<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="The K-Armed Bandit Problem">
<meta property="og:type" content="article">
<meta property="og:title" content="The K-Armed Bandit Problem">
<meta property="og:url" content="https://zhangruochi.com/The-K-Armed-Bandit-Problem/2020/09/03/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="The K-Armed Bandit Problem">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhangruochi.com/The-K-Armed-Bandit-Problem/2020/09/03/initial_values.png">
<meta property="article:published_time" content="2020-09-03T10:55:55.000Z">
<meta property="article:modified_time" content="2022-01-18T06:37:21.628Z">
<meta property="article:author" content="Ruochi Zhang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhangruochi.com/The-K-Armed-Bandit-Problem/2020/09/03/initial_values.png">

<link rel="canonical" href="https://zhangruochi.com/The-K-Armed-Bandit-Problem/2020/09/03/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>The K-Armed Bandit Problem | RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/The-K-Armed-Bandit-Problem/2020/09/03/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          The K-Armed Bandit Problem
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-09-03 18:55:55" itemprop="dateCreated datePublished" datetime="2020-09-03T18:55:55+08:00">2020-09-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-18 14:37:21" itemprop="dateModified" datetime="2022-01-18T14:37:21+08:00">2022-01-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/The-K-Armed-Bandit-Problem/2020/09/03/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/The-K-Armed-Bandit-Problem/2020/09/03/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <div class="post-description">The K-Armed Bandit Problem</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="lesson-1-the-k-armed-bandit-problem">Lesson 1: The K-Armed Bandit Problem</h2>
<h3 id="define-reward">Define reward</h3>
<p>In the k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the <strong>value</strong> of that action. We denote the action selected on time step t as <span class="math inline">\(A_t\)</span>, and the corresponding reward as <span class="math inline">\(R_t\)</span>. The value then of an arbitrary action a, denoted <span class="math inline">\(q\ast(a)\)</span>, is the expected reward given that a is selected:</p>
<p><span class="math display">\[q\ast(a) = \mathbb{E} [R_t | A_t = a ]\]</span></p>
<p>We denote the estimated value of action a at time step t as <span class="math inline">\(Q_t(a)\)</span>. We would like <span class="math inline">\(Q_t(a)\)</span> to be close to <span class="math inline">\(q\ast(a)\)</span>.</p>
<h3 id="understand-the-temporal-nature-of-the-bandit-problem">Understand the temporal nature of the bandit problem</h3>
<h3 id="define-k-armed-bandit">Define k-armed bandit</h3>
<p>Consider the following learning problem. You are faced repeatedly with a choice among k di↵erent options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.</p>
<h3 id="define-action-values">Define action-values</h3>
<p>We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods.</p>
<h2 id="lesson-2-what-to-learn-estimating-action-values">Lesson 2: What to Learn? Estimating Action Values</h2>
<h3 id="define-action-value-estimation-methods">Define action-value estimation methods</h3>
<p>One natural way to estimate this is by averaging the rewards actually received:</p>
<p><span class="math display">\[Q_t(a) = \frac{\text{sum of rewards when a taken prior to t} }{\text{ number of times a taken prior to t} }\]</span></p>
<h3 id="define-exploration-and-exploitation">Define exploration and exploitation</h3>
<p>If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are <strong>exploiting</strong> your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are <strong>exploring</strong>, because this enables you to improve your estimate of the nongreedy action’s value.</p>
<p>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</p>
<h3 id="select-actions-greedily-using-an-action-value-function">Select actions greedily using an action-value function</h3>
<p><span class="math display">\[A_t = argmax_a Q_t(a)\]</span></p>
<h3 id="define-online-learning">Define online learning</h3>
<p><span class="math display">\[Q_n = \frac{ R_1 + R_2 + \cdots + R_{n-1} }{ n - 1 }\]</span></p>
<p>The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, if this is done, then the memory and computational requirements would grow over time as more rewards are seen. Each additional reward would require additional memory to store it and additional computation to compute the sum in the numerator.</p>
<p>As you might suspect, this is not really necessary. It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward.</p>
<p><span class="math display">\[Q_(n+1) = Q_n + \frac{1}{n}[ R_n - Q_n ]\]</span></p>
<h3 id="define-the-general-online-update-equation">Define the general online update equation</h3>
<p><span class="math display">\[\text{NewEstimate} = \text{OldEstimate} + \text{StepSize} [ \text{target} - \text{OldEstimate} ]\]</span></p>
<h3 id="understand-why-we-might-use-a-constant-stepsize-in-the-case-of-non-stationarity">Understand why we might use a constant stepsize in the case of non-stationarity</h3>
<p>The averaging methods discussed so far are appropriate for stationary bandit problems, that is, for bandit problems in which the reward probabilities do not change over time. As noted earlier, we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to give more weight to recent rewards than to long-past rewards. One of the most popular ways of doing this is to use a constant step-size parameter.</p>
<p><span class="math display">\[Q_(n+1) = Q_n + \alpha [ R_n - Q_n ]\]</span></p>
<p>where the step-size parameter $ (0, 1]$ is constant.</p>
<p><span class="math display">\[Q_{n+1} = (1 - \alpha)^n Q_1 + \sum^{n}_{i=1}\alpha(1 - \alpha)^{n-i}R_i \]</span></p>
<p>Note that the weight, <span class="math inline">\(\alpha(1 - \alpha)^{n-i}\)</span> given the reward <span class="math inline">\(R_i\)</span> depends on how many rewards ago, <span class="math inline">\(n-1\)</span> it was observed. The quantity <span class="math inline">\(1-\alpha\)</span> is less than 1, and thus the weight given to <span class="math inline">\(R_i\)</span> decreases as the number of intervening rewards increases.</p>
<h2 id="lesson-3-exploration-vs.-exploitation-tradeoff">Lesson 3: Exploration vs. Exploitation Tradeoff</h2>
<h3 id="define-epsilon-greedy">Define epsilon-greedy</h3>
<p>Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability <span class="math inline">\(\alpha\)</span> instead select randomly from among all the actions with equal probability, independently of the action-value estimates. We call methods using this near-greedy action selection rule <span class="math inline">\(\alpha\)</span>-greedy methods.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">current_action = argmax(self.q_values) <span class="keyword">if</span> _ &gt;= self.epsilon <span class="keyword">else</span> np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(self.q_values))</span><br></pre></td></tr></table></figure>
<h3 id="understand-optimistic-initial-values-describe-the-benefits-of-optimistic-initial-values-for-early-exploration">Understand optimistic initial values, Describe the benefits of optimistic initial values for early exploration</h3>
<p>Initial action values can also be used as a simple way to encourage exploration. Suppose that instead of setting the initial action values to zero, as we did in the 10-armed testbed,we set them all to +5. Recall that the <span class="math inline">\(q\ast(a)\)</span> in this problem are selected from a normal distribution with mean 0 and variance 1. An initial estimate of +5 is thus wildly optimistic. But this optimism encourages action-value methods to explore. Whichever actions are initially selected, the reward is less than the starting estimates; the learner switches to other actions, being <strong>disappointed</strong> with the rewards it is receiving. The result is that all actions are tried several times before the value estimates converge. The system does a fair amount of exploration even if greedy actions are selected all the time.</p>
<p><img src="initial_values.png" /></p>
<h3 id="explain-the-criticisms-of-optimistic-initial-values">Explain the criticisms of optimistic initial values</h3>
<p>We call this technique for encouraging exploration optimistic initial values. We regard it as a simple trick that can be quite effective on stationary problems, but it is far from being a generally useful approach to encouraging exploration. For example, it is not well suited to nonstationary problems because its drive for exploration is inherently temporary. If the task changes, creating a renewed need for exploration, this method cannot help. Indeed, any method that focuses on the initial conditions in any special way is unlikely to help with the general nonstationary case. The beginning of time occurs only once, and thus we should not focus on it too much. This criticism applies as well to the sample-average methods, which also treat the beginning of time as a special event, averaging all subsequent rewards with equal weights.</p>
<h3 id="describe-the-upper-confidence-bound-action-selection-method">Describe the upper confidence bound action selection method</h3>
<p>It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates.One effective way of doing this is to select actions according to</p>
<p><span class="math display">\[A_t = argmax_a [ Q_t(a) + c\sqrt{\frac{\ln t}{ N_t(a)}} ]\]</span></p>
<h3 id="define-optimism-in-the-face-of-uncertainty">Define optimism in the face of uncertainty</h3>
<p>The idea of this upper confidence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value The quantity being max’ed over is thus a sort of upper bound on the possible true value of action <span class="math inline">\(a\)</span>, with <span class="math inline">\(c\)</span> determining the confidence level. Each time <span class="math inline">\(a\)</span> is selected the uncertainty is presumably reduced: <span class="math inline">\(N_t(a)\)</span> increments, and, as it appears in the denominator, the uncertainty term decreases. On the other hand, each time an action other than a is selected, <span class="math inline">\(t\)</span> increases but <span class="math inline">\(N_t(a)\)</span> does not; because <span class="math inline">\(t\)</span> appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates,or that have already been selected frequently, will be selected with decreasing frequency over time.</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Create-a-Siamese-Network-with-Triplet-Loss-in-Keras/2020/08/11/" rel="prev" title="Create a Siamese Network with Triplet Loss in Keras">
      <i class="fa fa-chevron-left"></i> Create a Siamese Network with Triplet Loss in Keras
    </a></div>
      <div class="post-nav-item">
    <a href="/Markov-Decision-Processes/2020/09/04/" rel="next" title="Markov Decision Processes I">
      Markov Decision Processes I <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#lesson-1-the-k-armed-bandit-problem"><span class="nav-number">1.</span> <span class="nav-text">Lesson 1: The K-Armed Bandit Problem</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#define-reward"><span class="nav-number">1.1.</span> <span class="nav-text">Define reward</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#understand-the-temporal-nature-of-the-bandit-problem"><span class="nav-number">1.2.</span> <span class="nav-text">Understand the temporal nature of the bandit problem</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#define-k-armed-bandit"><span class="nav-number">1.3.</span> <span class="nav-text">Define k-armed bandit</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#define-action-values"><span class="nav-number">1.4.</span> <span class="nav-text">Define action-values</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lesson-2-what-to-learn-estimating-action-values"><span class="nav-number">2.</span> <span class="nav-text">Lesson 2: What to Learn? Estimating Action Values</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#define-action-value-estimation-methods"><span class="nav-number">2.1.</span> <span class="nav-text">Define action-value estimation methods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#define-exploration-and-exploitation"><span class="nav-number">2.2.</span> <span class="nav-text">Define exploration and exploitation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#select-actions-greedily-using-an-action-value-function"><span class="nav-number">2.3.</span> <span class="nav-text">Select actions greedily using an action-value function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#define-online-learning"><span class="nav-number">2.4.</span> <span class="nav-text">Define online learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#define-the-general-online-update-equation"><span class="nav-number">2.5.</span> <span class="nav-text">Define the general online update equation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#understand-why-we-might-use-a-constant-stepsize-in-the-case-of-non-stationarity"><span class="nav-number">2.6.</span> <span class="nav-text">Understand why we might use a constant stepsize in the case of non-stationarity</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lesson-3-exploration-vs.-exploitation-tradeoff"><span class="nav-number">3.</span> <span class="nav-text">Lesson 3: Exploration vs. Exploitation Tradeoff</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#define-epsilon-greedy"><span class="nav-number">3.1.</span> <span class="nav-text">Define epsilon-greedy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#understand-optimistic-initial-values-describe-the-benefits-of-optimistic-initial-values-for-early-exploration"><span class="nav-number">3.2.</span> <span class="nav-text">Understand optimistic initial values, Describe the benefits of optimistic initial values for early exploration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#explain-the-criticisms-of-optimistic-initial-values"><span class="nav-number">3.3.</span> <span class="nav-text">Explain the criticisms of optimistic initial values</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#describe-the-upper-confidence-bound-action-selection-method"><span class="nav-number">3.4.</span> <span class="nav-text">Describe the upper confidence bound action selection method</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#define-optimism-in-the-face-of-uncertainty"><span class="nav-number">3.5.</span> <span class="nav-text">Define optimism in the face of uncertainty</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">216</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail → mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'en' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
