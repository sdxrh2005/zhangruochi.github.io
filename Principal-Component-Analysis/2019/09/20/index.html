<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="What is PCA and how to do dimensionality reduction based on it">
<meta property="og:type" content="article">
<meta property="og:title" content="Principal Component Analysis">
<meta property="og:url" content="https://zhangruochi.com/Principal-Component-Analysis/2019/09/20/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="What is PCA and how to do dimensionality reduction based on it">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhangruochi.com/Principal-Component-Analysis/2019/09/20/FC4110D01E2417B654EEF38783A59462.png">
<meta property="og:image" content="https://zhangruochi.com/Principal-Component-Analysis/2019/09/20/281DB10DBBA2A911809814DDBC34A624.png">
<meta property="og:image" content="https://zhangruochi.com/Principal-Component-Analysis/2019/09/20/1259CBBBA592F9731BB2753A218942FD.png">
<meta property="og:image" content="https://zhangruochi.com/Principal-Component-Analysis/2019/09/20/F94CA8E5F54491CE435380F760D081FA.png">
<meta property="article:published_time" content="2019-09-20T02:41:35.000Z">
<meta property="article:modified_time" content="2019-09-20T18:11:56.000Z">
<meta property="article:author" content="Ruochi Zhang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhangruochi.com/Principal-Component-Analysis/2019/09/20/FC4110D01E2417B654EEF38783A59462.png">

<link rel="canonical" href="https://zhangruochi.com/Principal-Component-Analysis/2019/09/20/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Principal Component Analysis | RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Principal-Component-Analysis/2019/09/20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Principal Component Analysis
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-09-20 10:41:35" itemprop="dateCreated datePublished" datetime="2019-09-20T10:41:35+08:00">2019-09-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-09-21 02:11:56" itemprop="dateModified" datetime="2019-09-21T02:11:56+08:00">2019-09-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Principal-Component-Analysis/2019/09/20/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Principal-Component-Analysis/2019/09/20/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <div class="post-description">What is PCA and how to do dimensionality reduction based on it</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="what-is-pca">What is PCA?</h2>
<p>High dimensional data, for example images, often have the property that it lies on a low dimensional subspace, and that many dimensions are highly correlated. Here's an illustration in two dimensions. We can think of dimensionality reduction as a way of compressing data with some loss, similar to jpg or mp3. Principal Component Analysis (PCA) is one of the most fundamental dimensionality reduction techniques that are used in machine learning.</p>
<h2 id="pre-knowledge">Pre-knowledge</h2>
<h3 id="orthogonal-projections">Orthogonal Projections</h3>
<p>Recall that for projection of a vector <span class="math inline">\(\boldsymbol x\)</span> onto a 1-dimensional subspace <span class="math inline">\(U\)</span> with basis vector <span class="math inline">\(\boldsymbol b\)</span> we have: <span class="math display">\[{\pi_U}(\boldsymbol x) = \frac{\boldsymbol b\boldsymbol b^T}{\lbrace\lVert\boldsymbol  b \rVert\rbrace^2}\boldsymbol x\]</span></p>
<p><span class="math display">\[\lambda = \frac{\langle \boldsymbol b,\boldsymbol x\rangle} {\lbrace\boldsymbol b\rbrace^{2}}\]</span></p>
<ul>
<li><span class="math inline">\(\frac{\boldsymbol b\boldsymbol b^T}{\lbrace\lVert\boldsymbol b \rVert^2}\)</span> is the projection matrix.</li>
<li><span class="math inline">\(\lambda\)</span> is he coordinate of our projection with respect to the basis b The procedure we get this equation is as follows:</li>
</ul>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
&amp; \langle \boldsymbol b, {\pi_U}(\boldsymbol x) - \boldsymbol x \rangle = 0 \\
&amp; \langle \boldsymbol b, {\pi_U}(\boldsymbol x) \rangle - \langle \boldsymbol b,\boldsymbol x \rangle = 0\\
&amp; \langle \boldsymbol b, \lambda \boldsymbol b\rangle - \langle \boldsymbol b, \boldsymbol x\rangle = 0\\
&amp; \lambda ||\boldsymbol b||^{2} - \langle \boldsymbol b, \boldsymbol x\rangle = 0 \\
&amp; \lambda = \frac{\langle \boldsymbol b,\boldsymbol x\rangle} {||\boldsymbol b||^{2}} 
\end{aligned}
\end{equation}\]</span></p>
<p><img src="FC4110D01E2417B654EEF38783A59462.png" alt="1.png" /> <img src="281DB10DBBA2A911809814DDBC34A624.png" alt="2.png" /></p>
<p>And for the general projection onto an M-dimensional subspace <span class="math inline">\(U\)</span> with basis vectors <span class="math inline">\(\boldsymbol b_1,\dotsc, \boldsymbol b_M\)</span> we have</p>
<p><span class="math display">\[{\pi_U}(\boldsymbol x) = \boldsymbol B(\boldsymbol B^T\boldsymbol B)^{-1}\boldsymbol B^T\boldsymbol x \]</span></p>
<p><span class="math display">\[\lambda = (\boldsymbol B^{T}\boldsymbol B )^{-1}B^{T} \boldsymbol x\]</span></p>
<p>where</p>
<p><span class="math display">\[\boldsymbol B = [\boldsymbol b_1,...,\boldsymbol b_M]\]</span> <img src="1259CBBBA592F9731BB2753A218942FD.png" alt="4.png" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">projection_matrix1D</span>(<span class="params">b</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (b @ b.T) / np.dot(b.T,b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">project1D</span>(<span class="params">projection_matrix,vector</span>):</span></span><br><span class="line">    <span class="keyword">return</span> projection_matrix @ vector</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">projection_matrix3D</span>(<span class="params">b</span>):</span></span><br><span class="line">    <span class="keyword">return</span> b @ np.linalg.inv(b.T @ b) @ b.T</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_lambda_3D</span>(<span class="params">b,vector</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.linalg.inv(b.T @ b) @ b.T @ vector</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">project3D</span>(<span class="params">projection_matrix,vector</span>):</span></span><br><span class="line">    <span class="keyword">return</span> projection_matrix @ vector</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_rank</span>(<span class="params">matrix</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.linalg.matrix_rank(matrix)</span><br></pre></td></tr></table></figure>
<h3 id="orthogonal-complement">Orthogonal complement</h3>
<p>If we look at an n-dimensional vector space V and K-dimensional subspace <span class="math inline">\(W\subset V\)</span>. then the orthogonal complement <span class="math inline">\(W^{\perp}\)</span> is an (n-k) dimensional subspace of V and contains all vectors in V that are orthogonal to every vector in W.</p>
<h3 id="lagrange-multipliers">Lagrange multipliers</h3>
<ol type="1">
<li>We can solve a constrained optimization problem of the form $ _{x}f(x), s.t.g(x) = 0 $ where g(x) is an equality constraint</li>
<li>The constraints can be absorbed into a single objective function, the Lagrangian, which combines the original loss function and the constraints as <span class="math inline">\(L(x,\lambda) = f(x) - \lambda g(x)\)</span>. <span class="math inline">\(\lambda\)</span> is called a Lagrange multiplier.</li>
<li>We solve the constrained optimization problem by computing the partial derivatives <span class="math inline">\(\frac{\partial L}{\partial x}\)</span> and <span class="math inline">\(\frac{\partial L}{\partial \lambda}\)</span>, setting them to 0 and solving for <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(x\)</span></li>
</ol>
<h2 id="dive-into-pca">Dive into PCA</h2>
<p>If we have a data set which contains <strong>N</strong> samples and <strong>D</strong> features, <span class="math inline">\(X = {x_i,\cdots,x_N}, x \in \mathbb{R}^{D}\)</span>, Our objective is to find a low dimensional representation of the data that is as similar to X as possible. Before getting start, let's briefly review three important concepts.</p>
<ol type="1">
<li>The first one is that every vector in <span class="math inline">\(\mathbb{R}^{D}\)</span> can be represented as a linear combination of the basis vectors. In the following we will assume that the <span class="math inline">\(b_i\)</span> are an orthonormal basis of <span class="math inline">\(\mathbb{R}^{D}\)</span> <span class="math display">\[x_n \sum_{i=1}^{D}\beta_{in}b_i\]</span></li>
<li>We can interpret <span class="math inline">\(\beta_{i_n}\)</span> to be the orthogonal projection of <span class="math inline">\(x_n\)</span> onto the one dimensional subspace spanned by the it's basis vector <span class="math display">\[\beta_{in} = x_n^{T}b_i \]</span></li>
<li>We define B to be the matrix that consists of these orthonormal basis vectors. <span class="math display">\[B = (b_1,\cdots,b_m)\]</span></li>
</ol>
<p>Then the projection of X onto the subspace is:</p>
<p><span class="math display">\[\widetilde{x} = BB^{T}x\]</span></p>
<p>So, we took our general way of writing any vector in <span class="math inline">\(\mathbb{R}^{D}\)</span> which comes from property one, and we split the sum in property one into two sums. One is living in an M-dimensional subspace and the other one is living in a (D-M) dimensional subspace which is an orthogonal complement to this particular subspace.</p>
<p><span class="math display">\[\widetilde{x} = \sum_{i=1}^{M}\beta_{in}b_i + \sum_{M+1}^{D}\beta_{in}b_i\]</span> <span class="math inline">\(b1,\cdots,b_{M}\)</span> span the principal subspace</p>
<p>Assuming we have data <span class="math inline">\(X_1,\cdots,X_n\)</span>, we want to find parameters <span class="math inline">\(beta i_n\)</span> and orthonormal basis vectors <span class="math inline">\(b_i\)</span>, such that the average squared reconstruction era is minimised. And we can write the average squared reconstruction error as follows.</p>
<p><span class="math display">\[H = \frac{1}{N} \sum_{n=1}^{N}||x_n - \widetilde{x_n}||^{2}\]</span></p>
<p>The parameters are the <span class="math inline">\(\beta_{i_n}\)</span> and the <span class="math inline">\(b_i\)</span>. We set the partial derivatives of J with respect to these parameters to zero and solve for the optimal parameters</p>
<p><span class="math display">\[\frac{\partial{J}}{\partial\lbrace\beta_{in},b_i\rbrace} = \frac{\partial J}{\partial \widetilde{x_n}} \frac{\partial \widetilde{x_n}}{\partial\lbrace\beta_{in},b_i\rbrace}\]</span></p>
<p><span class="math display">\[ \frac{\partial J}{\partial \widetilde{x_n}} = -\frac{2}{N}(x_n - \widetilde{x_n})^{T}\]</span></p>
<p>We can prove that the optimal coordinates of <span class="math inline">\(\widetilde{x_n}\)</span> with respect to our basis are the orthogonal projections of the coordinates of our original data point onto the ith basis vector that spans our principal subspace.</p>
<figure>
<img src="F94CA8E5F54491CE435380F760D081FA.png" alt="3.png" /><figcaption aria-hidden="true">3.png</figcaption>
</figure>
<p><span class="math display">\[\widetilde{x_n} = \sum_{j=1}^{M}\beta_{jn}b{i} = (\sum_{j=1}^{M}b_i b_j^{T})x_n\]</span> <span class="math display">\[x_n = (\sum_{j=1}^{M}b_i b_j^{T})x_n + (\sum_{j=M+1}^{D}b_jb_j^{T})x_n\]</span> <span class="math display">\[J = \frac{1}{N}\sum_{n=1}^{N}||x_n -\widetilde{x_n} ||^2 = \sum_{j=M+1}^{D}b_j^{T}(\frac{1}{N}\sum_{n=1}^{N}x_nx_n^{T})b_j = \sum_{j=M+1}^{D}b_j^{T}Sb_j = trace((\sum_{j=M+1}^{D}b_i b_j^{T})S)\]</span></p>
<p><strong>This projection matrix takes our data covariance matrix and project it onto the orthogonal compliment of the principal subspace. That means, we can reformulate the loss function as the variance of the data projected onto the subspace that we ignore. Therefore, minimising this loss is equivalent to minimising the variance of the data that lies in the subspace that is a orthogonal to the principal subspace. In other words, we are interested in retaining as much variance after projection as possibl.</strong></p>
<p>Using the results from earlier, We can write our loss function as the <span class="math inline">\(J = \sum_{j=M+1}^{D}b_j^{T}Sb_j\)</span>. Where S is the data covariance matrix.</p>
<p><span class="math display">\[J = \sum_{j=M+1}^{D}b_j^{T}Sb_j\]</span> <span class="math display">\[J = b_2^{T}Sb_2, b_2^{T}b_2 = 1\]</span> <span class="math display">\[L = b_2^{T}sb_{2} + \lambda(1 - b_2{T}b_2)\]</span> <span class="math display">\[\frac{\partial L}{\partial \lambda} = 1 - b_2^{T}b_2 = 0\]</span> <span class="math display">\[\frac{\partial L}{\partial b_2} = 2b_2^{T}S - 2\lambda b_2^{T} = 0\]</span> <span class="math display">\[SB_2 = \lambda b_2\]</span> <span class="math display">\[J = b_2^{T}Sb_2 = b_2^{T}b_2\lambda = \lambda\]</span> <span class="math display">\[J = \sum_{j=M+1}^D\lambda j\]</span></p>
<p><strong>We end up with <span class="math inline">\(\lambda\)</span> as our loss function. Therefore the average squared reconstruction error is minimised if <span class="math inline">\(\lambda\)</span> is the smallest eigen value of the data covariance matrix. And that means we need to choose <span class="math inline">\(B_2\)</span> as the corresponding eigen vector and that one will span the subspace that we will ignore. <span class="math inline">\(B_1\)</span> which spans the principal subspace is then the eigen vector that belongs to the largest eigen value of the data covariance matrix.</strong></p>
<h2 id="steps-of-pca">Steps of PCA</h2>
<ol type="1">
<li>We subtract the mean from the data and send it at zero to avoid numerical problems.</li>
<li>We divide by the standard deviation to make the data unit-free.</li>
<li>We Compute the eigenvalues and eigen vectors of the data covariance matrix.</li>
<li>And finally, we can project any data point onto the principal subspace that is spanned by the eigenvectors that belong to the largest eigenvalues.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span>(<span class="params">X</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Normalize the given dataset X</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X: ndarray, dataset</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (Xbar, mean, std): tuple of ndarray, Xbar is the normalized dataset</span></span><br><span class="line"><span class="string">        with mean 0 and standard deviation 1; mean and std are the </span></span><br><span class="line"><span class="string">        mean and standard deviation respectively.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note:</span></span><br><span class="line"><span class="string">        You will encounter dimensions where the standard deviation is</span></span><br><span class="line"><span class="string">        zero, for those when you do normalization the normalized data</span></span><br><span class="line"><span class="string">        will be NaN. Handle this by setting using `std = 1` for those </span></span><br><span class="line"><span class="string">        dimensions when doing normalization.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    mu = np.zeros(X.shape[<span class="number">1</span>]) <span class="comment"># &lt;-- EDIT THIS, compute the mean of X</span></span><br><span class="line">    std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    std_filled = std.copy()</span><br><span class="line">    std_filled[std==<span class="number">0</span>] = <span class="number">1.</span></span><br><span class="line">    Xbar = X                  <span class="comment"># &lt;-- EDIT THIS, compute the normalized data Xbar</span></span><br><span class="line">    <span class="keyword">return</span> Xbar, mu, std</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eig</span>(<span class="params">S</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute the eigenvalues and corresponding eigenvectors </span></span><br><span class="line"><span class="string">        for the covariance matrix S.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        S: ndarray, covariance matrix</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (eigvals, eigvecs): ndarray, the eigenvalues and eigenvectors</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note:</span></span><br><span class="line"><span class="string">        the eigenvals and eigenvecs should be sorted in descending</span></span><br><span class="line"><span class="string">        order of the eigen values</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    val,vec = np.linalg.eig(S)</span><br><span class="line">    idx = np.argsort(-val)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (val[idx], vec[:,idx]) <span class="comment"># EDIT THIS</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">projection_matrix</span>(<span class="params">B</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute the projection matrix onto the space spanned by `B`</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        B: ndarray of dimension (D, M), the basis for the subspace</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        P: the projection matrix</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>     </span><br><span class="line">    P = B @ np.linalg.inv(B.T @ B) @ B.T <span class="comment"># EDIT THIS</span></span><br><span class="line">    <span class="keyword">return</span> P</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PCA</span>(<span class="params">X, num_components</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X: ndarray of size (N, D), where D is the dimension of the data,</span></span><br><span class="line"><span class="string">           and N is the number of datapoints</span></span><br><span class="line"><span class="string">        num_components: the number of principal components to use.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        X_reconstruct: ndarray of the reconstruction</span></span><br><span class="line"><span class="string">        of X from the first `num_components` principal components.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Compute the data covariance matrix S</span></span><br><span class="line">    S = np.cov(X, rowvar=<span class="literal">False</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Next find eigenvalues and corresponding eigenvectors for S by implementing eig().</span></span><br><span class="line">    eig_vals, eig_vecs = eig(S)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Reconstruct the images from the lowerdimensional representation</span></span><br><span class="line">    <span class="comment"># To do this, we first need to find the projection_matrix (which you implemented earlier)</span></span><br><span class="line">    <span class="comment"># which projects our input data onto the vector space spanned by the eigenvectors</span></span><br><span class="line">    P = projection_matrix(eig_vecs[:,:num_components]) <span class="comment"># projection matrix</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Then for each data point x_i in the dataset X </span></span><br><span class="line">    <span class="comment">#   we can project the original x_i onto the eigenbasis.</span></span><br><span class="line">    X_reconstruct = (P @ X.T).T</span><br><span class="line">                          </span><br><span class="line">    <span class="keyword">return</span> X_reconstruct</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Vector-and-Space/2019/09/19/" rel="prev" title="Vector and Space">
      <i class="fa fa-chevron-left"></i> Vector and Space
    </a></div>
      <div class="post-nav-item">
    <a href="/Machine-Language/2019/09/22/" rel="next" title="Machine Language">
      Machine Language <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#what-is-pca"><span class="nav-number">1.</span> <span class="nav-text">What is PCA?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pre-knowledge"><span class="nav-number">2.</span> <span class="nav-text">Pre-knowledge</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#orthogonal-projections"><span class="nav-number">2.1.</span> <span class="nav-text">Orthogonal Projections</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#orthogonal-complement"><span class="nav-number">2.2.</span> <span class="nav-text">Orthogonal complement</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lagrange-multipliers"><span class="nav-number">2.3.</span> <span class="nav-text">Lagrange multipliers</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dive-into-pca"><span class="nav-number">3.</span> <span class="nav-text">Dive into PCA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#steps-of-pca"><span class="nav-number">4.</span> <span class="nav-text">Steps of PCA</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">212</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">42</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail → mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'en' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
