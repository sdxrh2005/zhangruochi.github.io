<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Markov Decision Processes I">
<meta property="og:type" content="article">
<meta property="og:title" content="Markov Decision Processes I">
<meta property="og:url" content="https://zhangruochi.com/Markov-Decision-Processes/2020/09/04/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="Markov Decision Processes I">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhangruochi.com/Markov-Decision-Processes/2020/09/04/1.png">
<meta property="article:published_time" content="2020-09-04T03:37:10.000Z">
<meta property="article:modified_time" content="2021-12-31T09:58:34.000Z">
<meta property="article:author" content="Ruochi Zhang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhangruochi.com/Markov-Decision-Processes/2020/09/04/1.png">

<link rel="canonical" href="https://zhangruochi.com/Markov-Decision-Processes/2020/09/04/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Markov Decision Processes I | RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Markov-Decision-Processes/2020/09/04/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Markov Decision Processes I
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-09-04 11:37:10" itemprop="dateCreated datePublished" datetime="2020-09-04T11:37:10+08:00">2020-09-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-31 17:58:34" itemprop="dateModified" datetime="2021-12-31T17:58:34+08:00">2021-12-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Markov-Decision-Processes/2020/09/04/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Markov-Decision-Processes/2020/09/04/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <div class="post-description">Markov Decision Processes I</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="lesson-1-introduction-to-markov-decision-processes">Lesson 1: Introduction to Markov Decision Processes</h2>
<h3 id="understand-markov-decision-processes-or-mdps">Understand Markov Decision Processes, or MDPs</h3>
<p>MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards.</p>
<h3 id="understand-the-graphical-representation-of-a-markov-decision-process">Understand the graphical representation of a Markov Decision Process</h3>
<p>MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal. The learner and decision maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the environment. These interact continually, the agent selecting actions and the environment responding to these actions and presenting new situations to the agent.1 The environment also gives rise to rewards, special numerical values that the agent seeks to maximize over time through its choice of actions.</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="1.png" width = "70%" height="70%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
The agent–environment interaction in a Markov decision process.
</div>
</center>
<h3 id="describe-how-the-dynamics-of-an-mdp-are-defined">Describe how the dynamics of an MDP are defined</h3>
<p>In a finite MDP, the sets of states, actions, and rewards (S, A and R) all have a finite number of elements. In this case, the random variables <span class="math inline">\(R_t\)</span> and <span class="math inline">\(S_t\)</span> have well defined discrete probability distributions dependent only on the preceding state and action. That is, for particular values of these random variables, <span class="math inline">\(s\prime \in S\)</span> and <span class="math inline">\(r \in R\)</span>, there is a probability of those values occurring at time t, given particular values of the preceding state and action:</p>
<p><span class="math display">\[p(s\prime, r | s, a) = Pr\{S_t = s\prime, R_t = r | S_{t-1} = s, A_{t-1} = a \}\]</span></p>
<p><span class="math display">\[\sum_{s\prime \in s}\sum_{r \in R} p(s\prime, r | s, a) = 1, \text{for all} \, s\in S, a \in A_{(s)}\]</span></p>
<p>for all <span class="math inline">\(s\prime, s \in S, r \in R \text{and} a \in A_{(s)}\)</span>. The function p defines the dynamics of the MDP. The dot over the equals sign in the equation reminds us that it is a definition (in this case of the function p) rather than a fact that follows from previous definitions. The dynamics function <span class="math inline">\(p: S x R x S x A \rightarrow [0, 1]\)</span> is an ordinary deterministic function of four arguments.</p>
<p>In a Markov decision process, the probabilities given by p completely characterize the environment’s dynamics. That is, the probability of each possible value for <span class="math inline">\(S_t\)</span> and <span class="math inline">\(A_t\)</span>, and, given them, not at all on earlier states and actions. This is best viewed a restriction not on the decision process, but on the state. The state must include information about all aspects of the past agent–environment interaction that make a difference for the future. If it does, then the state is said to have the Markov property.</p>
<p>From the four-argument dynamics function, p, one can compute anything else one might want to know about the environment, such as the <strong>state-transition probabilities</strong> (which we denote, with a slight abuse of notation, as a three-argument function <span class="math inline">\(p: S x S x A \rightarrow [0, 1]\)</span>)</p>
<p><span class="math display">\[p(s\prime | s, a) = Pr\{S_t = s\prime | S_{t-1} = s, A_{t-1} = a \} = \sum_{r \in R}p(s\prime, r | s, a)\]</span></p>
<p>We can also compute the expected rewards for state–action pairs as a two-argument function <span class="math inline">\(r : S x A \rightarrow \mathbb{R}\)</span></p>
<p><span class="math display">\[r(s,a) = \mathbb{R} [R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in R} r \sum_{s\prime \in S} p(s\prime, r | s, a)\]</span></p>
<p>and the expected rewards for <strong>state–action–next-state</strong> triples as a three-argument function <span class="math inline">\(r: S x A x S \rightarrow \mathbb{R}\)</span></p>
<p><span class="math display">\[r(s,a,s\prime) = \mathbb{E} [R_t | S_{t-1} = s, A_{t-1} = a, S_t = s\prime] = \sum_{r \in R} r \frac{ p(s\prime, r | s, a) }{p(s\prime | s, a)}\]</span></p>
<h3 id="explain-how-many-diverse-processes-can-be-written-in-terms-of-the-mdp-framework">Explain how many diverse processes can be written in terms of the MDP framework</h3>
<p>The MDP framework is abstract and flexible and can be applied to many di↵erent problems in many di↵erent ways.</p>
<ul>
<li>In general, actions can be any decisions we want to learn how to make, and the states can be anything we can know that might be useful in making them.</li>
<li>The general rule we follow is that anything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment.<br />
</li>
<li>The agent–environment boundary represents the limit of the agent’s absolute control, not of its knowledge.</li>
</ul>
<p>The MDP framework is a considerable abstraction of the problem of goal-directed learning from interaction. It proposes that whatever the details of the sensory, memory, and control apparatus, and whatever objective one is trying to achieve, any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment: one signal to represent the choices made by the agent (the <strong>actions</strong>), one signal to represent the basis on which the choices are made (the <strong>states</strong>), and one signal to define the agent’s goal (the <strong>rewards</strong>).</p>
<h2 id="lesson-2-goal-of-reinforcement-learning">Lesson 2: Goal of Reinforcement Learning</h2>
<h3 id="describe-how-rewards-relate-to-the-goal-of-an-agent">Describe how rewards relate to the goal of an agent</h3>
<p>Informally, the agent’s goal is to maximize the total amount of reward it receives. This means maximizing not immediate reward, but cumulative reward in the long run. We can clearly state this informal idea as the reward hypothesis:</p>
<blockquote>
<p>That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).</p>
</blockquote>
<h3 id="understand-episodes-and-identify-episodic-tasks">Understand episodes and identify episodic tasks</h3>
<p>when the agent–environment interaction breaks naturally into <strong>subsequences</strong>, which we call episodes,7 such as plays of a game, trips through a maze, or any sort of repeated interaction. Each episode ends in a special state called the <strong>terminal state</strong>, followed by a <strong>reset</strong> to a standard starting state or to a sample from a standard distribution of starting states.</p>
<p>In general, we seek to maximize the expected return, where the return, denoted <span class="math inline">\(G_t\)</span>, is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards:</p>
<p><span class="math display">\[G_t = R_{t+1} + R_{t+2} + R_{t+3} + \cdots + R_{T}\]</span></p>
<p>where T is a final time step.</p>
<h2 id="lesson-3-continuing-tasks">Lesson 3: Continuing Tasks</h2>
<h3 id="formulate-returns-for-continuing-tasks-using-discounting">Formulate returns for continuing tasks using discounting</h3>
<p>In many cases the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. For example, this would be the natural way to formulate an on-going process-control task, or an application to a robot with a long life span. We call these continuing tasks. The return formulation is problematic for continuing tasks because the final time step would be <span class="math inline">\(T = \infty\)</span> and the return, which is what we are trying to maximize, could itself easily be infinite.</p>
<p>The additional concept that we need is that of <strong>discounting</strong>. According to this approach, the agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses <span class="math inline">\(A_t\)</span> to maximize the expected discounted return:</p>
<p><span class="math display">\[G_t = R_{t+1} + \eta R_{t+2} + \eta^2 R_{t+3} + \cdots  = \sum_{k=0}^{\infty} \eta^k R_{t+k+1}\]</span></p>
<p>where is a parameter, $ 0 $, called the discount rate.</p>
<p>The discount rate determines the present value of future rewards: a reward received k time steps in the future is worth only <span class="math inline">\(\eta^{k-1}\)</span> times what it would be worth if it were received immediately. If <span class="math inline">\(\eta &lt; 1\)</span>, the infinite sum has a finite value as long as the reward sequence <span class="math inline">\(\{ R_k \}\)</span> is <strong>bounded</strong>.</p>
<p>Returns at successive time steps are related to each other in a way that is important for the theory and algorithms of reinforcement learning:</p>
<p><span class="math display">\[G_t =  R_{t+1} + \eta (R_{t+2} + \eta R_{t+3} + \eta^2 R_{t+4} + \cdots) = R_{t+1} + \eta G_{t+1}\]</span></p>
<h3 id="describe-how-returns-at-successive-time-steps-are-related-to-each-other">Describe how returns at successive time steps are related to each other</h3>
<p>If <span class="math inline">\(\eta = 0\)</span>, the agent is “myopic” in being concerned only with maximizing immediate rewards: its objective in this case is to learn how to choose At so as to maximize only <span class="math inline">\(R_{t+1}\)</span>. If each of the agent’s actions happened to influence only the immediate reward, not future rewards as well, then a myopic agent could maximize by separately maximizing each immediate reward. But in general, acting to maximize immediate reward can reduce access to future rewards so that the return is reduced. As <span class="math inline">\(\eta\)</span> approaches 1, the return objective takes future rewards into account more strongly; the agent becomes more farsighted.</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/The-K-Armed-Bandit-Problem/2020/09/03/" rel="prev" title="The K-Armed Bandit Problem">
      <i class="fa fa-chevron-left"></i> The K-Armed Bandit Problem
    </a></div>
      <div class="post-nav-item">
    <a href="/Markov-Decision-Processes-II/2020/09/06/" rel="next" title="Markov Decision Processes II">
      Markov Decision Processes II <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#lesson-1-introduction-to-markov-decision-processes"><span class="nav-number">1.</span> <span class="nav-text">Lesson 1: Introduction to Markov Decision Processes</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#understand-markov-decision-processes-or-mdps"><span class="nav-number">1.1.</span> <span class="nav-text">Understand Markov Decision Processes, or MDPs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#understand-the-graphical-representation-of-a-markov-decision-process"><span class="nav-number">1.2.</span> <span class="nav-text">Understand the graphical representation of a Markov Decision Process</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#describe-how-the-dynamics-of-an-mdp-are-defined"><span class="nav-number">1.3.</span> <span class="nav-text">Describe how the dynamics of an MDP are defined</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#explain-how-many-diverse-processes-can-be-written-in-terms-of-the-mdp-framework"><span class="nav-number">1.4.</span> <span class="nav-text">Explain how many diverse processes can be written in terms of the MDP framework</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lesson-2-goal-of-reinforcement-learning"><span class="nav-number">2.</span> <span class="nav-text">Lesson 2: Goal of Reinforcement Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#describe-how-rewards-relate-to-the-goal-of-an-agent"><span class="nav-number">2.1.</span> <span class="nav-text">Describe how rewards relate to the goal of an agent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#understand-episodes-and-identify-episodic-tasks"><span class="nav-number">2.2.</span> <span class="nav-text">Understand episodes and identify episodic tasks</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lesson-3-continuing-tasks"><span class="nav-number">3.</span> <span class="nav-text">Lesson 3: Continuing Tasks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#formulate-returns-for-continuing-tasks-using-discounting"><span class="nav-number">3.1.</span> <span class="nav-text">Formulate returns for continuing tasks using discounting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#describe-how-returns-at-successive-time-steps-are-related-to-each-other"><span class="nav-number">3.2.</span> <span class="nav-text">Describe how returns at successive time steps are related to each other</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">226</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail → mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'en' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
