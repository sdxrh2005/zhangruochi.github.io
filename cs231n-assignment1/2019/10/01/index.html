<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="In this assignment you will practice putting together a simple image classification pipeline, based on the k-Nearest Neighbor or the SVM&#x2F;Softmax classifier.">
<meta property="og:type" content="article">
<meta property="og:title" content="cs231n assignment1">
<meta property="og:url" content="https://zhangruochi.com/cs231n-assignment1/2019/10/01/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="In this assignment you will practice putting together a simple image classification pipeline, based on the k-Nearest Neighbor or the SVM&#x2F;Softmax classifier.">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-09-30T16:16:15.000Z">
<meta property="article:modified_time" content="2019-10-04T21:15:39.342Z">
<meta property="article:author" content="Ruochi Zhang">
<meta property="article:tag" content="cs231n">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhangruochi.com/cs231n-assignment1/2019/10/01/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>cs231n assignment1 | RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/cs231n-assignment1/2019/10/01/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          cs231n assignment1
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-01 00:16:15" itemprop="dateCreated datePublished" datetime="2019-10-01T00:16:15+08:00">2019-10-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-10-05 05:15:39" itemprop="dateModified" datetime="2019-10-05T05:15:39+08:00">2019-10-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/cs231n-assignment1/2019/10/01/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/cs231n-assignment1/2019/10/01/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <div class="post-description">In this assignment you will practice putting together a simple image classification pipeline, based on the k-Nearest Neighbor or the SVM/Softmax classifier.</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="the-goals-of-this-assignment-are-as-follows">The goals of this assignment are as follows</h2>
<ul>
<li>understand the basic Image Classification pipeline and the data-driven approach (train/predict stages)</li>
<li>understand the train/val/test splits and the use of validation data for hyperparameter tuning.</li>
<li>develop proficiency in writing efficient vectorized code with numpy</li>
<li>implement and apply a k-Nearest Neighbor (kNN) classifier</li>
<li>implement and apply a Multiclass Support Vector Machine (SVM) classifier</li>
<li>implement and apply a Softmax classifier</li>
<li>implement and apply a Two layer neural network classifier</li>
<li>understand the differences and tradeoffs between these classifiers</li>
<li>get a basic understanding of performance improvements from using higher-level representations than raw pixels (e.g. color histograms, Histogram of Gradient (HOG) features)</li>
</ul>
<h2 id="k-nearest-neighbor-classifier">k-Nearest Neighbor classifier</h2>
<p>The kNN classifier consists of two stages:</p>
<ul>
<li>During training, the classifier takes the training data and simply remembers it</li>
<li>During testing, kNN classifies every test image by comparing to all training images and transfering the labels of the k most similar training examples</li>
<li>The value of k is cross-validated</li>
</ul>
<p>In this exercise you will implement these steps and understand the basic Image Classification pipeline, cross-validation, and gain proficiency in writing efficient, vectorized code.</p>
<h3 id="key-point">Key point</h3>
<ol type="1">
<li>When we compute the distances wiout loop, these is a important observation</li>
</ol>
<p><span class="math display">\[(a-b)^{2} = a^{2} + b^{2} - 2ab\]</span></p>
<p>for example, if we want to compute the distance between <span class="math inline">\(\vec A\)</span> and <span class="math inline">\(\vec B\)</span>, the l2 norm is: <span class="math display">\[\sqrt{\sum_{i=1}^{D}(A_i - B_i)^{2}}\]</span> so we can factoring the equation above:</p>
<p><span class="math display">\[\sqrt{\sum_{i=1}^{D}(A_i)^{2} + \sum_{i=1}^{D}(B_i)^{2} - 2\sum_{i=1}^{D}(A_i\cdot B_i )}\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test_sum = np.<span class="built_in">sum</span>(np.square(X),axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">train_sum = np.<span class="built_in">sum</span>(np.square(self.X_train),axis = <span class="number">1</span>)</span><br><span class="line">cross_sum = X @ self.X_train.T</span><br><span class="line">dists = np.sqrt(train_sum + test_sum - <span class="number">2</span>*cross_sum)</span><br></pre></td></tr></table></figure>
<h3 id="knn-code">KNN code</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> <span class="built_in">range</span></span><br><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> <span class="built_in">object</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNearestNeighbor</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; a kNN classifier with L2 distance &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self, X, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Train the classifier. For k-nearest neighbors this is just</span></span><br><span class="line"><span class="string">        memorizing the training data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (num_train, D) containing the training data</span></span><br><span class="line"><span class="string">          consisting of num_train samples each of dimension D.</span></span><br><span class="line"><span class="string">        - y: A numpy array of shape (N,) containing the training labels, where</span></span><br><span class="line"><span class="string">             y[i] is the label for X[i].</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.X_train = X</span><br><span class="line">        self.y_train = y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, X, k=<span class="number">1</span>, num_loops=<span class="number">0</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Predict labels for test data using this classifier.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (num_test, D) containing test data consisting</span></span><br><span class="line"><span class="string">             of num_test samples each of dimension D.</span></span><br><span class="line"><span class="string">        - k: The number of nearest neighbors that vote for the predicted labels.</span></span><br><span class="line"><span class="string">        - num_loops: Determines which implementation to use to compute distances</span></span><br><span class="line"><span class="string">          between training points and testing points.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        - y: A numpy array of shape (num_test,) containing predicted labels for the</span></span><br><span class="line"><span class="string">          test data, where y[i] is the predicted label for the test point X[i].</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> num_loops == <span class="number">0</span>:</span><br><span class="line">            dists = self.compute_distances_no_loops(X)</span><br><span class="line">        <span class="keyword">elif</span> num_loops == <span class="number">1</span>:</span><br><span class="line">            dists = self.compute_distances_one_loop(X)</span><br><span class="line">        <span class="keyword">elif</span> num_loops == <span class="number">2</span>:</span><br><span class="line">            dists = self.compute_distances_two_loops(X)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Invalid value %d for num_loops&#x27;</span> % num_loops)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.predict_labels(dists, k=k)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_distances_two_loops</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">        in self.X_train using a nested loop over both the training data and the</span></span><br><span class="line"><span class="string">        test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (num_test, D) containing test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span></span><br><span class="line"><span class="string">          is the Euclidean distance between the ith test point and the jth training</span></span><br><span class="line"><span class="string">          point.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">        num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">        dists = np.zeros((num_test, num_train))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_test):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_train):</span><br><span class="line">                <span class="comment">#####################################################################</span></span><br><span class="line">                <span class="comment"># <span class="doctag">TODO:</span>                                                             #</span></span><br><span class="line">                <span class="comment"># Compute the l2 distance between the ith test point and the jth    #</span></span><br><span class="line">                <span class="comment"># training point, and store the result in dists[i, j]. You should   #</span></span><br><span class="line">                <span class="comment"># not use a loop over dimension, nor use np.linalg.norm().          #</span></span><br><span class="line">                <span class="comment">#####################################################################</span></span><br><span class="line">                <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">                dists[i,j] = np.sqrt(np.<span class="built_in">sum</span>(np.square(X[i] - self.X_train[j])))</span><br><span class="line"></span><br><span class="line">                <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">        <span class="keyword">return</span> dists</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_distances_one_loop</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">        in self.X_train using a single loop over the test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Input / Output: Same as compute_distances_two_loops</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">        num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">        dists = np.zeros((num_test, num_train))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_test):</span><br><span class="line">            <span class="comment">#######################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                               #</span></span><br><span class="line">            <span class="comment"># Compute the l2 distance between the ith test point and all training #</span></span><br><span class="line">            <span class="comment"># points, and store the result in dists[i, :].                        #</span></span><br><span class="line">            <span class="comment"># Do not use np.linalg.norm().                                        #</span></span><br><span class="line">            <span class="comment">#######################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">            dists[i] = np.sqrt(np.<span class="built_in">sum</span>(np.square(self.X_train-X[i]), axis = <span class="number">1</span>))</span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">        <span class="keyword">return</span> dists</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_distances_no_loops</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">        in self.X_train using no explicit loops.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Input / Output: Same as compute_distances_two_loops</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">        num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">        dists = np.zeros((num_test, num_train))</span><br><span class="line">        <span class="comment">#########################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">        <span class="comment"># Compute the l2 distance between all test points and all training      #</span></span><br><span class="line">        <span class="comment"># points without using any explicit loops, and store the result in      #</span></span><br><span class="line">        <span class="comment"># dists.                                                                #</span></span><br><span class="line">        <span class="comment">#                                                                       #</span></span><br><span class="line">        <span class="comment"># You should implement this function using only basic array operations; #</span></span><br><span class="line">        <span class="comment"># in particular you should not use functions from scipy,                #</span></span><br><span class="line">        <span class="comment"># nor use np.linalg.norm().                                             #</span></span><br><span class="line">        <span class="comment">#                                                                       #</span></span><br><span class="line">        <span class="comment"># HINT: Try to formulate the l2 distance using matrix multiplication    #</span></span><br><span class="line">        <span class="comment">#       and two broadcast sums.                                         #</span></span><br><span class="line">        <span class="comment">#########################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">        test_sum = np.<span class="built_in">sum</span>(np.square(X),axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">        train_sum = np.<span class="built_in">sum</span>(np.square(self.X_train),axis = <span class="number">1</span>)</span><br><span class="line">        cross_sum = X @ self.X_train.T</span><br><span class="line"></span><br><span class="line">        dists = np.sqrt(train_sum + test_sum - <span class="number">2</span>*cross_sum)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">        <span class="keyword">return</span> dists</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_labels</span>(<span class="params">self, dists, k=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Given a matrix of distances between test points and training points,</span></span><br><span class="line"><span class="string">        predict a label for each test point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span></span><br><span class="line"><span class="string">          gives the distance betwen the ith test point and the jth training point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        - y: A numpy array of shape (num_test,) containing predicted labels for the</span></span><br><span class="line"><span class="string">          test data, where y[i] is the predicted label for the test point X[i].</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        num_test = dists.shape[<span class="number">0</span>]</span><br><span class="line">        y_pred = np.zeros(num_test)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_test):</span><br><span class="line">            <span class="comment"># A list of length k storing the labels of the k nearest neighbors to</span></span><br><span class="line">            <span class="comment"># the ith test point.</span></span><br><span class="line">            closest_y = []</span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">            <span class="comment"># Use the distance matrix to find the k nearest neighbors of the ith    #</span></span><br><span class="line">            <span class="comment"># testing point, and use self.y_train to find the labels of these       #</span></span><br><span class="line">            <span class="comment"># neighbors. Store these labels in closest_y.                           #</span></span><br><span class="line">            <span class="comment"># Hint: Look up the function numpy.argsort.                             #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            closest_y = self.y_train[np.argsort(dists[i])[:k]]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">            <span class="comment"># Now that you have found the labels of the k nearest neighbors, you    #</span></span><br><span class="line">            <span class="comment"># need to find the most common label in the list closest_y of labels.   #</span></span><br><span class="line">            <span class="comment"># Store this label in y_pred[i]. Break ties by choosing the smaller     #</span></span><br><span class="line">            <span class="comment"># label.                                                                #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            y_pred[i] = np.bincount(closest_y).argmax()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
<h2 id="multiclass-support-vector-machine-exercise"># Multiclass Support Vector Machine exercise</h2>
<ul>
<li>implement a fully-vectorized <strong>loss function</strong> for the SVM</li>
<li>implement the fully-vectorized expression for its <strong>analytic gradient</strong></li>
<li><strong>check your implementation</strong> using numerical gradient</li>
<li>use a validation set to <strong>tune the learning rate and regularization</strong> strength</li>
<li><strong>optimize</strong> the loss function with <strong>SGD</strong></li>
<li><strong>visualize</strong> the final learned weights</li>
</ul>
<h3 id="key-points">Key points</h3>
<p>Recall that for the i-th example we are given the pixels of image <span class="math inline">\(x_i\)</span>,and the label <span class="math inline">\(y_i\)</span> that specifies the index of the correct class. The score function takes the pixels and computes the vector <span class="math inline">\(f(x_i,w)\)</span> of class scores, which we will abbreviate to <span class="math inline">\(s\)</span> (short for scores). For example, the score for the j-th class is the j-th element: <span class="math inline">\(s\_j = f(x\_i, W)\_j\)</span>, The Multiclass SVM loss for the i-th example is then formalized as follows:</p>
<p><span class="math display">\[L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta)\]</span></p>
<p>Note that in this particular module we are working with linear score functions (<span class="math inline">\(f(x_i; W) = W x_i\)</span>), so we can also rewrite the loss function in this equivalent form:</p>
<p><span class="math display">\[L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)\]</span></p>
<p>add regularization term and expanding this out in its full form:</p>
<p><span class="math display">\[L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2\]</span></p>
<p>In addition to the motivation we provided above there are many desirable properties to include the regularization penalty, many of which we will come back to in later sections. For example, it turns out that including the L2 penalty leads to the appealing max margin property in SVMs</p>
<ol type="1">
<li>Calculate the derivative for svm</li>
</ol>
<p><span class="math display">\[\begin{equation}
\left\{ 
\begin{array}{lr}
&amp; \frac{\partial {L_{i j}}}{\partial w_j} = - x_i^{T}  \quad j = y_i \\
&amp; \frac{\partial {L_{i j}}}{\partial w_j} = x_i^{T}   \quad j \neq y_i 
\end{array}
\right.
\end{equation}\]</span></p>
<ol start="2" type="1">
<li>calculate the loss and gradient by vector</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">scores = X @ W</span><br><span class="line">    <span class="comment"># print(scores.shape)</span></span><br><span class="line">    <span class="comment"># print(y)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">## difference of each class for every sample</span></span><br><span class="line">    margins = np.maximum(<span class="number">0</span>,scores - scores[<span class="built_in">range</span>(num_train),y].reshape(-<span class="number">1</span>,<span class="number">1</span>) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">## true class have no loss</span></span><br><span class="line">    margins[<span class="built_in">range</span>(num_train),y] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">## calculate the loss</span></span><br><span class="line">    loss = np.<span class="built_in">sum</span>(margins) / num_train + <span class="number">0.5</span> * reg * np.<span class="built_in">sum</span>(W * W)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">## calcute the times we should repeate when calculate the gradient for true class</span></span><br><span class="line">    margins[margins &gt; <span class="number">0</span>] = <span class="number">1.0</span></span><br><span class="line">    row_sum = np.<span class="built_in">sum</span>(margins, axis=<span class="number">1</span>)                  </span><br><span class="line">    margins[np.arange(num_train), y] = -row_sum   </span><br><span class="line"></span><br><span class="line">    <span class="comment">## calcute the gradient</span></span><br><span class="line">    dW += np.dot(X.T, margins)/num_train + reg * W  </span><br></pre></td></tr></table></figure>
<h3 id="svm-code">SVM code</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> <span class="built_in">range</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_naive</span>(<span class="params">W, X, y, reg</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Structured SVM loss function, naive implementation (with loops).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">    of N examples.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - reg: (float) regularization strength</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss as single float</span></span><br><span class="line"><span class="string">    - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dW = np.zeros(W.shape) <span class="comment"># initialize the gradient as zero</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the loss and the gradient</span></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_train):</span><br><span class="line">        scores = X[i].dot(W)</span><br><span class="line">        correct_class_score = scores[y[i]]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            margin = scores[j] - correct_class_score + <span class="number">1</span> <span class="comment"># note delta = 1</span></span><br><span class="line">            <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</span><br><span class="line">                loss += margin</span><br><span class="line">                dW[:,j] += X[i,:].T</span><br><span class="line">                dW[:,y[i]] -= X[i,:].T</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Right now the loss is a sum over all training examples, but we want it</span></span><br><span class="line">    <span class="comment"># to be an average instead so we divide by num_train.</span></span><br><span class="line">    loss /= num_train</span><br><span class="line">    dW /= num_train</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add regularization to the loss.</span></span><br><span class="line">    loss += <span class="number">0.5</span> * reg * np.<span class="built_in">sum</span>(W * W)</span><br><span class="line">    dW += reg * W</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Compute the gradient of the loss function and store it dW.                #</span></span><br><span class="line">    <span class="comment"># Rather that first computing the loss and then computing the derivative,   #</span></span><br><span class="line">    <span class="comment"># it may be simpler to compute the derivative at the same time that the     #</span></span><br><span class="line">    <span class="comment"># loss is being computed. As a result you may need to modify some of the    #</span></span><br><span class="line">    <span class="comment"># code above to compute the gradient.                                       #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_vectorized</span>(<span class="params">W, X, y, reg</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Structured SVM loss function, vectorized implementation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs and outputs are the same as svm_loss_naive.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros(W.shape) <span class="comment"># initialize the gradient as zero</span></span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Implement a vectorized version of the structured SVM loss, storing the    #</span></span><br><span class="line">    <span class="comment"># result in loss.                                                           #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    scores = X @ W</span><br><span class="line">    <span class="comment"># print(scores.shape)</span></span><br><span class="line">    <span class="comment"># print(y)</span></span><br><span class="line"></span><br><span class="line">    margins = np.maximum(<span class="number">0</span>,scores - scores[<span class="built_in">range</span>(num_train),y].reshape(-<span class="number">1</span>,<span class="number">1</span>) + <span class="number">1</span>)</span><br><span class="line">    margins[<span class="built_in">range</span>(num_train),y] = <span class="number">0</span></span><br><span class="line">    loss = np.<span class="built_in">sum</span>(margins) / num_train + <span class="number">0.5</span> * reg * np.<span class="built_in">sum</span>(W * W)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Implement a vectorized version of the gradient for the structured SVM     #</span></span><br><span class="line">    <span class="comment"># loss, storing the result in dW.                                           #</span></span><br><span class="line">    <span class="comment">#                                                                           #</span></span><br><span class="line">    <span class="comment"># Hint: Instead of computing the gradient from scratch, it may be easier    #</span></span><br><span class="line">    <span class="comment"># to reuse some of the intermediate values that you used to compute the     #</span></span><br><span class="line">    <span class="comment"># loss.                                                                     #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    margins[margins &gt; <span class="number">0</span>] = <span class="number">1.0</span></span><br><span class="line">    row_sum = np.<span class="built_in">sum</span>(margins, axis=<span class="number">1</span>)                  </span><br><span class="line">    margins[np.arange(num_train), y] = -row_sum        </span><br><span class="line">    dW += np.dot(X.T, margins)/num_train + reg * W  </span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<h2 id="implement-a-softmax-classifier">Implement a Softmax classifier</h2>
<ul>
<li>implement a fully-vectorized <strong>loss function</strong> for the Softmax classifier</li>
<li>implement the fully-vectorized expression for its <strong>analytic gradient</strong></li>
<li><strong>check your implementation</strong> with numerical gradient</li>
<li>use a validation set to <strong>tune the learning rate and regularization</strong> strength</li>
<li><strong>optimize</strong> the loss function with <strong>SGD</strong></li>
<li><strong>visualize</strong> the final learned weights</li>
</ul>
<p>Unlike the SVM which treats the outputs <span class="math inline">\(f(x_i,W)\)</span> as (uncalibrated and possibly difficult to interpret) scores for each class, the Softmax classifier gives a slightly more intuitive output (normalized class probabilities) and also has a probabilistic interpretation that we will describe shortly. In the Softmax classifier, the function mapping <span class="math inline">\(f(x_i; W) = W x_i\)</span> stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss that has the form: <span class="math display">\[L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}\]</span></p>
<p><span class="math display">\[L =  \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + \underbrace{ \frac{1}{2} \lambda \sum_k\sum_l W_{k,l}^2 }\_\text{regularization loss}\]</span></p>
<p>The function <span class="math inline">\(\frac{e^{z_j}}{\sum_k e^{z_k}}\)</span>, is called the softmax function: It takes a vector of arbitrary real-valued scores (in <span class="math inline">\(z\)</span>) and squashes it to a vector of values between zero and one that sum to one. The full cross-entropy loss that involves the softmax function might look scary if you’re seeing it for the first time but it is relatively easy to motivate.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">num_examples = X.shape[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># get unnormalized probabilities</span></span><br><span class="line">exp_scores = np.exp(scores)</span><br><span class="line"><span class="comment"># normalize them for each example</span></span><br><span class="line">probs = exp_scores / np.<span class="built_in">sum</span>(exp_scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">correct_logprobs = -np.log(probs[<span class="built_in">range</span>(num_examples),y])</span><br><span class="line"><span class="comment"># compute the loss: average cross-entropy loss and regularization</span></span><br><span class="line">data_loss = np.<span class="built_in">sum</span>(correct_logprobs)/num_examples</span><br><span class="line">reg_loss = <span class="number">0.5</span>*reg*np.<span class="built_in">sum</span>(W*W)</span><br><span class="line">loss = data_loss + reg_loss</span><br></pre></td></tr></table></figure>
<p>We have a way of evaluating the loss, and now we have to minimize it. We’ll do so with gradient descent. That is, we start with random parameters (as shown above), and evaluate the gradient of the loss function with respect to the parameters, so that we know how we should change the parameters to decrease the loss. Lets introduce the intermediate variable <span class="math inline">\(P\)</span>,which is a vector of the (normalized) probabilities. The loss for one example is:</p>
<p><span class="math display">\[p_k = \frac{e^{f_k}}{ \sum_j e^{f_j} } \hspace{1in} L_i =-\log\left(p_{y_i}\right)\]</span></p>
<p>We now wish to understand how the computed scores inside <span class="math inline">\(f\)</span> should change to decrease the loss <span class="math inline">\(L_i\)</span> that this example contributes to the full objective. In other words, we want to derive the gradient <span class="math inline">\(\partial L_i / \partial f_k\)</span>. The loss <span class="math inline">\(Li\)</span> is computed from <span class="math inline">\(p\)</span> which in turn depends on <span class="math inline">\(f\)</span> It’s a fun exercise to the reader to use the chain rule to derive the gradient, but it turns out to be extremely simple and interpretible in the end, after a lot of things cancel out: <span class="math display">\[\frac{\partial L_i }{ \partial f_k } = p_k - \mathbb{1}(y_i = k)\]</span> <span class="math display">\[\frac{\partial L_i }{ \partial f_j } = p_j - \mathbb{1}(y_i = k)\]</span></p>
<p>Notice how elegant and simple this expression is. Suppose the probabilities we computed were <code>p = [0.2, 0.3, 0.5]</code>, and that the correct class was the middle one (with probability 0.3). According to this derivation the gradient on the scores would be <code>df = [0.2, -0.7, 0.5]</code>. Recalling what the interpretation of the gradient, we see that this result is highly intuitive: increasing the first or last element of the score vector f (the scores of the incorrect classes) leads to an increased loss (due to the positive signs +0.2 and +0.5) - and increasing the loss is bad, as expected. However, increasing the score of the correct class has negative influence on the loss. The gradient of -0.7 is telling us that increasing the correct class score would lead to a decrease of the loss <span class="math inline">\(L_i\)</span>, which makes sense.</p>
<p>To get the gradient on the scores, which we call <span class="math inline">\(dscores\)</span>, we proceed as follows: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dscores = probs</span><br><span class="line">dscores[<span class="built_in">range</span>(num_examples),y] -= <span class="number">1</span></span><br><span class="line">dscores /= num_examples</span><br></pre></td></tr></table></figure></p>
<p>Lastly, we had that <span class="math display">\[scores = np.dot(X, W) + b\]</span>, so armed with the gradient on scores (stored in dscores), we can now backpropagate into W and b:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dW = np.dot(X.T, dscores)</span><br><span class="line">db = np.<span class="built_in">sum</span>(dscores, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">dW += reg*W <span class="comment"># don&#x27;t forget the regularization gradient</span></span><br></pre></td></tr></table></figure>
<h3 id="softmax-code">Softmax code</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> <span class="built_in">range</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span>(<span class="params">W, X, y, reg</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Softmax loss function, naive implementation (with loops)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">    of N examples.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - reg: (float) regularization strength</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss as single float</span></span><br><span class="line"><span class="string">    - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Compute the softmax loss and its gradient using explicit loops.     #</span></span><br><span class="line">    <span class="comment"># Store the loss in loss and the gradient in dW. If you are not careful     #</span></span><br><span class="line">    <span class="comment"># here, it is easy to run into numeric instability. Don&#x27;t forget the        #</span></span><br><span class="line">    <span class="comment"># regularization!                                                           #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    num_trains = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_trains):</span><br><span class="line">        scores = X[i].T @ W</span><br><span class="line">        shift_scores = scores - np.<span class="built_in">max</span>(scores)</span><br><span class="line">        loss_i = - shift_scores[y[i]] + np.log(np.<span class="built_in">sum</span>(np.exp(shift_scores)))</span><br><span class="line">        loss += loss_i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_classes):</span><br><span class="line">            softmax_output = np.exp(shift_scores[j])/<span class="built_in">sum</span>(np.exp(shift_scores))</span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                dW[:,j] += (-<span class="number">1</span> + softmax_output) *X[i] </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                dW[:,j] += softmax_output *X[i] </span><br><span class="line"></span><br><span class="line">        loss /= num_trains</span><br><span class="line">        loss +=  <span class="number">0.5</span>* reg * np.<span class="built_in">sum</span>(W * W)</span><br><span class="line">        dW = dW/num_trains + reg* W </span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span>(<span class="params">W, X, y, reg</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Softmax loss function, vectorized version.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs and outputs are the same as softmax_loss_naive.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Compute the softmax loss and its gradient using no explicit loops.  #</span></span><br><span class="line">    <span class="comment"># Store the loss in loss and the gradient in dW. If you are not careful     #</span></span><br><span class="line">    <span class="comment"># here, it is easy to run into numeric instability. Don&#x27;t forget the        #</span></span><br><span class="line">    <span class="comment"># regularization!                                                           #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    scores = X.dot(W)</span><br><span class="line">    shift_scores = scores - np.<span class="built_in">max</span>(scores, axis = <span class="number">1</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    softmax_output = np.exp(shift_scores)/np.<span class="built_in">sum</span>(np.exp(shift_scores), axis = <span class="number">1</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    loss = -np.<span class="built_in">sum</span>(np.log(softmax_output[<span class="built_in">range</span>(num_train), <span class="built_in">list</span>(y)]))</span><br><span class="line">    loss /= num_train </span><br><span class="line">    loss +=  <span class="number">0.5</span>* reg * np.<span class="built_in">sum</span>(W * W)</span><br><span class="line"></span><br><span class="line">    dS = softmax_output.copy()</span><br><span class="line">    dS[<span class="built_in">range</span>(num_train), <span class="built_in">list</span>(y)] += -<span class="number">1</span></span><br><span class="line">    dW = (X.T).dot(dS)</span><br><span class="line">    dW = dW/num_train + reg* W </span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<h2 id="two-layer-neural-network">Two-Layer Neural Network</h2>
<p>Clearly, a linear classifier is inadequate for this dataset and we would like to use a Neural Network. One additional hidden layer will suffice for this toy data. We will now need two sets of weights and biases (for the first and second layers):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialize parameters randomly</span></span><br><span class="line">h = <span class="number">100</span> <span class="comment"># size of hidden layer</span></span><br><span class="line">W = <span class="number">0.01</span> * np.random.randn(D,h)</span><br><span class="line">b = np.zeros((<span class="number">1</span>,h))</span><br><span class="line">W2 = <span class="number">0.01</span> * np.random.randn(h,K)</span><br><span class="line">b2 = np.zeros((<span class="number">1</span>,K))</span><br></pre></td></tr></table></figure>
<p>The forward pass to compute scores now changes form:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># evaluate class scores with a 2-layer Neural Network</span></span><br><span class="line">hidden_layer = np.maximum(<span class="number">0</span>, np.dot(X, W) + b) <span class="comment"># note, ReLU activation</span></span><br><span class="line">scores = np.dot(hidden_layer, W2) + b2</span><br></pre></td></tr></table></figure>
<p>Notice that the only change from before is one <strong>extra line of code</strong>, where we first compute the hidden layer representation and then the scores based on this hidden layer. Crucially, we’ve also added a non-linearity, which in this case is simple ReLU that thresholds the activations on the hidden layer at zero.</p>
<p>Everything else remains the same. We compute the loss based on the scores exactly as before, and get the gradient for the scores dscores exactly as before. However, the way we backpropagate that gradient into the model parameters now changes form, of course. First lets backpropagate the second layer of the Neural Network. This looks identical to the code we had for the Softmax classifier, except we’re replacing X (the raw data), with the variable hidden_layer):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># backpropate the gradient to the parameters</span></span><br><span class="line"><span class="comment"># first backprop into parameters W2 and b2</span></span><br><span class="line">dW2 = np.dot(hidden_layer.T, dscores)</span><br><span class="line">db2 = np.<span class="built_in">sum</span>(dscores, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>However, unlike before we are not yet done, because hidden_layer is itself a function of other parameters and the data! We need to continue backpropagation through this variable. Its gradient can be computed as:</p>
<p><span class="math display">\[relu = max(0,a)\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dhidden = np.dot(dscores, W2.T)</span><br></pre></td></tr></table></figure>
<p>Now we have the gradient on the outputs of the hidden layer. Next, we have to backpropagate the ReLU non-linearity. This turns out to be easy because ReLU during the backward pass is effectively a switch. Since <span class="math inline">\(r = max(0, x)\)</span>, we have that <span class="math inline">\(\frac{dr}{dx} = 1(x &gt; 0)\)</span>, Combined with the chain rule, we see that the ReLU unit lets the gradient pass through unchanged if its input was greater than 0, but kills it if its input was less than zero during the forward pass. Hence, we can backpropagate the ReLU in place simply with:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># backprop the ReLU non-linearity</span></span><br><span class="line">dhidden[hidden_layer &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"><span class="comment"># finally into W,b</span></span><br><span class="line">dW = np.dot(X.T, dhidden)</span><br><span class="line">db = np.<span class="built_in">sum</span>(dhidden, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="neural-net-code">Neural Net code</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> <span class="built_in">range</span></span><br><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> <span class="built_in">object</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A two-layer fully-connected neural network. The net has an input dimension of</span></span><br><span class="line"><span class="string">    N, a hidden layer dimension of H, and performs classification over C classes.</span></span><br><span class="line"><span class="string">    We train the network with a softmax loss function and L2 regularization on the</span></span><br><span class="line"><span class="string">    weight matrices. The network uses a ReLU nonlinearity after the first fully</span></span><br><span class="line"><span class="string">    connected layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    In other words, the network has the following architecture:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    input - fully connected layer - ReLU - fully connected layer - softmax</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The outputs of the second fully-connected layer are the scores for each class.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size, std=<span class="number">1e-4</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialize the model. Weights are initialized to small random values and</span></span><br><span class="line"><span class="string">        biases are initialized to zero. Weights and biases are stored in the</span></span><br><span class="line"><span class="string">        variable self.params, which is a dictionary with the following keys:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        W1: First layer weights; has shape (D, H)</span></span><br><span class="line"><span class="string">        b1: First layer biases; has shape (H,)</span></span><br><span class="line"><span class="string">        W2: Second layer weights; has shape (H, C)</span></span><br><span class="line"><span class="string">        b2: Second layer biases; has shape (C,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - input_size: The dimension D of the input data.</span></span><br><span class="line"><span class="string">        - hidden_size: The number of neurons H in the hidden layer.</span></span><br><span class="line"><span class="string">        - output_size: The number of classes C.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.params[<span class="string">&#x27;W1&#x27;</span>] = std * np.random.randn(input_size, hidden_size)</span><br><span class="line">        self.params[<span class="string">&#x27;b1&#x27;</span>] = np.zeros(hidden_size)</span><br><span class="line">        self.params[<span class="string">&#x27;W2&#x27;</span>] = std * np.random.randn(hidden_size, output_size)</span><br><span class="line">        self.params[<span class="string">&#x27;b2&#x27;</span>] = np.zeros(output_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">relu</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.maximum(<span class="number">0</span>,x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, X, y=<span class="literal">None</span>, reg=<span class="number">0.0</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Compute the loss and gradients for a two layer fully connected neural</span></span><br><span class="line"><span class="string">        network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: Input data of shape (N, D). Each X[i] is a training sample.</span></span><br><span class="line"><span class="string">        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is</span></span><br><span class="line"><span class="string">          an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it</span></span><br><span class="line"><span class="string">          is not passed then we only return scores, and if it is passed then we</span></span><br><span class="line"><span class="string">          instead return the loss and gradients.</span></span><br><span class="line"><span class="string">        - reg: Regularization strength.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is</span></span><br><span class="line"><span class="string">        the score for class c on input X[i].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        If y is not None, instead return a tuple of:</span></span><br><span class="line"><span class="string">        - loss: Loss (data loss and regularization loss) for this batch of training</span></span><br><span class="line"><span class="string">          samples.</span></span><br><span class="line"><span class="string">        - grads: Dictionary mapping parameter names to gradients of those parameters</span></span><br><span class="line"><span class="string">          with respect to the loss function; has the same keys as self.params.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Unpack variables from the params dictionary</span></span><br><span class="line">        W1, b1 = self.params[<span class="string">&#x27;W1&#x27;</span>], self.params[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line">        W2, b2 = self.params[<span class="string">&#x27;W2&#x27;</span>], self.params[<span class="string">&#x27;b2&#x27;</span>]</span><br><span class="line">        N, D = X.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the forward pass</span></span><br><span class="line">        scores = <span class="literal">None</span></span><br><span class="line">        <span class="comment">#############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Perform the forward pass, computing the class scores for the input. #</span></span><br><span class="line">        <span class="comment"># Store the result in the scores variable, which should be an array of      #</span></span><br><span class="line">        <span class="comment"># shape (N, C).                                                             #</span></span><br><span class="line">        <span class="comment">#############################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># (n,d) * (d,h) + (h,) = (n,h)</span></span><br><span class="line">        h_output =  self.relu(X @ W1 + b1)    </span><br><span class="line">        scores = h_output @ W2 + b2</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># If the targets are not given then jump out, we&#x27;re done</span></span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the loss</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="comment">#############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Finish the forward pass, and compute the loss. This should include  #</span></span><br><span class="line">        <span class="comment"># both the data loss and L2 regularization for W1 and W2. Store the result  #</span></span><br><span class="line">        <span class="comment"># in the variable loss, which should be a scalar. Use the Softmax           #</span></span><br><span class="line">        <span class="comment"># classifier loss.                                                          #</span></span><br><span class="line">        <span class="comment">#############################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        shift_scores = scores - np.<span class="built_in">max</span>(scores, axis = <span class="number">1</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">        softmax_output = np.exp(shift_scores)/np.<span class="built_in">sum</span>(np.exp(shift_scores), axis = <span class="number">1</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">        loss = -np.<span class="built_in">sum</span>(np.log(softmax_output[<span class="built_in">range</span>(N), <span class="built_in">list</span>(y)]))</span><br><span class="line">        loss /= N</span><br><span class="line">        loss +=  <span class="number">0.5</span>* reg * (np.<span class="built_in">sum</span>(W1 * W1) + np.<span class="built_in">sum</span>(W2 * W2))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward pass: compute gradients</span></span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        <span class="comment">#############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Compute the backward pass, computing the derivatives of the weights #</span></span><br><span class="line">        <span class="comment"># and biases. Store the results in the grads dictionary. For example,       #</span></span><br><span class="line">        <span class="comment"># grads[&#x27;W1&#x27;] should store the gradient on W1, and be a matrix of same size #</span></span><br><span class="line">        <span class="comment">#############################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        dscores = softmax_output.copy()</span><br><span class="line">        dscores[<span class="built_in">range</span>(N), <span class="built_in">list</span>(y)] -= <span class="number">1</span></span><br><span class="line">        dscores /= N</span><br><span class="line">        grads[<span class="string">&#x27;W2&#x27;</span>] = h_output.T.dot(dscores) + reg * W2</span><br><span class="line">        grads[<span class="string">&#x27;b2&#x27;</span>] = np.<span class="built_in">sum</span>(dscores, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        dh = dscores.dot(W2.T)</span><br><span class="line">        dh_ReLu = (h_output &gt; <span class="number">0</span>) * dh</span><br><span class="line">        grads[<span class="string">&#x27;W1&#x27;</span>] = X.T.dot(dh_ReLu) + reg * W1</span><br><span class="line">        grads[<span class="string">&#x27;b1&#x27;</span>] = np.<span class="built_in">sum</span>(dh_ReLu, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, grads</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self, X, y, X_val, y_val,</span></span></span><br><span class="line"><span class="params"><span class="function">              learning_rate=<span class="number">1e-3</span>, learning_rate_decay=<span class="number">0.95</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">              reg=<span class="number">5e-6</span>, num_iters=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">              batch_size=<span class="number">200</span>, verbose=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Train this neural network using stochastic gradient descent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (N, D) giving training data.</span></span><br><span class="line"><span class="string">        - y: A numpy array f shape (N,) giving training labels; y[i] = c means that</span></span><br><span class="line"><span class="string">          X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">        - X_val: A numpy array of shape (N_val, D) giving validation data.</span></span><br><span class="line"><span class="string">        - y_val: A numpy array of shape (N_val,) giving validation labels.</span></span><br><span class="line"><span class="string">        - learning_rate: Scalar giving learning rate for optimization.</span></span><br><span class="line"><span class="string">        - learning_rate_decay: Scalar giving factor used to decay the learning rate</span></span><br><span class="line"><span class="string">          after each epoch.</span></span><br><span class="line"><span class="string">        - reg: Scalar giving regularization strength.</span></span><br><span class="line"><span class="string">        - num_iters: Number of steps to take when optimizing.</span></span><br><span class="line"><span class="string">        - batch_size: Number of training examples to use per step.</span></span><br><span class="line"><span class="string">        - verbose: boolean; if true print progress during optimization.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">        iterations_per_epoch = <span class="built_in">max</span>(num_train / batch_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Use SGD to optimize the parameters in self.model</span></span><br><span class="line">        loss_history = []</span><br><span class="line">        train_acc_history = []</span><br><span class="line">        val_acc_history = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">            X_batch = <span class="literal">None</span></span><br><span class="line">            y_batch = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span> Create a random minibatch of training data and labels, storing  #</span></span><br><span class="line">            <span class="comment"># them in X_batch and y_batch respectively.                             #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            idx = np.random.choice(num_train, batch_size, replace=<span class="literal">True</span>)</span><br><span class="line">            X_batch = X[idx]</span><br><span class="line">            y_batch = y[idx]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute loss and gradients using the current minibatch</span></span><br><span class="line">            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)</span><br><span class="line">            loss_history.append(loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span> Use the gradients in the grads dictionary to update the         #</span></span><br><span class="line">            <span class="comment"># parameters of the network (stored in the dictionary self.params)      #</span></span><br><span class="line">            <span class="comment"># using stochastic gradient descent. You&#x27;ll need to use the gradients   #</span></span><br><span class="line">            <span class="comment"># stored in the grads dictionary defined above.                         #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            self.params[<span class="string">&#x27;W2&#x27;</span>] += - learning_rate * grads[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">            self.params[<span class="string">&#x27;b2&#x27;</span>] += - learning_rate * grads[<span class="string">&#x27;b2&#x27;</span>]</span><br><span class="line">            self.params[<span class="string">&#x27;W1&#x27;</span>] += - learning_rate * grads[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">            self.params[<span class="string">&#x27;b1&#x27;</span>] += - learning_rate * grads[<span class="string">&#x27;b1&#x27;</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;iteration %d / %d: loss %f&#x27;</span> % (it, num_iters, loss))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Every epoch, check train and val accuracy and decay learning rate.</span></span><br><span class="line">            <span class="keyword">if</span> it % iterations_per_epoch == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># Check accuracy</span></span><br><span class="line">                train_acc = (self.predict(X_batch) == y_batch).mean()</span><br><span class="line">                val_acc = (self.predict(X_val) == y_val).mean()</span><br><span class="line">                train_acc_history.append(train_acc)</span><br><span class="line">                val_acc_history.append(val_acc)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Decay learning rate</span></span><br><span class="line">                learning_rate *= learning_rate_decay</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">          <span class="string">&#x27;loss_history&#x27;</span>: loss_history,</span><br><span class="line">          <span class="string">&#x27;train_acc_history&#x27;</span>: train_acc_history,</span><br><span class="line">          <span class="string">&#x27;val_acc_history&#x27;</span>: val_acc_history,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Use the trained weights of this two-layer network to predict labels for</span></span><br><span class="line"><span class="string">        data points. For each data point we predict scores for each of the C</span></span><br><span class="line"><span class="string">        classes, and assign each data point to the class with the highest score.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (N, D) giving N D-dimensional data points to</span></span><br><span class="line"><span class="string">          classify.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        - y_pred: A numpy array of shape (N,) giving predicted labels for each of</span></span><br><span class="line"><span class="string">          the elements of X. For all i, y_pred[i] = c means that X[i] is predicted</span></span><br><span class="line"><span class="string">          to have class c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        y_pred = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">###########################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement this function; it should be VERY simple!                #</span></span><br><span class="line">        <span class="comment">###########################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        h = np.maximum(<span class="number">0</span>, X.dot(self.params[<span class="string">&#x27;W1&#x27;</span>]) + self.params[<span class="string">&#x27;b1&#x27;</span>])</span><br><span class="line">        scores = h.dot(self.params[<span class="string">&#x27;W2&#x27;</span>]) + self.params[<span class="string">&#x27;b2&#x27;</span>]</span><br><span class="line">        y_pred = np.argmax(scores, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
<h2 id="higher-level-representations-image-features">Higher Level Representations: Image Features</h2>
<p>An important way to gain intuition about how an algorithm works is to visualize the mistakes that it makes. In this visualization, we show examples of images that are misclassified by our current system. The first column shows images that our system labeled as "plane" but whose true label is something other than "plane".</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">examples_per_class = <span class="number">8</span></span><br><span class="line">classes = [<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> cls, cls_name <span class="keyword">in</span> <span class="built_in">enumerate</span>(classes):</span><br><span class="line">    idxs = np.where((y_test != cls) &amp; (y_test_pred == cls))[<span class="number">0</span>]</span><br><span class="line">    idxs = np.random.choice(idxs, examples_per_class, replace=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">for</span> i, idx <span class="keyword">in</span> <span class="built_in">enumerate</span>(idxs):</span><br><span class="line">        plt.subplot(examples_per_class, <span class="built_in">len</span>(classes), i * <span class="built_in">len</span>(classes) + cls + <span class="number">1</span>)</span><br><span class="line">        plt.imshow(X_test[idx].astype(<span class="string">&#x27;uint8&#x27;</span>))</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            plt.title(cls_name)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cs231n.classifiers.neural_net <span class="keyword">import</span> TwoLayerNet</span><br><span class="line"></span><br><span class="line">input_dim = X_train_feats.shape[<span class="number">1</span>]</span><br><span class="line">hidden_dim = <span class="number">500</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#net = TwoLayerNet(input_dim, hidden_dim, num_classes)</span></span><br><span class="line">best_net = <span class="literal">None</span></span><br><span class="line">best_val = -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Train a two-layer neural network on image features. You may want to    #</span></span><br><span class="line"><span class="comment"># cross-validate various parameters as in previous sections. Store your best   #</span></span><br><span class="line"><span class="comment"># model in the best_net variable.                                              #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_random_hyperparams</span>(<span class="params">lr_min, lr_max, reg_min, reg_max, h_min, h_max</span>):</span></span><br><span class="line">    lr = <span class="number">10</span>**np.random.uniform(lr_min,lr_max)</span><br><span class="line">    reg = <span class="number">10</span>**np.random.uniform(reg_min,reg_max)</span><br><span class="line">    hidden = np.random.randint(h_min, h_max)</span><br><span class="line">    <span class="keyword">return</span> lr, reg, hidden</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use of random search for hyperparameter search</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    lr, reg, hidden_dim = generate_random_hyperparams(-<span class="number">1</span>, <span class="number">0</span>, -<span class="number">7</span>, -<span class="number">4</span>, <span class="number">10</span>, <span class="number">500</span>)</span><br><span class="line">    <span class="comment"># Create a two-layer network</span></span><br><span class="line">    net = TwoLayerNet(input_dim, hidden_dim, num_classes)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Train the network</span></span><br><span class="line">    stats = net.train(X_train_feats, y_train, X_val_feats, y_val,</span><br><span class="line">                num_iters=<span class="number">3000</span>, batch_size=<span class="number">200</span>,</span><br><span class="line">                learning_rate=lr, learning_rate_decay=<span class="number">0.95</span>,</span><br><span class="line">                reg=reg, verbose=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predict on the training set</span></span><br><span class="line">    train_accuracy = (net.predict(X_train_feats) == y_train).mean()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Predict on the validation set</span></span><br><span class="line">    val_accuracy = (net.predict(X_val_feats) == y_val).mean()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save best values</span></span><br><span class="line">    <span class="keyword">if</span> val_accuracy &gt; best_val:</span><br><span class="line">        best_val = val_accuracy</span><br><span class="line">        best_net = net</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Print results</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;lr %e reg %e hid %d  train accuracy: %f val accuracy: %f&#x27;</span> % (</span><br><span class="line">                lr, reg, hidden_dim, train_accuracy, val_accuracy))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;best validation accuracy achieved: %f&#x27;</span> % best_val)</span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                              END OF YOUR CODE                                #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/cs231n/" rel="tag"># cs231n</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Machine-Language/2019/09/22/" rel="prev" title="Machine Language">
      <i class="fa fa-chevron-left"></i> Machine Language
    </a></div>
      <div class="post-nav-item">
    <a href="/Naive-Bayes/2019/10/02/" rel="next" title="Naive Bayes">
      Naive Bayes <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#the-goals-of-this-assignment-are-as-follows"><span class="nav-number">1.</span> <span class="nav-text">The goals of this assignment are as follows</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k-nearest-neighbor-classifier"><span class="nav-number">2.</span> <span class="nav-text">k-Nearest Neighbor classifier</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#key-point"><span class="nav-number">2.1.</span> <span class="nav-text">Key point</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#knn-code"><span class="nav-number">2.2.</span> <span class="nav-text">KNN code</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multiclass-support-vector-machine-exercise"><span class="nav-number">3.</span> <span class="nav-text"># Multiclass Support Vector Machine exercise</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#key-points"><span class="nav-number">3.1.</span> <span class="nav-text">Key points</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#svm-code"><span class="nav-number">3.2.</span> <span class="nav-text">SVM code</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#implement-a-softmax-classifier"><span class="nav-number">4.</span> <span class="nav-text">Implement a Softmax classifier</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax-code"><span class="nav-number">4.1.</span> <span class="nav-text">Softmax code</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#two-layer-neural-network"><span class="nav-number">5.</span> <span class="nav-text">Two-Layer Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#neural-net-code"><span class="nav-number">5.1.</span> <span class="nav-text">Neural Net code</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#higher-level-representations-image-features"><span class="nav-number">6.</span> <span class="nav-text">Higher Level Representations: Image Features</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">221</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail → mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'en' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
