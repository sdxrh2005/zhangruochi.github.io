<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="RUOCHI.AI">
<meta property="og:url" content="https://zhangruochi.com/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Ruochi Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhangruochi.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/hello-world/2021/12/31/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/hello-world/2021/12/31/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-31 13:18:28" itemprop="dateCreated datePublished" datetime="2021-12-31T13:18:28+08:00">2021-12-31</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/hello-world/2021/12/31/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/hello-world/2021/12/31/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Computer-Network/2021/09/02/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Computer-Network/2021/09/02/" class="post-title-link" itemprop="url">Computer Network</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-02 11:54:32" itemprop="dateCreated datePublished" datetime="2021-09-02T11:54:32+08:00">2021-09-02</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Computer-Network/2021/09/02/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Computer-Network/2021/09/02/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Product-Manager-Concept/2021/05/06/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Product-Manager-Concept/2021/05/06/" class="post-title-link" itemprop="url">Product Manager - Concept</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-06 23:53:15" itemprop="dateCreated datePublished" datetime="2021-05-06T23:53:15+08:00">2021-05-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-05-07 00:12:16" itemprop="dateModified" datetime="2021-05-07T00:12:16+08:00">2021-05-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Product-Management/" itemprop="url" rel="index"><span itemprop="name">Product Management</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Product-Manager-Concept/2021/05/06/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Product-Manager-Concept/2021/05/06/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>《人人都是产品经理2.0》读书笔记</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/Product-Manager-Concept/2021/05/06/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Product-Manager-Keywords/2021/05/06/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Product-Manager-Keywords/2021/05/06/" class="post-title-link" itemprop="url">Product Manager - Keywords</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-05-06 22:57:53 / Modified: 23:48:41" itemprop="dateCreated datePublished" datetime="2021-05-06T22:57:53+08:00">2021-05-06</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Product-Management/" itemprop="url" rel="index"><span itemprop="name">Product Management</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Product-Manager-Keywords/2021/05/06/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Product-Manager-Keywords/2021/05/06/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>《人人都是产品经理2.0》读书笔记</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/Product-Manager-Keywords/2021/05/06/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Product-Manager-Introduction/2021/05/06/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Product-Manager-Introduction/2021/05/06/" class="post-title-link" itemprop="url">Product Manager - Introduction</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-05-06 22:25:13 / Modified: 23:06:02" itemprop="dateCreated datePublished" datetime="2021-05-06T22:25:13+08:00">2021-05-06</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Product-Management/" itemprop="url" rel="index"><span itemprop="name">Product Management</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Product-Manager-Introduction/2021/05/06/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Product-Manager-Introduction/2021/05/06/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>《人人都是产品经理2.0》读书笔记</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/Product-Manager-Introduction/2021/05/06/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/CS224W-Colab-1/2021/01/22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/CS224W-Colab-1/2021/01/22/" class="post-title-link" itemprop="url">CS224W - Colab 1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-01-22 16:39:10" itemprop="dateCreated datePublished" datetime="2021-01-22T16:39:10+08:00">2021-01-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-31 15:04:09" itemprop="dateModified" datetime="2021-12-31T15:04:09+08:00">2021-12-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Graph-Neural-Network/" itemprop="url" rel="index"><span itemprop="name">Graph Neural Network</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/CS224W-Colab-1/2021/01/22/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/CS224W-Colab-1/2021/01/22/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>CS224W - Colab 1 Homework</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/CS224W-Colab-1/2021/01/22/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Implement-your-agent/2020/10/19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Implement-your-agent/2020/10/19/" class="post-title-link" itemprop="url">Implement your agent</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-10-19 16:45:27 / Modified: 16:46:39" itemprop="dateCreated datePublished" datetime="2020-10-19T16:45:27+08:00">2020-10-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Implement-your-agent/2020/10/19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Implement-your-agent/2020/10/19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="assignment-2---implement-your-agent">Assignment 2 - Implement your agent</h1>
<p>Welcome to Course 4, Programming Assignment 2! We have learned about reinforcement learning algorithms for prediction and control in previous courses and extended those algorithms to large state spaces using function approximation. One example of this was in assignment 2 of course 3 where we implemented semi-gradient TD for prediction and used a neural network as the function approximator. In this notebook, we will build a reinforcement learning agent for control, again using a neural network for function approximation. This combination of neural network function approximators and reinforcement learning algorithms, often referred to as Deep RL, is an active area of research and has led to many impressive results (e. g., AlphaGo: https://deepmind.com/research/case-studies/alphago-the-story-so-far).</p>
<p><strong>In this assignment, you will:</strong> 1. Extend the neural network code from assignment 2 of course 3 to output action-values instead of state-values. 2. Write up the Adam algorithm for neural network optimization. 3. Understand experience replay buffers. 4. Implement Softmax action-selection. 5. Build an Expected Sarsa agent by putting all the pieces together. 6. Solve Lunar Lander with your agent.</p>
<h2 id="packages">Packages</h2>
<ul>
<li><a href="www.numpy.org">numpy</a> : Fundamental package for scientific computing with Python.</li>
<li><a target="_blank" rel="noopener" href="http://matplotlib.org">matplotlib</a> : Library for plotting graphs in Python.</li>
<li><a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/v10/tanner09a.html">RL-Glue</a>, BaseEnvironment, BaseAgent : Library and abstract classes to inherit from for reinforcement learning experiments.</li>
<li><a target="_blank" rel="noopener" href="https://gym.openai.com/envs/LunarLander-v2/">LunarLanderEnvironment</a> : An RLGlue environment that wraps a LundarLander environment implementation from OpenAI Gym.</li>
<li><a target="_blank" rel="noopener" href="https://docs.python.org/3/library/collections.html#collections.deque">collections.deque</a>: a double-ended queue implementation. We use deque to implement the experience replay buffer.</li>
<li><a target="_blank" rel="noopener" href="https://docs.python.org/3/library/copy.html#copy.deepcopy">copy.deepcopy</a>: As objects are not passed by value in python, we often need to make copies of mutable objects. copy.deepcopy allows us to make a new object with the same contents as another object. (Take a look at this link if you are interested to learn more: https://robertheaton.com/2014/02/09/pythons-pass-by-object-reference-as-explained-by-philip-k-dick/)</li>
<li><a target="_blank" rel="noopener" href="https://github.com/tqdm/tqdm">tqdm</a> : A package to display progress bar when running experiments</li>
<li><a target="_blank" rel="noopener" href="https://docs.python.org/3/library/os.html">os</a>: Package used to interface with the operating system. Here we use it for creating a results folder when it does not exist.</li>
<li><a target="_blank" rel="noopener" href="https://docs.python.org/3/library/shutil.html">shutil</a>: Package used to operate on files and folders. Here we use it for creating a zip file of the results folder.</li>
<li>plot_script: Used for plotting learning curves using matplotlib.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="comment"># DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> environment <span class="keyword">import</span> BaseEnvironment</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> lunar_lander <span class="keyword">import</span> LunarLanderEnvironment</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> plot_script <span class="keyword">import</span> plot_result</span><br></pre></td></tr></table></figure>
<h2 id="section-1-action-value-network">Section 1: Action-Value Network</h2>
<p>This section includes the function approximator that we use in our agent, a neural network. In Course 3 Assignment 2, we used a neural network as the function approximator for a policy evaluation problem. In this assignment, we will use a neural network for approximating the action-value function in a control problem. The main difference between approximating a state-value function and an action-value function using a neural network is that in the former the output layer only includes one unit whereas in the latter the output layer includes as many units as the number of actions.</p>
<p>In the cell below, you will specify the architecture of the action-value neural network. More specifically, you will specify <code>self.layer_size</code> in the <code>__init__()</code> function.</p>
<p>We have already provided <code>get_action_values()</code> and <code>get_TD_update()</code> methods. The former computes the action-value function by doing a forward pass and the latter computes the gradient of the action-value function with respect to the weights times the TD error. These <code>get_action_values()</code> and <code>get_TD_update()</code> methods are similar to the <code>get_value()</code> and <code>get_gradient()</code> methods that you implemented in Course 3 Assignment 2. The main difference is that in this notebook, they are designed to be applied to batches of states instead of one state. You will later use these functions for implementing the agent.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Work Required: Yes. Fill in the code for layer_sizes in __init__ (~1 Line). </span></span><br><span class="line"><span class="comment"># Also go through the rest of the code to ensure your understanding is correct.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActionValueNetwork</span>:</span></span><br><span class="line">    <span class="comment"># Work Required: Yes. Fill in the layer_sizes member variable (~1 Line).</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, network_config</span>):</span></span><br><span class="line">        self.state_dim = network_config.get(<span class="string">&quot;state_dim&quot;</span>)</span><br><span class="line">        self.num_hidden_units = network_config.get(<span class="string">&quot;num_hidden_units&quot;</span>)</span><br><span class="line">        self.num_actions = network_config.get(<span class="string">&quot;num_actions&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        self.rand_generator = np.random.RandomState(network_config.get(<span class="string">&quot;seed&quot;</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Specify self.layer_size which shows the number of nodes in each layer</span></span><br><span class="line">        <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">        self.layer_sizes = [self.state_dim,self.num_hidden_units, self.num_actions]</span><br><span class="line">        <span class="comment">### END CODE HERE</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize the weights of the neural network</span></span><br><span class="line">        <span class="comment"># self.weights is an array of dictionaries with each dictionary corresponding to </span></span><br><span class="line">        <span class="comment"># the weights from one layer to the next. Each dictionary includes W and b</span></span><br><span class="line">        self.weights = [<span class="built_in">dict</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(self.layer_sizes) - <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(self.layer_sizes) - <span class="number">1</span>):</span><br><span class="line">            self.weights[i][<span class="string">&#x27;W&#x27;</span>] = self.init_saxe(self.layer_sizes[i], self.layer_sizes[i + <span class="number">1</span>])</span><br><span class="line">            self.weights[i][<span class="string">&#x27;b&#x27;</span>] = np.zeros((<span class="number">1</span>, self.layer_sizes[i + <span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Work Required: No.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_action_values</span>(<span class="params">self, s</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            s (Numpy array): The state.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The action-values (Numpy array) calculated using the network&#x27;s weights.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        W0, b0 = self.weights[<span class="number">0</span>][<span class="string">&#x27;W&#x27;</span>], self.weights[<span class="number">0</span>][<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">        psi = np.dot(s, W0) + b0</span><br><span class="line">        x = np.maximum(psi, <span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        W1, b1 = self.weights[<span class="number">1</span>][<span class="string">&#x27;W&#x27;</span>], self.weights[<span class="number">1</span>][<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">        q_vals = np.dot(x, W1) + b1</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> q_vals</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Work Required: No.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_TD_update</span>(<span class="params">self, s, delta_mat</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            s (Numpy array): The state.</span></span><br><span class="line"><span class="string">            delta_mat (Numpy array): A 2D array of shape (batch_size, num_actions). Each row of delta_mat  </span></span><br><span class="line"><span class="string">            correspond to one state in the batch. Each row has only one non-zero element </span></span><br><span class="line"><span class="string">            which is the TD-error corresponding to the action taken.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The TD update (Array of dictionaries with gradient times TD errors) for the network&#x27;s weights</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        W0, b0 = self.weights[<span class="number">0</span>][<span class="string">&#x27;W&#x27;</span>], self.weights[<span class="number">0</span>][<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">        W1, b1 = self.weights[<span class="number">1</span>][<span class="string">&#x27;W&#x27;</span>], self.weights[<span class="number">1</span>][<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">        </span><br><span class="line">        psi = np.dot(s, W0) + b0</span><br><span class="line">        x = np.maximum(psi, <span class="number">0</span>)</span><br><span class="line">        dx = (psi &gt; <span class="number">0</span>).astype(<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># td_update has the same structure as self.weights, that is an array of dictionaries.</span></span><br><span class="line">        <span class="comment"># td_update[0][&quot;W&quot;], td_update[0][&quot;b&quot;], td_update[1][&quot;W&quot;], and td_update[1][&quot;b&quot;] have the same shape as </span></span><br><span class="line">        <span class="comment"># self.weights[0][&quot;W&quot;], self.weights[0][&quot;b&quot;], self.weights[1][&quot;W&quot;], and self.weights[1][&quot;b&quot;] respectively</span></span><br><span class="line">        td_update = [<span class="built_in">dict</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.weights))]</span><br><span class="line">         </span><br><span class="line">        v = delta_mat</span><br><span class="line">        td_update[<span class="number">1</span>][<span class="string">&#x27;W&#x27;</span>] = np.dot(x.T, v) * <span class="number">1.</span> / s.shape[<span class="number">0</span>]</span><br><span class="line">        td_update[<span class="number">1</span>][<span class="string">&#x27;b&#x27;</span>] = np.<span class="built_in">sum</span>(v, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>) * <span class="number">1.</span> / s.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        v = np.dot(v, W1.T) * dx</span><br><span class="line">        td_update[<span class="number">0</span>][<span class="string">&#x27;W&#x27;</span>] = np.dot(s.T, v) * <span class="number">1.</span> / s.shape[<span class="number">0</span>]</span><br><span class="line">        td_update[<span class="number">0</span>][<span class="string">&#x27;b&#x27;</span>] = np.<span class="built_in">sum</span>(v, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>) * <span class="number">1.</span> / s.shape[<span class="number">0</span>]</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> td_update</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Work Required: No. You may wish to read the relevant paper for more information on this weight initialization</span></span><br><span class="line">    <span class="comment"># (Exact solutions to the nonlinear dynamics of learning in deep linear neural networks by Saxe, A et al., 2013)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_saxe</span>(<span class="params">self, rows, cols</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            rows (int): number of input units for layer.</span></span><br><span class="line"><span class="string">            cols (int): number of output units for layer.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            NumPy Array consisting of weights for the layer based on the initialization in Saxe et al.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        tensor = self.rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (rows, cols))</span><br><span class="line">        <span class="keyword">if</span> rows &lt; cols:</span><br><span class="line">            tensor = tensor.T</span><br><span class="line">        tensor, r = np.linalg.qr(tensor)</span><br><span class="line">        d = np.diag(r, <span class="number">0</span>)</span><br><span class="line">        ph = np.sign(d)</span><br><span class="line">        tensor *= ph</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rows &lt; cols:</span><br><span class="line">            tensor = tensor.T</span><br><span class="line">        <span class="keyword">return</span> tensor</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Work Required: No.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Returns: </span></span><br><span class="line"><span class="string">            A copy of the current weights of this network.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> deepcopy(self.weights)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Work Required: No.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_weights</span>(<span class="params">self, weights</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args: </span></span><br><span class="line"><span class="string">            weights (list of dictionaries): Consists of weights that this network will set as its own weights.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.weights = deepcopy(weights)</span><br></pre></td></tr></table></figure>
<p>Run the cell below to test your implementation of the <code>__init__()</code> function for ActionValueNetwork:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for ActionValueNetwork __init__() ## </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Additional tests are used in the autograder, so it is recommended </span></span><br><span class="line"><span class="comment"># to test your implementations more carefully for correctness.</span></span><br><span class="line"></span><br><span class="line">network_config = &#123;</span><br><span class="line">    <span class="string">&quot;state_dim&quot;</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">&quot;num_hidden_units&quot;</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="string">&quot;num_actions&quot;</span>: <span class="number">3</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_network = ActionValueNetwork(network_config)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;layer_sizes:&quot;</span>, test_network.layer_sizes)</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_network.layer_sizes, np.array([<span class="number">5</span>, <span class="number">20</span>, <span class="number">3</span>])))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>layer_sizes: [5, 20, 3]
Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>layer_sizes: [ 5 20  3]</code></pre>
<h2 id="section-2-adam-optimizer">Section 2: Adam Optimizer</h2>
<p>In this assignment, you will use the Adam algorithm for updating the weights of your action-value network. As you may remember from Course 3 Assignment 2, the Adam algorithm is a more advanced variant of stochastic gradient descent (SGD). The Adam algorithm improves the SGD update with two concepts: adaptive vector stepsizes and momentum. It keeps running estimates of the mean and second moment of the updates, denoted by <span class="math inline">\(\mathbf{m}\)</span> and <span class="math inline">\(\mathbf{v}\)</span> respectively: <span class="math display">\[\mathbf{m_t} = \beta_m \mathbf{m_{t-1}} + (1 - \beta_m)g_t \\
\mathbf{v_t} = \beta_v \mathbf{v_{t-1}} + (1 - \beta_v)g^2_t
\]</span></p>
<p>Here, <span class="math inline">\(\beta_m\)</span> and <span class="math inline">\(\beta_v\)</span> are fixed parameters controlling the linear combinations above and <span class="math inline">\(g_t\)</span> is the update at time <span class="math inline">\(t\)</span> (generally the gradients, but here the TD error times the gradients).</p>
<p>Given that <span class="math inline">\(\mathbf{m}\)</span> and <span class="math inline">\(\mathbf{v}\)</span> are initialized to zero, they are biased toward zero. To get unbiased estimates of the mean and second moment, Adam defines <span class="math inline">\(\mathbf{\hat{m}}\)</span> and <span class="math inline">\(\mathbf{\hat{v}}\)</span> as: <span class="math display">\[ \mathbf{\hat{m}_t} = \frac{\mathbf{m_t}}{1 - \beta_m^t} \\
\mathbf{\hat{v}_t} = \frac{\mathbf{v_t}}{1 - \beta_v^t}
\]</span></p>
<p>The weights are then updated as follows: <span class="math display">\[ \mathbf{w_t} = \mathbf{w_{t-1}} + \frac{\alpha}{\sqrt{\mathbf{\hat{v}_t}}+\epsilon} \mathbf{\hat{m}_t}
\]</span></p>
<p>Here, <span class="math inline">\(\alpha\)</span> is the step size parameter and <span class="math inline">\(\epsilon\)</span> is another small parameter to keep the denominator from being zero.</p>
<p>In the cell below, you will implement the <code>__init__()</code> and <code>update_weights()</code> methods for the Adam algorithm. In <code>__init__()</code>, you will initialize <code>self.m</code> and <code>self.v</code>. In <code>update_weights()</code>, you will compute new weights given the input weights and an update <span class="math inline">\(g\)</span> (here <code>td_errors_times_gradients</code>) according to the equations above.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Work Required: Yes. Fill in code in __init__ and update_weights (~9-11 Lines).</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adam</span>():</span></span><br><span class="line">    <span class="comment"># Work Required: Yes. Fill in the initialization for self.m and self.v (~4 Lines).</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer_sizes, </span></span></span><br><span class="line"><span class="params"><span class="function">                 optimizer_info</span>):</span></span><br><span class="line">        self.layer_sizes = layer_sizes</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Specify Adam algorithm&#x27;s hyper parameters</span></span><br><span class="line">        self.step_size = optimizer_info.get(<span class="string">&quot;step_size&quot;</span>)</span><br><span class="line">        self.beta_m = optimizer_info.get(<span class="string">&quot;beta_m&quot;</span>)</span><br><span class="line">        self.beta_v = optimizer_info.get(<span class="string">&quot;beta_v&quot;</span>)</span><br><span class="line">        self.epsilon = optimizer_info.get(<span class="string">&quot;epsilon&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize Adam algorithm&#x27;s m and v</span></span><br><span class="line">        self.m = [<span class="built_in">dict</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(self.layer_sizes))]</span><br><span class="line">        self.v = [<span class="built_in">dict</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(self.layer_sizes))]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(self.layer_sizes) - <span class="number">1</span>):</span><br><span class="line">            <span class="comment">### START CODE HERE (~4 Lines)</span></span><br><span class="line">            <span class="comment"># Hint: The initialization for m and v should look very much like the initializations of the weights</span></span><br><span class="line">            <span class="comment"># except for the fact that initialization here is to zeroes (see description above.)</span></span><br><span class="line">            self.m[i][<span class="string">&quot;W&quot;</span>] = np.zeros((self.layer_sizes[i],self.layer_sizes[i+<span class="number">1</span>]))</span><br><span class="line">            self.m[i][<span class="string">&quot;b&quot;</span>] = np.zeros((<span class="number">1</span>,self.layer_sizes[i+<span class="number">1</span>]))</span><br><span class="line">            self.v[i][<span class="string">&quot;W&quot;</span>] = np.zeros((self.layer_sizes[i],self.layer_sizes[i+<span class="number">1</span>]))</span><br><span class="line">            self.v[i][<span class="string">&quot;b&quot;</span>] = np.zeros((<span class="number">1</span>,self.layer_sizes[i+<span class="number">1</span>]))</span><br><span class="line">            <span class="comment">### END CODE HERE</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Notice that to calculate m_hat and v_hat, we use powers of beta_m and beta_v to </span></span><br><span class="line">        <span class="comment"># the time step t. We can calculate these powers using an incremental product. At initialization then, </span></span><br><span class="line">        <span class="comment"># beta_m_product and beta_v_product should be ...? (Note that timesteps start at 1 and if we were to </span></span><br><span class="line">        <span class="comment"># start from 0, the denominator would be 0.)</span></span><br><span class="line">        self.beta_m_product = self.beta_m</span><br><span class="line">        self.beta_v_product = self.beta_v</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Work Required: Yes. Fill in the weight updates (~5-7 lines).</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_weights</span>(<span class="params">self, weights, td_errors_times_gradients</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            weights (Array of dictionaries): The weights of the neural network.</span></span><br><span class="line"><span class="string">            td_errors_times_gradients (Array of dictionaries): The gradient of the </span></span><br><span class="line"><span class="string">            action-values with respect to the network&#x27;s weights times the TD-error</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The updated weights (Array of dictionaries).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(weights)):</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> weights[i].keys():</span><br><span class="line">                <span class="comment">### START CODE HERE (~5-7 Lines)</span></span><br><span class="line">                <span class="comment"># Hint: Follow the equations above. First, you should update m and v and then compute </span></span><br><span class="line">                <span class="comment"># m_hat and v_hat. Finally, compute how much the weights should be incremented by.</span></span><br><span class="line">                <span class="comment"># self.m[i][param] = None</span></span><br><span class="line">                <span class="comment"># self.v[i][param] = None</span></span><br><span class="line">                <span class="comment"># m_hat = None</span></span><br><span class="line">                <span class="comment"># v_hat = None</span></span><br><span class="line">                self.m[i][param] = self.beta_m * self.m[i][param] + (<span class="number">1</span>-self.beta_m)*td_errors_times_gradients[i][param]</span><br><span class="line">                self.v[i][param] = self.beta_v * self.v[i][param] + (<span class="number">1</span>-self.beta_v)*(td_errors_times_gradients[i][param] * td_errors_times_gradients[i][param])</span><br><span class="line">                </span><br><span class="line">                m_hat = self.m[i][param]/(<span class="number">1</span> - self.beta_m_product)</span><br><span class="line">                v_hat = self.v[i][param]/(<span class="number">1</span> - self.beta_v_product)</span><br><span class="line">                </span><br><span class="line">                weight_update = (self.step_size * m_hat) / (np.sqrt(v_hat) + self.epsilon)</span><br><span class="line">                <span class="comment">### END CODE HERE</span></span><br><span class="line">                </span><br><span class="line">                weights[i][param] = weights[i][param] + weight_update</span><br><span class="line">        <span class="comment"># Notice that to calculate m_hat and v_hat, we use powers of beta_m and beta_v to </span></span><br><span class="line">        <span class="comment">### update self.beta_m_product and self.beta_v_product</span></span><br><span class="line">        self.beta_m_product *= self.beta_m</span><br><span class="line">        self.beta_v_product *= self.beta_v</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
<p>Run the following code to test your implementation of the <code>__init__()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for Adam __init__() ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Additional tests are used in the autograder, so it is recommended </span></span><br><span class="line"><span class="comment"># to test your implementations more carefully for correctness.</span></span><br><span class="line"></span><br><span class="line">network_config = &#123;<span class="string">&quot;state_dim&quot;</span>: <span class="number">5</span>,</span><br><span class="line">                  <span class="string">&quot;num_hidden_units&quot;</span>: <span class="number">2</span>,</span><br><span class="line">                  <span class="string">&quot;num_actions&quot;</span>: <span class="number">3</span></span><br><span class="line">                 &#125;</span><br><span class="line"></span><br><span class="line">optimizer_info = &#123;<span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line">                  <span class="string">&quot;beta_m&quot;</span>: <span class="number">0.99</span>,</span><br><span class="line">                  <span class="string">&quot;beta_v&quot;</span>: <span class="number">0.999</span>,</span><br><span class="line">                  <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.0001</span></span><br><span class="line">                 &#125;</span><br><span class="line"></span><br><span class="line">network = ActionValueNetwork(network_config)</span><br><span class="line">test_adam = Adam(network.layer_sizes, optimizer_info)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;m[0][\&quot;W\&quot;] shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_adam.m[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>].shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;m[0][\&quot;b\&quot;] shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_adam.m[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>].shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;m[1][\&quot;W\&quot;] shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_adam.m[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>].shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;m[1][\&quot;b\&quot;] shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_adam.m[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>].shape), <span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_adam.m[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>].shape, np.array([<span class="number">5</span>, <span class="number">2</span>])))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_adam.m[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>].shape, np.array([<span class="number">1</span>, <span class="number">2</span>])))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_adam.m[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>].shape, np.array([<span class="number">2</span>, <span class="number">3</span>])))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_adam.m[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>].shape, np.array([<span class="number">1</span>, <span class="number">3</span>])))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[0][\&quot;W\&quot;] shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_adam.v[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>].shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[0][\&quot;b\&quot;] shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_adam.v[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>].shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[1][\&quot;W\&quot;] shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_adam.v[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>].shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[1][\&quot;b\&quot;] shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_adam.v[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>].shape), <span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_adam.v[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>].shape, np.array([<span class="number">5</span>, <span class="number">2</span>])))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_adam.v[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>].shape, np.array([<span class="number">1</span>, <span class="number">2</span>])))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_adam.v[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>].shape, np.array([<span class="number">2</span>, <span class="number">3</span>])))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_adam.v[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>].shape, np.array([<span class="number">1</span>, <span class="number">3</span>])))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.<span class="built_in">all</span>(test_adam.m[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>]==<span class="number">0</span>))</span><br><span class="line"><span class="keyword">assert</span>(np.<span class="built_in">all</span>(test_adam.m[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>]==<span class="number">0</span>))</span><br><span class="line"><span class="keyword">assert</span>(np.<span class="built_in">all</span>(test_adam.m[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>]==<span class="number">0</span>))</span><br><span class="line"><span class="keyword">assert</span>(np.<span class="built_in">all</span>(test_adam.m[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>]==<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.<span class="built_in">all</span>(test_adam.v[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>]==<span class="number">0</span>))</span><br><span class="line"><span class="keyword">assert</span>(np.<span class="built_in">all</span>(test_adam.v[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>]==<span class="number">0</span>))</span><br><span class="line"><span class="keyword">assert</span>(np.<span class="built_in">all</span>(test_adam.v[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>]==<span class="number">0</span>))</span><br><span class="line"><span class="keyword">assert</span>(np.<span class="built_in">all</span>(test_adam.v[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>]==<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>m[0][&quot;W&quot;] shape: (5, 2)
m[0][&quot;b&quot;] shape: (1, 2)
m[1][&quot;W&quot;] shape: (2, 3)
m[1][&quot;b&quot;] shape: (1, 3) 

v[0][&quot;W&quot;] shape: (5, 2)
v[0][&quot;b&quot;] shape: (1, 2)
v[1][&quot;W&quot;] shape: (2, 3)
v[1][&quot;b&quot;] shape: (1, 3) 

Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>m[0][&quot;W&quot;] shape: (5, 2)
m[0][&quot;b&quot;] shape: (1, 2)
m[1][&quot;W&quot;] shape: (2, 3)
m[1][&quot;b&quot;] shape: (1, 3) 

v[0][&quot;W&quot;] shape: (5, 2)
v[0][&quot;b&quot;] shape: (1, 2)
v[1][&quot;W&quot;] shape: (2, 3)
v[1][&quot;b&quot;] shape: (1, 3) </code></pre>
<p>Run the following code to test your implementation of the <code>update_weights()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for Adam update_weights() ##</span></span><br><span class="line"></span><br><span class="line">network_config = &#123;<span class="string">&quot;state_dim&quot;</span>: <span class="number">5</span>,</span><br><span class="line">                  <span class="string">&quot;num_hidden_units&quot;</span>: <span class="number">2</span>,</span><br><span class="line">                  <span class="string">&quot;num_actions&quot;</span>: <span class="number">3</span></span><br><span class="line">                 &#125;</span><br><span class="line"></span><br><span class="line">optimizer_info = &#123;<span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line">                  <span class="string">&quot;beta_m&quot;</span>: <span class="number">0.99</span>,</span><br><span class="line">                  <span class="string">&quot;beta_v&quot;</span>: <span class="number">0.999</span>,</span><br><span class="line">                  <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.0001</span></span><br><span class="line">                 &#125;</span><br><span class="line"></span><br><span class="line">network = ActionValueNetwork(network_config)</span><br><span class="line">test_adam = Adam(network.layer_sizes, optimizer_info)</span><br><span class="line"></span><br><span class="line">rand_generator = np.random.RandomState(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize m and v</span></span><br><span class="line">test_adam.m[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line">test_adam.m[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">test_adam.m[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">test_adam.m[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">test_adam.v[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>] = np.<span class="built_in">abs</span>(rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">5</span>, <span class="number">2</span>)))</span><br><span class="line">test_adam.v[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>] = np.<span class="built_in">abs</span>(rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">2</span>)))</span><br><span class="line">test_adam.v[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>] = np.<span class="built_in">abs</span>(rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>)))</span><br><span class="line">test_adam.v[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>] = np.<span class="built_in">abs</span>(rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">3</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify weights</span></span><br><span class="line">weights = [<span class="built_in">dict</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(test_adam.layer_sizes))]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify g</span></span><br><span class="line">g = [<span class="built_in">dict</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(test_adam.layer_sizes))]</span><br><span class="line">g[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">5</span>, <span class="number">2</span>))</span><br><span class="line">g[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">g[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">g[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>] = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Update weights</span></span><br><span class="line">updated_weights = test_adam.update_weights(weights, g)</span><br><span class="line"></span><br><span class="line"><span class="comment"># updated weights asserts</span></span><br><span class="line">updated_weights_answer = np.load(<span class="string">&quot;asserts/update_weights.npz&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;updated_weights[0][\&quot;W\&quot;]\n&quot;</span>, updated_weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>], <span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;updated_weights[0][\&quot;b\&quot;]\n&quot;</span>, updated_weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>], <span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;updated_weights[1][\&quot;W\&quot;]\n&quot;</span>, updated_weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>], <span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;updated_weights[1][\&quot;b\&quot;]\n&quot;</span>, updated_weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>], <span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>], updated_weights_answer[<span class="string">&quot;W0&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>], updated_weights_answer[<span class="string">&quot;b0&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>], updated_weights_answer[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>], updated_weights_answer[<span class="string">&quot;b1&quot;</span>]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>updated_weights[0][&quot;W&quot;]
 [[-1.03112528  2.08618453]
 [-0.15531623  0.02412129]
 [-0.76656476 -0.65405898]
 [-0.92569612 -0.24916335]
 [-0.92180119  0.72137957]] 

updated_weights[0][&quot;b&quot;]
 [[-0.44392532 -0.69588495]] 

updated_weights[1][&quot;W&quot;]
 [[ 0.13962892  0.48820826  0.41311548]
 [ 0.3958054  -0.20738072 -0.47172585]] 

updated_weights[1][&quot;b&quot;]
 [[-0.48917533 -0.61934122 -1.48771198]] 

Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>updated_weights[0][&quot;W&quot;]
 [[-1.03112528  2.08618453]
 [-0.15531623  0.02412129]
 [-0.76656476 -0.65405898]
 [-0.92569612 -0.24916335]
 [-0.92180119  0.72137957]] 

updated_weights[0][&quot;b&quot;]
 [[-0.44392532 -0.69588495]] 

updated_weights[1][&quot;W&quot;]
 [[ 0.13962892  0.48820826  0.41311548]
 [ 0.3958054  -0.20738072 -0.47172585]] 

updated_weights[1][&quot;b&quot;]
 [[-0.48917533 -0.61934122 -1.48771198]] </code></pre>
<h2 id="section-3-experience-replay-buffers">Section 3: Experience Replay Buffers</h2>
<p>In Course 3, you implemented agents that update value functions once for each sample. We can use a more efficient approach for updating value functions. You have seen an example of an efficient approach in Course 2 when implementing Dyna. The idea behind Dyna is to learn a model using sampled experience, obtain simulated experience from the model, and improve the value function using the simulated experience.</p>
<p>Experience replay is a simple method that can get some of the advantages of Dyna by saving a buffer of experience and using the data stored in the buffer as a model. This view of prior data as a model works because the data represents actual transitions from the underlying MDP. Furthermore, as a side note, this kind of model that is not learned and simply a collection of experience can be called non-parametric as it can be ever-growing as opposed to a parametric model where the transitions are learned to be represented with a fixed set of parameters or weights.</p>
<p>We have provided the implementation of the experience replay buffer in the cell below. ReplayBuffer includes two main functions: <code>append()</code> and <code>sample()</code>. <code>append()</code> adds an experience transition to the buffer as an array that includes the state, action, reward, terminal flag (indicating termination of the episode), and next_state. <code>sample()</code> gets a batch of experiences from the buffer with size <code>minibatch_size</code>.</p>
<p>You will use the <code>append()</code> and <code>sample()</code> functions when implementing the agent.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell! </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No. However, do go through the code to ensure your understanding is correct.</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, minibatch_size, seed</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            size (integer): The size of the replay buffer.              </span></span><br><span class="line"><span class="string">            minibatch_size (integer): The sample size.</span></span><br><span class="line"><span class="string">            seed (integer): The seed for the random number generator. </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.buffer = []</span><br><span class="line">        self.minibatch_size = minibatch_size</span><br><span class="line">        self.rand_generator = np.random.RandomState(seed)</span><br><span class="line">        self.max_size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">append</span>(<span class="params">self, state, action, reward, terminal, next_state</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            state (Numpy array): The state.              </span></span><br><span class="line"><span class="string">            action (integer): The action.</span></span><br><span class="line"><span class="string">            reward (float): The reward.</span></span><br><span class="line"><span class="string">            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.</span></span><br><span class="line"><span class="string">            next_state (Numpy array): The next state.           </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.buffer) == self.max_size:</span><br><span class="line">            <span class="keyword">del</span> self.buffer[<span class="number">0</span>]</span><br><span class="line">        self.buffer.append([state, action, reward, terminal, next_state])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            A list of transition tuples including state, action, reward, terinal, and next_state</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        idxs = self.rand_generator.choice(np.arange(<span class="built_in">len</span>(self.buffer)), size=self.minibatch_size)</span><br><span class="line">        <span class="keyword">return</span> [self.buffer[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> idxs]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br></pre></td></tr></table></figure>
<h2 id="section-4-softmax-policy">Section 4: Softmax Policy</h2>
<p>In this assignment, you will use a softmax policy. One advantage of a softmax policy is that it explores according to the action-values, meaning that an action with a moderate value has a higher chance of getting selected compared to an action with a lower value. Contrast this with an <span class="math inline">\(\epsilon\)</span>-greedy policy which does not consider the individual action values when choosing an exploratory action in a state and instead chooses randomly when doing so.</p>
<p>The probability of selecting each action according to the softmax policy is shown below: <span class="math display">\[Pr{(A_t=a | S_t=s)} \hspace{0.1cm} \dot{=} \hspace{0.1cm} \frac{e^{Q(s, a)/\tau}}{\sum_{b \in A}e^{Q(s, b)/\tau}}\]</span> where <span class="math inline">\(\tau\)</span> is the temperature parameter which controls how much the agent focuses on the highest valued actions. The smaller the temperature, the more the agent selects the greedy action. Conversely, when the temperature is high, the agent selects among actions more uniformly random.</p>
<p>Given that a softmax policy exponentiates action values, if those values are large, exponentiating them could get very large. To implement the softmax policy in a numerically stable way, we often subtract the maximum action-value from the action-values. If we do so, the probability of selecting each action looks as follows:</p>
<p><span class="math display">\[Pr{(A_t=a | S_t=s)} \hspace{0.1cm} \dot{=} \hspace{0.1cm} \frac{e^{Q(s, a)/\tau - max_{c}Q(s, c)/\tau}}{\sum_{b \in A}e^{Q(s, b)/\tau - max_{c}Q(s, c)/\tau}}\]</span></p>
<p>In the cell below, you will implement the <code>softmax()</code> function. In order to do so, you could break the above computation into smaller steps: - compute the preference, <span class="math inline">\(H(a)\)</span>, for taking each action by dividing the action-values by the temperature parameter <span class="math inline">\(\tau\)</span>, - subtract the maximum preference across the actions from the preferences to avoid overflow, and, - compute the probability of taking each action.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span>(<span class="params">action_values, tau=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        action_values (Numpy array): A 2D array of shape (batch_size, num_actions). </span></span><br><span class="line"><span class="string">                       The action-values computed by an action-value network.              </span></span><br><span class="line"><span class="string">        tau (float): The temperature parameter scalar.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over</span></span><br><span class="line"><span class="string">        the actions representing the policy.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (~2 Lines)</span></span><br><span class="line">    <span class="comment"># Compute the preferences by dividing the action-values by the temperature parameter tau</span></span><br><span class="line">    preferences = action_values / tau</span><br><span class="line">    <span class="comment"># Compute the maximum preference across the actions</span></span><br><span class="line">    max_preference = np.<span class="built_in">max</span>(preferences,axis = <span class="number">1</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Reshape max_preference array which has shape [Batch,] to [Batch, 1]. This allows NumPy broadcasting </span></span><br><span class="line">    <span class="comment"># when subtracting the maximum preference from the preference of each action.</span></span><br><span class="line">    reshaped_max_preference = max_preference.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~2 Lines)</span></span><br><span class="line">    <span class="comment"># Compute the numerator, i.e., the exponential of the preference - the max preference.</span></span><br><span class="line">    exp_preferences = np.exp(preferences - reshaped_max_preference)</span><br><span class="line">    <span class="comment"># Compute the denominator, i.e., the sum over the numerator along the actions axis.</span></span><br><span class="line">    sum_of_exp_preferences = np.<span class="built_in">sum</span>(exp_preferences,axis = <span class="number">1</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Reshape sum_of_exp_preferences array which has shape [Batch,] to [Batch, 1] to  allow for NumPy broadcasting </span></span><br><span class="line">    <span class="comment"># when dividing the numerator by the denominator.</span></span><br><span class="line">    reshaped_sum_of_exp_preferences = sum_of_exp_preferences.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~1 Lines)</span></span><br><span class="line">    <span class="comment"># Compute the action probabilities according to the equation in the previous cell.</span></span><br><span class="line">    action_probs = exp_preferences / reshaped_sum_of_exp_preferences</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># squeeze() removes any singleton dimensions. It is used here because this function is used in the </span></span><br><span class="line">    <span class="comment"># agent policy when selecting an action (for which the batch dimension is 1.) As np.random.choice is used in </span></span><br><span class="line">    <span class="comment"># the agent policy and it expects 1D arrays, we need to remove this singleton batch dimension.</span></span><br><span class="line">    action_probs = action_probs.squeeze()</span><br><span class="line">    <span class="keyword">return</span> action_probs</span><br></pre></td></tr></table></figure>
<p>Run the cell below to test your implementation of the <code>softmax()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for softmax() ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Additional tests are used in the autograder, so it is recommended </span></span><br><span class="line"><span class="comment"># to test your implementations more carefully for correctness.</span></span><br><span class="line"></span><br><span class="line">rand_generator = np.random.RandomState(<span class="number">0</span>)</span><br><span class="line">action_values = rand_generator.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">tau = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">action_probs = softmax(action_values, tau)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;action_probs&quot;</span>, action_probs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(action_probs, np.array([</span><br><span class="line">    [<span class="number">0.25849645</span>, <span class="number">0.01689625</span>, <span class="number">0.05374514</span>, <span class="number">0.67086216</span>],</span><br><span class="line">    [<span class="number">0.84699852</span>, <span class="number">0.00286345</span>, <span class="number">0.13520063</span>, <span class="number">0.01493741</span>]</span><br><span class="line">])))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>action_probs [[0.25849645 0.01689625 0.05374514 0.67086216]
 [0.84699852 0.00286345 0.13520063 0.01493741]]
Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)</code></pre>
<p><strong>Expected output:</strong></p>
<pre><code>action_probs [[0.25849645 0.01689625 0.05374514 0.67086216]
 [0.84699852 0.00286345 0.13520063 0.01493741]]</code></pre>
<h2 id="section-5-putting-the-pieces-together">Section 5: Putting the pieces together</h2>
<p>In this section, you will combine components from the previous sections to write up an RL-Glue Agent. The main component that you will implement is the action-value network updates with experience sampled from the experience replay buffer.</p>
<p>At time <span class="math inline">\(t\)</span>, we have an action-value function represented as a neural network, say <span class="math inline">\(Q_t\)</span>. We want to update our action-value function and get a new one we can use at the next timestep. We will get this <span class="math inline">\(Q_{t+1}\)</span> using multiple replay steps that each result in an intermediate action-value function <span class="math inline">\(Q_{t+1}^{i}\)</span> where <span class="math inline">\(i\)</span> indexes which replay step we are at.</p>
<p>In each replay step, we sample a batch of experiences from the replay buffer and compute a minibatch Expected-SARSA update. Across these N replay steps, we will use the current "un-updated" action-value network at time <span class="math inline">\(t\)</span>, <span class="math inline">\(Q_t\)</span>, for computing the action-values of the next-states. This contrasts using the most recent action-values from the last replay step <span class="math inline">\(Q_{t+1}^{i}\)</span>. We make this choice to have targets that are stable across replay steps. Here is the pseudocode for performing the updates:</p>
<p><span class="math display">\[
\begin{align}
&amp; Q_t \leftarrow \text{action-value network at timestep t (current action-value network)}\\
&amp; \text{Initialize } Q_{t+1}^1 \leftarrow Q_t\\
&amp; \text{For } i \text{ in } [1, ..., N] \text{ (i.e. N} \text{  replay steps)}:\\
&amp; \hspace{1cm} s, a, r, t, s&#39;
\leftarrow \text{Sample batch of experiences from experience replay buffer} \\
&amp; \hspace{1cm} \text{Do Expected Sarsa update with } Q_t: Q_{t+1}^{i+1}(s, a) \leftarrow Q_{t+1}^{i}(s, a) + \alpha \cdot \left[r + \gamma \left(\sum_{b} \pi(b | s&#39;) Q_t(s&#39;, b)\right) - Q_{t+1}^{i}(s, a)\right]\\
&amp; \hspace{1.5cm} \text{ making sure to add the } \gamma \left(\sum_{b} \pi(b | s&#39;) Q_t(s&#39;, b)\right) \text{ for non-terminal transitions only.} \\
&amp; \text{After N replay steps, we set } Q_{t+1}^{N} \text{ as } Q_{t+1} \text{ and have a new } Q_{t+1} \text{for time step } t + 1 \text{ that we will fix in the next set of updates. }
\end{align}
\]</span></p>
<p>As you can see in the pseudocode, after sampling a batch of experiences, we do many computations. The basic idea however is that we are looking to compute a form of a TD error. In order to so, we can take the following steps: - compute the action-values for the next states using the action-value network <span class="math inline">\(Q_{t}\)</span>, - compute the policy <span class="math inline">\(\pi(b | s&#39;)\)</span> induced by the action-values <span class="math inline">\(Q_{t}\)</span> (using the softmax function you implemented before), - compute the Expected sarsa targets <span class="math inline">\(r + \gamma \left(\sum_{b} \pi(b | s&#39;) Q_t(s&#39;, b)\right)\)</span>, - compute the action-values for the current states using the latest <span class="math inline">\(Q_{t + 1}\)</span>, and, - compute the TD-errors with the Expected Sarsa targets.</p>
<p>For the third step above, you can start by computing <span class="math inline">\(\pi(b | s&#39;) Q_t(s&#39;, b)\)</span> followed by summation to get <span class="math inline">\(\hat{v}_\pi(s&#39;) = \left(\sum_{b} \pi(b | s&#39;) Q_t(s&#39;, b)\right)\)</span>. <span class="math inline">\(\hat{v}_\pi(s&#39;)\)</span> is an estimate of the value of the next state. Note for terminal next states, <span class="math inline">\(\hat{v}_\pi(s&#39;) = 0\)</span>. Finally, we add the rewards to the discount times <span class="math inline">\(\hat{v}_\pi(s&#39;)\)</span>.</p>
<p>You will implement these steps in the <code>get_td_error()</code> function below which given a batch of experiences (including states, next_states, actions, rewards, terminals), fixed action-value network (current_q), and action-value network (network), computes the TD error in the form of a 1D array of size batch_size.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Work Required: Yes. Fill in code in get_td_error (~9 Lines).</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_td_error</span>(<span class="params">states, next_states, actions, rewards, discount, terminals, network, current_q, tau</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        states (Numpy array): The batch of states with the shape (batch_size, state_dim).</span></span><br><span class="line"><span class="string">        next_states (Numpy array): The batch of next states with the shape (batch_size, state_dim).</span></span><br><span class="line"><span class="string">        actions (Numpy array): The batch of actions with the shape (batch_size,).</span></span><br><span class="line"><span class="string">        rewards (Numpy array): The batch of rewards with the shape (batch_size,).</span></span><br><span class="line"><span class="string">        discount (float): The discount factor.</span></span><br><span class="line"><span class="string">        terminals (Numpy array): The batch of terminals with the shape (batch_size,).</span></span><br><span class="line"><span class="string">        network (ActionValueNetwork): The latest state of the network that is getting replay updates.</span></span><br><span class="line"><span class="string">        current_q (ActionValueNetwork): The fixed network used for computing the targets, </span></span><br><span class="line"><span class="string">                                        and particularly, the action-values at the next-states.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The TD errors (Numpy array) for actions taken, of shape (batch_size,)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Note: Here network is the latest state of the network that is getting replay updates. In other words, </span></span><br><span class="line">    <span class="comment"># the network represents Q_&#123;t+1&#125;^&#123;i&#125; whereas current_q represents Q_t, the fixed network used for computing the </span></span><br><span class="line">    <span class="comment"># targets, and particularly, the action-values at the next-states.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute action values at next states using current_q network</span></span><br><span class="line">    <span class="comment"># Note that q_next_mat is a 2D array of shape (batch_size, num_actions)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">    q_next_mat = current_q.get_action_values(next_states)</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute policy at next state by passing the action-values in q_next_mat to softmax()</span></span><br><span class="line">    <span class="comment"># Note that probs_mat is a 2D array of shape (batch_size, num_actions)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">    probs_mat = softmax(q_next_mat,tau)</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the estimate of the next state value, v_next_vec.</span></span><br><span class="line">    <span class="comment"># Hint: sum the action-values for the next_states weighted by the policy, probs_mat. Then, multiply by</span></span><br><span class="line">    <span class="comment"># (1 - terminals) to make sure v_next_vec is zero for terminal next states.</span></span><br><span class="line">    <span class="comment"># Note that v_next_vec is a 1D array of shape (batch_size,)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~3 Lines)</span></span><br><span class="line">    v_next_vec = np.<span class="built_in">sum</span>(q_next_mat * probs_mat , axis = <span class="number">1</span>) * (<span class="number">1</span>-terminals)</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute Expected Sarsa target</span></span><br><span class="line">    <span class="comment"># Note that target_vec is a 1D array of shape (batch_size,)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">    target_vec = rewards + discount * v_next_vec</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute action values at the current states for all actions using network</span></span><br><span class="line">    <span class="comment"># Note that q_mat is a 2D array of shape (batch_size, num_actions)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">    q_mat = network.get_action_values(states)</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Batch Indices is an array from 0 to the batch size - 1. </span></span><br><span class="line">    batch_indices = np.arange(q_mat.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute q_vec by selecting q(s, a) from q_mat for taken actions</span></span><br><span class="line">    <span class="comment"># Use batch_indices as the index for the first dimension of q_mat</span></span><br><span class="line">    <span class="comment"># Note that q_vec is a 1D array of shape (batch_size)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">    q_vec = q_mat[batch_indices,actions]</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute TD errors for actions taken</span></span><br><span class="line">    <span class="comment"># Note that delta_vec is a 1D array of shape (batch_size)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">    delta_vec = target_vec - q_vec</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> delta_vec</span><br></pre></td></tr></table></figure>
<p>Run the following code to test your implementation of the <code>get_td_error()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for get_td_error() ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Additional tests are used in the autograder, so it is recommended </span></span><br><span class="line"><span class="comment"># to test your implementations more carefully for correctness.</span></span><br><span class="line"></span><br><span class="line">data = np.load(<span class="string">&quot;asserts/get_td_error_1.npz&quot;</span>, allow_pickle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">states = data[<span class="string">&quot;states&quot;</span>]</span><br><span class="line">next_states = data[<span class="string">&quot;next_states&quot;</span>]</span><br><span class="line">actions = data[<span class="string">&quot;actions&quot;</span>]</span><br><span class="line">rewards = data[<span class="string">&quot;rewards&quot;</span>]</span><br><span class="line">discount = data[<span class="string">&quot;discount&quot;</span>]</span><br><span class="line">terminals = data[<span class="string">&quot;terminals&quot;</span>]</span><br><span class="line">tau = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line">network_config = &#123;<span class="string">&quot;state_dim&quot;</span>: <span class="number">8</span>,</span><br><span class="line">                  <span class="string">&quot;num_hidden_units&quot;</span>: <span class="number">512</span>,</span><br><span class="line">                  <span class="string">&quot;num_actions&quot;</span>: <span class="number">4</span></span><br><span class="line">                  &#125;</span><br><span class="line"></span><br><span class="line">network = ActionValueNetwork(network_config)</span><br><span class="line">network.set_weights(data[<span class="string">&quot;network_weights&quot;</span>])</span><br><span class="line"></span><br><span class="line">current_q = ActionValueNetwork(network_config)</span><br><span class="line">current_q.set_weights(data[<span class="string">&quot;current_q_weights&quot;</span>])</span><br><span class="line"></span><br><span class="line">delta_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau)</span><br><span class="line">answer_delta_vec = data[<span class="string">&quot;delta_vec&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(delta_vec, answer_delta_vec))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)</code></pre>
<p>Now that you implemented the <code>get_td_error()</code> function, you can use it to implement the <code>optimize_network()</code> function. In this function, you will: - get the TD-errors vector from <code>get_td_error()</code>, - make the TD-errors into a matrix using zeroes for actions not taken in the transitions, - pass the TD-errors matrix to the <code>get_TD_update()</code> function of network to calculate the gradients times TD errors, and, - perform an ADAM optimizer step.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Work Required: Yes. Fill in code in optimize_network (~2 Lines).</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize_network</span>(<span class="params">experiences, discount, optimizer, network, current_q, tau</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        experiences (Numpy array): The batch of experiences including the states, actions, </span></span><br><span class="line"><span class="string">                                   rewards, terminals, and next_states.</span></span><br><span class="line"><span class="string">        discount (float): The discount factor.</span></span><br><span class="line"><span class="string">        network (ActionValueNetwork): The latest state of the network that is getting replay updates.</span></span><br><span class="line"><span class="string">        current_q (ActionValueNetwork): The fixed network used for computing the targets, </span></span><br><span class="line"><span class="string">                                        and particularly, the action-values at the next-states.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get states, action, rewards, terminals, and next_states from experiences</span></span><br><span class="line">    states, actions, rewards, terminals, next_states = <span class="built_in">map</span>(<span class="built_in">list</span>, <span class="built_in">zip</span>(*experiences))</span><br><span class="line">    states = np.concatenate(states)</span><br><span class="line">    next_states = np.concatenate(next_states)</span><br><span class="line">    rewards = np.array(rewards)</span><br><span class="line">    terminals = np.array(terminals)</span><br><span class="line">    batch_size = states.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute TD error using the get_td_error function</span></span><br><span class="line">    <span class="comment"># Note that q_vec is a 1D array of shape (batch_size)</span></span><br><span class="line">    delta_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Batch Indices is an array from 0 to the batch_size - 1. </span></span><br><span class="line">    batch_indices = np.arange(batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Make a td error matrix of shape (batch_size, num_actions)</span></span><br><span class="line">    <span class="comment"># delta_mat has non-zero value only for actions taken</span></span><br><span class="line">    delta_mat = np.zeros((batch_size, network.num_actions))</span><br><span class="line">    delta_mat[batch_indices, actions] = delta_vec</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pass delta_mat to compute the TD errors times the gradients of the network&#x27;s weights from back-propagation</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE</span></span><br><span class="line">    td_update = network.get_TD_update(states,delta_mat)</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Pass network.get_weights and the td_update to the optimizer to get updated weights</span></span><br><span class="line">    <span class="comment">### START CODE HERE</span></span><br><span class="line">    weights = optimizer.update_weights(network.get_weights(), td_update)</span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    network.set_weights(weights)</span><br></pre></td></tr></table></figure>
<p>Run the following code to test your implementation of the <code>optimize_network()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for optimize_network() ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Additional tests are used in the autograder, so it is recommended </span></span><br><span class="line"><span class="comment"># to test your implementations more carefully for correctness.</span></span><br><span class="line"></span><br><span class="line">input_data = np.load(<span class="string">&quot;asserts/optimize_network_input_1.npz&quot;</span>, allow_pickle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">experiences = <span class="built_in">list</span>(input_data[<span class="string">&quot;experiences&quot;</span>])</span><br><span class="line">discount = input_data[<span class="string">&quot;discount&quot;</span>]</span><br><span class="line">tau = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line">network_config = &#123;<span class="string">&quot;state_dim&quot;</span>: <span class="number">8</span>,</span><br><span class="line">                  <span class="string">&quot;num_hidden_units&quot;</span>: <span class="number">512</span>,</span><br><span class="line">                  <span class="string">&quot;num_actions&quot;</span>: <span class="number">4</span></span><br><span class="line">                  &#125;</span><br><span class="line"></span><br><span class="line">network = ActionValueNetwork(network_config)</span><br><span class="line">network.set_weights(input_data[<span class="string">&quot;network_weights&quot;</span>])</span><br><span class="line"></span><br><span class="line">current_q = ActionValueNetwork(network_config)</span><br><span class="line">current_q.set_weights(input_data[<span class="string">&quot;current_q_weights&quot;</span>])</span><br><span class="line"></span><br><span class="line">optimizer_config = &#123;<span class="string">&#x27;step_size&#x27;</span>: <span class="number">3e-5</span>, </span><br><span class="line">                    <span class="string">&#x27;beta_m&#x27;</span>: <span class="number">0.9</span>, </span><br><span class="line">                    <span class="string">&#x27;beta_v&#x27;</span>: <span class="number">0.999</span>,</span><br><span class="line">                    <span class="string">&#x27;epsilon&#x27;</span>: <span class="number">1e-8</span></span><br><span class="line">                   &#125;</span><br><span class="line">optimizer = Adam(network.layer_sizes, optimizer_config)</span><br><span class="line">optimizer.m = input_data[<span class="string">&quot;optimizer_m&quot;</span>]</span><br><span class="line">optimizer.v = input_data[<span class="string">&quot;optimizer_v&quot;</span>]</span><br><span class="line">optimizer.beta_m_product = input_data[<span class="string">&quot;optimizer_beta_m_product&quot;</span>]</span><br><span class="line">optimizer.beta_v_product = input_data[<span class="string">&quot;optimizer_beta_v_product&quot;</span>]</span><br><span class="line"></span><br><span class="line">optimize_network(experiences, discount, optimizer, network, current_q, tau)</span><br><span class="line">updated_weights = network.get_weights()</span><br><span class="line"></span><br><span class="line">output_data = np.load(<span class="string">&quot;asserts/optimize_network_output_1.npz&quot;</span>, allow_pickle=<span class="literal">True</span>)</span><br><span class="line">answer_updated_weights = output_data[<span class="string">&quot;updated_weights&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>], answer_updated_weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>], answer_updated_weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>], answer_updated_weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>], answer_updated_weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)</code></pre>
<p>Now that you implemented the <code>optimize_network()</code> function, you can implement the agent. In the cell below, you will fill the <code>agent_step()</code> and <code>agent_end()</code> functions. You should: - select an action (only in <code>agent_step()</code>), - add transitions (consisting of the state, action, reward, terminal, and next state) to the replay buffer, and, - update the weights of the neural network by doing multiple replay steps and calling the <code>optimize_network()</code> function that you implemented above.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Work Required: Yes. Fill in code in agent_step and agent_end (~7 Lines).</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span>(<span class="params">BaseAgent</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.name = <span class="string">&quot;expected_sarsa_agent&quot;</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Work Required: No.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span>(<span class="params">self, agent_config</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Set parameters needed to setup the agent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Assume agent_config dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            network_config: dictionary,</span></span><br><span class="line"><span class="string">            optimizer_config: dictionary,</span></span><br><span class="line"><span class="string">            replay_buffer_size: integer,</span></span><br><span class="line"><span class="string">            minibatch_sz: integer, </span></span><br><span class="line"><span class="string">            num_replay_updates_per_step: float</span></span><br><span class="line"><span class="string">            discount_factor: float,</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.replay_buffer = ReplayBuffer(agent_config[<span class="string">&#x27;replay_buffer_size&#x27;</span>], </span><br><span class="line">                                          agent_config[<span class="string">&#x27;minibatch_sz&#x27;</span>], agent_config.get(<span class="string">&quot;seed&quot;</span>))</span><br><span class="line">        self.network = ActionValueNetwork(agent_config[<span class="string">&#x27;network_config&#x27;</span>])</span><br><span class="line">        self.optimizer = Adam(self.network.layer_sizes, agent_config[<span class="string">&quot;optimizer_config&quot;</span>])</span><br><span class="line">        self.num_actions = agent_config[<span class="string">&#x27;network_config&#x27;</span>][<span class="string">&#x27;num_actions&#x27;</span>]</span><br><span class="line">        self.num_replay = agent_config[<span class="string">&#x27;num_replay_updates_per_step&#x27;</span>]</span><br><span class="line">        self.discount = agent_config[<span class="string">&#x27;gamma&#x27;</span>]</span><br><span class="line">        self.tau = agent_config[<span class="string">&#x27;tau&#x27;</span>]</span><br><span class="line">        </span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_config.get(<span class="string">&quot;seed&quot;</span>))</span><br><span class="line">        </span><br><span class="line">        self.last_state = <span class="literal">None</span></span><br><span class="line">        self.last_action = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        self.sum_rewards = <span class="number">0</span></span><br><span class="line">        self.episode_steps = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Work Required: No.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            state (Numpy array): the state.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            the action. </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        action_values = self.network.get_action_values(state)</span><br><span class="line">        probs_batch = softmax(action_values, self.tau)</span><br><span class="line">        action = self.rand_generator.choice(self.num_actions, p=probs_batch.squeeze())</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Work Required: No.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;The first method called when the experiment starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">                environment&#x27;s evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The first action the agent takes.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.sum_rewards = <span class="number">0</span></span><br><span class="line">        self.episode_steps = <span class="number">0</span></span><br><span class="line">        self.last_state = np.array([state])</span><br><span class="line">        self.last_action = self.policy(self.last_state)</span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Work Required: Yes. Fill in the action selection, replay-buffer update, </span></span><br><span class="line">    <span class="comment"># weights update using optimize_network, and updating last_state and last_action (~5 lines).</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, state</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">                environment&#x27;s step based, where the agent ended up after the</span></span><br><span class="line"><span class="string">                last step</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The action the agent is taking.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        self.sum_rewards += reward</span><br><span class="line">        self.episode_steps += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Make state an array of shape (1, state_dim) to add a batch dimension and</span></span><br><span class="line">        <span class="comment"># to later match the get_action_values() and get_TD_update() functions</span></span><br><span class="line">        state = np.array([state])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Select action</span></span><br><span class="line">        <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">        action = self.policy(state)</span><br><span class="line">        <span class="comment">### END CODE HERE</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Append new experience to replay buffer</span></span><br><span class="line">        <span class="comment"># Note: look at the replay_buffer append function for the order of arguments</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">        self.replay_buffer.append(self.last_state,self.last_action,reward,<span class="number">0</span>,state)</span><br><span class="line">        <span class="comment">### END CODE HERE</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform replay steps:</span></span><br><span class="line">        <span class="keyword">if</span> self.replay_buffer.size() &gt; self.replay_buffer.minibatch_size:</span><br><span class="line">            current_q = deepcopy(self.network)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_replay):</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Get sample experiences from the replay buffer</span></span><br><span class="line">                experiences = self.replay_buffer.sample()</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Call optimize_network to update the weights of the network (~1 Line)</span></span><br><span class="line">                <span class="comment">### START CODE HERE</span></span><br><span class="line">                optimize_network(experiences,self.discount,self.optimizer,self.network,current_q,self.tau)</span><br><span class="line">                <span class="comment">### END CODE HERE</span></span><br><span class="line">                </span><br><span class="line">        <span class="comment"># Update the last state and last action.</span></span><br><span class="line">        <span class="comment">### START CODE HERE (~2 Lines)</span></span><br><span class="line">        self.last_state = state</span><br><span class="line">        self.last_action = action</span><br><span class="line">        <span class="comment">### END CODE HERE</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Work Required: Yes. Fill in the replay-buffer update and</span></span><br><span class="line">    <span class="comment"># update of the weights using optimize_network (~2 lines).</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.sum_rewards += reward</span><br><span class="line">        self.episode_steps += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set terminal state to an array of zeros</span></span><br><span class="line">        state = np.zeros_like(self.last_state)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Append new experience to replay buffer</span></span><br><span class="line">        <span class="comment"># Note: look at the replay_buffer append function for the order of arguments</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">        self.replay_buffer.append(self.last_state,self.last_action,reward,<span class="number">1</span>,state)</span><br><span class="line">        <span class="comment">### END CODE HERE</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform replay steps:</span></span><br><span class="line">        <span class="keyword">if</span> self.replay_buffer.size() &gt; self.replay_buffer.minibatch_size:</span><br><span class="line">            current_q = deepcopy(self.network)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_replay):</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Get sample experiences from the replay buffer</span></span><br><span class="line">                experiences = self.replay_buffer.sample()</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Call optimize_network to update the weights of the network</span></span><br><span class="line">                <span class="comment">### START CODE HERE (~1 Line)</span></span><br><span class="line">                optimize_network(experiences,self.discount,self.optimizer,self.network,current_q,self.tau)</span><br><span class="line">                <span class="comment">### END CODE HERE</span></span><br><span class="line">                </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_message</span>(<span class="params">self, message</span>):</span></span><br><span class="line">        <span class="keyword">if</span> message == <span class="string">&quot;get_sum_reward&quot;</span>:</span><br><span class="line">            <span class="keyword">return</span> self.sum_rewards</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">&quot;Unrecognized Message!&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>Run the following code to test your implementation of the <code>agent_step()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for agent_step() ## </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Additional tests are used in the autograder, so it is recommended </span></span><br><span class="line"><span class="comment"># to test your implementations more carefully for correctness.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">             <span class="string">&#x27;network_config&#x27;</span>: &#123;</span><br><span class="line">                 <span class="string">&#x27;state_dim&#x27;</span>: <span class="number">8</span>,</span><br><span class="line">                 <span class="string">&#x27;num_hidden_units&#x27;</span>: <span class="number">256</span>,</span><br><span class="line">                 <span class="string">&#x27;num_hidden_layers&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">                 <span class="string">&#x27;num_actions&#x27;</span>: <span class="number">4</span></span><br><span class="line">             &#125;,</span><br><span class="line">             <span class="string">&#x27;optimizer_config&#x27;</span>: &#123;</span><br><span class="line">                 <span class="string">&#x27;step_size&#x27;</span>: <span class="number">3e-5</span>, </span><br><span class="line">                 <span class="string">&#x27;beta_m&#x27;</span>: <span class="number">0.9</span>, </span><br><span class="line">                 <span class="string">&#x27;beta_v&#x27;</span>: <span class="number">0.999</span>,</span><br><span class="line">                 <span class="string">&#x27;epsilon&#x27;</span>: <span class="number">1e-8</span></span><br><span class="line">             &#125;,</span><br><span class="line">             <span class="string">&#x27;replay_buffer_size&#x27;</span>: <span class="number">32</span>,</span><br><span class="line">             <span class="string">&#x27;minibatch_sz&#x27;</span>: <span class="number">32</span>,</span><br><span class="line">             <span class="string">&#x27;num_replay_updates_per_step&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">             <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0.99</span>,</span><br><span class="line">             <span class="string">&#x27;tau&#x27;</span>: <span class="number">1000.0</span>,</span><br><span class="line">             <span class="string">&#x27;seed&#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize agent</span></span><br><span class="line">agent = Agent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load agent network, optimizer, replay_buffer from the agent_input_1.npz file</span></span><br><span class="line">input_data = np.load(<span class="string">&quot;asserts/agent_input_1.npz&quot;</span>, allow_pickle=<span class="literal">True</span>)</span><br><span class="line">agent.network.set_weights(input_data[<span class="string">&quot;network_weights&quot;</span>])</span><br><span class="line">agent.optimizer.m = input_data[<span class="string">&quot;optimizer_m&quot;</span>]</span><br><span class="line">agent.optimizer.v = input_data[<span class="string">&quot;optimizer_v&quot;</span>]</span><br><span class="line">agent.optimizer.beta_m_product = input_data[<span class="string">&quot;optimizer_beta_m_product&quot;</span>]</span><br><span class="line">agent.optimizer.beta_v_product = input_data[<span class="string">&quot;optimizer_beta_v_product&quot;</span>]</span><br><span class="line">agent.replay_buffer.rand_generator.seed(<span class="built_in">int</span>(input_data[<span class="string">&quot;replay_buffer_seed&quot;</span>]))</span><br><span class="line"><span class="keyword">for</span> experience <span class="keyword">in</span> input_data[<span class="string">&quot;replay_buffer&quot;</span>]:</span><br><span class="line">    agent.replay_buffer.buffer.append(experience)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Perform agent_step multiple times</span></span><br><span class="line">last_state_array = input_data[<span class="string">&quot;last_state_array&quot;</span>]</span><br><span class="line">last_action_array = input_data[<span class="string">&quot;last_action_array&quot;</span>]</span><br><span class="line">state_array = input_data[<span class="string">&quot;state_array&quot;</span>]</span><br><span class="line">reward_array = input_data[<span class="string">&quot;reward_array&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    agent.last_state = last_state_array[i]</span><br><span class="line">    agent.last_action = last_action_array[i]</span><br><span class="line">    state = state_array[i]</span><br><span class="line">    reward = reward_array[i]</span><br><span class="line">    </span><br><span class="line">    agent.agent_step(reward, state)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Load expected values for last_state, last_action, weights, and replay_buffer </span></span><br><span class="line">    output_data = np.load(<span class="string">&quot;asserts/agent_step_output_&#123;&#125;.npz&quot;</span>.<span class="built_in">format</span>(i), allow_pickle=<span class="literal">True</span>)</span><br><span class="line">    answer_last_state = output_data[<span class="string">&quot;last_state&quot;</span>]</span><br><span class="line">    answer_last_action = output_data[<span class="string">&quot;last_action&quot;</span>]</span><br><span class="line">    answer_updated_weights = output_data[<span class="string">&quot;updated_weights&quot;</span>]</span><br><span class="line">    answer_replay_buffer = output_data[<span class="string">&quot;replay_buffer&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Asserts for last_state and last_action</span></span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(answer_last_state, agent.last_state))</span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(answer_last_action, agent.last_action))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Asserts for replay_buffer </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(answer_replay_buffer.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(answer_replay_buffer.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">assert</span>(np.allclose(np.asarray(agent.replay_buffer.buffer)[i, j], answer_replay_buffer[i, j]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Asserts for network.weights</span></span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(agent.network.weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>], answer_updated_weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>]))</span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(agent.network.weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>], answer_updated_weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>]))</span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(agent.network.weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>], answer_updated_weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>]))</span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(agent.network.weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>], answer_updated_weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)</code></pre>
<p>Run the following code to test your implementation of the <code>agent_end()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for agent_end() ## </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Additional tests are used in the autograder, so it is recommended </span></span><br><span class="line"><span class="comment"># to test your implementations more carefully for correctness.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">             <span class="string">&#x27;network_config&#x27;</span>: &#123;</span><br><span class="line">                 <span class="string">&#x27;state_dim&#x27;</span>: <span class="number">8</span>,</span><br><span class="line">                 <span class="string">&#x27;num_hidden_units&#x27;</span>: <span class="number">256</span>,</span><br><span class="line">                 <span class="string">&#x27;num_hidden_layers&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">                 <span class="string">&#x27;num_actions&#x27;</span>: <span class="number">4</span></span><br><span class="line">             &#125;,</span><br><span class="line">             <span class="string">&#x27;optimizer_config&#x27;</span>: &#123;</span><br><span class="line">                 <span class="string">&#x27;step_size&#x27;</span>: <span class="number">3e-5</span>, </span><br><span class="line">                 <span class="string">&#x27;beta_m&#x27;</span>: <span class="number">0.9</span>, </span><br><span class="line">                 <span class="string">&#x27;beta_v&#x27;</span>: <span class="number">0.999</span>,</span><br><span class="line">                 <span class="string">&#x27;epsilon&#x27;</span>: <span class="number">1e-8</span></span><br><span class="line">             &#125;,</span><br><span class="line">             <span class="string">&#x27;replay_buffer_size&#x27;</span>: <span class="number">32</span>,</span><br><span class="line">             <span class="string">&#x27;minibatch_sz&#x27;</span>: <span class="number">32</span>,</span><br><span class="line">             <span class="string">&#x27;num_replay_updates_per_step&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">             <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0.99</span>,</span><br><span class="line">             <span class="string">&#x27;tau&#x27;</span>: <span class="number">1000</span>,</span><br><span class="line">             <span class="string">&#x27;seed&#x27;</span>: <span class="number">0</span></span><br><span class="line">             &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize agent</span></span><br><span class="line">agent = Agent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load agent network, optimizer, replay_buffer from the agent_input_1.npz file</span></span><br><span class="line">input_data = np.load(<span class="string">&quot;asserts/agent_input_1.npz&quot;</span>, allow_pickle=<span class="literal">True</span>)</span><br><span class="line">agent.network.set_weights(input_data[<span class="string">&quot;network_weights&quot;</span>])</span><br><span class="line">agent.optimizer.m = input_data[<span class="string">&quot;optimizer_m&quot;</span>]</span><br><span class="line">agent.optimizer.v = input_data[<span class="string">&quot;optimizer_v&quot;</span>]</span><br><span class="line">agent.optimizer.beta_m_product = input_data[<span class="string">&quot;optimizer_beta_m_product&quot;</span>]</span><br><span class="line">agent.optimizer.beta_v_product = input_data[<span class="string">&quot;optimizer_beta_v_product&quot;</span>]</span><br><span class="line">agent.replay_buffer.rand_generator.seed(<span class="built_in">int</span>(input_data[<span class="string">&quot;replay_buffer_seed&quot;</span>]))</span><br><span class="line"><span class="keyword">for</span> experience <span class="keyword">in</span> input_data[<span class="string">&quot;replay_buffer&quot;</span>]:</span><br><span class="line">    agent.replay_buffer.buffer.append(experience)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Perform agent_step multiple times</span></span><br><span class="line">last_state_array = input_data[<span class="string">&quot;last_state_array&quot;</span>]</span><br><span class="line">last_action_array = input_data[<span class="string">&quot;last_action_array&quot;</span>]</span><br><span class="line">state_array = input_data[<span class="string">&quot;state_array&quot;</span>]</span><br><span class="line">reward_array = input_data[<span class="string">&quot;reward_array&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    agent.last_state = last_state_array[i]</span><br><span class="line">    agent.last_action = last_action_array[i]</span><br><span class="line">    reward = reward_array[i]</span><br><span class="line">    </span><br><span class="line">    agent.agent_end(reward)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load expected values for last_state, last_action, weights, and replay_buffer </span></span><br><span class="line">    output_data = np.load(<span class="string">&quot;asserts/agent_end_output_&#123;&#125;.npz&quot;</span>.<span class="built_in">format</span>(i), allow_pickle=<span class="literal">True</span>)</span><br><span class="line">    answer_updated_weights = output_data[<span class="string">&quot;updated_weights&quot;</span>]</span><br><span class="line">    answer_replay_buffer = output_data[<span class="string">&quot;replay_buffer&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Asserts for replay_buffer </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(answer_replay_buffer.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(answer_replay_buffer.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">assert</span>(np.allclose(np.asarray(agent.replay_buffer.buffer)[i, j], answer_replay_buffer[i, j]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Asserts for network.weights</span></span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(agent.network.weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>], answer_updated_weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>]))</span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(agent.network.weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>], answer_updated_weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>]))</span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(agent.network.weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>], answer_updated_weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>]))</span><br><span class="line">    <span class="keyword">assert</span>(np.allclose(agent.network.weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>], answer_updated_weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)</code></pre>
<h2 id="section-6-run-experiment">Section 6: Run Experiment</h2>
<p>Now that you implemented the agent, we can use it to run an experiment on the Lunar Lander problem. We will plot the learning curve of the agent to visualize learning progress. To plot the learning curve, we use the sum of rewards in an episode as the performance measure. We have provided for you the experiment/plot code in the cell below which you can go ahead and run. Note that running the cell below has taken approximately 10 minutes in prior testing.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span>(<span class="params">environment, agent, environment_parameters, agent_parameters, experiment_parameters</span>):</span></span><br><span class="line">    </span><br><span class="line">    rl_glue = RLGlue(environment, agent)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># save sum of reward at the end of each episode</span></span><br><span class="line">    agent_sum_reward = np.zeros((experiment_parameters[<span class="string">&quot;num_runs&quot;</span>], </span><br><span class="line">                                 experiment_parameters[<span class="string">&quot;num_episodes&quot;</span>]))</span><br><span class="line"></span><br><span class="line">    env_info = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    agent_info = agent_parameters</span><br><span class="line"></span><br><span class="line">    <span class="comment"># one agent setting</span></span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, experiment_parameters[<span class="string">&quot;num_runs&quot;</span>]+<span class="number">1</span>):</span><br><span class="line">        agent_info[<span class="string">&quot;seed&quot;</span>] = run</span><br><span class="line">        agent_info[<span class="string">&quot;network_config&quot;</span>][<span class="string">&quot;seed&quot;</span>] = run</span><br><span class="line">        env_info[<span class="string">&quot;seed&quot;</span>] = run</span><br><span class="line"></span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">1</span>, experiment_parameters[<span class="string">&quot;num_episodes&quot;</span>]+<span class="number">1</span>)):</span><br><span class="line">            <span class="comment"># run episode</span></span><br><span class="line">            rl_glue.rl_episode(experiment_parameters[<span class="string">&quot;timeout&quot;</span>])</span><br><span class="line">            </span><br><span class="line">            episode_reward = rl_glue.rl_agent_message(<span class="string">&quot;get_sum_reward&quot;</span>)</span><br><span class="line">            agent_sum_reward[run - <span class="number">1</span>, episode - <span class="number">1</span>] = episode_reward</span><br><span class="line">    save_name = <span class="string">&quot;&#123;&#125;&quot;</span>.<span class="built_in">format</span>(rl_glue.agent.name)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;results&#x27;</span>):</span><br><span class="line">        os.makedirs(<span class="string">&#x27;results&#x27;</span>)</span><br><span class="line">    np.save(<span class="string">&quot;results/sum_reward_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(save_name), agent_sum_reward)</span><br><span class="line">    shutil.make_archive(<span class="string">&#x27;results&#x27;</span>, <span class="string">&#x27;zip&#x27;</span>, <span class="string">&#x27;results&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run Experiment</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;num_runs&quot;</span> : <span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;num_episodes&quot;</span> : <span class="number">300</span>,</span><br><span class="line">    <span class="comment"># OpenAI Gym environments allow for a timestep limit timeout, causing episodes to end after </span></span><br><span class="line">    <span class="comment"># some number of timesteps. Here we use the default of 1000.</span></span><br><span class="line">    <span class="string">&quot;timeout&quot;</span> : <span class="number">1000</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123;&#125;</span><br><span class="line"></span><br><span class="line">current_env = LunarLanderEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;</span><br><span class="line">    <span class="string">&#x27;network_config&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;state_dim&#x27;</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">&#x27;num_hidden_units&#x27;</span>: <span class="number">256</span>,</span><br><span class="line">        <span class="string">&#x27;num_actions&#x27;</span>: <span class="number">4</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&#x27;optimizer_config&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;step_size&#x27;</span>: <span class="number">1e-3</span>,</span><br><span class="line">        <span class="string">&#x27;beta_m&#x27;</span>: <span class="number">0.9</span>, </span><br><span class="line">        <span class="string">&#x27;beta_v&#x27;</span>: <span class="number">0.999</span>,</span><br><span class="line">        <span class="string">&#x27;epsilon&#x27;</span>: <span class="number">1e-8</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&#x27;replay_buffer_size&#x27;</span>: <span class="number">50000</span>,</span><br><span class="line">    <span class="string">&#x27;minibatch_sz&#x27;</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">&#x27;num_replay_updates_per_step&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">&#x27;tau&#x27;</span>: <span class="number">0.001</span></span><br><span class="line">&#125;</span><br><span class="line">current_agent = Agent</span><br><span class="line"></span><br><span class="line"><span class="comment"># run experiment</span></span><br><span class="line">run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br></pre></td></tr></table></figure>
<pre><code> 51%|█████     | 152/300 [07:08&lt;09:09,  3.71s/it]</code></pre>
<p>Run the cell below to see the comparison between the agent that you implemented and a random agent for the one run and 300 episodes. Note that the <code>plot_result()</code> function smoothes the learning curve by applying a sliding window on the performance measure.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_result([<span class="string">&quot;expected_sarsa_agent&quot;</span>, <span class="string">&quot;random_agent&quot;</span>])</span><br></pre></td></tr></table></figure>
<p>In the following cell you can visualize the performance of the agent with a correct implementation. As you can see, the agent initially crashes quite quickly (Episode 0). Then, the agent learns to avoid crashing by expending fuel and staying far above the ground. Finally however, it learns to land smoothly within the landing zone demarcated by the two flags (Episode 275).</p>
<p>In the learning curve above, you can see that sum of reward over episode has quite a high-variance at the beginning. However, the performance seems to be improving. The experiment that you ran was for 300 episodes and 1 run. To understand how the agent performs in the long run, we provide below the learning curve for the agent trained for 3000 episodes with performance averaged over 30 runs. <img src="3000_episodes.png" alt="Drawing" style="width: 500px;"/> You can see that the agent learns a reasonably good policy within 3000 episodes, gaining sum of reward bigger than 200. Note that because of the high-variance in the agent performance, we also smoothed the learning curve.</p>
<h3 id="wrapping-up">Wrapping up!</h3>
<p>You have successfully implemented Course 4 Programming Assignment 2.</p>
<p>You have implemented an <strong>Expected Sarsa agent with a neural network and the Adam optimizer</strong> and used it for solving the Lunar Lander problem! You implemented different components of the agent including:</p>
<ul>
<li>a neural network for function approximation,</li>
<li>the Adam algorithm for optimizing the weights of the neural network,</li>
<li>a Softmax policy,</li>
<li>the replay steps for updating the action-value function using the experiences sampled from a replay buffer</li>
</ul>
<p>You tested the agent for a single parameter setting. In the next assignment, you will perform a parameter study on the step-size parameter to gain insight about the effect of step-size on the performance of your agent.</p>
<p>Note: Apart from using the <code>Submit</code> button in the notebook, you have to submit an additional zip file containing the 'npy' files that were generated from running the experiment cells. In order to do so: 1. Generate the zip file by running the experiment cells in the notebook. On the top of the notebook, navigate to <code>File-&gt;Open</code> to open the directory view of this assignment. Select the checkbox next to <code>results.zip</code> and click on <code>Download.</code> Alternatively, you can download the results folder and run <code>zip -jr results.zip results/</code> (The flag 'j' is required by the grader!). 2. Go to the "My submission" tab on the programming assignment and click on "+ Create submission". 3. Click on "PA2 Data-file Grader" and upload your results.zip.</p>
<p><strong><em>These account for 25% of the marks, so don't forget to do so!</em></strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Average-Reward-Softmax-Actor-Critic/2020/10/16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Average-Reward-Softmax-Actor-Critic/2020/10/16/" class="post-title-link" itemprop="url">Average Reward Softmax Actor-Critic</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-10-16 15:06:08 / Modified: 15:07:03" itemprop="dateCreated datePublished" datetime="2020-10-16T15:06:08+08:00">2020-10-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Average-Reward-Softmax-Actor-Critic/2020/10/16/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Average-Reward-Softmax-Actor-Critic/2020/10/16/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="assignment-4---average-reward-softmax-actor-critic">Assignment 4 - Average Reward Softmax Actor-Critic</h1>
<p>Welcome to your Course 3 Programming Assignment 4. In this assignment, you will implement <strong>Average Reward Softmax Actor-Critic</strong> in the Pendulum Swing-Up problem that you have seen earlier in the lecture. Through this assignment you will get hands-on experience in implementing actor-critic methods on a continuing task.</p>
<p><strong>In this assignment, you will:</strong> 1. Implement softmax actor-critic agent on a continuing task using the average reward formulation. 2. Understand how to parameterize the policy as a function to learn, in a discrete action environment. 3. Understand how to (approximately) sample the gradient of this objective to update the actor. 4. Understand how to update the critic using differential TD error.</p>
<h2 id="pendulum-swing-up-environment">Pendulum Swing-Up Environment</h2>
<p>In this assignment, we will be using a Pendulum environment, adapted from <a target="_blank" rel="noopener" href="http://www.incompleteideas.net/papers/SSR-98.pdf">Santamaría et al. (1998)</a>. This is also the same environment that we used in the lecture. The diagram below illustrates the environment.</p>
<p><img src="pendulum_env.png" alt="Drawing" style="width: 400px;"/></p>
<p>The environment consists of single pendulum that can swing 360 degrees. The pendulum is actuated by applying a torque on its pivot point. The goal is to get the pendulum to balance up-right from its resting position (hanging down at the bottom with no velocity) and maintain it as long as possible. The pendulum can move freely, subject only to gravity and the action applied by the agent.</p>
<p>The state is 2-dimensional, which consists of the current angle <span class="math inline">\(\beta \in [-\pi, \pi]\)</span> (angle from the vertical upright position) and current angular velocity <span class="math inline">\(\dot{\beta} \in (-2\pi, 2\pi)\)</span>. The angular velocity is constrained in order to avoid damaging the pendulum system. If the angular velocity reaches this limit during simulation, the pendulum is reset to the resting position. The action is the angular acceleration, with discrete values <span class="math inline">\(a \in \{-1, 0, 1\}\)</span> applied to the pendulum. For more details on environment dynamics you can refer to the original paper.</p>
<p>The goal is to swing-up the pendulum and maintain its upright angle. Hence, the reward is the negative absolute angle from the vertical position: <span class="math inline">\(R_{t} = -|\beta_{t}|\)</span></p>
<p>Furthermore, since the goal is to reach and maintain a vertical position, there are no terminations nor episodes. Thus this problem can be formulated as a continuing task.</p>
<p>Similar to the Mountain Car task, the action in this pendulum environment is not strong enough to move the pendulum directly to the desired position. The agent must learn to first move the pendulum away from its desired position and gain enough momentum to successfully swing-up the pendulum. And even after reaching the upright position the agent must learn to continually balance the pendulum in this unstable position.</p>
<h2 id="packages">Packages</h2>
<p>You will use the following packages in this assignment.</p>
<ul>
<li><a href="www.numpy.org">numpy</a> : Fundamental package for scientific computing with Python.</li>
<li><a target="_blank" rel="noopener" href="http://matplotlib.org">matplotlib</a> : Library for plotting graphs in Python.</li>
<li><a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/v10/tanner09a.html">RL-Glue</a> : Library for reinforcement learning experiments.</li>
<li><a target="_blank" rel="noopener" href="https://alexhagen.github.io/jdc/">jdc</a> : Jupyter magic that allows defining classes over multiple jupyter notebook cells.</li>
<li><a target="_blank" rel="noopener" href="https://tqdm.github.io/">tqdm</a> : A package to display progress bar when running experiments</li>
<li>plot_script : custom script to plot results</li>
<li><a target="_blank" rel="noopener" href="http://incompleteideas.net/tiles/tiles3.html">tiles3</a> : A package that implements tile-coding.</li>
<li>pendulum_env : Pendulum Swing-up Environment</li>
</ul>
<p><strong>Please do not import other libraries</strong> — this will break the autograder.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="comment"># DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> pendulum_env <span class="keyword">import</span> PendulumEnvironment</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">import</span> plot_script</span><br><span class="line"><span class="keyword">import</span> tiles3 <span class="keyword">as</span> tc</span><br></pre></td></tr></table></figure>
<h2 id="section-1-create-tile-coding-helper-function">Section 1: Create Tile Coding Helper Function</h2>
<p>In this section, we are going to build a tile coding class for our agent that will make it easier to make calls to our tile coder.</p>
<p>Tile-coding is introduced in Section 9.5.4 of the textbook as a way to create features that can both provide good generalization and discrimination. We have already used it in our last programming assignment as well.</p>
<p>Similar to the last programming assignment, we are going to make a function specific for tile coding for our Pendulum Swing-up environment. We will also use the <a target="_blank" rel="noopener" href="http://incompleteideas.net/tiles/tiles3.html">Tiles3 library</a>.</p>
<p>To get the tile coder working we need to:</p>
<pre><code>1) create an index hash table using tc.IHT(), 
2) scale the inputs for the tile coder based on number of tiles and range of values each input could take
3) call tc.tileswrap to get active tiles back.</code></pre>
<p>However, we need to make one small change to this tile coder. Note that in this environment the state space contains angle, which is between <span class="math inline">\([-\pi, \pi]\)</span>. If we tile-code this state space in the usual way, the agent may think the value of states corresponding to an angle of <span class="math inline">\(-\pi\)</span> is very different from angle of <span class="math inline">\(\pi\)</span> when in fact they are the same! To remedy this and allow generalization between angle <span class="math inline">\(= -\pi\)</span> and angle <span class="math inline">\(= \pi\)</span>, we need to use <strong>wrap tile coder</strong>.</p>
<p>The usage of wrap tile coder is almost identical to the original tile coder, except that we also need to provide the <code>wrapwidth</code> argument for the dimension we want to wrap over (hence only for angle, and <code>None</code> for angular velocity). More details of wrap tile coder is also provided in <a target="_blank" rel="noopener" href="http://incompleteideas.net/tiles/tiles3.html">Tiles3 library</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PendulumTileCoder</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, iht_size=<span class="number">4096</span>, num_tilings=<span class="number">32</span>, num_tiles=<span class="number">8</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initializes the MountainCar Tile Coder</span></span><br><span class="line"><span class="string">        Initializers:</span></span><br><span class="line"><span class="string">        iht_size -- int, the size of the index hash table, typically a power of 2</span></span><br><span class="line"><span class="string">        num_tilings -- int, the number of tilings</span></span><br><span class="line"><span class="string">        num_tiles -- int, the number of tiles. Here both the width and height of the tiles are the same</span></span><br><span class="line"><span class="string">                            </span></span><br><span class="line"><span class="string">        Class Variables:</span></span><br><span class="line"><span class="string">        self.iht -- tc.IHT, the index hash table that the tile coder will use</span></span><br><span class="line"><span class="string">        self.num_tilings -- int, the number of tilings the tile coder will use</span></span><br><span class="line"><span class="string">        self.num_tiles -- int, the number of tiles the tile coder will use</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        self.num_tilings = num_tilings</span><br><span class="line">        self.num_tiles = num_tiles </span><br><span class="line">        self.iht = tc.IHT(iht_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_tiles</span>(<span class="params">self, angle, ang_vel</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Takes in an angle and angular velocity from the pendulum environment</span></span><br><span class="line"><span class="string">        and returns a numpy array of active tiles.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">        angle -- float, the angle of the pendulum between -np.pi and np.pi</span></span><br><span class="line"><span class="string">        ang_vel -- float, the angular velocity of the agent between -2*np.pi and 2*np.pi</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        returns:</span></span><br><span class="line"><span class="string">        tiles -- np.array, active tiles</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### Use the ranges above and scale the angle and angular velocity between [0, 1]</span></span><br><span class="line">        <span class="comment"># then multiply by the number of tiles so they are scaled between [0, self.num_tiles]</span></span><br><span class="line">        </span><br><span class="line">        angle_scaled = <span class="number">0</span></span><br><span class="line">        ang_vel_scaled = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        angle_scaled = (angle + np.pi) / (np.pi + np.pi) * self.num_tiles</span><br><span class="line">        ang_vel_scaled = (ang_vel + <span class="number">2</span>*np.pi) / ( <span class="number">2</span>*np.pi + <span class="number">2</span>*np.pi) * self.num_tiles</span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get tiles by calling tc.tileswrap method</span></span><br><span class="line">        <span class="comment"># wrapwidths specify which dimension to wrap over and its wrapwidth</span></span><br><span class="line">        tiles = tc.tileswrap(self.iht, self.num_tilings, [angle_scaled, ang_vel_scaled], wrapwidths=[self.num_tiles, <span class="literal">False</span>])</span><br><span class="line">                    </span><br><span class="line">        <span class="keyword">return</span> np.array(tiles)</span><br></pre></td></tr></table></figure>
<p>Run the following code to verify <code>PendulumTilecoder</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for PendulumTileCoder ##</span></span><br><span class="line"><span class="comment"># Your tile coder should also work for other num. tilings and num. tiles</span></span><br><span class="line">angles = np.linspace(-np.pi, np.pi, num=<span class="number">5</span>)</span><br><span class="line">vels = np.linspace(-<span class="number">2</span> * np.pi, <span class="number">2</span> * np.pi, num=<span class="number">5</span>)</span><br><span class="line">test_obs = <span class="built_in">list</span>(itertools.product(angles, vels))</span><br><span class="line"></span><br><span class="line">pdtc = PendulumTileCoder(iht_size=<span class="number">4096</span>, num_tilings=<span class="number">8</span>, num_tiles=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">result=[]</span><br><span class="line"><span class="keyword">for</span> obs <span class="keyword">in</span> test_obs:</span><br><span class="line">    angle, ang_vel = obs</span><br><span class="line">    tiles = pdtc.get_tiles(angle=angle, ang_vel=ang_vel)</span><br><span class="line">    result.append(tiles)</span><br><span class="line">    </span><br><span class="line">expected = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">6</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">8</span>, <span class="number">14</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">14</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">15</span>, <span class="number">19</span>],</span><br><span class="line">    [<span class="number">20</span>, <span class="number">21</span>, <span class="number">16</span>, <span class="number">22</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">23</span>, <span class="number">19</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">26</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">8</span>, <span class="number">14</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">31</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">14</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">31</span>, <span class="number">34</span>],</span><br><span class="line">    [<span class="number">20</span>, <span class="number">21</span>, <span class="number">16</span>, <span class="number">22</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">35</span>, <span class="number">34</span>],</span><br><span class="line">    [<span class="number">36</span>, <span class="number">37</span>, <span class="number">38</span>, <span class="number">39</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>],</span><br><span class="line">    [<span class="number">36</span>, <span class="number">37</span>, <span class="number">40</span>, <span class="number">39</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">26</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">41</span>, <span class="number">42</span>, <span class="number">40</span>, <span class="number">43</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">31</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">41</span>, <span class="number">42</span>, <span class="number">44</span>, <span class="number">43</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">31</span>, <span class="number">34</span>],</span><br><span class="line">    [<span class="number">45</span>, <span class="number">46</span>, <span class="number">44</span>, <span class="number">47</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">35</span>, <span class="number">34</span>],</span><br><span class="line">    [<span class="number">36</span>, <span class="number">37</span>, <span class="number">38</span>, <span class="number">39</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">    [<span class="number">36</span>, <span class="number">37</span>, <span class="number">40</span>, <span class="number">39</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">6</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">41</span>, <span class="number">42</span>, <span class="number">40</span>, <span class="number">43</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">41</span>, <span class="number">42</span>, <span class="number">44</span>, <span class="number">43</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">15</span>, <span class="number">19</span>],</span><br><span class="line">    [<span class="number">45</span>, <span class="number">46</span>, <span class="number">44</span>, <span class="number">47</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">23</span>, <span class="number">19</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">6</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">8</span>, <span class="number">14</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">14</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">15</span>, <span class="number">19</span>],</span><br><span class="line">    [<span class="number">20</span>, <span class="number">21</span>, <span class="number">16</span>, <span class="number">22</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">23</span>, <span class="number">19</span>],</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(expected == np.array(result))</span><br></pre></td></tr></table></figure>
<h2 id="section-2-create-average-reward-softmax-actor-critic-agent">Section 2: Create Average Reward Softmax Actor-Critic Agent</h2>
<p>Now that we implemented PendulumTileCoder let's create the agent that interacts with the environment. We will implement the same average reward Actor-Critic algorithm presented in the videos.</p>
<p>This agent has two components: an Actor and a Critic. The Actor learns a parameterized policy while the Critic learns a state-value function. The environment has discrete actions; your Actor implementation will use a softmax policy with exponentiated action-preferences. The Actor learns with the sample-based estimate for the gradient of the average reward objective. The Critic learns using the average reward version of the semi-gradient TD(0) algorithm.</p>
<p>In this section, you will be implementing <code>agent_policy</code>, <code>agent_start</code>, <code>agent_step</code>, and <code>agent_end</code>.</p>
<h2 id="section-2-1-implement-helper-functions">Section 2-1: Implement Helper Functions</h2>
<p>Let's first define a couple of useful helper functions.</p>
<h2 id="section-2-1a-compute-softmax-probability">Section 2-1a: Compute Softmax Probability</h2>
<p>In this part you will implement <code>compute_softmax_prob</code>.</p>
<p>This function computes softmax probability for all actions, given actor weights <code>actor_w</code> and active tiles <code>tiles</code>. This function will be later used in <code>agent_policy</code> to sample appropriate action.</p>
<p>First, recall how the softmax policy is represented from state-action preferences: <span class="math inline">\(\large \pi(a|s, \mathbf{\theta}) \doteq \frac{e^{h(s,a,\mathbf{\theta})}}{\sum_{b}e^{h(s,b,\mathbf{\theta})}}\)</span>.</p>
<p><strong>state-action preference</strong> is defined as <span class="math inline">\(h(s,a, \mathbf{\theta}) \doteq \mathbf{\theta}^T \mathbf{x}_h(s,a)\)</span>.</p>
<p>Given active tiles <code>tiles</code> for state <code>s</code>, state-action preference <span class="math inline">\(\mathbf{\theta}^T \mathbf{x}_h(s,a)\)</span> can be computed by <code>actor_w[a][tiles].sum()</code>.</p>
<p>We will also use <strong>exp-normalize trick</strong>, in order to avoid possible numerical overflow. Consider the following:</p>
<p><span class="math inline">\(\large \pi(a|s, \mathbf{\theta}) \doteq \frac{e^{h(s,a,\mathbf{\theta})}}{\sum_{b}e^{h(s,b,\mathbf{\theta})}} = \frac{e^{h(s,a,\mathbf{\theta}) - c} e^c}{\sum_{b}e^{h(s,b,\mathbf{\theta}) - c} e^c} = \frac{e^{h(s,a,\mathbf{\theta}) - c}}{\sum_{b}e^{h(s,b,\mathbf{\theta}) - c}}\)</span></p>
<p><span class="math inline">\(\pi(\cdot|s, \mathbf{\theta})\)</span> is shift-invariant, and the policy remains the same when we subtract a constant <span class="math inline">\(c \in \mathbb{R}\)</span> from state-action preferences.</p>
<p>Normally we use <span class="math inline">\(c = \max_b h(s,b, \mathbf{\theta})\)</span>, to prevent any overflow due to exponentiating large numbers.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_softmax_prob</span>(<span class="params">actor_w, tiles</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes softmax probability for all actions</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">    actor_w - np.array, an array of actor weights</span></span><br><span class="line"><span class="string">    tiles - np.array, an array of active tiles</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    softmax_prob - np.array, an array of size equal to num. actions, and sums to 1.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First compute the list of state-action preferences (1~2 lines)</span></span><br><span class="line">    <span class="comment"># state_action_preferences = ? (list of size 3)</span></span><br><span class="line">    state_action_preferences = []</span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    state_action_preferences = actor_w[:,tiles].<span class="built_in">sum</span>(axis = <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the constant c by finding the maximum of state-action preferences (use np.max) (1 line)</span></span><br><span class="line">    <span class="comment"># c = ? (float)</span></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    c = np.<span class="built_in">max</span>(state_action_preferences)</span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the numerator by subtracting c from state-action preferences and exponentiating it (use np.exp) (1 line)</span></span><br><span class="line">    <span class="comment"># numerator = ? (list of size 3)</span></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    numerator = np.exp( state_action_preferences - c)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Next compute the denominator by summing the values in the numerator (use np.sum) (1 line)</span></span><br><span class="line">    <span class="comment"># denominator = ? (float)</span></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    denominator = np.<span class="built_in">sum</span>(numerator)</span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a probability array by dividing each element in numerator array by denominator (1 line)</span></span><br><span class="line">    <span class="comment"># We will store this probability array in self.softmax_prob as it will be useful later when updating the Actor</span></span><br><span class="line">    <span class="comment"># softmax_prob = ? (list of size 3)</span></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    softmax_prob = numerator / denominator</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> softmax_prob</span><br></pre></td></tr></table></figure>
<p>Run the following code to verify <code>compute_softmax_prob</code>.</p>
<p>We will test the method by building a softmax policy from state-action preferences [-1,1,2].</p>
<p>The sampling probability should then roughly match <span class="math inline">\([\frac{e^{-1}}{e^{-1}+e^1+e^2}, \frac{e^{1}}{e^{-1}+e^1+e^2}, \frac{e^2}{e^{-1}+e^1+e^2}] \approx\)</span> [0.0351, 0.2595, 0.7054]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set tile-coder</span></span><br><span class="line">iht_size = <span class="number">4096</span></span><br><span class="line">num_tilings = <span class="number">8</span></span><br><span class="line">num_tiles = <span class="number">8</span></span><br><span class="line">test_tc = PendulumTileCoder(iht_size=iht_size, num_tilings=num_tilings, num_tiles=num_tiles)</span><br><span class="line"></span><br><span class="line">num_actions = <span class="number">3</span></span><br><span class="line">actions = <span class="built_in">list</span>(<span class="built_in">range</span>(num_actions))</span><br><span class="line">actor_w = np.zeros((<span class="built_in">len</span>(actions), iht_size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># setting actor weights such that state-action preferences are always [-1, 1, 2]</span></span><br><span class="line">actor_w[<span class="number">0</span>] = -<span class="number">1.</span>/num_tilings</span><br><span class="line">actor_w[<span class="number">1</span>] = <span class="number">1.</span>/num_tilings</span><br><span class="line">actor_w[<span class="number">2</span>] = <span class="number">2.</span>/num_tilings</span><br><span class="line"></span><br><span class="line"><span class="comment"># obtain active_tiles from state</span></span><br><span class="line">state = [-np.pi, <span class="number">0.</span>]</span><br><span class="line">angle, ang_vel = state</span><br><span class="line">active_tiles = test_tc.get_tiles(angle, ang_vel)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute softmax probability</span></span><br><span class="line">softmax_prob = compute_softmax_prob(actor_w, active_tiles) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;softmax probability: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(softmax_prob))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(softmax_prob, [<span class="number">0.03511903</span>, <span class="number">0.25949646</span>, <span class="number">0.70538451</span>])</span><br></pre></td></tr></table></figure>
<pre><code>softmax probability: [0.03511903 0.25949646 0.70538451]</code></pre>
<h2 id="section-2-2-implement-agent-methods">Section 2-2: Implement Agent Methods</h2>
<p>Let's first define methods that initialize the agent. <code>agent_init()</code> initializes all the variables that the agent will need.</p>
<p>Now that we have implemented helper functions, let's create an agent. In this part, you will implement <code>agent_start()</code> and <code>agent_step()</code>. We do not need to implement <code>agent_end()</code> because there is no termination in our continuing task.</p>
<p><code>compute_softmax_prob()</code> is used in <code>agent_policy()</code>, which in turn will be used in <code>agent_start()</code> and <code>agent_step()</code>. We have implemented <code>agent_policy()</code> for you.</p>
<p>When performing updates to the Actor and Critic, recall their respective updates in the Actor-Critic algorithm video.</p>
<p>We approximate <span class="math inline">\(q_\pi\)</span> in the Actor update using one-step bootstrapped return(<span class="math inline">\(R_{t+1} - \bar{R} + \hat{v}(S_{t+1}, \mathbf{w})\)</span>) subtracted by current state-value(<span class="math inline">\(\hat{v}(S_{t}, \mathbf{w})\)</span>), equivalent to TD error <span class="math inline">\(\delta\)</span>.</p>
<p><span class="math inline">\(\delta_t = R_{t+1} - \bar{R} + \hat{v}(S_{t+1}, \mathbf{w}) - \hat{v}(S_{t}, \mathbf{w}) \hspace{6em} (1)\)</span></p>
<p><strong>Average Reward update rule</strong>: <span class="math inline">\(\bar{R} \leftarrow \bar{R} + \alpha^{\bar{R}}\delta \hspace{4.3em} (2)\)</span></p>
<p><strong>Critic weight update rule</strong>: <span class="math inline">\(\mathbf{w} \leftarrow \mathbf{w} + \alpha^{\mathbf{w}}\delta\nabla \hat{v}(s,\mathbf{w}) \hspace{2.5em} (3)\)</span></p>
<p><strong>Actor weight update rule</strong>: <span class="math inline">\(\mathbf{\theta} \leftarrow \mathbf{\theta} + \alpha^{\mathbf{\theta}}\delta\nabla ln \pi(A|S,\mathbf{\theta}) \hspace{1.4em} (4)\)</span></p>
<p>However, since we are using linear function approximation and parameterizing a softmax policy, the above update rule can be further simplified using:</p>
<p><span class="math inline">\(\nabla \hat{v}(s,\mathbf{w}) = \mathbf{x}(s) \hspace{14.2em} (5)\)</span></p>
<p><span class="math inline">\(\nabla ln \pi(A|S,\mathbf{\theta}) = \mathbf{x}_h(s,a) - \sum_b \pi(b|s, \mathbf{\theta})\mathbf{x}_h(s,b) \hspace{3.3em} (6)\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActorCriticSoftmaxAgent</span>(<span class="params">BaseAgent</span>):</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.rand_generator = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.actor_step_size = <span class="literal">None</span></span><br><span class="line">        self.critic_step_size = <span class="literal">None</span></span><br><span class="line">        self.avg_reward_step_size = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.tc = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.avg_reward = <span class="literal">None</span></span><br><span class="line">        self.critic_w = <span class="literal">None</span></span><br><span class="line">        self.actor_w = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.actions = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.softmax_prob = <span class="literal">None</span></span><br><span class="line">        self.prev_tiles = <span class="literal">None</span></span><br><span class="line">        self.last_action = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span>(<span class="params">self, agent_info=&#123;&#125;</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Set parameters needed to setup the semi-gradient TD(0) state aggregation agent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Assume agent_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            &quot;iht_size&quot;: int</span></span><br><span class="line"><span class="string">            &quot;num_tilings&quot;: int,</span></span><br><span class="line"><span class="string">            &quot;num_tiles&quot;: int,</span></span><br><span class="line"><span class="string">            &quot;actor_step_size&quot;: float,</span></span><br><span class="line"><span class="string">            &quot;critic_step_size&quot;: float,</span></span><br><span class="line"><span class="string">            &quot;avg_reward_step_size&quot;: float,</span></span><br><span class="line"><span class="string">            &quot;num_actions&quot;: int,</span></span><br><span class="line"><span class="string">            &quot;seed&quot;: int</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># set random seed for each run</span></span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">&quot;seed&quot;</span>)) </span><br><span class="line"></span><br><span class="line">        iht_size = agent_info.get(<span class="string">&quot;iht_size&quot;</span>)</span><br><span class="line">        num_tilings = agent_info.get(<span class="string">&quot;num_tilings&quot;</span>)</span><br><span class="line">        num_tiles = agent_info.get(<span class="string">&quot;num_tiles&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># initialize self.tc to the tile coder we created</span></span><br><span class="line">        self.tc = PendulumTileCoder(iht_size=iht_size, num_tilings=num_tilings, num_tiles=num_tiles)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># set step-size accordingly (we normally divide actor and critic step-size by num. tilings (p.217-218 of textbook))</span></span><br><span class="line">        self.actor_step_size = agent_info.get(<span class="string">&quot;actor_step_size&quot;</span>)/num_tilings</span><br><span class="line">        self.critic_step_size = agent_info.get(<span class="string">&quot;critic_step_size&quot;</span>)/num_tilings</span><br><span class="line">        self.avg_reward_step_size = agent_info.get(<span class="string">&quot;avg_reward_step_size&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.actions = <span class="built_in">list</span>(<span class="built_in">range</span>(agent_info.get(<span class="string">&quot;num_actions&quot;</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set initial values of average reward, actor weights, and critic weights</span></span><br><span class="line">        <span class="comment"># We initialize actor weights to three times the iht_size. </span></span><br><span class="line">        <span class="comment"># Recall this is because we need to have one set of weights for each of the three actions.</span></span><br><span class="line">        self.avg_reward = <span class="number">0.0</span></span><br><span class="line">        self.actor_w = np.zeros((<span class="built_in">len</span>(self.actions), iht_size))</span><br><span class="line">        self.critic_w = np.zeros(iht_size)</span><br><span class="line"></span><br><span class="line">        self.softmax_prob = <span class="literal">None</span></span><br><span class="line">        self.prev_tiles = <span class="literal">None</span></span><br><span class="line">        self.last_action = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_policy</span>(<span class="params">self, active_tiles</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; policy of the agent</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            active_tiles (Numpy array): active tiles returned by tile coder</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The action selected according to the policy</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># compute softmax probability</span></span><br><span class="line">        softmax_prob = compute_softmax_prob(self.actor_w, active_tiles)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Sample action from the softmax probability array</span></span><br><span class="line">        <span class="comment"># self.rand_generator.choice() selects an element from the array with the specified probability</span></span><br><span class="line">        chosen_action = self.rand_generator.choice(self.actions, p=softmax_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># save softmax_prob as it will be useful later when updating the Actor</span></span><br><span class="line">        self.softmax_prob = softmax_prob</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> chosen_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;The first method called when the experiment starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the environment&#x27;s env_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The first action the agent takes.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        angle, ang_vel = state</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Use self.tc to get active_tiles using angle and ang_vel (2 lines)</span></span><br><span class="line">        <span class="comment"># set current_action by calling self.agent_policy with active_tiles</span></span><br><span class="line">        <span class="comment"># active_tiles = ?</span></span><br><span class="line">        <span class="comment"># current_action = ?</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        active_tiles = self.tc.get_tiles(angle, ang_vel)</span><br><span class="line">        current_action = self.agent_policy(active_tiles)</span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        self.last_action = current_action</span><br><span class="line">        self.prev_tiles = np.copy(active_tiles)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, state</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the environment&#x27;s step based on </span></span><br><span class="line"><span class="string">                                where the agent ended up after the</span></span><br><span class="line"><span class="string">                                last step.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The action the agent is taking.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        angle, ang_vel = state</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Use self.tc to get active_tiles using angle and ang_vel (1 line)</span></span><br><span class="line">        <span class="comment"># active_tiles = ?    </span></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        </span><br><span class="line">        active_tiles = self.tc.get_tiles(angle, ang_vel)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Compute delta using Equation (1) (1 line)</span></span><br><span class="line">        <span class="comment"># delta = ?</span></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">    </span><br><span class="line">        delta = reward - self.avg_reward + self.critic_w[active_tiles].<span class="built_in">sum</span>() - self.critic_w[self.last_action].<span class="built_in">sum</span>()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### update average reward using Equation (2) (1 line)</span></span><br><span class="line">        <span class="comment"># self.avg_reward += ?</span></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.avg_reward += self.avg_reward_step_size * delta</span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># update critic weights using Equation (3) and (5) (1 line)</span></span><br><span class="line">        <span class="comment"># self.critic_w[self.prev_tiles] += ?</span></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.critic_w[self.prev_tiles] += self.critic_step_size * delta</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># update actor weights using Equation (4) and (6)</span></span><br><span class="line">        <span class="comment"># We use self.softmax_prob saved from the previous timestep</span></span><br><span class="line">        <span class="comment"># We leave it as an exercise to verify that the code below corresponds to the equation.</span></span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> self.actions:</span><br><span class="line">            <span class="keyword">if</span> a == self.last_action:</span><br><span class="line">                self.actor_w[a][self.prev_tiles] += self.actor_step_size * delta * (<span class="number">1</span> - self.softmax_prob[a])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.actor_w[a][self.prev_tiles] += self.actor_step_size * delta * (<span class="number">0</span> - self.softmax_prob[a])</span><br><span class="line"></span><br><span class="line">        <span class="comment">### set current_action by calling self.agent_policy with active_tiles (1 line)</span></span><br><span class="line">        <span class="comment"># current_action = ? </span></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        current_action = self.agent_policy(active_tiles)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        self.prev_tiles = active_tiles</span><br><span class="line">        self.last_action = current_action</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_message</span>(<span class="params">self, message</span>):</span></span><br><span class="line">        <span class="keyword">if</span> message == <span class="string">&#x27;get avg reward&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> self.avg_reward</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Run the following code to verify <code>agent_start()</code>. Although there is randomness due to <code>self.rand_generator.choice()</code> in <code>agent_policy()</code>, we control the seed so your output should match the expected output.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">&quot;iht_size&quot;</span>: <span class="number">4096</span>,</span><br><span class="line">    <span class="string">&quot;num_tilings&quot;</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">&quot;num_tiles&quot;</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">&quot;actor_step_size&quot;</span>: <span class="number">1e-1</span>,</span><br><span class="line">    <span class="string">&quot;critic_step_size&quot;</span>: <span class="number">1e-0</span>,</span><br><span class="line">    <span class="string">&quot;avg_reward_step_size&quot;</span>: <span class="number">1e-2</span>,</span><br><span class="line">    <span class="string">&quot;num_actions&quot;</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;seed&quot;</span>: <span class="number">99</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_agent = ActorCriticSoftmaxAgent()</span><br><span class="line">test_agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">state = [-np.pi, <span class="number">0.</span>]</span><br><span class="line"></span><br><span class="line">test_agent.agent_start(state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(test_agent.prev_tiles == [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line"><span class="keyword">assert</span> test_agent.last_action == <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;agent active_tiles: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_agent.prev_tiles))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;agent selected action: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_agent.last_action))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>agent active_tiles: [0 1 2 3 4 5 6 7]
agent selected action: 2</code></pre>
<p>Run the following code to verify <code>agent_step()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Make sure agent_start() and agent_policy() are working correctly first.</span></span><br><span class="line"><span class="comment"># agent_step() should work correctly for other arbitrary state transitions in addition to this test case.</span></span><br><span class="line"></span><br><span class="line">env_info = &#123;<span class="string">&quot;seed&quot;</span>: <span class="number">99</span>&#125;</span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">&quot;iht_size&quot;</span>: <span class="number">4096</span>,</span><br><span class="line">    <span class="string">&quot;num_tilings&quot;</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">&quot;num_tiles&quot;</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="string">&quot;actor_step_size&quot;</span>: <span class="number">1e-1</span>,</span><br><span class="line">    <span class="string">&quot;critic_step_size&quot;</span>: <span class="number">1e-0</span>,</span><br><span class="line">    <span class="string">&quot;avg_reward_step_size&quot;</span>: <span class="number">1e-2</span>,</span><br><span class="line">    <span class="string">&quot;num_actions&quot;</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;seed&quot;</span>: <span class="number">99</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">rl_glue = RLGlue(PendulumEnvironment, ActorCriticSoftmaxAgent)</span><br><span class="line">rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># start env/agent</span></span><br><span class="line">rl_glue.rl_start()</span><br><span class="line">rl_glue.rl_step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># simple alias</span></span><br><span class="line">agent = rl_glue.agent</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;agent next_action: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(agent.last_action))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;agent avg reward: &#123;&#125;\n&quot;</span>.<span class="built_in">format</span>(agent.avg_reward))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> agent.last_action == <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> agent.avg_reward == -<span class="number">0.03139092653589793</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;agent first 10 values of actor weights[0]: \n&#123;&#125;\n&quot;</span>.<span class="built_in">format</span>(agent.actor_w[<span class="number">0</span>][:<span class="number">10</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;agent first 10 values of actor weights[1]: \n&#123;&#125;\n&quot;</span>.<span class="built_in">format</span>(agent.actor_w[<span class="number">1</span>][:<span class="number">10</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;agent first 10 values of actor weights[2]: \n&#123;&#125;\n&quot;</span>.<span class="built_in">format</span>(agent.actor_w[<span class="number">2</span>][:<span class="number">10</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;agent first 10 values of critic weights: \n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(agent.critic_w[:<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.actor_w[<span class="number">0</span>][:<span class="number">10</span>], [<span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.actor_w[<span class="number">1</span>][:<span class="number">10</span>], [<span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.01307955</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.actor_w[<span class="number">2</span>][:<span class="number">10</span>], [-<span class="number">0.02615911</span>, -<span class="number">0.02615911</span>, -<span class="number">0.02615911</span>, -<span class="number">0.02615911</span>, -<span class="number">0.02615911</span>, -<span class="number">0.02615911</span>, -<span class="number">0.02615911</span>, -<span class="number">0.02615911</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.critic_w[:<span class="number">10</span>], [-<span class="number">0.39238658</span>, -<span class="number">0.39238658</span>, -<span class="number">0.39238658</span>, -<span class="number">0.39238658</span>, -<span class="number">0.39238658</span>, -<span class="number">0.39238658</span>, -<span class="number">0.39238658</span>, -<span class="number">0.39238658</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br></pre></td></tr></table></figure>
<pre><code>agent next_action: 1
agent avg reward: -0.03139092653589793

agent first 10 values of actor weights[0]: 
[0.01307955 0.01307955 0.01307955 0.01307955 0.01307955 0.01307955
 0.01307955 0.01307955 0.         0.        ]

agent first 10 values of actor weights[1]: 
[0.01307955 0.01307955 0.01307955 0.01307955 0.01307955 0.01307955
 0.01307955 0.01307955 0.         0.        ]

agent first 10 values of actor weights[2]: 
[-0.02615911 -0.02615911 -0.02615911 -0.02615911 -0.02615911 -0.02615911
 -0.02615911 -0.02615911  0.          0.        ]

agent first 10 values of critic weights: 
[-0.39238658 -0.39238658 -0.39238658 -0.39238658 -0.39238658 -0.39238658
 -0.39238658 -0.39238658  0.          0.        ]</code></pre>
<h2 id="section-3-run-experiment">Section 3: Run Experiment</h2>
<p>Now that we've implemented all the components of environment and agent, let's run an experiment! We want to see whether our agent is successful at learning the optimal policy of balancing the pendulum upright. We will plot total return over time, as well as the exponential average of the reward over time. We also do multiple runs in order to be confident about our results.</p>
<p>The experiment/plot code is provided in the cell below.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define function to run experiment</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span>(<span class="params">environment, agent, environment_parameters, agent_parameters, experiment_parameters</span>):</span></span><br><span class="line"></span><br><span class="line">    rl_glue = RLGlue(environment, agent)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># sweep agent parameters</span></span><br><span class="line">    <span class="keyword">for</span> num_tilings <span class="keyword">in</span> agent_parameters[<span class="string">&#x27;num_tilings&#x27;</span>]:</span><br><span class="line">        <span class="keyword">for</span> num_tiles <span class="keyword">in</span> agent_parameters[<span class="string">&quot;num_tiles&quot;</span>]:</span><br><span class="line">            <span class="keyword">for</span> actor_ss <span class="keyword">in</span> agent_parameters[<span class="string">&quot;actor_step_size&quot;</span>]:</span><br><span class="line">                <span class="keyword">for</span> critic_ss <span class="keyword">in</span> agent_parameters[<span class="string">&quot;critic_step_size&quot;</span>]:</span><br><span class="line">                    <span class="keyword">for</span> avg_reward_ss <span class="keyword">in</span> agent_parameters[<span class="string">&quot;avg_reward_step_size&quot;</span>]:</span><br><span class="line">                        </span><br><span class="line">                        env_info = &#123;&#125;</span><br><span class="line">                        agent_info = &#123;<span class="string">&quot;num_tilings&quot;</span>: num_tilings,</span><br><span class="line">                                      <span class="string">&quot;num_tiles&quot;</span>: num_tiles,</span><br><span class="line">                                      <span class="string">&quot;actor_step_size&quot;</span>: actor_ss,</span><br><span class="line">                                      <span class="string">&quot;critic_step_size&quot;</span>: critic_ss,</span><br><span class="line">                                      <span class="string">&quot;avg_reward_step_size&quot;</span>: avg_reward_ss,</span><br><span class="line">                                      <span class="string">&quot;num_actions&quot;</span>: agent_parameters[<span class="string">&quot;num_actions&quot;</span>],</span><br><span class="line">                                      <span class="string">&quot;iht_size&quot;</span>: agent_parameters[<span class="string">&quot;iht_size&quot;</span>]&#125;            </span><br><span class="line">            </span><br><span class="line">                        <span class="comment"># results to save</span></span><br><span class="line">                        return_per_step = np.zeros((experiment_parameters[<span class="string">&quot;num_runs&quot;</span>], experiment_parameters[<span class="string">&quot;max_steps&quot;</span>]))</span><br><span class="line">                        exp_avg_reward_per_step = np.zeros((experiment_parameters[<span class="string">&quot;num_runs&quot;</span>], experiment_parameters[<span class="string">&quot;max_steps&quot;</span>]))</span><br><span class="line"></span><br><span class="line">                        <span class="comment"># using tqdm we visualize progress bars </span></span><br><span class="line">                        <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">1</span>, experiment_parameters[<span class="string">&quot;num_runs&quot;</span>]+<span class="number">1</span>)):</span><br><span class="line">                            env_info[<span class="string">&quot;seed&quot;</span>] = run</span><br><span class="line">                            agent_info[<span class="string">&quot;seed&quot;</span>] = run</span><br><span class="line">                </span><br><span class="line">                            rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">                            rl_glue.rl_start()</span><br><span class="line"></span><br><span class="line">                            num_steps = <span class="number">0</span></span><br><span class="line">                            total_return = <span class="number">0.</span></span><br><span class="line">                            return_arr = []</span><br><span class="line"></span><br><span class="line">                            <span class="comment"># exponential average reward without initial bias</span></span><br><span class="line">                            exp_avg_reward = <span class="number">0.0</span></span><br><span class="line">                            exp_avg_reward_ss = <span class="number">0.01</span></span><br><span class="line">                            exp_avg_reward_normalizer = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">                            <span class="keyword">while</span> num_steps &lt; experiment_parameters[<span class="string">&#x27;max_steps&#x27;</span>]:</span><br><span class="line">                                num_steps += <span class="number">1</span></span><br><span class="line">                                </span><br><span class="line">                                rl_step_result = rl_glue.rl_step()</span><br><span class="line">                                </span><br><span class="line">                                reward = rl_step_result[<span class="number">0</span>]</span><br><span class="line">                                total_return += reward</span><br><span class="line">                                return_arr.append(reward)</span><br><span class="line">                                avg_reward = rl_glue.rl_agent_message(<span class="string">&quot;get avg reward&quot;</span>)</span><br><span class="line"></span><br><span class="line">                                exp_avg_reward_normalizer = exp_avg_reward_normalizer + exp_avg_reward_ss * (<span class="number">1</span> - exp_avg_reward_normalizer)</span><br><span class="line">                                ss = exp_avg_reward_ss / exp_avg_reward_normalizer</span><br><span class="line">                                exp_avg_reward += ss * (reward - exp_avg_reward)</span><br><span class="line">                                </span><br><span class="line">                                return_per_step[run-<span class="number">1</span>][num_steps-<span class="number">1</span>] = total_return</span><br><span class="line">                                exp_avg_reward_per_step[run-<span class="number">1</span>][num_steps-<span class="number">1</span>] = exp_avg_reward</span><br><span class="line">                                                        </span><br><span class="line">                        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;results&#x27;</span>):</span><br><span class="line">                            os.makedirs(<span class="string">&#x27;results&#x27;</span>)</span><br><span class="line">                </span><br><span class="line">                        save_name = <span class="string">&quot;ActorCriticSoftmax_tilings_&#123;&#125;_tiledim_&#123;&#125;_actor_ss_&#123;&#125;_critic_ss_&#123;&#125;_avg_reward_ss_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(num_tilings, num_tiles, actor_ss, critic_ss, avg_reward_ss)</span><br><span class="line">                        total_return_filename = <span class="string">&quot;results/&#123;&#125;_total_return.npy&quot;</span>.<span class="built_in">format</span>(save_name)</span><br><span class="line">                        exp_avg_reward_filename = <span class="string">&quot;results/&#123;&#125;_exp_avg_reward.npy&quot;</span>.<span class="built_in">format</span>(save_name)</span><br><span class="line"></span><br><span class="line">                        np.save(total_return_filename, return_per_step)</span><br><span class="line">                        np.save(exp_avg_reward_filename, exp_avg_reward_per_step)</span><br><span class="line">                        </span><br></pre></td></tr></table></figure>
<h2 id="section-3-1-run-experiment-with-32-tilings-size-8x8">Section 3-1: Run Experiment with 32 tilings, size 8x8</h2>
<p>We will first test our implementation using 32 tilings, of size 8x8. We saw from the earlier assignment using tile-coding that many tilings promote fine discrimination, and broad tiles allows more generalization. We conducted a wide sweep of meta-parameters in order to find the best meta-parameters for our Pendulum Swing-up task.</p>
<p>We swept over the following range of meta-parameters and the best meta-parameter is boldfaced below:</p>
<p>actor step-size: <span class="math inline">\(\{\frac{2^{-6}}{32}, \frac{2^{-5}}{32}, \frac{2^{-4}}{32}, \frac{2^{-3}}{32}, \mathbf{\frac{2^{-2}}{32}}, \frac{2^{-1}}{32}, \frac{2^{0}}{32}, \frac{2^{1}}{32}\}\)</span></p>
<p>critic step-size: <span class="math inline">\(\{\frac{2^{-4}}{32}, \frac{2^{-3}}{32}, \frac{2^{-2}}{32}, \frac{2^{-1}}{32}, \frac{2^{0}}{32}, \mathbf{\frac{2^{1}}{32}}, \frac{3}{32}, \frac{2^{2}}{32}\}\)</span></p>
<p>avg reward step-size: <span class="math inline">\(\{2^{-11}, 2^{-10} , 2^{-9} , 2^{-8}, 2^{-7}, \mathbf{2^{-6}}, 2^{-5}, 2^{-4}, 2^{-3}, 2^{-2}\}\)</span></p>
<p>We will do 50 runs using the above best meta-parameter setting to verify your agent. Note that running the experiment cell below will take <strong><em>approximately 5 min</em></strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Run Experiment</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;max_steps&quot;</span> : <span class="number">20000</span>,</span><br><span class="line">    <span class="string">&quot;num_runs&quot;</span> : <span class="number">50</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line"><span class="comment"># Each element is an array because we will be later sweeping over multiple values</span></span><br><span class="line"><span class="comment"># actor and critic step-sizes are divided by num. tilings inside the agent</span></span><br><span class="line">agent_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;num_tilings&quot;</span>: [<span class="number">32</span>],</span><br><span class="line">    <span class="string">&quot;num_tiles&quot;</span>: [<span class="number">8</span>],</span><br><span class="line">    <span class="string">&quot;actor_step_size&quot;</span>: [<span class="number">2</span>**(-<span class="number">2</span>)],</span><br><span class="line">    <span class="string">&quot;critic_step_size&quot;</span>: [<span class="number">2</span>**<span class="number">1</span>],</span><br><span class="line">    <span class="string">&quot;avg_reward_step_size&quot;</span>: [<span class="number">2</span>**(-<span class="number">6</span>)],</span><br><span class="line">    <span class="string">&quot;num_actions&quot;</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;iht_size&quot;</span>: <span class="number">4096</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = PendulumEnvironment</span><br><span class="line">current_agent = ActorCriticSoftmaxAgent</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br><span class="line">plot_script.plot_result(agent_parameters, <span class="string">&#x27;results&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>  0%|          | 0/50 [00:00&lt;?, ?it/s]</code></pre>
<p>Run the following code to verify your experimental result.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test Code for experimental result ##</span></span><br><span class="line">filename = <span class="string">&#x27;ActorCriticSoftmax_tilings_32_tiledim_8_actor_ss_0.25_critic_ss_2_avg_reward_ss_0.015625_exp_avg_reward&#x27;</span></span><br><span class="line">agent_exp_avg_reward = np.load(<span class="string">&#x27;results/&#123;&#125;.npy&#x27;</span>.<span class="built_in">format</span>(filename), allow_pickle=<span class="literal">True</span>)</span><br><span class="line">result_med = np.median(agent_exp_avg_reward, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">answer_range = np.load(<span class="string">&#x27;correct_npy/exp_avg_reward_answer_range.npy&#x27;</span>, allow_pickle=<span class="literal">True</span>)</span><br><span class="line">upper_bound = answer_range.item()[<span class="string">&#x27;upper-bound&#x27;</span>]</span><br><span class="line">lower_bound = answer_range.item()[<span class="string">&#x27;lower-bound&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># check if result is within answer range</span></span><br><span class="line">all_correct = np.<span class="built_in">all</span>(result_med &lt;= upper_bound) <span class="keyword">and</span> np.<span class="built_in">all</span>(result_med &gt;= lower_bound)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> all_correct:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Your experiment results are correct!&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Your experiment results does not match with ours. Please check if you have implemented all methods correctly.&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h2 id="section-3-2-performance-metric-and-meta-parameter-sweeps">Section 3-2: Performance Metric and Meta-Parameter Sweeps</h2>
<h3 id="performance-metric">Performance Metric</h3>
<p>To evaluate performance, we plotted both the return and exponentially weighted average reward over time.</p>
<p>In the first plot, the return is negative because the reward is negative at every state except when the pendulum is in the upright position. As the policy improves over time, the agent accumulates less negative reward, and thus the return decreases slowly. Towards the end the slope is almost flat indicating the policy has stabilized to a good policy. When using this plot however, it can be difficult to distinguish whether it has learned an optimal policy. The near-optimal policy in this Pendulum Swing-up Environment is to maintain the pendulum in the upright position indefinitely, getting near 0 reward at each time step. We would have to examine the slope of the curve but it can be hard to compare the slope of different curves.</p>
<p>The second plot using exponential average reward gives a better visualization. We can see that towards the end the value is near 0, indicating it is getting near 0 reward at each time step. Here, the exponentially weighted average reward shouldn't be confused with the agent’s internal estimate of the average reward. To be more specific, we used an exponentially weighted average of the actual reward without initial bias (Refer to Exercise 2.7 from the textbook (p.35) to read more about removing the initial bias). If we used sample averages instead, later rewards would have decreasing impact on the average and would not be able to represent the agent's performance with respect to its current policy effectively.</p>
<p>It is easier to see whether the agent has learned a good policy in the second plot than the first plot. If the learned policy is optimal, the exponential average reward would be close to 0.</p>
<p>Furthermore, how did we pick the best meta-parameter from the sweeps? A common method would be to pick the meta-parameter that results in the largest Area Under the Curve (AUC). However, this is not always what we want. We want to find a set of meta-parameters that learns a good final policy. When using AUC as the criteria, we may pick meta-parameters that allows the agent to learn fast but converge to a worse policy. In our case, we selected the meta-parameter setting that obtained the most exponential average reward over the last 5000 time steps.</p>
<h3 id="parameter-sensitivity">Parameter Sensitivity</h3>
<p>In addition to finding the best meta-parameters it is also equally important to plot <strong>parameter sensitivity curves</strong> to understand how our algorithm behaves.</p>
<p>In our simulated Pendulum problem, we can extensively test our agent with different meta-parameter configurations but it would be quite expensive to do so in real life. Parameter sensitivity curves can provide us insight into how our algorithms might behave in general. It can help us identify a good range of each meta-parameters as well as how sensitive the performance is with respect to each meta-parameter.</p>
<p>Here are the sensitivity curves for the three step-sizes we swept over:</p>
<p><img src="sensitivity_combined.png" alt="Drawing" style="width: 1000px;"/></p>
<p>On the y-axis we use the performance measure, which is the average of the exponential average reward over the 5000 time steps, averaged over 50 different runs. On the x-axis is the meta-parameter we are testing. For the given meta-parameter, the remaining meta-parameters are chosen such that it obtains the best performance.</p>
<p>The curves are quite rounded, indicating the agent performs well for these wide range of values. It indicates that the agent is not too sensitive to these meta-parameters. Furthermore, looking at the y-axis values we can observe that average reward step-size is particularly less sensitive than actor step-size and critic step-size.</p>
<p>But how do we know that we have sufficiently covered a wide range of meta-parameters? It is important that the best value is not on the edge but in the middle of the meta-parameter sweep range in these sensitivity curves. Otherwise this may indicate that there could be better meta-parameter values that we did not sweep over.</p>
<h2 id="wrapping-up">Wrapping up</h2>
<h3 id="congratulations-you-have-successfully-implemented-course-3-programming-assignment-4."><strong>Congratulations!</strong> You have successfully implemented Course 3 Programming Assignment 4.</h3>
<p>You have implemented your own <strong>Average Reward Actor-Critic with Softmax Policy</strong> agent in the Pendulum Swing-up Environment. You implemented the environment based on information about the state/action space and transition dynamics. Furthermore, you have learned how to implement an agent in a continuing task using the average reward formulation. We parameterized the policy using softmax of action-preferences over discrete action spaces, and used Actor-Critic to learn the policy.</p>
<p>To summarize, you have learned how to: 1. Implement softmax actor-critic agent on a continuing task using the average reward formulation. 2. Understand how to parameterize the policy as a function to learn, in a discrete action environment. 3. Understand how to (approximately) sample the gradient of this objective to update the actor. 4. Understand how to update the critic using differential TD error.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Function-Approximation-and-Control/2020/10/15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Function-Approximation-and-Control/2020/10/15/" class="post-title-link" itemprop="url">Function Approximation and Control</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-10-15 12:00:24 / Modified: 12:01:26" itemprop="dateCreated datePublished" datetime="2020-10-15T12:00:24+08:00">2020-10-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Function-Approximation-and-Control/2020/10/15/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Function-Approximation-and-Control/2020/10/15/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="assignment-3-function-approximation-and-control">Assignment 3: Function Approximation and Control</h1>
<p>Welcome to Assignment 3. In this notebook you will learn how to: - Use function approximation in the control setting - Implement the Sarsa algorithm using tile coding - Compare three settings for tile coding to see their effect on our agent</p>
<p>As with the rest of the notebooks do not import additional libraries or adjust grading cells as this will break the grader.</p>
<p>MAKE SURE TO RUN ALL OF THE CELLS SO THE GRADER GETS THE OUTPUT IT NEEDS</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import Necessary Libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tiles3 <span class="keyword">as</span> tc</span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> argmax</span><br><span class="line"><span class="keyword">import</span> mountaincar_env</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure>
<p>In the above cell, we import the libraries we need for this assignment. You may have noticed that we import mountaincar_env. This is the <strong>Mountain Car Task</strong> introduced in <a target="_blank" rel="noopener" href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=267">Section 10.1 of the textbook</a>. The task is for an under powered car to make it to the top of a hill: <img src="mountaincar.png" title="Mountain Car" alt="Mountain Car" /> The car is under-powered so the agent needs to learn to rock back and forth to get enough momentum to reach the goal. At each time step the agent receives from the environment its current velocity (a float between -0.07 and 0.07), and it's current position (a float between -1.2 and 0.5). Because our state is continuous there are a potentially infinite number of states that our agent could be in. We need a function approximation method to help the agent deal with this. In this notebook we will use tile coding. We provide a tile coding implementation for you to use, imported above with tiles3.</p>
<h2 id="section-0-tile-coding-helper-function">Section 0: Tile Coding Helper Function</h2>
<p>To begin we are going to build a tile coding class for our Sarsa agent that will make it easier to make calls to our tile coder.</p>
<h3 id="tile-coding-function">Tile Coding Function</h3>
<p>Tile coding is introduced in <a target="_blank" rel="noopener" href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=239">Section 9.5.4 of the textbook</a> of the textbook as a way to create features that can both provide good generalization and discrimination. It consists of multiple overlapping tilings, where each tiling is a partitioning of the space into tiles. <img src="tilecoding.png" title="Tile Coding" alt="Tile Coding" /></p>
<p>To help keep our agent code clean we are going to make a function specific for tile coding for our Mountain Car environment. To help we are going to use the Tiles3 library. This is a Python 3 implementation of the tile coder. To start take a look at the documentation: <a target="_blank" rel="noopener" href="http://incompleteideas.net/tiles/tiles3.html">Tiles3 documentation</a> To get the tile coder working we need to implement a few pieces: - First: create an index hash table - this is done for you in the init function using tc.IHT. - Second is to scale the inputs for the tile coder based on the number of tiles and the range of values each input could take. The tile coder needs to take in a number in range [0, 1], or scaled to be [0, 1] * num_tiles. For more on this refer to the <a target="_blank" rel="noopener" href="http://incompleteideas.net/tiles/tiles3.html">Tiles3 documentation</a>. - Finally we call tc.tiles to get the active tiles back.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MountainCarTileCoder</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, iht_size=<span class="number">4096</span>, num_tilings=<span class="number">8</span>, num_tiles=<span class="number">8</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initializes the MountainCar Tile Coder</span></span><br><span class="line"><span class="string">        Initializers:</span></span><br><span class="line"><span class="string">        iht_size -- int, the size of the index hash table, typically a power of 2</span></span><br><span class="line"><span class="string">        num_tilings -- int, the number of tilings</span></span><br><span class="line"><span class="string">        num_tiles -- int, the number of tiles. Here both the width and height of the</span></span><br><span class="line"><span class="string">                     tile coder are the same</span></span><br><span class="line"><span class="string">        Class Variables:</span></span><br><span class="line"><span class="string">        self.iht -- tc.IHT, the index hash table that the tile coder will use</span></span><br><span class="line"><span class="string">        self.num_tilings -- int, the number of tilings the tile coder will use</span></span><br><span class="line"><span class="string">        self.num_tiles -- int, the number of tiles the tile coder will use</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.iht = tc.IHT(iht_size)</span><br><span class="line">        self.num_tilings = num_tilings</span><br><span class="line">        self.num_tiles = num_tiles</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_tiles</span>(<span class="params">self, position, velocity</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Takes in a position and velocity from the mountaincar environment</span></span><br><span class="line"><span class="string">        and returns a numpy array of active tiles.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">        position -- float, the position of the agent between -1.2 and 0.5</span></span><br><span class="line"><span class="string">        velocity -- float, the velocity of the agent between -0.07 and 0.07</span></span><br><span class="line"><span class="string">        returns:</span></span><br><span class="line"><span class="string">        tiles - np.array, active tiles</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Use the ranges above and self.num_tiles to scale position and velocity to the range [0, 1]</span></span><br><span class="line">        <span class="comment"># then multiply that range with self.num_tiles so it scales from [0, num_tiles]</span></span><br><span class="line">        </span><br><span class="line">        position_scaled = <span class="number">0</span></span><br><span class="line">        velocity_scaled = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        position_scaled = ( position + <span class="number">1.2</span> ) / (<span class="number">0.5</span> + <span class="number">1.2</span>) * self.num_tiles</span><br><span class="line">        velocity_scaled = ( velocity + <span class="number">0.07</span> ) / ( <span class="number">0.07</span> + <span class="number">0.07</span> ) * self.num_tiles</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get the tiles using tc.tiles, with self.iht, self.num_tilings and [scaled position, scaled velocity]</span></span><br><span class="line">        <span class="comment"># nothing to implment here</span></span><br><span class="line">        tiles = tc.tiles(self.iht, self.num_tilings, [position_scaled, velocity_scaled])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> np.array(tiles)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># create a range of positions and velocities to test</span></span><br><span class="line"><span class="comment"># then test every element in the cross-product between these lists</span></span><br><span class="line">pos_tests = np.linspace(-<span class="number">1.2</span>, <span class="number">0.5</span>, num=<span class="number">5</span>)</span><br><span class="line">vel_tests = np.linspace(-<span class="number">0.07</span>, <span class="number">0.07</span>, num=<span class="number">5</span>)</span><br><span class="line">tests = <span class="built_in">list</span>(itertools.product(pos_tests, vel_tests))</span><br><span class="line"></span><br><span class="line">mctc = MountainCarTileCoder(iht_size=<span class="number">1024</span>, num_tilings=<span class="number">8</span>, num_tiles=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">t = []</span><br><span class="line"><span class="keyword">for</span> test <span class="keyword">in</span> tests:</span><br><span class="line">    position, velocity = test</span><br><span class="line">    tiles = mctc.get_tiles(position=position, velocity=velocity)</span><br><span class="line">    t.append(tiles)</span><br><span class="line"></span><br><span class="line">expected = [</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">6</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">8</span>, <span class="number">14</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">11</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">14</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">15</span>, <span class="number">19</span>],</span><br><span class="line">    [<span class="number">20</span>, <span class="number">21</span>, <span class="number">16</span>, <span class="number">22</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">23</span>, <span class="number">19</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">26</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">8</span>, <span class="number">14</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">31</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">12</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">14</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">31</span>, <span class="number">34</span>],</span><br><span class="line">    [<span class="number">20</span>, <span class="number">21</span>, <span class="number">16</span>, <span class="number">22</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">35</span>, <span class="number">34</span>],</span><br><span class="line">    [<span class="number">36</span>, <span class="number">37</span>, <span class="number">38</span>, <span class="number">39</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>],</span><br><span class="line">    [<span class="number">36</span>, <span class="number">37</span>, <span class="number">40</span>, <span class="number">39</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">26</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">41</span>, <span class="number">42</span>, <span class="number">40</span>, <span class="number">43</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">31</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">41</span>, <span class="number">42</span>, <span class="number">44</span>, <span class="number">43</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">31</span>, <span class="number">34</span>],</span><br><span class="line">    [<span class="number">45</span>, <span class="number">46</span>, <span class="number">44</span>, <span class="number">47</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">35</span>, <span class="number">34</span>],</span><br><span class="line">    [<span class="number">36</span>, <span class="number">37</span>, <span class="number">38</span>, <span class="number">39</span>, <span class="number">48</span>, <span class="number">49</span>, <span class="number">50</span>, <span class="number">51</span>],</span><br><span class="line">    [<span class="number">36</span>, <span class="number">37</span>, <span class="number">40</span>, <span class="number">39</span>, <span class="number">52</span>, <span class="number">53</span>, <span class="number">50</span>, <span class="number">54</span>],</span><br><span class="line">    [<span class="number">41</span>, <span class="number">42</span>, <span class="number">40</span>, <span class="number">43</span>, <span class="number">52</span>, <span class="number">53</span>, <span class="number">55</span>, <span class="number">54</span>],</span><br><span class="line">    [<span class="number">41</span>, <span class="number">42</span>, <span class="number">44</span>, <span class="number">43</span>, <span class="number">56</span>, <span class="number">57</span>, <span class="number">55</span>, <span class="number">58</span>],</span><br><span class="line">    [<span class="number">45</span>, <span class="number">46</span>, <span class="number">44</span>, <span class="number">47</span>, <span class="number">56</span>, <span class="number">57</span>, <span class="number">59</span>, <span class="number">58</span>],</span><br><span class="line">    [<span class="number">60</span>, <span class="number">61</span>, <span class="number">62</span>, <span class="number">63</span>, <span class="number">48</span>, <span class="number">49</span>, <span class="number">50</span>, <span class="number">51</span>],</span><br><span class="line">    [<span class="number">60</span>, <span class="number">61</span>, <span class="number">64</span>, <span class="number">63</span>, <span class="number">52</span>, <span class="number">53</span>, <span class="number">50</span>, <span class="number">54</span>],</span><br><span class="line">    [<span class="number">65</span>, <span class="number">66</span>, <span class="number">64</span>, <span class="number">67</span>, <span class="number">52</span>, <span class="number">53</span>, <span class="number">55</span>, <span class="number">54</span>],</span><br><span class="line">    [<span class="number">65</span>, <span class="number">66</span>, <span class="number">68</span>, <span class="number">67</span>, <span class="number">56</span>, <span class="number">57</span>, <span class="number">55</span>, <span class="number">58</span>],</span><br><span class="line">    [<span class="number">69</span>, <span class="number">70</span>, <span class="number">68</span>, <span class="number">71</span>, <span class="number">56</span>, <span class="number">57</span>, <span class="number">59</span>, <span class="number">58</span>],</span><br><span class="line">]</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(expected == np.array(t))</span><br></pre></td></tr></table></figure>
<h2 id="section-1-sarsa-agent">Section 1: Sarsa Agent</h2>
<p>We are now going to use the functions that we just created to implement the Sarsa algorithm. Recall from class that Sarsa stands for State, Action, Reward, State, Action.</p>
<p>For this case we have given you an argmax function similar to what you wrote back in Course 1 Assignment 1. Recall, this is different than the argmax function that is used by numpy, which returns the first index of a maximum value. We want our argmax function to arbitrarily break ties, which is what the imported argmax function does. The given argmax function takes in an array of values and returns an int of the chosen action: argmax(action values)</p>
<p>There are multiple ways that we can deal with actions for the tile coder. Here we are going to use one simple method - make the size of the weight vector equal to (iht_size, num_actions). This will give us one weight vector for each action and one weight for each tile.</p>
<p>Use the above function to help fill in select_action, agent_start, agent_step, and agent_end.</p>
<p>Hints:</p>
<ol type="1">
<li>The tile coder returns a list of active indexes (e.g. [1, 12, 22]). You can index a numpy array using an array of values - this will return an array of the values at each of those indices. So in order to get the value of a state we can index our weight vector using the action and the array of tiles that the tile coder returns:</li>
</ol>
<p><code>self.w[action][active_tiles]</code></p>
<p>This will give us an array of values, one for each active tile, and we sum the result to get the value of that state-action pair.</p>
<ol start="2" type="1">
<li>In the case of a binary feature vector (such as the tile coder), the derivative is 1 at each of the active tiles, and zero otherwise.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaAgent</span>(<span class="params">BaseAgent</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Initialization of Sarsa Agent. All values are set to None so they can</span></span><br><span class="line"><span class="string">    be initialized in the agent_init method.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.last_action = <span class="literal">None</span></span><br><span class="line">        self.last_state = <span class="literal">None</span></span><br><span class="line">        self.epsilon = <span class="literal">None</span></span><br><span class="line">        self.gamma = <span class="literal">None</span></span><br><span class="line">        self.iht_size = <span class="literal">None</span></span><br><span class="line">        self.w = <span class="literal">None</span></span><br><span class="line">        self.alpha = <span class="literal">None</span></span><br><span class="line">        self.num_tilings = <span class="literal">None</span></span><br><span class="line">        self.num_tiles = <span class="literal">None</span></span><br><span class="line">        self.mctc = <span class="literal">None</span></span><br><span class="line">        self.initial_weights = <span class="literal">None</span></span><br><span class="line">        self.num_actions = <span class="literal">None</span></span><br><span class="line">        self.previous_tiles = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span>(<span class="params">self, agent_info=&#123;&#125;</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Setup for the agent called when the experiment first starts.&quot;&quot;&quot;</span></span><br><span class="line">        self.num_tilings = agent_info.get(<span class="string">&quot;num_tilings&quot;</span>, <span class="number">8</span>)</span><br><span class="line">        self.num_tiles = agent_info.get(<span class="string">&quot;num_tiles&quot;</span>, <span class="number">8</span>)</span><br><span class="line">        self.iht_size = agent_info.get(<span class="string">&quot;iht_size&quot;</span>, <span class="number">4096</span>)</span><br><span class="line">        self.epsilon = agent_info.get(<span class="string">&quot;epsilon&quot;</span>, <span class="number">0.0</span>)</span><br><span class="line">        self.gamma = agent_info.get(<span class="string">&quot;gamma&quot;</span>, <span class="number">1.0</span>)</span><br><span class="line">        self.alpha = agent_info.get(<span class="string">&quot;alpha&quot;</span>, <span class="number">0.5</span>) / self.num_tilings</span><br><span class="line">        self.initial_weights = agent_info.get(<span class="string">&quot;initial_weights&quot;</span>, <span class="number">0.0</span>)</span><br><span class="line">        self.num_actions = agent_info.get(<span class="string">&quot;num_actions&quot;</span>, <span class="number">3</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># We initialize self.w to three times the iht_size. Recall this is because</span></span><br><span class="line">        <span class="comment"># we need to have one set of weights for each action.</span></span><br><span class="line">        self.w = np.ones((self.num_actions, self.iht_size)) * self.initial_weights</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># We initialize self.mctc to the mountaincar verions of the </span></span><br><span class="line">        <span class="comment"># tile coder that we created</span></span><br><span class="line">        self.tc = MountainCarTileCoder(iht_size=self.iht_size, </span><br><span class="line">                                         num_tilings=self.num_tilings, </span><br><span class="line">                                         num_tiles=self.num_tiles)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">select_action</span>(<span class="params">self, tiles</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Selects an action using epsilon greedy</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">        tiles - np.array, an array of active tiles</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        (chosen_action, action_value) - (int, float), tuple of the chosen action</span></span><br><span class="line"><span class="string">                                        and it&#x27;s value</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        action_values = []</span><br><span class="line">        chosen_action = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># First loop through the weights of each action and populate action_values</span></span><br><span class="line">        <span class="comment"># with the action value for each action and tiles instance</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use np.random.random to decide if an exploritory action should be taken</span></span><br><span class="line">        <span class="comment"># and set chosen_action to a random action if it is</span></span><br><span class="line">        <span class="comment"># Otherwise choose the greedy action using the given argmax </span></span><br><span class="line">        <span class="comment"># function and the action values (don&#x27;t use numpy&#x27;s armax)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_actions):</span><br><span class="line">            action_values.append(self.w[i][tiles].<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> np.random.random() &lt; self.epsilon:</span><br><span class="line">            chosen_action = np.random.choice(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            chosen_action = argmax(action_values)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> chosen_action, action_values[chosen_action]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;The first method called when the experiment starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            state (Numpy array): the state observation from the</span></span><br><span class="line"><span class="string">                environment&#x27;s evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The first action the agent takes.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        position, velocity = state</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use self.tc to set active_tiles using position and velocity</span></span><br><span class="line">        <span class="comment"># set current_action to the epsilon greedy chosen action using</span></span><br><span class="line">        <span class="comment"># the select_action function above with the active tiles</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        active_tiles = self.tc.get_tiles(position, velocity)</span><br><span class="line">        current_action, _ = self.select_action(active_tiles)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        self.last_action = current_action</span><br><span class="line">        self.previous_tiles = np.copy(active_tiles)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, state</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            state (Numpy array): the state observation from the</span></span><br><span class="line"><span class="string">                environment&#x27;s step based, where the agent ended up after the</span></span><br><span class="line"><span class="string">                last step</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The action the agent is taking.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># choose the action here</span></span><br><span class="line">        position, velocity = state</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use self.tc to set active_tiles using position and velocity</span></span><br><span class="line">        <span class="comment"># set current_action and action_value to the epsilon greedy chosen action using</span></span><br><span class="line">        <span class="comment"># the select_action function above with the active tiles</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update self.w at self.previous_tiles and self.previous action</span></span><br><span class="line">        <span class="comment"># using the reward, action_value, self.gamma, self.w,</span></span><br><span class="line">        <span class="comment"># self.alpha, and the Sarsa update from the textbook</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        active_tiles = self.tc.get_tiles(position, velocity)</span><br><span class="line">        current_action, action_value = self.select_action(active_tiles)</span><br><span class="line">        last_action_value = self.w[self.last_action][self.previous_tiles].<span class="built_in">sum</span>()</span><br><span class="line">        delta = reward + self.gamma * action_value - last_action_value</span><br><span class="line">        grad = np.zeros_like(self.w)</span><br><span class="line">        grad[self.last_action][self.previous_tiles] = <span class="number">1</span></span><br><span class="line">        self.w += self.alpha * delta * grad</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        </span><br><span class="line">        self.last_action = current_action</span><br><span class="line">        self.previous_tiles = np.copy(active_tiles)</span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Update self.w at self.previous_tiles and self.previous action</span></span><br><span class="line">        <span class="comment"># using the reward, self.gamma, self.w,</span></span><br><span class="line">        <span class="comment"># self.alpha, and the Sarsa update from the textbook</span></span><br><span class="line">        <span class="comment"># Hint - there is no action_value used here because this is the end</span></span><br><span class="line">        <span class="comment"># of the episode.</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        last_action_value = self.w[self.last_action][self.previous_tiles].<span class="built_in">sum</span>()</span><br><span class="line">        </span><br><span class="line">        grad = np.zeros_like(self.w)</span><br><span class="line">        grad[self.last_action][self.previous_tiles] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        self.w += self.alpha * (reward - last_action_value) * grad</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_cleanup</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Cleanup done after the agent ends.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_message</span>(<span class="params">self, message</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;A function used to pass information from the agent to the experiment.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            message: The message passed to the agent.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The response (or answer) to the message.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">agent = SarsaAgent()</span><br><span class="line">agent.agent_init(&#123;<span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line">agent.w = np.array([np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]), np.array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]), np.array([<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])])</span><br><span class="line"></span><br><span class="line">action_distribution = np.zeros(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    chosen_action, action_value = agent.select_action(np.array([<span class="number">0</span>,<span class="number">1</span>]))</span><br><span class="line">    action_distribution[chosen_action] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;action distribution:&quot;</span>, action_distribution)</span><br><span class="line"><span class="comment"># notice that the two non-greedy actions are roughly uniformly distributed</span></span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(action_distribution == [<span class="number">29</span>, <span class="number">35</span>, <span class="number">936</span>])</span><br><span class="line"></span><br><span class="line">agent = SarsaAgent()</span><br><span class="line">agent.agent_init(&#123;<span class="string">&quot;epsilon&quot;</span>: <span class="number">0.0</span>&#125;)</span><br><span class="line">agent.w = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">chosen_action, action_value = agent.select_action([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"><span class="keyword">assert</span> chosen_action == <span class="number">2</span></span><br><span class="line"><span class="keyword">assert</span> action_value == <span class="number">15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># test update</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line">agent = SarsaAgent()</span><br><span class="line">agent.agent_init(&#123;<span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line"></span><br><span class="line">agent.agent_start((<span class="number">0.1</span>, <span class="number">0.3</span>))</span><br><span class="line">agent.agent_step(<span class="number">1</span>, (<span class="number">0.02</span>, <span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(agent.w[<span class="number">0</span>,<span class="number">0</span>:<span class="number">8</span>] == <span class="number">0.0625</span>)</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(agent.w[<span class="number">1</span>:] == <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<pre><code>action distribution: [ 29.  35. 936.]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">num_runs = <span class="number">10</span></span><br><span class="line">num_episodes = <span class="number">50</span></span><br><span class="line">env_info = &#123;<span class="string">&quot;num_tiles&quot;</span>: <span class="number">8</span>, <span class="string">&quot;num_tilings&quot;</span>: <span class="number">8</span>&#125;</span><br><span class="line">agent_info = &#123;&#125;</span><br><span class="line">all_steps = []</span><br><span class="line"></span><br><span class="line">agent = SarsaAgent</span><br><span class="line">env = mountaincar_env.Environment</span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> <span class="built_in">range</span>(num_runs):</span><br><span class="line">    <span class="keyword">if</span> run % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;RUN: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(run))</span><br><span class="line"></span><br><span class="line">    rl_glue = RLGlue(env, agent)</span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">    steps_per_episode = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(num_episodes):</span><br><span class="line">        rl_glue.rl_episode(<span class="number">15000</span>)</span><br><span class="line">        steps_per_episode.append(rl_glue.num_steps)</span><br><span class="line"></span><br><span class="line">    all_steps.append(np.array(steps_per_episode))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Run time: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(time.time() - start))</span><br><span class="line"></span><br><span class="line">mean = np.mean(all_steps, axis=<span class="number">0</span>)</span><br><span class="line">plt.plot(mean)</span><br><span class="line"></span><br><span class="line"><span class="comment"># because we set the random seed, these values should be *exactly* the same</span></span><br><span class="line"><span class="keyword">assert</span> np.allclose(mean, [<span class="number">1432.5</span>, <span class="number">837.9</span>, <span class="number">694.4</span>, <span class="number">571.4</span>, <span class="number">515.2</span>, <span class="number">380.6</span>, <span class="number">379.4</span>, <span class="number">369.6</span>, <span class="number">357.2</span>, <span class="number">316.5</span>, <span class="number">291.1</span>, <span class="number">305.3</span>, <span class="number">250.1</span>, <span class="number">264.9</span>, <span class="number">235.4</span>, <span class="number">242.1</span>, <span class="number">244.4</span>, <span class="number">245.</span>, <span class="number">221.2</span>, <span class="number">229.</span>, <span class="number">238.3</span>, <span class="number">211.2</span>, <span class="number">201.1</span>, <span class="number">208.3</span>, <span class="number">185.3</span>, <span class="number">207.1</span>, <span class="number">191.6</span>, <span class="number">204.</span>, <span class="number">214.5</span>, <span class="number">207.9</span>, <span class="number">195.9</span>, <span class="number">206.4</span>, <span class="number">194.9</span>, <span class="number">191.1</span>, <span class="number">195.</span>, <span class="number">186.6</span>, <span class="number">171.</span>, <span class="number">177.8</span>, <span class="number">171.1</span>, <span class="number">174.</span>, <span class="number">177.1</span>, <span class="number">174.5</span>, <span class="number">156.9</span>, <span class="number">174.3</span>, <span class="number">164.1</span>, <span class="number">179.3</span>, <span class="number">167.4</span>, <span class="number">156.1</span>, <span class="number">158.4</span>, <span class="number">154.4</span>])</span><br></pre></td></tr></table></figure>
<pre><code>RUN: 0
RUN: 5
Run time: 13.416615009307861</code></pre>
<figure>
<img src="output_15_1.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<p>The learning rate of your agent should look similar to ours, though it will not look exactly the same.If there are some spikey points that is okay. Due to stochasticity, a few episodes may have taken much longer, causing some spikes in the plot. The trend of the line should be similar, though, generally decreasing to about 200 steps per run. <img src="sarsa_agent_initial.png" title="Logo Title Text 1" alt="alt text" /></p>
<p>This result was using 8 tilings with 8x8 tiles on each. Let's see if we can do better, and what different tilings look like. We will also text 2 tilings of 16x16 and 4 tilings of 32x32. These three choices produce the same number of features (512), but distributed quite differently.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compare the three</span></span><br><span class="line">num_runs = <span class="number">20</span></span><br><span class="line">num_episodes = <span class="number">100</span></span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line"></span><br><span class="line">agent_runs = []</span><br><span class="line"><span class="comment"># alphas = [0.2, 0.4, 0.5, 1.0]</span></span><br><span class="line">alphas = [<span class="number">0.5</span>]</span><br><span class="line">agent_info_options = [&#123;<span class="string">&quot;num_tiles&quot;</span>: <span class="number">16</span>, <span class="string">&quot;num_tilings&quot;</span>: <span class="number">2</span>, <span class="string">&quot;alpha&quot;</span>: <span class="number">0.5</span>&#125;,</span><br><span class="line">                      &#123;<span class="string">&quot;num_tiles&quot;</span>: <span class="number">4</span>, <span class="string">&quot;num_tilings&quot;</span>: <span class="number">32</span>, <span class="string">&quot;alpha&quot;</span>: <span class="number">0.5</span>&#125;,</span><br><span class="line">                      &#123;<span class="string">&quot;num_tiles&quot;</span>: <span class="number">8</span>, <span class="string">&quot;num_tilings&quot;</span>: <span class="number">8</span>, <span class="string">&quot;alpha&quot;</span>: <span class="number">0.5</span>&#125;]</span><br><span class="line">agent_info_options = [&#123;<span class="string">&quot;num_tiles&quot;</span> : agent[<span class="string">&quot;num_tiles&quot;</span>], </span><br><span class="line">                       <span class="string">&quot;num_tilings&quot;</span>: agent[<span class="string">&quot;num_tilings&quot;</span>],</span><br><span class="line">                       <span class="string">&quot;alpha&quot;</span> : alpha&#125; <span class="keyword">for</span> agent <span class="keyword">in</span> agent_info_options <span class="keyword">for</span> alpha <span class="keyword">in</span> alphas]</span><br><span class="line"></span><br><span class="line">agent = SarsaAgent</span><br><span class="line">env = mountaincar_env.Environment</span><br><span class="line"><span class="keyword">for</span> agent_info <span class="keyword">in</span> agent_info_options:</span><br><span class="line">    all_steps = []</span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> <span class="built_in">range</span>(num_runs):</span><br><span class="line">        <span class="keyword">if</span> run % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;RUN: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(run))</span><br><span class="line">        env = mountaincar_env.Environment</span><br><span class="line">        </span><br><span class="line">        rl_glue = RLGlue(env, agent)</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">        steps_per_episode = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(num_episodes):</span><br><span class="line">            rl_glue.rl_episode(<span class="number">15000</span>)</span><br><span class="line">            steps_per_episode.append(rl_glue.num_steps)</span><br><span class="line">        all_steps.append(np.array(steps_per_episode))</span><br><span class="line">    </span><br><span class="line">    agent_runs.append(np.mean(np.array(all_steps), axis=<span class="number">0</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;stepsize:&quot;</span>, rl_glue.agent.alpha)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Run Time: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(time.time() - start))</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">10</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">&#x27;w&#x27;</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.plot(np.array(agent_runs).T)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Episode&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Steps Per Episode&quot;</span>)</span><br><span class="line">plt.yscale(<span class="string">&quot;linear&quot;</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">1000</span>)</span><br><span class="line">plt.legend([<span class="string">&quot;num_tiles: &#123;&#125;, num_tilings: &#123;&#125;, alpha: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(agent_info[<span class="string">&quot;num_tiles&quot;</span>], </span><br><span class="line">                                                               agent_info[<span class="string">&quot;num_tilings&quot;</span>],</span><br><span class="line">                                                               agent_info[<span class="string">&quot;alpha&quot;</span>])</span><br><span class="line">            <span class="keyword">for</span> agent_info <span class="keyword">in</span> agent_info_options])</span><br></pre></td></tr></table></figure>
<pre><code>RUN: 0
RUN: 5
RUN: 10
RUN: 15
stepsize: 0.25
Run Time: 71.2762451171875
RUN: 0
RUN: 5
RUN: 10
RUN: 15
stepsize: 0.015625
Run Time: 38.23225665092468
RUN: 0
RUN: 5
RUN: 10
RUN: 15
stepsize: 0.0625
Run Time: 42.84800481796265





&lt;matplotlib.legend.Legend at 0x7fbda7e76910&gt;</code></pre>
<figure>
<img src="output_18_2.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<p>Here we can see that using 32 tilings and 4 x 4 tiles does a little better than 8 tilings with 8x8 tiles. Both seem to do much better than using 2 tilings, with 16 x 16 tiles.</p>
<h2 id="section-3-conclusion">Section 3: Conclusion</h2>
<p>Congratulations! You have learned how to implement a control agent using function approximation. In this notebook you learned how to:</p>
<ul>
<li>Use function approximation in the control setting</li>
<li>Implement the Sarsa algorithm using tile coding</li>
<li>Compare three settings for tile coding to see their effect on our agent</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/" class="post-title-link" itemprop="url">Semi Gradient TD with a Neural Network</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-14 17:18:55" itemprop="dateCreated datePublished" datetime="2020-10-14T17:18:55+08:00">2020-10-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-10-16 15:04:08" itemprop="dateModified" datetime="2020-10-16T15:04:08+08:00">2020-10-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Semi-gradient-TD-with-a-Neural-Network/2020/10/14/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="assignment-2---semi-gradient-td-with-a-neural-network">Assignment 2 - Semi-gradient TD with a Neural Network</h1>
<p>Welcome to Course 3 Programming Assignment 2. In the previous assignment, you implemented semi-gradient TD with State Aggregation for solving a <strong>policy evaluation task</strong>. In this assignment, you will implement <strong>semi-gradient TD with a simple Neural Network</strong> and use it for the same policy evaluation problem.</p>
<p>You will implement an agent to evaluate a fixed policy on the 500-State Randomwalk. As you may remember from the previous assignment, the 500-state Randomwalk includes 500 states. Each episode begins with the agent at the center and terminates when the agent goes far left beyond state 1 or far right beyond state 500. At each time step, the agent selects to move either left or right with equal probability. The environment determines how much the agent moves in the selected direction.</p>
<p><strong>In this assignment, you will:</strong> - Implement stochastic gradient descent method for state-value prediction. - Implement semi-gradient TD with a neural network as the function approximator and Adam algorithm. - Compare performance of semi-gradient TD with a neural network and semi-gradient TD with tile-coding.</p>
<h2 id="packages">Packages</h2>
<p>We import the following libraries that are required for this assignment:</p>
<ul>
<li><a href="www.numpy.org">numpy</a> : Fundamental package for scientific computing with Python.</li>
<li><a target="_blank" rel="noopener" href="http://matplotlib.org">matplotlib</a> : Library for plotting graphs in Python.</li>
<li><a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/v10/tanner09a.html">RL-Glue</a> : Library for reinforcement learning experiments.</li>
<li><a target="_blank" rel="noopener" href="https://tqdm.github.io/">tqdm</a> : A package to display progress bar when running experiments.</li>
<li>BaseOptimizer : An abstract class that specifies the optimizer API for Agent.</li>
<li>plot_script : Custom script to plot results.</li>
<li>RandomWalkEnvironment : The Randomwalk environment script from Course 3 Assignment 1.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="comment"># DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os, shutil</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> environment <span class="keyword">import</span> BaseEnvironment</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">from</span> optimizer <span class="keyword">import</span> BaseOptimizer</span><br><span class="line"><span class="keyword">import</span> plot_script</span><br><span class="line"><span class="keyword">from</span> randomwalk_environment <span class="keyword">import</span> RandomWalkEnvironment</span><br></pre></td></tr></table></figure>
<h2 id="section-1-create-semi-gradient-td-with-a-neural-network">Section 1: Create semi-gradient TD with a Neural Network</h2>
<p>In this section, you will implement an Agent that learns with semi-gradient TD with a neural network. You will use a neural network with one hidden layer. The input of the neural network is the one-hot encoding of the state number. We use the one-hot encoding of the state number instead of the state number itself because we do not want to build the prior knowledge that integer number inputs close to each other have similar values. The hidden layer contains 100 rectifier linear units (ReLUs) which pass their input if it is bigger than one and return 0 otherwise. ReLU gates are commonly used in neural networks due to their nice properties such as the sparsity of the activation and having non-vanishing gradients. The output of the neural network is the estimated state value. It is a linear function of the hidden units as is commonly the case when estimating the value of a continuous target using neural networks.</p>
<p>The neural network looks like this: <img src="nn_structure.png" /></p>
<p>For a given input, <span class="math inline">\(s\)</span>, value of <span class="math inline">\(s\)</span> is computed by: <span class="math display">\[
\begin{align} 
\psi &amp;= sW^{[0]} + b^{[0]} \\
x &amp;= \textit{max}(0, \psi) \\
v &amp;= xW^{[1]} + b^{[1]}
\end{align} 
\]</span></p>
<p>where <span class="math inline">\(W^{[0]}\)</span>, <span class="math inline">\(b^{[0]}\)</span>, <span class="math inline">\(W^{[1]}\)</span>, <span class="math inline">\(b^{[1]}\)</span> are the parameters of the network and will be learned when training the agent.</p>
<h2 id="implement-helper-methods">1-1: Implement helper methods</h2>
<p>Before implementing the agent, you first implement some helper functions which you will later use in agent's main methods.</p>
<h3 id="implement-get_value">Implement <code>get_value()</code></h3>
<p>First, you will implement get_value() method which feeds an input <span class="math inline">\(s\)</span> into the neural network and returns the output of the network <span class="math inline">\(v\)</span> according to the equations above. To implement get_value(), take into account the following notes:</p>
<ul>
<li><code>get_value()</code> gets the one-hot encoded state number denoted by s as an input.</li>
<li><code>get_value()</code> receives the weights of the neural network as input, denoted by weights and structured as an array of dictionaries. Each dictionary corresponds to weights from one layer of the neural network to the next. Each dictionary includes <span class="math inline">\(W\)</span> and <span class="math inline">\(b\)</span>. The shape of the elements in weights are as follows:
<ul>
<li>weights[0]["W"]: num_states <span class="math inline">\(\times\)</span> num_hidden_units</li>
<li>weights[0]["b"]: 1 <span class="math inline">\(\times\)</span> num_hidden_units</li>
<li>weights[1]["W"]: num_hidden_units <span class="math inline">\(\times\)</span> 1</li>
<li>weights[1]["b"]: 1 <span class="math inline">\(\times\)</span> 1</li>
</ul></li>
<li>The input of the neural network is a sparse vector. To make computation faster, we take advantage of input sparsity. To do so, we provided a helper method <code>my_matmul()</code>. <strong>Make sure that you use <code>my_matmul()</code> for all matrix multiplications except for element-wise multiplications in this notebook.</strong></li>
<li>The max operator used for computing <span class="math inline">\(x\)</span> is element-wise.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_matmul</span>(<span class="params">x1, x2</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Given matrices x1 and x2, return the multiplication of them</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    result = np.zeros((x1.shape[<span class="number">0</span>], x2.shape[<span class="number">1</span>]))</span><br><span class="line">    x1_non_zero_indices = x1.nonzero()</span><br><span class="line">    <span class="keyword">if</span> x1.shape[<span class="number">0</span>] == <span class="number">1</span> <span class="keyword">and</span> <span class="built_in">len</span>(x1_non_zero_indices[<span class="number">1</span>]) == <span class="number">1</span>:</span><br><span class="line">        result = x2[x1_non_zero_indices[<span class="number">1</span>], :]</span><br><span class="line">    <span class="keyword">elif</span> x1.shape[<span class="number">1</span>] == <span class="number">1</span> <span class="keyword">and</span> <span class="built_in">len</span>(x1_non_zero_indices[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        result[x1_non_zero_indices[<span class="number">0</span>], :] = x2 * x1[x1_non_zero_indices[<span class="number">0</span>], <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = np.matmul(x1, x2)</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_value</span>(<span class="params">s, weights</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute value of input s given the weights of a neural network</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### Compute the ouput of the neural network, v, for input s</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    hidden = my_matmul(s, weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>]) + weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>]</span><br><span class="line">    x = np.maximum(hidden, np.zeros_like(hidden))</span><br><span class="line">    v = my_matmul(x, weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>]) + weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure>
<p>Run the following code to test your implementation of the <code>get_value()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose num_states = 5, num_hidden_layer = 1, and num_hidden_units = 10 </span></span><br><span class="line">num_hidden_layer = <span class="number">1</span></span><br><span class="line">s = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">weights_data = np.load(<span class="string">&quot;asserts/get_value_weights.npz&quot;</span>)</span><br><span class="line">weights = [<span class="built_in">dict</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>] = weights_data[<span class="string">&quot;W0&quot;</span>]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>] = weights_data[<span class="string">&quot;b0&quot;</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>] = weights_data[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>] = weights_data[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line"></span><br><span class="line">estimated_value = get_value(s, weights)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Estimated value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(estimated_value))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(estimated_value, [[-<span class="number">0.21915705</span>]]))</span><br></pre></td></tr></table></figure>
<pre><code>Estimated value: [[-0.21915705]]</code></pre>
<p><strong>Expected output</strong>:</p>
<pre><code>Estimated value: [[-0.21915705]]</code></pre>
<h3 id="implement-get_gradient">Implement <code>get_gradient()</code></h3>
<p>You will also implement <code>get_gradient()</code> method which computes the gradient of the value function for a given input, using backpropagation. You will later use this function to update the value function.</p>
<p>As you know, we compute the value of a state <span class="math inline">\(s\)</span> according to: <span class="math display">\[
\begin{align} 
\psi &amp;= sW^{[0]} + b^{[0]} \\
x &amp;= \textit{max}(0, \psi) \\
v &amp;= xW^{[1]} + b^{[1]}
\end{align} 
\]</span></p>
<p>To update the weights of the neural network (<span class="math inline">\(W^{[0]}\)</span>, <span class="math inline">\(b^{[0]}\)</span>, <span class="math inline">\(W^{[1]}\)</span>, <span class="math inline">\(b^{[1]}\)</span>), we compute the gradient of <span class="math inline">\(v\)</span> with respect to the weights according to:</p>
<p><span class="math display">\[
\begin{align} 
\frac{\partial v}{\partial W^{[0]}} &amp;= s^T(W^{[1]T} \odot I_{x&gt;0}) \\
\frac{\partial v}{\partial b^{[0]}} &amp;= W^{[1]T} \odot I_{x&gt;0} \\
\frac{\partial v}{\partial W^{[1]}} &amp;= x^T \\
\frac{\partial v}{\partial b^{[1]}} &amp;= 1
\end{align}
\]</span> where <span class="math inline">\(\odot\)</span> denotes element-wise matrix multiplication and <span class="math inline">\(I_{x&gt;0}\)</span> is the gradient of the ReLU activation function which is an indicator whose <span class="math inline">\(i\)</span>th element is 1 if <span class="math inline">\(x[i]&gt;0\)</span> and 0 otherwise.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gradient</span>(<span class="params">s, weights</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Given inputs s and weights, return the gradient of v with respect to the weights</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### Compute the gradient of the value function with respect to W0, b0, W1, b1 for input s</span></span><br><span class="line">    <span class="comment"># grads[0][&quot;W&quot;] = ?</span></span><br><span class="line">    <span class="comment"># grads[0][&quot;b&quot;] = ?</span></span><br><span class="line">    <span class="comment"># grads[1][&quot;W&quot;] = ?</span></span><br><span class="line">    <span class="comment"># grads[1][&quot;b&quot;] = ?</span></span><br><span class="line">    <span class="comment"># Note that grads[0][&quot;W&quot;], grads[0][&quot;b&quot;], grads[1][&quot;W&quot;], and grads[1][&quot;b&quot;] should have the same shape as </span></span><br><span class="line">    <span class="comment"># weights[0][&quot;W&quot;], weights[0][&quot;b&quot;], weights[1][&quot;W&quot;], and weights[1][&quot;b&quot;] respectively</span></span><br><span class="line">    <span class="comment"># Note that to compute the gradients, you need to compute the activation of the hidden layer (x)</span></span><br><span class="line"></span><br><span class="line">    grads = [<span class="built_in">dict</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(weights))]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    hidden = my_matmul(s, weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>]) + weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>]</span><br><span class="line">    x = np.maximum(hidden, np.zeros_like(hidden))</span><br><span class="line">    </span><br><span class="line">    grads[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>] = my_matmul(np.transpose(s),np.maximum(np.transpose(weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>]),<span class="number">0</span>))</span><br><span class="line">    grads[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>] = np.maximum(np.transpose(weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>]),<span class="number">0</span>)</span><br><span class="line">    grads[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>] = np.transpose(x)</span><br><span class="line">    grads[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Run the following code to test your implementation of the <code>get_gradient()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose num_states = 5, num_hidden_layer = 1, and num_hidden_units = 2 </span></span><br><span class="line">num_hidden_layer = <span class="number">1</span></span><br><span class="line">s = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">weights_data = np.load(<span class="string">&quot;asserts/get_gradient_weights.npz&quot;</span>)</span><br><span class="line">weights = [<span class="built_in">dict</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>] = weights_data[<span class="string">&quot;W0&quot;</span>]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>] = weights_data[<span class="string">&quot;b0&quot;</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>] = weights_data[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>] = weights_data[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line"></span><br><span class="line">grads = get_gradient(s, weights)</span><br><span class="line"></span><br><span class="line">grads_answer = np.load(<span class="string">&quot;asserts/get_gradient_grads.npz&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(grads[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>], grads_answer[<span class="string">&quot;W0&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(grads[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>], grads_answer[<span class="string">&quot;b0&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(grads[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>], grads_answer[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(grads[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>], grads_answer[<span class="string">&quot;b1&quot;</span>]))</span><br></pre></td></tr></table></figure>
<p><strong>Expected output</strong>:</p>
<pre><code>grads[0][&quot;W&quot;]
 [[0.         0.        ]
 [0.         0.        ]
 [0.         0.        ]
 [0.76103773 0.12167502]
 [0.         0.        ]] 

grads[0][&quot;b&quot;]
 [[0.76103773 0.12167502]] 

grads[1][&quot;W&quot;]
 [[0.69198983]
 [0.82403662]] 

grads[1][&quot;b&quot;]
 [[1.]] </code></pre>
<h3 id="implement-stochastic-gradient-descent-method-for-state-value-prediction">Implement stochastic gradient descent method for state-value prediction</h3>
<p>In this section, you will implement stochastic gradient descent (SGD) method for state_value prediction. Here is the basic SGD update for state-value prediction with TD:</p>
<p><span class="math display">\[\mathbf{w_{t+1}} = \mathbf{w_{t}} + \alpha \delta_t \nabla \hat{v}(S_t,\mathbf{w_{t}})\]</span></p>
<p>At each time step, we update the weights in the direction <span class="math inline">\(g_t = \delta_t \nabla \hat{v}(S_t,\mathbf{w_t})\)</span> using a fixed step-size <span class="math inline">\(\alpha\)</span>. <span class="math inline">\(\delta_t = R_{t+1} + \gamma \hat{v}(S_{t+1},\mathbf{w_{t}}) - \hat{v}(S_t,\mathbf{w_t})\)</span> is the TD-error. <span class="math inline">\(\nabla \hat{v}(S_t,\mathbf{w_{t}})\)</span> is the gradient of the value function with respect to the weights.</p>
<p>The following cell includes the SGD class. You will complete the <code>update_weight()</code> method of SGD assuming that the weights and update g are provided.</p>
<p><strong>As you know, in this assignment, we structured the weights as an array of dictionaries. Note that the updates <span class="math inline">\(g_t\)</span>, in the case of TD, is <span class="math inline">\(\delta_t \nabla \hat{v}(S_t,\mathbf{w_t})\)</span>. As a result, <span class="math inline">\(g_t\)</span> has the same structure as <span class="math inline">\(\nabla \hat{v}(S_t,\mathbf{w_t})\)</span> which is also an array of dictionaries.</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SGD</span>(<span class="params">BaseOptimizer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">optimizer_init</span>(<span class="params">self, optimizer_info</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Setup for the optimizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Set parameters needed to setup the stochastic gradient descent method.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Assume optimizer_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            step_size: float</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.step_size = optimizer_info.get(<span class="string">&quot;step_size&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_weights</span>(<span class="params">self, weights, g</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Given weights and update g, return updated weights</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(weights)):</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> weights[i].keys():</span><br><span class="line">                </span><br><span class="line">                <span class="comment">### update weights</span></span><br><span class="line">                <span class="comment"># weights[i][param] = None</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line">                <span class="comment"># your code here</span></span><br><span class="line">                weights[i][param] += self.step_size * g[i][param]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
<p>Run the following code to test your implementation of the <code>update_weights()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose num_states = 5, num_hidden_layer = 1, and num_hidden_units = 2 </span></span><br><span class="line">num_hidden_layer = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">weights_data = np.load(<span class="string">&quot;asserts/update_weights_weights.npz&quot;</span>)</span><br><span class="line">weights = [<span class="built_in">dict</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>] = weights_data[<span class="string">&quot;W0&quot;</span>]</span><br><span class="line">weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>] = weights_data[<span class="string">&quot;b0&quot;</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>] = weights_data[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>] = weights_data[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line"></span><br><span class="line">g_data = np.load(<span class="string">&quot;asserts/update_weights_g.npz&quot;</span>)</span><br><span class="line">g = [<span class="built_in">dict</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">g[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>] = g_data[<span class="string">&quot;W0&quot;</span>]</span><br><span class="line">g[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>] = g_data[<span class="string">&quot;b0&quot;</span>]</span><br><span class="line">g[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>] = g_data[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">g[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>] = g_data[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line"></span><br><span class="line">test_sgd = SGD()</span><br><span class="line">optimizer_info = &#123;<span class="string">&quot;step_size&quot;</span>: <span class="number">0.3</span>&#125;</span><br><span class="line">test_sgd.optimizer_init(optimizer_info)</span><br><span class="line">updated_weights = test_sgd.update_weights(weights, g)</span><br><span class="line"></span><br><span class="line"><span class="comment"># updated weights asserts</span></span><br><span class="line">updated_weights_answer = np.load(<span class="string">&quot;asserts/update_weights_updated_weights.npz&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>], updated_weights_answer[<span class="string">&quot;W0&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>], updated_weights_answer[<span class="string">&quot;b0&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>], updated_weights_answer[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(updated_weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>], updated_weights_answer[<span class="string">&quot;b1&quot;</span>]))</span><br></pre></td></tr></table></figure>
<p><strong>Expected output</strong>:</p>
<pre><code>updated_weights[0][&quot;W&quot;]
 [[ 1.17899492  0.53656321]
 [ 0.58008221  1.47666572]
 [ 1.01909411 -1.10248056]
 [ 0.72490408  0.06828853]
 [-0.20609725  0.69034095]] 

updated_weights[0][&quot;b&quot;]
 [[-0.18484533  0.92844539]] 

updated_weights[1][&quot;W&quot;]
 [[0.70488257]
 [0.58150878]] 

updated_weights[1][&quot;b&quot;]
 [[0.88467086]] </code></pre>
<h3 id="adam-algorithm">Adam Algorithm</h3>
<p>In this assignment, instead of using SGD for updating the weights, we use a more advanced algorithm called Adam. The Adam algorithm improves the SGD update with two concepts: adaptive vector step-sizes and momentum. It keeps estimates of the mean and second moment of the updates, denoted by <span class="math inline">\(\mathbf{m}\)</span> and <span class="math inline">\(\mathbf{v}\)</span> respectively: <span class="math display">\[\mathbf{m_t} = \beta_m \mathbf{m_{t-1}} + (1 - \beta_m)g_t \\
\mathbf{v_t} = \beta_v \mathbf{v_{t-1}} + (1 - \beta_v)g^2_t
\]</span></p>
<p>Given that <span class="math inline">\(\mathbf{m}\)</span> and <span class="math inline">\(\mathbf{v}\)</span> are initialized to zero, they are biased toward zero. To get unbiased estimates of the mean and second moment, Adam defines <span class="math inline">\(\mathbf{\hat{m}}\)</span> and <span class="math inline">\(\mathbf{\hat{v}}\)</span> as: <span class="math display">\[ \mathbf{\hat{m_t}} = \frac{\mathbf{m_t}}{1 - \beta_m^t} \\
\mathbf{\hat{v_t}} = \frac{\mathbf{v_t}}{1 - \beta_v^t}
\]</span></p>
<p>The weights are then updated as follows: <span class="math display">\[ \mathbf{w_t} = \mathbf{w_{t-1}} + \frac{\alpha}{\sqrt{\mathbf{\hat{v_t}}}+\epsilon} \mathbf{\hat{m_t}}
\]</span></p>
<p>When implementing the agent you will use the Adam algorithm instead of SGD because it is more efficient. We have already provided you the implementation of the Adam algorithm in the cell below. You will use it when implementing your agent.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adam</span>(<span class="params">BaseOptimizer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">optimizer_init</span>(<span class="params">self, optimizer_info</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Setup for the optimizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Set parameters needed to setup the Adam algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Assume optimizer_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states: integer,</span></span><br><span class="line"><span class="string">            num_hidden_layer: integer,</span></span><br><span class="line"><span class="string">            num_hidden_units: integer,</span></span><br><span class="line"><span class="string">            step_size: float, </span></span><br><span class="line"><span class="string">            self.beta_m: float</span></span><br><span class="line"><span class="string">            self.beta_v: float</span></span><br><span class="line"><span class="string">            self.epsilon: float</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        self.num_states = optimizer_info.get(<span class="string">&quot;num_states&quot;</span>)</span><br><span class="line">        self.num_hidden_layer = optimizer_info.get(<span class="string">&quot;num_hidden_layer&quot;</span>)</span><br><span class="line">        self.num_hidden_units = optimizer_info.get(<span class="string">&quot;num_hidden_units&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Specify Adam algorithm&#x27;s hyper parameters</span></span><br><span class="line">        self.step_size = optimizer_info.get(<span class="string">&quot;step_size&quot;</span>)</span><br><span class="line">        self.beta_m = optimizer_info.get(<span class="string">&quot;beta_m&quot;</span>)</span><br><span class="line">        self.beta_v = optimizer_info.get(<span class="string">&quot;beta_v&quot;</span>)</span><br><span class="line">        self.epsilon = optimizer_info.get(<span class="string">&quot;epsilon&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.layer_size = np.array([self.num_states, self.num_hidden_units, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize Adam algorithm&#x27;s m and v</span></span><br><span class="line">        self.m = [<span class="built_in">dict</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">        self.v = [<span class="built_in">dict</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_hidden_layer+<span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Initialize self.m[i][&quot;W&quot;], self.m[i][&quot;b&quot;], self.v[i][&quot;W&quot;], self.v[i][&quot;b&quot;] to zero</span></span><br><span class="line">            self.m[i][<span class="string">&quot;W&quot;</span>] = np.zeros((self.layer_size[i], self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line">            self.m[i][<span class="string">&quot;b&quot;</span>] = np.zeros((<span class="number">1</span>, self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line">            self.v[i][<span class="string">&quot;W&quot;</span>] = np.zeros((self.layer_size[i], self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line">            self.v[i][<span class="string">&quot;b&quot;</span>] = np.zeros((<span class="number">1</span>, self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize beta_m_product and beta_v_product to be later used for computing m_hat and v_hat</span></span><br><span class="line">        self.beta_m_product = self.beta_m</span><br><span class="line">        self.beta_v_product = self.beta_v</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_weights</span>(<span class="params">self, weights, g</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Given weights and update g, return updated weights</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(weights)):</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> weights[i].keys():</span><br><span class="line"></span><br><span class="line">                <span class="comment">### update self.m and self.v</span></span><br><span class="line">                self.m[i][param] = self.beta_m * self.m[i][param] + (<span class="number">1</span> - self.beta_m) * g[i][param]</span><br><span class="line">                self.v[i][param] = self.beta_v * self.v[i][param] + (<span class="number">1</span> - self.beta_v) * (g[i][param] * g[i][param])</span><br><span class="line"></span><br><span class="line">                <span class="comment">### compute m_hat and v_hat</span></span><br><span class="line">                m_hat = self.m[i][param] / (<span class="number">1</span> - self.beta_m_product)</span><br><span class="line">                v_hat = self.v[i][param] / (<span class="number">1</span> - self.beta_v_product)</span><br><span class="line"></span><br><span class="line">                <span class="comment">### update weights</span></span><br><span class="line">                weights[i][param] += self.step_size * m_hat / (np.sqrt(v_hat) + self.epsilon)</span><br><span class="line">                </span><br><span class="line">        <span class="comment">### update self.beta_m_product and self.beta_v_product</span></span><br><span class="line">        self.beta_m_product *= self.beta_m</span><br><span class="line">        self.beta_v_product *= self.beta_v</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> weights</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="implement-agent-methods">1-2: Implement Agent Methods</h2>
<p>In this section, you will implement <code>agent_init()</code>, <code>agent_start()</code>, <code>agent_step()</code>, and <code>agent_end()</code>.</p>
<p>In <code>agent_init()</code>, you will:</p>
<ul>
<li>specify the neural network structure by filling self.layer_size with the size of the input layer, hidden layer, and output layer.</li>
<li>initialize the network's parameters. We show the parameters as an array of dictionaries, self.weights, where each dictionary corresponds to weights from one layer to the next. Each dictionary includes <span class="math inline">\(W\)</span> and <span class="math inline">\(b\)</span>.</li>
</ul>
<p><span class="math display">\[\sqrt{ \frac{2}{ input \, of \, each \, node } }\]</span></p>
<p>This initialization heuristic is commonly used when using ReLU gates and helps keep the output of a neuron from getting too big or too small. To initialize the network's parameters, use <strong>self.rand_generator.normal()</strong> which draws random samples from a normal distribution. The parameters of self.rand_generator.normal are mean of the distribution, standard deviation of the distribution, and output shape in the form of tuple of integers.</p>
<p>In <code>agent_start()</code>, you will: - specify self.last_state and self.last_action.</p>
<p>In <code>agent_step()</code> and <code>agent_end()</code>, you will: - compute the TD error using <span class="math inline">\(v(S_t)\)</span> and <span class="math inline">\(v(S_{t+1})\)</span>. To compute the value function for <span class="math inline">\(S_t\)</span> and <span class="math inline">\(S_{t+1}\)</span>, you will get their one-hot encoding using <code>one_hot()</code> method that we provided below. You feed the one-hot encoded state number to the neural networks using <code>get_value()</code> method that you implemented above. Note that <code>one_hot()</code> method returns the one-hot encoding of a state as a numpy array of shape (1, num_states). - retrieve the gradients using <code>get_gradient()</code> function that you implemented. - use Adam_algorithm that we provided to update the neural network's parameters, self.weights. - use <code>agent_policy()</code> method to select actions with. (only in <code>agent_step()</code>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot</span>(<span class="params">state, num_states</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Given num_state and a state, return the one-hot encoding of the state</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Create the one-hot encoding of state</span></span><br><span class="line">    <span class="comment"># one_hot_vector is a numpy array of shape (1, num_states)</span></span><br><span class="line">    </span><br><span class="line">    one_hot_vector = np.zeros((<span class="number">1</span>, num_states))</span><br><span class="line">    one_hot_vector[<span class="number">0</span>, <span class="built_in">int</span>((state - <span class="number">1</span>))] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> one_hot_vector</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TDAgent</span>(<span class="params">BaseAgent</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.name = <span class="string">&quot;td_agent&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span>(<span class="params">self, agent_info=&#123;&#125;</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Set parameters needed to setup the semi-gradient TD with a Neural Network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Assume agent_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states: integer,</span></span><br><span class="line"><span class="string">            num_hidden_layer: integer,</span></span><br><span class="line"><span class="string">            num_hidden_units: integer,</span></span><br><span class="line"><span class="string">            step_size: float, </span></span><br><span class="line"><span class="string">            discount_factor: float,</span></span><br><span class="line"><span class="string">            self.beta_m: float</span></span><br><span class="line"><span class="string">            self.beta_v: float</span></span><br><span class="line"><span class="string">            self.epsilon: float</span></span><br><span class="line"><span class="string">            seed: int</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Set random seed for weights initialization for each run</span></span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">&quot;seed&quot;</span>)) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set random seed for policy for each run</span></span><br><span class="line">        self.policy_rand_generator = np.random.RandomState(agent_info.get(<span class="string">&quot;seed&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set attributes according to agent_info</span></span><br><span class="line">        self.num_states = agent_info.get(<span class="string">&quot;num_states&quot;</span>)</span><br><span class="line">        self.num_hidden_layer = agent_info.get(<span class="string">&quot;num_hidden_layer&quot;</span>)</span><br><span class="line">        self.num_hidden_units = agent_info.get(<span class="string">&quot;num_hidden_units&quot;</span>)</span><br><span class="line">        self.discount_factor = agent_info.get(<span class="string">&quot;discount_factor&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Define the neural network&#x27;s structure</span></span><br><span class="line">        <span class="comment"># Specify self.layer_size which shows the number of nodes in each layer</span></span><br><span class="line">        <span class="comment"># self.layer_size = np.array([None, None, None])</span></span><br><span class="line">        <span class="comment"># Hint: Checkout the NN diagram at the beginning of the notebook</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.layer_size = np.array([self.num_states, self.num_hidden_units, <span class="number">1</span>])</span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize the neural network&#x27;s parameter</span></span><br><span class="line">        self.weights = [<span class="built_in">dict</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_hidden_layer+<span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">            <span class="comment">### Initialize self.weights[i][&quot;W&quot;] and self.weights[i][&quot;b&quot;] using self.rand_generator.normal()</span></span><br><span class="line">            <span class="comment"># Note that The parameters of self.rand_generator.normal are mean of the distribution, </span></span><br><span class="line">            <span class="comment"># standard deviation of the distribution, and output shape in the form of tuple of integers.</span></span><br><span class="line">            <span class="comment"># To specify output shape, use self.layer_size.</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># ----------------</span></span><br><span class="line">            <span class="comment"># your code here</span></span><br><span class="line">            self.weights[i][<span class="string">&quot;W&quot;</span>] = self.rand_generator.normal(loc=<span class="number">0</span>, scale= np.sqrt(<span class="number">2.0</span> / self.layer_size[i]) ,size = (self.layer_size[i],self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line">            self.weights[i][<span class="string">&quot;b&quot;</span>] = self.rand_generator.normal(loc=<span class="number">0</span>, scale= np.sqrt(<span class="number">2.0</span> / self.layer_size[i]) ,size = (<span class="number">1</span>,self.layer_size[i+<span class="number">1</span>]))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># ----------------</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Specify the optimizer</span></span><br><span class="line">        self.optimizer = Adam()</span><br><span class="line">        self.optimizer.optimizer_init(&#123;</span><br><span class="line">            <span class="string">&quot;num_states&quot;</span>: agent_info[<span class="string">&quot;num_states&quot;</span>],</span><br><span class="line">            <span class="string">&quot;num_hidden_layer&quot;</span>: agent_info[<span class="string">&quot;num_hidden_layer&quot;</span>],</span><br><span class="line">            <span class="string">&quot;num_hidden_units&quot;</span>: agent_info[<span class="string">&quot;num_hidden_units&quot;</span>],</span><br><span class="line">            <span class="string">&quot;step_size&quot;</span>: agent_info[<span class="string">&quot;step_size&quot;</span>],</span><br><span class="line">            <span class="string">&quot;beta_m&quot;</span>: agent_info[<span class="string">&quot;beta_m&quot;</span>],</span><br><span class="line">            <span class="string">&quot;beta_v&quot;</span>: agent_info[<span class="string">&quot;beta_v&quot;</span>],</span><br><span class="line">            <span class="string">&quot;epsilon&quot;</span>: agent_info[<span class="string">&quot;epsilon&quot;</span>],</span><br><span class="line">        &#125;)</span><br><span class="line">        </span><br><span class="line">        self.last_state = <span class="literal">None</span></span><br><span class="line">        self.last_action = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_policy</span>(<span class="params">self, state</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Set chosen_action as 0 or 1 with equal probability. </span></span><br><span class="line">        chosen_action = self.policy_rand_generator.choice([<span class="number">0</span>,<span class="number">1</span>])    </span><br><span class="line">        <span class="keyword">return</span> chosen_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;The first method called when the experiment starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">                environment&#x27;s evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The first action the agent takes.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">### select action given state (using self.agent_policy()), and save current state and action</span></span><br><span class="line">        <span class="comment"># self.last_state = ?</span></span><br><span class="line">        <span class="comment"># self.last_action = ?</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.last_action = self.agent_policy(state)</span><br><span class="line">        self.last_state = state</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, state</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">                environment&#x27;s step based, where the agent ended up after the</span></span><br><span class="line"><span class="string">                last step</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The action the agent is taking.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        last_state_vec = one_hot(self.last_state, self.num_states)</span><br><span class="line">        last_value = get_value(last_state_vec, self.weights)</span><br><span class="line"></span><br><span class="line">        state_vec = one_hot(state, self.num_states)</span><br><span class="line">        value = get_value(state_vec, self.weights)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### Compute TD error</span></span><br><span class="line">        <span class="comment"># delta = None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        delta = reward + self.discount_factor * value - last_value</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Retrieve gradients</span></span><br><span class="line">        <span class="comment"># grads = None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        grads = get_gradient(last_state_vec, self.weights)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Compute g (1 line)</span></span><br><span class="line">        g = [<span class="built_in">dict</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_hidden_layer+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> self.weights[i].keys():</span><br><span class="line"></span><br><span class="line">                <span class="comment"># g[i][param] = None</span></span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line">                <span class="comment"># your code here</span></span><br><span class="line">                g[i][param] = grads[i][param] * delta</span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### update the weights using self.optimizer</span></span><br><span class="line">        <span class="comment"># self.weights = None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.optimizer.update_weights(self.weights, g)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### update self.last_state and self.last_action</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.last_action = self.agent_policy(state)</span><br><span class="line">        self.last_state = state</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        last_state_vec = one_hot(self.last_state, self.num_states)</span><br><span class="line">        last_value = get_value(last_state_vec, self.weights)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### compute TD error</span></span><br><span class="line">        <span class="comment"># delta = None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        delta = reward - last_value</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Retrieve gradients</span></span><br><span class="line">        <span class="comment"># grads = None</span></span><br><span class="line">        grads = get_gradient(last_state_vec, self.weights)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### Compute g</span></span><br><span class="line">        g = [<span class="built_in">dict</span>() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_hidden_layer+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_hidden_layer+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> self.weights[i].keys():</span><br><span class="line"></span><br><span class="line">                <span class="comment"># g[i][param] = None</span></span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line">                <span class="comment"># your code here</span></span><br><span class="line">                g[i][param] = grads[i][param] * delta</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### update the weights using self.optimizer</span></span><br><span class="line">        <span class="comment"># self.weights = None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.optimizer.update_weights(self.weights, g)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_message</span>(<span class="params">self, message</span>):</span></span><br><span class="line">        <span class="keyword">if</span> message == <span class="string">&#x27;get state value&#x27;</span>:</span><br><span class="line">            state_value = np.zeros(self.num_states)</span><br><span class="line">            <span class="keyword">for</span> state <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, self.num_states + <span class="number">1</span>):</span><br><span class="line">                s = one_hot(state, self.num_states)</span><br><span class="line">                state_value[state - <span class="number">1</span>] = get_value(s, self.weights)</span><br><span class="line">            <span class="keyword">return</span> state_value</span><br></pre></td></tr></table></figure>
<p>Run the following code to test your implementation of the <code>agent_init()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">&quot;num_states&quot;</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">&quot;num_hidden_layer&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;num_hidden_units&quot;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">&quot;step_size&quot;</span>: <span class="number">0.25</span>,</span><br><span class="line">    <span class="string">&quot;discount_factor&quot;</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">&quot;beta_m&quot;</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">&quot;beta_v&quot;</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.0001</span>,</span><br><span class="line">    <span class="string">&quot;seed&quot;</span>: <span class="number">0</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_agent = TDAgent()</span><br><span class="line">test_agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;layer_size: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_agent.layer_size))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.layer_size, np.array([agent_info[<span class="string">&quot;num_states&quot;</span>], </span><br><span class="line">                                                    agent_info[<span class="string">&quot;num_hidden_units&quot;</span>], </span><br><span class="line">                                                    <span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(test_agent.weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>].shape == (agent_info[<span class="string">&quot;num_states&quot;</span>], agent_info[<span class="string">&quot;num_hidden_units&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(test_agent.weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>].shape == (<span class="number">1</span>, agent_info[<span class="string">&quot;num_hidden_units&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(test_agent.weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>].shape == (agent_info[<span class="string">&quot;num_hidden_units&quot;</span>], <span class="number">1</span>))</span><br><span class="line"><span class="keyword">assert</span>(test_agent.weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>].shape == (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">agent_weight_answer = np.load(<span class="string">&quot;asserts/agent_init_weights_1.npz&quot;</span>)</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>], agent_weight_answer[<span class="string">&quot;W0&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>], agent_weight_answer[<span class="string">&quot;b0&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>], agent_weight_answer[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>], agent_weight_answer[<span class="string">&quot;b1&quot;</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>layer_size: [5 2 1]</code></pre>
<p><strong>Expected output</strong>:</p>
<pre><code>layer_size: [5 2 1]
weights[0][&quot;W&quot;] shape: (5, 2)
weights[0][&quot;b&quot;] shape: (1, 2)
weights[1][&quot;W&quot;] shape: (2, 1)
weights[1][&quot;b&quot;] shape: (1, 1) 

weights[0][&quot;W&quot;]
 [[ 1.11568467  0.25308164]
 [ 0.61900825  1.4172653 ]
 [ 1.18114738 -0.6180848 ]
 [ 0.60088868 -0.0957267 ]
 [-0.06528133  0.25968529]] 

weights[0][&quot;b&quot;]
 [[0.09110115 0.91976332]] 

weights[1][&quot;W&quot;]
 [[0.76103773]
 [0.12167502]] 

weights[1][&quot;b&quot;]
 [[0.44386323]]</code></pre>
<p>Run the following code to test your implementation of the <code>agent_start()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">&quot;num_states&quot;</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">&quot;num_hidden_layer&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;num_hidden_units&quot;</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">&quot;discount_factor&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&quot;beta_m&quot;</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">&quot;beta_v&quot;</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.0001</span>,</span><br><span class="line">    <span class="string">&quot;seed&quot;</span>: <span class="number">10</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose state = 250</span></span><br><span class="line">state = <span class="number">250</span></span><br><span class="line"></span><br><span class="line">test_agent = TDAgent()</span><br><span class="line">test_agent.agent_init(agent_info)</span><br><span class="line">test_agent.agent_start(state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(test_agent.last_state == <span class="number">250</span>)</span><br><span class="line"><span class="keyword">assert</span>(test_agent.last_action == <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Expected output</strong>:</p>
<pre><code>Agent state: 250
Agent selected action: 1</code></pre>
<p>Run the following code to test your implementation of the <code>agent_step()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">&quot;num_states&quot;</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">&quot;num_hidden_layer&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;num_hidden_units&quot;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">&quot;discount_factor&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&quot;beta_m&quot;</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">&quot;beta_v&quot;</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.0001</span>,</span><br><span class="line">    <span class="string">&quot;seed&quot;</span>: <span class="number">0</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_agent = TDAgent()</span><br><span class="line">test_agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load initial weights</span></span><br><span class="line">agent_initial_weight = np.load(<span class="string">&quot;asserts/agent_step_initial_weights.npz&quot;</span>)</span><br><span class="line">test_agent.weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>] = agent_initial_weight[<span class="string">&quot;W0&quot;</span>]</span><br><span class="line">test_agent.weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>] = agent_initial_weight[<span class="string">&quot;b0&quot;</span>]</span><br><span class="line">test_agent.weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>] = agent_initial_weight[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">test_agent.weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>] = agent_initial_weight[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># load m and v for the optimizer</span></span><br><span class="line">m_data = np.load(<span class="string">&quot;asserts/agent_step_initial_m.npz&quot;</span>)</span><br><span class="line">test_agent.optimizer.m[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>] = m_data[<span class="string">&quot;W0&quot;</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>] = m_data[<span class="string">&quot;b0&quot;</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>] = m_data[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>] = m_data[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line"></span><br><span class="line">v_data = np.load(<span class="string">&quot;asserts/agent_step_initial_v.npz&quot;</span>)</span><br><span class="line">test_agent.optimizer.v[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>] = v_data[<span class="string">&quot;W0&quot;</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>] = v_data[<span class="string">&quot;b0&quot;</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>] = v_data[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>] = v_data[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the agent started at State 3</span></span><br><span class="line">start_state = <span class="number">3</span></span><br><span class="line">test_agent.agent_start(start_state)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the reward was 10.0 and the next state observed was State 1</span></span><br><span class="line">reward = <span class="number">10.0</span></span><br><span class="line">next_state = <span class="number">1</span></span><br><span class="line">test_agent.agent_step(reward, next_state)</span><br><span class="line"></span><br><span class="line">agent_updated_weight_answer = np.load(<span class="string">&quot;asserts/agent_step_updated_weights.npz&quot;</span>)</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>], agent_updated_weight_answer[<span class="string">&quot;W0&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>], agent_updated_weight_answer[<span class="string">&quot;b0&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>], agent_updated_weight_answer[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>], agent_updated_weight_answer[<span class="string">&quot;b1&quot;</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(test_agent.last_state == <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span>(test_agent.last_action == <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Expected output</strong>:</p>
<pre><code>updated_weights[0][&quot;W&quot;]
 [[ 1.10893459  0.30763738]
 [ 0.63690565  1.14778865]
 [ 1.23397791 -0.48152743]
 [ 0.72792093 -0.15829832]
 [ 0.15021996  0.39822163]] 

updated_weights[0][&quot;b&quot;]
 [[0.29798822 0.96254535]] 

updated_weights[1][&quot;W&quot;]
 [[0.76628754]
 [0.11486511]] 

updated_weights[1][&quot;b&quot;]
 [[0.58530057]] 

Agent last state: 1
Agent last action: 1 </code></pre>
<p>Run the following code to test your implementation of the <code>agent_end()</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">&quot;num_states&quot;</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">&quot;num_hidden_layer&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;num_hidden_units&quot;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">&quot;discount_factor&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&quot;beta_m&quot;</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">&quot;beta_v&quot;</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.0001</span>,</span><br><span class="line">    <span class="string">&quot;seed&quot;</span>: <span class="number">0</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">test_agent = TDAgent()</span><br><span class="line">test_agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load initial weights</span></span><br><span class="line">agent_initial_weight = np.load(<span class="string">&quot;asserts/agent_end_initial_weights.npz&quot;</span>)</span><br><span class="line">test_agent.weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>] = agent_initial_weight[<span class="string">&quot;W0&quot;</span>]</span><br><span class="line">test_agent.weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>] = agent_initial_weight[<span class="string">&quot;b0&quot;</span>]</span><br><span class="line">test_agent.weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>] = agent_initial_weight[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">test_agent.weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>] = agent_initial_weight[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># load m and v for the optimizer</span></span><br><span class="line">m_data = np.load(<span class="string">&quot;asserts/agent_step_initial_m.npz&quot;</span>)</span><br><span class="line">test_agent.optimizer.m[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>] = m_data[<span class="string">&quot;W0&quot;</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>] = m_data[<span class="string">&quot;b0&quot;</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>] = m_data[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">test_agent.optimizer.m[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>] = m_data[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line"></span><br><span class="line">v_data = np.load(<span class="string">&quot;asserts/agent_step_initial_v.npz&quot;</span>)</span><br><span class="line">test_agent.optimizer.v[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>] = v_data[<span class="string">&quot;W0&quot;</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>] = v_data[<span class="string">&quot;b0&quot;</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>] = v_data[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">test_agent.optimizer.v[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>] = v_data[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the agent started at State 4</span></span><br><span class="line">start_state = <span class="number">4</span></span><br><span class="line">test_agent.agent_start(start_state)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the reward was 10.0 and reached the terminal state</span></span><br><span class="line">reward = <span class="number">10.0</span></span><br><span class="line">test_agent.agent_end(reward)</span><br><span class="line"></span><br><span class="line"><span class="comment"># updated weights asserts</span></span><br><span class="line">agent_updated_weight_answer = np.load(<span class="string">&quot;asserts/agent_end_updated_weights.npz&quot;</span>)</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">&quot;W&quot;</span>], agent_updated_weight_answer[<span class="string">&quot;W0&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">0</span>][<span class="string">&quot;b&quot;</span>], agent_updated_weight_answer[<span class="string">&quot;b0&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">&quot;W&quot;</span>], agent_updated_weight_answer[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="keyword">assert</span>(np.allclose(test_agent.weights[<span class="number">1</span>][<span class="string">&quot;b&quot;</span>], agent_updated_weight_answer[<span class="string">&quot;b1&quot;</span>]))</span><br></pre></td></tr></table></figure>
<p><strong>Expected output:</strong></p>
<pre><code>updated_weights[0][&quot;W&quot;]
 [[ 1.10893459  0.30763738]
 [ 0.63690565  1.14778865]
 [ 1.17531054 -0.51043162]
 [ 0.75062903 -0.13736817]
 [ 0.15021996  0.39822163]] 

updated_weights[0][&quot;b&quot;]
 [[0.30846523 0.95937346]] 

updated_weights[1][&quot;W&quot;]
 [[0.68861703]
 [0.15986364]] 

updated_weights[1][&quot;b&quot;]
 [[0.586074]] </code></pre>
<h2 id="section-2---run-experiment">Section 2 - Run Experiment</h2>
<p>Now that you implemented the agent, we can run the experiment. Similar to Course 3 Programming Assignment 1, we will plot the learned state value function and the learning curve of the TD agent. To plot the learning curve, we use Root Mean Squared Value Error (RMSVE).</p>
<h2 id="run-experiment-for-semi-gradient-td-with-a-neural-network">2-1: Run Experiment for Semi-gradient TD with a Neural Network</h2>
<p>We have already provided you the experiment/plot code, so you can go ahead and run the two cells below.</p>
<p>Note that running the cell below will take <strong>approximately 12 minutes</strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">true_state_val = np.load(<span class="string">&#x27;data/true_V.npy&#x27;</span>)    </span><br><span class="line">state_distribution = np.load(<span class="string">&#x27;data/state_distribution.npy&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_RMSVE</span>(<span class="params">learned_state_val</span>):</span></span><br><span class="line">    <span class="keyword">assert</span>(<span class="built_in">len</span>(true_state_val) == <span class="built_in">len</span>(learned_state_val) == <span class="built_in">len</span>(state_distribution))</span><br><span class="line">    MSVE = np.<span class="built_in">sum</span>(np.multiply(state_distribution, np.square(true_state_val - learned_state_val)))</span><br><span class="line">    RMSVE = np.sqrt(MSVE)</span><br><span class="line">    <span class="keyword">return</span> RMSVE</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define function to run experiment</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span>(<span class="params">environment, agent, environment_parameters, agent_parameters, experiment_parameters</span>):</span></span><br><span class="line">    </span><br><span class="line">    rl_glue = RLGlue(environment, agent)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># save rmsve at the end of each episode</span></span><br><span class="line">    agent_rmsve = np.zeros((experiment_parameters[<span class="string">&quot;num_runs&quot;</span>], </span><br><span class="line">                            <span class="built_in">int</span>(experiment_parameters[<span class="string">&quot;num_episodes&quot;</span>]/experiment_parameters[<span class="string">&quot;episode_eval_frequency&quot;</span>]) + <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># save learned state value at the end of each run</span></span><br><span class="line">    agent_state_val = np.zeros((experiment_parameters[<span class="string">&quot;num_runs&quot;</span>], </span><br><span class="line">                                environment_parameters[<span class="string">&quot;num_states&quot;</span>]))</span><br><span class="line"></span><br><span class="line">    env_info = &#123;<span class="string">&quot;num_states&quot;</span>: environment_parameters[<span class="string">&quot;num_states&quot;</span>],</span><br><span class="line">                <span class="string">&quot;start_state&quot;</span>: environment_parameters[<span class="string">&quot;start_state&quot;</span>],</span><br><span class="line">                <span class="string">&quot;left_terminal_state&quot;</span>: environment_parameters[<span class="string">&quot;left_terminal_state&quot;</span>],</span><br><span class="line">                <span class="string">&quot;right_terminal_state&quot;</span>: environment_parameters[<span class="string">&quot;right_terminal_state&quot;</span>]&#125;</span><br><span class="line"></span><br><span class="line">    agent_info = &#123;<span class="string">&quot;num_states&quot;</span>: environment_parameters[<span class="string">&quot;num_states&quot;</span>],</span><br><span class="line">                  <span class="string">&quot;num_hidden_layer&quot;</span>: agent_parameters[<span class="string">&quot;num_hidden_layer&quot;</span>],</span><br><span class="line">                  <span class="string">&quot;num_hidden_units&quot;</span>: agent_parameters[<span class="string">&quot;num_hidden_units&quot;</span>],</span><br><span class="line">                  <span class="string">&quot;step_size&quot;</span>: agent_parameters[<span class="string">&quot;step_size&quot;</span>],</span><br><span class="line">                  <span class="string">&quot;discount_factor&quot;</span>: environment_parameters[<span class="string">&quot;discount_factor&quot;</span>],</span><br><span class="line">                  <span class="string">&quot;beta_m&quot;</span>: agent_parameters[<span class="string">&quot;beta_m&quot;</span>],</span><br><span class="line">                  <span class="string">&quot;beta_v&quot;</span>: agent_parameters[<span class="string">&quot;beta_v&quot;</span>],</span><br><span class="line">                  <span class="string">&quot;epsilon&quot;</span>: agent_parameters[<span class="string">&quot;epsilon&quot;</span>]</span><br><span class="line">                 &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Setting - Neural Network with 100 hidden units&#x27;</span>)</span><br><span class="line">    os.system(<span class="string">&#x27;sleep 1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># one agent setting</span></span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">1</span>, experiment_parameters[<span class="string">&quot;num_runs&quot;</span>]+<span class="number">1</span>)):</span><br><span class="line">        env_info[<span class="string">&quot;seed&quot;</span>] = run</span><br><span class="line">        agent_info[<span class="string">&quot;seed&quot;</span>] = run</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute initial RMSVE before training</span></span><br><span class="line">        current_V = rl_glue.rl_agent_message(<span class="string">&quot;get state value&quot;</span>)</span><br><span class="line">        agent_rmsve[run-<span class="number">1</span>, <span class="number">0</span>] = calc_RMSVE(current_V)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, experiment_parameters[<span class="string">&quot;num_episodes&quot;</span>]+<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># run episode</span></span><br><span class="line">            rl_glue.rl_episode(<span class="number">0</span>) <span class="comment"># no step limit</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> episode % experiment_parameters[<span class="string">&quot;episode_eval_frequency&quot;</span>] == <span class="number">0</span>:</span><br><span class="line">                current_V = rl_glue.rl_agent_message(<span class="string">&quot;get state value&quot;</span>)</span><br><span class="line">                agent_rmsve[run-<span class="number">1</span>, <span class="built_in">int</span>(episode/experiment_parameters[<span class="string">&quot;episode_eval_frequency&quot;</span>])] = calc_RMSVE(current_V)</span><br><span class="line">            <span class="keyword">elif</span> episode == experiment_parameters[<span class="string">&quot;num_episodes&quot;</span>]: <span class="comment"># if last episode</span></span><br><span class="line">                current_V = rl_glue.rl_agent_message(<span class="string">&quot;get state value&quot;</span>)</span><br><span class="line"></span><br><span class="line">        agent_state_val[run-<span class="number">1</span>, :] = current_V</span><br><span class="line"></span><br><span class="line">    save_name = <span class="string">&quot;&#123;&#125;&quot;</span>.<span class="built_in">format</span>(rl_glue.agent.name).replace(<span class="string">&#x27;.&#x27;</span>,<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;results&#x27;</span>):</span><br><span class="line">                os.makedirs(<span class="string">&#x27;results&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># save avg. state value</span></span><br><span class="line">    np.save(<span class="string">&quot;results/V_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(save_name), agent_state_val)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># save avg. rmsve</span></span><br><span class="line">    np.savez(<span class="string">&quot;results/RMSVE_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(save_name), rmsve = agent_rmsve,</span><br><span class="line">                                                   eval_freq = experiment_parameters[<span class="string">&quot;episode_eval_frequency&quot;</span>],</span><br><span class="line">                                                   num_episodes = experiment_parameters[<span class="string">&quot;num_episodes&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Run Experiment</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;num_runs&quot;</span> : <span class="number">20</span>,</span><br><span class="line">    <span class="string">&quot;num_episodes&quot;</span> : <span class="number">1000</span>,</span><br><span class="line">    <span class="string">&quot;episode_eval_frequency&quot;</span> : <span class="number">10</span> <span class="comment"># evaluate every 10 episode</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;num_states&quot;</span> : <span class="number">500</span>,</span><br><span class="line">    <span class="string">&quot;start_state&quot;</span> : <span class="number">250</span>,</span><br><span class="line">    <span class="string">&quot;left_terminal_state&quot;</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="string">&quot;right_terminal_state&quot;</span> : <span class="number">501</span>,</span><br><span class="line">    <span class="string">&quot;discount_factor&quot;</span> : <span class="number">1.0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;num_hidden_layer&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;num_hidden_units&quot;</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="string">&quot;step_size&quot;</span>: <span class="number">0.001</span>,</span><br><span class="line">    <span class="string">&quot;beta_m&quot;</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">&quot;beta_v&quot;</span>: <span class="number">0.999</span>,</span><br><span class="line">    <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.0001</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = RandomWalkEnvironment</span><br><span class="line">current_agent = TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># run experiment</span></span><br><span class="line">run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot result</span></span><br><span class="line">plot_script.plot_result([<span class="string">&quot;td_agent&quot;</span>])</span><br><span class="line"></span><br><span class="line">shutil.make_archive(<span class="string">&#x27;results&#x27;</span>, <span class="string">&#x27;zip&#x27;</span>, <span class="string">&#x27;results&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Setting - Neural Network with 100 hidden units</code></pre>
<figure>
<img src="output_39_2.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<pre><code>&#39;/home/jovyan/work/release/TD-NN/results.zip&#39;</code></pre>
<p>You plotted the learning curve for 1000 episodes. As you can see the RMSVE is still decreasing. Here we provide the pre-computed result for 5000 episodes and 20 runs so that you can see the performance of semi-gradient TD with a neural network after being trained for a long time.</p>
<p><img src="nn_5000_episodes.png" /></p>
<p>Does semi-gradient TD with a neural network find a good approximation within 5000 episodes?</p>
<p>As you may remember from the previous assignment, semi-gradient TD with 10-state aggregation converged within 100 episodes. Why is TD with a neural network slower?</p>
<p>Would it be faster if we decrease the number of hidden units? Or what about if we increase the number of hidden units?</p>
<h2 id="compare-performance-of-semi-gradient-td-with-a-neural-network-and-semi-gradient-td-with-tile-coding">2-2: Compare Performance of Semi-gradient TD with a Neural Network and Semi-gradient TD with Tile-coding</h2>
<p>In this section, we compare the performance of semi-gradient TD with a Neural Network and semi-gradient TD with tile-coding. Tile-coding is a kind of coarse coding that uses multiple overlapping partitions of the state space to produce features. For tile-coding, we used 50 tilings each with 6 tiles. We set the step-size for semi-gradient TD with tile-coding to <span class="math inline">\(\frac{0.1}{tilings}\)</span>. See the figure below for the comparison between semi-gradient TD with tile-coding and semi-gradient TD with a neural network and Adam algorithm. This result is for 5000 episodes and 20 runs:</p>
<p><img src="nn_vs_tc.png" /></p>
<p>How are the results?</p>
<p>Semi-gradient TD with tile-coding is much faster than semi-gradient TD with a neural network. Why?</p>
<p>Which method has a lower RMSVE at the end of 5000 episodes?</p>
<h3 id="wrapping-up">Wrapping up!</h3>
<p>You have successfully implemented Course 3 Programming Assignment 2.</p>
<p>You have implemented <strong>semi-gradient TD with a Neural Network and Adam algorithm</strong> in 500-state Random Walk.</p>
<p>You also compared semi-gradient TD with a neural network and semi-gradient TD with tile-coding.</p>
<p>From the experiments and lectures, you should be more familiar with some of the strengths and weaknesses of using neural networks as the function approximator for an RL agent. On one hand, neural networks are powerful function approximators capable of representing a wide class of functions. They are also capable of producing features without exclusively relying on hand-crafted mechanisms. On the other hand, compared to a linear function approximator with tile-coding, neural networks can be less sample efficient. When implementing your own Reinforcement Learning agents, you may consider these strengths and weaknesses to choose the proper function approximator for your problems.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">268</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail → mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'en' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
