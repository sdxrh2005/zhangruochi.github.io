<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="This is the lecture note of Upitt course INFSCI 2160 DATA MINING">
<meta property="og:type" content="article">
<meta property="og:title" content="INFSCI 2160 DATA MINING(PART ONE)">
<meta property="og:url" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="This is the lecture note of Upitt course INFSCI 2160 DATA MINING">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/29AF0C1A006683910109C1BA949541C4.png">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/83DD4C0B5649752C8A1EAE6A78058344.png">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/9784E656D9E9D2613CF3D07CA064D3AA.jpg">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/CA80BF1FC8D3AA010CDDFE5A646489BF.png">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/BFE33EA434D8355137098AEF9241AEEC.png">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/3FB3E0B341C2DD82395CB962430B6E34.png">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/youden.png">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/IMG_77C6064B4F18.png">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/BEC28A552A36163DACC7817A18999E6B.png">
<meta property="og:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/8227ABDFF7CF20D89F91BFAAE7C2C7F8.png">
<meta property="article:published_time" content="2019-02-18T05:11:34.000Z">
<meta property="article:modified_time" content="2019-07-05T10:09:09.000Z">
<meta property="article:author" content="Ruochi Zhang">
<meta property="article:tag" content="Course Note">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/29AF0C1A006683910109C1BA949541C4.png">

<link rel="canonical" href="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>INFSCI 2160 DATA MINING(PART ONE) | RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          INFSCI 2160 DATA MINING(PART ONE)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-02-18 13:11:34" itemprop="dateCreated datePublished" datetime="2019-02-18T13:11:34+08:00">2019-02-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-07-05 18:09:09" itemprop="dateModified" datetime="2019-07-05T18:09:09+08:00">2019-07-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Data-Mining/" itemprop="url" rel="index"><span itemprop="name">Data Mining</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/INFSCI-2160-DATA-MINING-PART-ONE/2019/02/18/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <div class="post-description">This is the lecture note of Upitt course INFSCI 2160 DATA MINING</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><strong>Reference from some lecture slides of INFSCI 2160 DATA MINING lectured by Matt Berezo</strong></p>
<h2 id="introduction">Introduction</h2>
<ol type="1">
<li><p>What is Artificial Intelligence? <img src="29AF0C1A006683910109C1BA949541C4.png" alt="Screen Shot 2019-02-18 at 13.13.28.png" /> The goal of machine learning/AI/data mining is to develop an algorithm that performs well on new, <strong>unseen inputs</strong>. The ability to perform well on previously unobserved inputs is called <strong>generalization</strong></p></li>
<li><p>Data Mining process <img src="83DD4C0B5649752C8A1EAE6A78058344.png" alt="Screen Shot 2019-02-18 at 13.15.58.png" /></p></li>
</ol>
<ul>
<li><strong>Business undertanding</strong> is important</li>
<li><strong>Data understranding</strong> is important</li>
</ul>
<h2 id="regression">Regression</h2>
<ul>
<li>Simple linear regression involves 2 variables:
<ul>
<li>A predictor variable, x</li>
<li>A response variable, y</li>
</ul></li>
</ul>
<p><span class="math display">\[\hat{y_{i}} = \hat{\alpha} + \hat{\beta_{i}}X_{i}\]</span> - <span class="math inline">\(\hat{y_{i}}\)</span> = Estimated prediction of y - <span class="math inline">\(\hat{\alpha}\)</span> = Intercept - <span class="math inline">\(\hat{\beta_{i}}\)</span> = coefficient/parameter</p>
<p><strong>Goal</strong>: Obtain coefficient estimates that the linear model fits the available data well, and will also perform well (generalize) on unseen data</p>
<h3 id="the-least-square-approach">The least square approach</h3>
<p><span class="math display">\[\begin{align*}
&amp; \hat{\beta_{1}} = \frac{\sum_{i=1}^{n}{(x_{i}-\bar{x})(y_{i}-\bar{y})}}{\sum_{i=1}^{n}{(x_{i} - \bar{x})^{2}}}\\
&amp; \hat{\beta_{0}} = \bar{y} - \hat{\beta_{1}}\bar{x}
\end{align*}
\]</span></p>
<h3 id="coefficient-accuracy">Coefficient Accuracy</h3>
<p>We can compute the standard error of our coefficients</p>
<h4 id="what-is-se-standaed-error">what is SE (standaed error)?</h4>
<blockquote>
<p>If the purpose is <strong>Descriptive</strong>, use standard Deviation; if the purpose is <strong>Estimation</strong>, use standard Error.</p>
</blockquote>
<p>很容易混淆,我们拿到一个样本,对样本观察值离散程度的量化是<span class="math inline">\(SD:sd(x)\)</span>; 而我们可以从很多个样本中得到很多个均值，这些均值的离散度用SE来量化,也就是<span class="math inline">\(SE=sd(\bar{x})\)</span></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## generate 1000 sample with sample size 100</span></span><br><span class="line">a = sapply(<span class="built_in">rep</span>(<span class="number">100</span>, <span class="number">1000</span>), rnorm)</span><br><span class="line">a.mean = colMeans(a)</span><br><span class="line"><span class="comment">## estimate SEM by simulation</span></span><br><span class="line">sd(a.mean)</span><br><span class="line"></span><br><span class="line"><span class="comment">## estimate SEM by sigma/sqrt(n), sigma = 1</span></span><br><span class="line"><span class="number">1</span>/<span class="built_in">sqrt</span>(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## estimate SEM by sample 1</span></span><br><span class="line">sd(a[, <span class="number">1</span>])/<span class="built_in">sqrt</span>(<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p><strong>We have established that the average of <span class="math inline">\(\hat{\mu}\)</span> over many data sets will be very close to <span class="math inline">\(\mu\)</span>, but that a single estimate <span class="math inline">\(\hat{\mu}\)</span> may be a substantial underestimate or overestimate of <span class="math inline">\(\mu\)</span>. How far off will that single estimate of <span class="math inline">\(\hat{\mu}\)</span> be?</strong></p>
<p><span class="math display">\[\begin{align*}
&amp; SE(\hat{\beta_{0}})^{2} = \sigma^{2}\lbrack\frac{1}{n} + \frac{\bar{x}^{2}}{\sum_{i=1}^{n}{(x_{i} - \bar{x})^{2}}}\rbrack\\
&amp; SE(\hat{\beta_{1}})^{2} = \frac{\sigma^{2}}{\sum_{i=1}^{n}{(x_{i} - \bar{x})^{2}}}\\
\end{align*}\]</span></p>
<p>When we get the SE of parameters, we can calculate the 95% confidence interval</p>
<p><span class="math display">\[\hat{B_{i}} = +/- 2 * SE(\hat{B_{i}})\]</span></p>
<p>Standard errors can also be used to perform <strong>hypothesis tests</strong> on the coefficients. The most common hypothesis test involves testing the null hypothesis</p>
<ul>
<li><strong>Null hypothesis(H0)</strong>: there is <strong>no</strong> relationship between x and y</li>
<li><strong>Alternative hypothesis(Ha)</strong>: there is a relationship between x and y</li>
</ul>
<p>Mathematically, this corresponds to testing <span class="math display">\[\begin{align*}
H_{0}: \beta_{1} = 0 \\
H_{a}: \beta_{1} \ne 0 \\
\end{align*}\]</span></p>
<p>To test the null hypothesis, we need to determine whether <span class="math inline">\(\hat{β_{1}}\)</span>, our estimate for <span class="math inline">\(\hat{β_{1}}\)</span> , is sufficiently far from zero that we can be confident that <span class="math inline">\(\hat{β_{1}}\)</span> is non-zero</p>
<p><span class="math display">\[t = \frac{\hat{β_{1}} - 0}{SE(\hat{\beta_{1}})}\]</span></p>
<h3 id="model-accuracy">Model Accuracy</h3>
<ul>
<li><p>RSS: Residual Sum of Squares <span class="math display">\[e_{1}^{2} + e_{2}^{2} + e_{3}^{2} + .....\]</span></p></li>
<li><p>RSE: Residual standard error <span class="math display">\[\sqrt\frac{RSS}{(N-2)}\]</span></p></li>
<li><p>R squared How much better does your model do than simply using the mean, in terms of SSE? <span class="math display">\[R^{2} = 1 - (\frac{\sum{(y_{i} - \hat{y}_{i})^{2}}}{\sum{(y_{i} - \overline{y}_{i})^{2}}})\]</span></p>
<ul>
<li>R-square takes form of a proportion and gives a value between 0 and 1 (1 = perfect model)</li>
</ul></li>
</ul>
<h3 id="multiple-linear-regression">Multiple Linear Regression</h3>
<ul>
<li>F-stat If the F-stat is larger than 1 and the p-value is &lt;= 0.05, we can determine that our predictors and model have a relationship with the response variable</li>
</ul>
<p><span class="math display">\[\frac{TSS - RSS}{p} / \frac{RSS}{n-p-1}\]</span> - Where p = our number of predictors - N = number of observations</p>
<figure>
<img src="9784E656D9E9D2613CF3D07CA064D3AA.jpg" alt="IMG_0310.jpg" /><figcaption aria-hidden="true">IMG_0310.jpg</figcaption>
</figure>
<ul>
<li>R-squared <span class="math display">\[R^{2} = 1 - \frac{\sum{(y_{i} - \hat{y}_{i})^{2}} / (n-d-1)}{\sum{(y_{i} - \overline{y}_{i})^{2}}/(n-d)}\]</span></li>
</ul>
<h3 id="feature-selection">Feature Selection</h3>
<h4 id="stepwise-procedures">Stepwise Procedures</h4>
<ul>
<li>Backward Elimination This is the simplest of all variable selection procedures and can be easily implemented without special software. In situations where there is a complex hierarchy, backward elimination can be run manually while taking account of what variables are eligible for removal.
<ol type="1">
<li>Start with all the predictors in the model</li>
<li>Remove the predictor with highest p-value greater than <span class="math inline">\(\alpha\)</span></li>
<li>Refit the model and goto 2</li>
<li>Stop when all p-values are less than <span class="math inline">\(\alpha\)</span></li>
</ol></li>
<li>Forward Selection This just reverses the backward method.
<ol type="1">
<li>Start with no variables in the model.</li>
<li>For all predictors not in the model, check their p-value if they are added to the model. Choose the one with lowest p-value less than αcrit .</li>
<li>Continue until no new predictors can be added.</li>
</ol></li>
</ul>
<h4 id="ridge-regression-i.e.-l2-norm-regulizar">Ridge regression (i.e., L2 norm regulizar)</h4>
<p>Ridge looks to minimize: <span class="math display">\[RSS + \lambda\sum_{j=1}^{p}{\beta_{j}^{2}}\]</span></p>
<ul>
<li><span class="math inline">\(\lambda\)</span> is a tuning parameter</li>
</ul>
<h3 id="bias-vs.-variance-trade-off">Bias vs. Variance Trade-off</h3>
<p>Ideally, we want to derive a model that has low bias, low variance, and low MSE on test data <img src="CA80BF1FC8D3AA010CDDFE5A646489BF.png" alt="Screen Shot 2019-02-18 at 20.01.09.png" /></p>
<h3 id="local-polynomial-regression">Local Polynomial Regression</h3>
<ul>
<li>The fitted value changes with x in a nonparametric manner</li>
<li>Define a weight function so that only values within a smoothing window [𝑥0 - h(𝑥0 ), 𝑥0 + h(𝑥0 )] will be considered in the estimate of <span class="math inline">\(\hat{y}\)</span></li>
</ul>
<h3 id="model-performance">Model Performance</h3>
<h3 id="cross-validation">Cross-validation</h3>
<p>The goal of cross-validation is to test the model’s ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias[6] and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).</p>
<h4 id="advantages-of-loocv">Advantages of LOOCV</h4>
<p>Advantages of LOOCV: - Works well on small datasets - Meticulously tests the data</p>
<p>Disadvantages of LOOCV: - Computationally expensive on “big data” sets - Can result in high variability since model is only tested on one observation</p>
<h4 id="overfitting">Overfitting</h4>
<ul>
<li>Use cross-validation</li>
<li>Ensemble/combine models together</li>
<li>Use regularization techniques to penalize models that are too complex</li>
</ul>
<h3 id="non-parametric-methods">Non-parametric Methods</h3>
<p>Advantages of Non-parametric Methods: - Do not assume an explicit form of f(x), so the model is more "flexible"</p>
<p>Disadvantages of Non-Parametric Methods: - Often are more complex and thus more difficult to interpret</p>
<h4 id="k-nearest-neighbors">K Nearest-Neighbors</h4>
<ul>
<li>KNN is a <strong>non-parametric method</strong>, vs. linear and logistic regression which are parametric approaches since they assume a linear functional form for f(x) <img src="BFE33EA434D8355137098AEF9241AEEC.png" alt="Screen Shot 2019-02-18 at 22.26.44.png" /></li>
</ul>
<h3 id="accuracy-and-error-rate">Accuracy and Error Rate</h3>
<p><img src="3FB3E0B341C2DD82395CB962430B6E34.png" alt="Screen Shot 2019-02-19 at 00.26.12.png" /> <img src="youden.png" alt="youden.png" /> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">scores = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.35</span>, <span class="number">0.8</span>])</span><br><span class="line">fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fpr</span><br><span class="line">array([<span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">1.</span> ])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tpr</span><br><span class="line">array([<span class="number">0.</span> , <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">1.</span> , <span class="number">1.</span> ])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>thresholds</span><br><span class="line">array([<span class="number">1.8</span> , <span class="number">0.8</span> , <span class="number">0.4</span> , <span class="number">0.35</span>, <span class="number">0.1</span> ])</span><br></pre></td></tr></table></figure></p>
<h3 id="classification">Classification</h3>
<h4 id="naïve-bayes">Naïve Bayes</h4>
<ul>
<li>a Naïve Bayes classifier assumes independence between features</li>
<li>Naïve Bayes assumes that the continuous variables are normally distributed</li>
<li>For continuous random variables, probabilities are areas under the curve <img src="IMG_77C6064B4F18.png" alt="IMG_77C6064B4F18.png" /></li>
</ul>
<h4 id="decision-trees">Decision Trees</h4>
<p>Constructing Decision Trees for Regression 1. First, we divide the predictor space into distinct and non-overlapping regions (𝑅1, 𝑅2,𝑅3 ... 𝑅𝑛) 2. To make a prediction, we typically use the mean of the training data in the region to which it belongs</p>
<p><strong>How do we construct R1 and R2?</strong> The goal is to find regions that minimize the residual sum of squares (RSS)</p>
<p>Decision trees can get too complex, memorize the training data, and overfit on test data - It is advised to first build a very large tree and then <strong>prune</strong> it back to obtain a subtree - Given a subtree, we can estimate the test error rate using cross-validation - <strong>Cost complexity pruning</strong> i.e., <strong>weakest link pruning</strong> gives us the most efficient way to choose our subset of trees</p>
<h4 id="decision-trees-advantages-and-disadvantages">Decision Trees Advantages and Disadvantages</h4>
<p>Advantages: - Trees are easy to explain and are intuitive - Trees can be displayed graphically and are easy to interpret - Trees can handle qualitative predictors without dummy variables</p>
<p>Disadvantages: - Trees to not usually have the same level of predictive accuracy as other regression and classification methods - Trees can be non-robust, i.e., a small change in the data can cause a large change in the tree</p>
<h4 id="bagging-and-random-forests">Bagging and Random Forests</h4>
<p><strong>Bootstrap aggregation</strong>, also known as <strong>bagging</strong>, is a procedure of reducing the variance of a statistical learning method - This is a good way to reduce variance→by taking many training sets from the population and build separate learning methods using each set - We can then calculate f1,f2,f3... and average them in order to obtain a low-variance statistical model - We can do this by bootstrapping, or taking repeated random samples from the training set</p>
<p><strong>Ensemble learning</strong> is a machine learning paradigm where multiple learners are trained to solve the same problem</p>
<p><strong>Random forests</strong> provide an improvement over bagged trees by decorrelating them - Like bagging, decision trees are made on bootstrapped training samples - Random forests are an <strong>ensemble</strong> method for decision trees - The difference is, each time a split in the tree is considered, a random sample of predictors is chosen as split candidates from the full set of predictors. So, at each split of the tree, the algorithm can’t even consider a majority of the predictors</p>
<h3 id="support-vector-machines">Support Vector Machines</h3>
<ul>
<li>SVM’s use a classifying tool called <strong>maximum margin classifier</strong></li>
<li>maximum margin classifiers can’t be applied to most datasets because they require the classes to be <strong>separated by a linear boundary</strong></li>
<li>Support vector classifiers are an extension of maximum margin classifiers that can be applied to a broader range of datasets</li>
</ul>
<h4 id="hyperplane">hyperplane</h4>
<p>A hyperplane is a flat subspace in p-dimensional space with p – 1 dimensions <img src="BEC28A552A36163DACC7817A18999E6B.png" alt="Screen Shot 2019-02-19 at 02.46.26.png" /></p>
<h4 id="parameters">parameters</h4>
<ul>
<li>C = a nonnegative tuning parameter
<ul>
<li>C can be thought of as a budget for the amount the margin can be violated by n observations. If C = 0, there is no budget for violations to the margin</li>
<li>large C, Overfitting</li>
<li>small C, underfitting</li>
</ul></li>
</ul>
<h4 id="what-if-the-decision-boundary-for-the-two-classes-is-not-linear">What if the decision boundary for the two classes is not linear?</h4>
<ul>
<li>enlarging the feature space with kernels</li>
<li>A kernel is a function that quantifies the similarity between two observations</li>
</ul>
<h3 id="multinomial-logistic-regression">Multinomial Logistic Regression</h3>
<ul>
<li>Similar to binary logistic regression, all probabilities in the output will sum to 1</li>
<li>This is just an extension of the same math from logistic regression</li>
</ul>
<h4 id="drawbacks">Drawbacks</h4>
<ul>
<li>Models involve many parameters, which makes their interpretation tedious</li>
<li>Maximum-likelihood estimation can encounter numerical problems if the data is separable and if the predicted probabilities are close to either 0 or 1</li>
</ul>
<h3 id="xgboost">XGBoost</h3>
<h4 id="overview">overview</h4>
<p>Advantages of XGBoost:</p>
<ul>
<li>Scalability: XGBoost system runs 10x faster than existing popular solutions on a single machine</li>
<li>XGBoost accepts null values: users don’t have to impute missing values, drop records, etc.</li>
<li>Less time spent on feature selection and more time spent on hyperparametric tuning</li>
</ul>
<p>Typically, one tree is not as strong as an ensemble/combination of other trees. XGBoost uses an ensemble method to gather information from other trees.</p>
<h4 id="objective-function-and-regularization">Objective Function and Regularization</h4>
<p>The additive function fixes what we have already learned, and adds one new tree at a time <img src="8227ABDFF7CF20D89F91BFAAE7C2C7F8.png" alt="Screen Shot 2019-02-19 at 04.46.06.png" /></p>
<p>But how do we choose which tree we want at each step? &gt; We pick the one that optimizes our objective function! This is known as an <strong>additive function</strong></p>
<h4 id="objective-functions">Objective Functions</h4>
<ul>
<li>Linear: Continuous numeric prediction</li>
<li>Binary: logistic,binary classification</li>
<li>Multi:softmax: multiclassification</li>
</ul>
<h4 id="tree-boosting-parameters">Tree Boosting Parameters</h4>
<p><strong>Reference from <a target="_blank" rel="noopener" href="https://www.cnblogs.com/sarahp/p/6900572.html">https://www.cnblogs.com/sarahp/p/6900572.html</a></strong></p>
<ul>
<li><strong>Eta</strong> (i.e., learning rate): Step shrinkage use in update to prevent overfitting. After each boosting step, we can get the weights of new features. Eta shrinks the weights to make the boosting process more conservative</li>
<li>Gamma: Minimum loss reduction required to make a further partition on a leaf node of a tree. Larger gamma = more conservative model (这个指定了一个结点被分割时，所需要的最小损失函数减小的大小)</li>
<li>Max depth: Maximum depth of a tree. Increasing this value will make the model more complex (树的最大深度，值越大，树越复杂)</li>
<li>Minimum child weight: Minimum sum of instance weight needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than this set parameter, the building process will stop partitioning. Larger weight = more conservative model (定义了一个子集的所有观察值的最小权重和)</li>
<li>Subsample: A subsample ratio of the training instances. Setting to 0.5 would make XGBoost randomly sample half of the training data prior to growing trees and will help prevent overfitting (样本的采样率，如果设置成0.5，那么Xgboost会随机选择一般的样本作为训练集)</li>
<li>Column sample by tree: Subsample ratio of columns when constructing a tree</li>
<li>Column sample by level: Subsample ratio of columns for each level of the tree</li>
<li>Column sample by node: Subsample ratio of columns for each node (split)</li>
<li>Lambda: L2 regularization</li>
<li>Alpha: L1 regularization</li>
<li>Scale positive weight: Control the balance of positive and negative weights</li>
</ul>
<h3 id="review">REVIEW</h3>
<h4 id="what-is-the-difference-between-boost-ensemble-bootstrap-and-bagging">What is the difference between boost, ensemble, bootstrap and bagging?</h4>
<p><strong>Reference from <a target="_blank" rel="noopener" href="https://www.quora.com/What-is-the-difference-between-boost-ensemble-bootstrap-and-bagging">https://www.quora.com/What-is-the-difference-between-boost-ensemble-bootstrap-and-bagging</a></strong></p>
<ul>
<li>Boosting is the idea of training iteratively the same “weak” classifier, so that at each iteration, the i-th classifier is supposed to correct the mistakes made by the previous classifier (i-1). It is done by weighting more the misclassified observations.</li>
<li>The final classifier is calculated by a weighted mean of all the “weak” classifiers, the weights being close to the accuracies calculated for each classifier.</li>
<li>Ensembling is quite general and encompasses simple methods like Averaging, and more complicated ones like Boosting, Bagging, Stacking, etc.</li>
<li>Bootstrapping means taking a sample of a population by drawing with replacement. It is one of the main ideas behind Bagging (which stands for Bootstrap AGGregatING).</li>
<li>Bagging means training the same classifier on different subsets (that may be overlapping) of one dataset. You do so with bootstrap.</li>
</ul>
<h4 id="rf-vs-xgboost">RF vs XGBoost</h4>
<p><strong>Reference from <a target="_blank" rel="noopener" href="https://www.cnblogs.com/sarahp/p/6900572.html">https://www.cnblogs.com/sarahp/p/6900572.html</a></strong></p>
<ul>
<li>RF use bagging:
<ul>
<li>种集成学习算法，基于bootstrap sampling 自助采样法，重复性有放回的随机采用部分样本进行训练最后再将结果 voting 或者 averaging</li>
<li>它是并行式算法，因为不同基学习器是独立</li>
<li>训练一个bagging集成学习器时间复杂度与基学习器同阶（n倍，n为基学习器个数）。</li>
<li>bagging可以用于二分类／多分类／回归</li>
<li>每个基学习器的未用作训练样本可用来做包外估计，评价泛化性能。</li>
<li>bagging主要关注降低<strong>方差</strong></li>
<li>两个步骤 1. 抽样训练（采样样本，采样特征） 2 融合</li>
</ul></li>
<li>XGBoost use boosting(Gradient Boosting Decision Tree):
<ul>
<li>gbdt的基本原理是boost 里面的 boosting tree（提升树），并使用 gradient boost。</li>
<li>Gradient Boosting是一种Boosting的方法，其与传统的Boosting的区别是，每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，可以在残差减少的梯度(Gradient)方向上建立一个新的模型。所以说，在Gradient Boosting中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boosting对正确、错误样本进行加权有着很大的区别。这个梯度代表上一轮学习器损失函数对预测值求导。</li>
<li>与Boosting Tree的区别：Boosting Tree的适合于损失函数为平方损失或者指数损失。而Gradient Boosting适合各类损失函数（损失函数为：平方损失则相当于Boosting Tree拟合残差、损失函数为：使用指数损失则可以近似于Adaboost，但树是回归树）</li>
</ul></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Course-Note/" rel="tag"># Course Note</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Machine-Learning-Andraw-Ng/2019/02/15/" rel="prev" title="Machine Learning (Andrew Ng)">
      <i class="fa fa-chevron-left"></i> Machine Learning (Andrew Ng)
    </a></div>
      <div class="post-nav-item">
    <a href="/Linear-models-Optimization/2019/02/22/" rel="next" title="Linear models, Optimization">
      Linear models, Optimization <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#regression"><span class="nav-number">2.</span> <span class="nav-text">Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the-least-square-approach"><span class="nav-number">2.1.</span> <span class="nav-text">The least square approach</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#coefficient-accuracy"><span class="nav-number">2.2.</span> <span class="nav-text">Coefficient Accuracy</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#what-is-se-standaed-error"><span class="nav-number">2.2.1.</span> <span class="nav-text">what is SE (standaed error)?</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-accuracy"><span class="nav-number">2.3.</span> <span class="nav-text">Model Accuracy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multiple-linear-regression"><span class="nav-number">2.4.</span> <span class="nav-text">Multiple Linear Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#feature-selection"><span class="nav-number">2.5.</span> <span class="nav-text">Feature Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#stepwise-procedures"><span class="nav-number">2.5.1.</span> <span class="nav-text">Stepwise Procedures</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ridge-regression-i.e.-l2-norm-regulizar"><span class="nav-number">2.5.2.</span> <span class="nav-text">Ridge regression (i.e., L2 norm regulizar)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bias-vs.-variance-trade-off"><span class="nav-number">2.6.</span> <span class="nav-text">Bias vs. Variance Trade-off</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#local-polynomial-regression"><span class="nav-number">2.7.</span> <span class="nav-text">Local Polynomial Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-performance"><span class="nav-number">2.8.</span> <span class="nav-text">Model Performance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cross-validation"><span class="nav-number">2.9.</span> <span class="nav-text">Cross-validation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#advantages-of-loocv"><span class="nav-number">2.9.1.</span> <span class="nav-text">Advantages of LOOCV</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#overfitting"><span class="nav-number">2.9.2.</span> <span class="nav-text">Overfitting</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#non-parametric-methods"><span class="nav-number">2.10.</span> <span class="nav-text">Non-parametric Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#k-nearest-neighbors"><span class="nav-number">2.10.1.</span> <span class="nav-text">K Nearest-Neighbors</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#accuracy-and-error-rate"><span class="nav-number">2.11.</span> <span class="nav-text">Accuracy and Error Rate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#classification"><span class="nav-number">2.12.</span> <span class="nav-text">Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#na%C3%AFve-bayes"><span class="nav-number">2.12.1.</span> <span class="nav-text">Naïve Bayes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#decision-trees"><span class="nav-number">2.12.2.</span> <span class="nav-text">Decision Trees</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#decision-trees-advantages-and-disadvantages"><span class="nav-number">2.12.3.</span> <span class="nav-text">Decision Trees Advantages and Disadvantages</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bagging-and-random-forests"><span class="nav-number">2.12.4.</span> <span class="nav-text">Bagging and Random Forests</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#support-vector-machines"><span class="nav-number">2.13.</span> <span class="nav-text">Support Vector Machines</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#hyperplane"><span class="nav-number">2.13.1.</span> <span class="nav-text">hyperplane</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#parameters"><span class="nav-number">2.13.2.</span> <span class="nav-text">parameters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#what-if-the-decision-boundary-for-the-two-classes-is-not-linear"><span class="nav-number">2.13.3.</span> <span class="nav-text">What if the decision boundary for the two classes is not linear?</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multinomial-logistic-regression"><span class="nav-number">2.14.</span> <span class="nav-text">Multinomial Logistic Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#drawbacks"><span class="nav-number">2.14.1.</span> <span class="nav-text">Drawbacks</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xgboost"><span class="nav-number">2.15.</span> <span class="nav-text">XGBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#overview"><span class="nav-number">2.15.1.</span> <span class="nav-text">overview</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#objective-function-and-regularization"><span class="nav-number">2.15.2.</span> <span class="nav-text">Objective Function and Regularization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#objective-functions"><span class="nav-number">2.15.3.</span> <span class="nav-text">Objective Functions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tree-boosting-parameters"><span class="nav-number">2.15.4.</span> <span class="nav-text">Tree Boosting Parameters</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#review"><span class="nav-number">2.16.</span> <span class="nav-text">REVIEW</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#what-is-the-difference-between-boost-ensemble-bootstrap-and-bagging"><span class="nav-number">2.16.1.</span> <span class="nav-text">What is the difference between boost, ensemble, bootstrap and bagging?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#rf-vs-xgboost"><span class="nav-number">2.16.2.</span> <span class="nav-text">RF vs XGBoost</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">218</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail → mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'en' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
