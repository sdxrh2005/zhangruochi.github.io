<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Summary of Variational Auto-Encoder">
<meta property="og:type" content="article">
<meta property="og:title" content="Variational Auto-Encoder">
<meta property="og:url" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="Summary of Variational Auto-Encoder">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/3.png">
<meta property="og:image" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/4.png">
<meta property="og:image" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/5.png">
<meta property="og:image" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/1.png">
<meta property="og:image" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/2.png">
<meta property="og:image" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/6.png">
<meta property="og:image" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/output_5_0.png">
<meta property="og:image" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/output_15_0.svg">
<meta property="og:image" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/output_16_1.png">
<meta property="og:image" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/output_17_0.png">
<meta property="og:image" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/output_17_1.png">
<meta property="og:image" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/output_17_2.png">
<meta property="og:image" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/output_17_3.png">
<meta property="og:image" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/output_17_4.png">
<meta property="og:image" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/output_17_5.png">
<meta property="article:published_time" content="2019-10-10T13:37:02.000Z">
<meta property="article:modified_time" content="2021-12-31T07:29:27.000Z">
<meta property="article:author" content="Ruochi Zhang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/3.png">

<link rel="canonical" href="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Variational Auto-Encoder | RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Variational-Auto-Encoder/2019/10/10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Variational Auto-Encoder
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-10 21:37:02" itemprop="dateCreated datePublished" datetime="2019-10-10T21:37:02+08:00">2019-10-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-31 15:29:27" itemprop="dateModified" datetime="2021-12-31T15:29:27+08:00">2021-12-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Variational-Auto-Encoder/2019/10/10/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Variational-Auto-Encoder/2019/10/10/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <div class="post-description">Summary of Variational Auto-Encoder</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="pre-knowledge">Pre-knowledge</h2>
<h3 id="什么是独立同分布i.i.d">什么是独立同分布(i.i.d)？</h3>
<p>在概率论与统计学中，独立同分布（英語：Independent and identically distributed，缩写为IID）是指一组随机变量中每个变量的概率分布都相同，且这些随机变量互相独立。</p>
<h3 id="什么是数据集的分布">什么是数据集的分布？</h3>
<ul>
<li>对于supervised learning，分布是指关于特征<span class="math inline">\(X\)</span>和结果<span class="math inline">\(Y\)</span>的联合分布<span class="math inline">\(F(X,Y)\)</span>或者条件分布<span class="math inline">\(F(Y|X)\)</span>。 我们说训练集和测试集服从同分布的意思是训练集和测试集都是由服从同一个分布的随机样本组成的，也就是：</li>
</ul>
<p><span class="math display">\[(X_{train},Y_{train}), (X_{test},Y_{test}) \text{ i.i.d. } \sim F(X,Y)\]</span></p>
<ul>
<li><p>对于unsupervised learning，分布是指特征<span class="math inline">\(X\)</span>的分布 F(X)，也就是： <span class="math display">\[X_{train},X_{test} \text{ i.i.d. } \sim F(X) \]</span></p></li>
<li><p>但是现实中比较难做到这点，特别是当训练集是过去的数据，测试集是当下的数据，由于时间的因素，它们很可能不是完全同分布的，这就增加了预测难度。这也是为什么一般交叉验证的误差往往小于实际的测试误差。因为交叉验证中每折数据都是来自训练集，它们肯定是同分布的。如果训练集和测试集的分布风马牛不相及，那么根据训练集学习得到的模型在测试集上就几乎没有什么用了。所以我们训练模型和应用模型时一个重要的前提假设就是训练集和测试集是同分布的。另外一个方面是牵涉到过拟合问题，即使训练集和测试集是同分布的，由于数据量的问题，训练集的分布可能无法完整体现真实分布，当我们过分去学习训练集分布的时候，我们反而会远离真实分布（以及测试集的分布），造成预测不准确，这就造成过拟合。</p></li>
</ul>
<h3 id="隐变量latent-variable">隐变量（latent variable）</h3>
<p>什么是隐变量呢，让我们先简单的说一下，我们估计算法在做的一些事情，我们要做的其实就是估算出概率模型的参数，概率模型是什么呢？你可以简单把它理解成一个分布，甚至说可以把它理解成一个函数，我们的估计算法就是为了求解出这些函数的参数而存在的。</p>
<blockquote>
<p>如果你站在这个人旁边，你目睹了整个过程：这个人选了哪个袋子、抓出来的球是什么颜色的。然后你把每次选择的袋子和抓出来的球的颜色都记录下来（样本观察值），那个人不停地抓，你不停地记。最终你就可以通过你的记录，推测出每个袋子里每种球颜色的大致比例。并且你记录的越多，推测的就越准（中心极限定理）。然而，抓球的人觉得这样很不爽，于是决定不告诉你他从哪个袋子里抓的球，只告诉你抓出来的球的颜色是什么。这时候，“选袋子”的过程由于你看不见，其实就相当于是一个隐变量。隐变量在很多地方都是能够出现的。现在我们经常说的隐变量主要强调它的“latent”。所以广义上的隐变量主要就是指“不能被直接观察到，但是对系统的状态和能观察到的输出存在影响的一种东西”。所以说，很多人在研究隐变量。以及设计出各种更优(比如如可解释、可计算距离、可定义运算等性质)的隐变量的表示。</p>
</blockquote>
<h2 id="分布变换">分布变换</h2>
<p>通常我们会拿VAE跟GAN比较，的确，它们两个的目标基本是一致的——希望构建一个从隐变量<span class="math inline">\(Z\)</span>,生成目标数据<span class="math inline">\(X\)</span>的模型，但是实现上有所不同。更准确地讲，它们是假设了<span class="math inline">\(Z\)</span>服从某些常见的分布（比如正态分布或均匀分布），然后希望训练一个模型<span class="math inline">\(X=g(Z)\)</span>这个模型能够将原来的概率分布映射到训练集的概率分布，也就是说，它们的目的都是进行<strong>分布之间的变换</strong>。</p>
<p>现在假设<span class="math inline">\(Z\)</span>服从标准的正态分布，那么我就可以从中采样得到若干个<span class="math inline">\(Z_1\)</span>,<span class="math inline">\(Z_2\)</span>,...,<span class="math inline">\(Z_n\)</span>, 然后对它做变换得到<span class="math inline">\(hat{x_i} = g(Z_i)\)</span>，我们怎么判断这个通过<span class="math inline">\(g\)</span>构造出来的数据集，它的分布跟我们目标的数据集分布是不是一样的呢？KL离散度不行，因为KL散度是根据两个概率分布的表达式来算它们的相似度的，然而目前我们并不知道它们的概率分布的表达式，我们只有一批从构造的分布采样而来的数据<span class="math inline">\({\hat{x_i}}\)</span>，还有一批从真实的分布采样而来的数据<span class="math inline">\({x_i}\)</span>。我们只有样本本身，没有分布表达式，当然也就没有方法算KL散度。</p>
<p><strong>GAN的思路很直接粗犷：既然没有合适的度量，那我干脆把这个度量也用神经网络训练出来吧。</strong></p>
<h2 id="auto-encoder">Auto-Encoder</h2>
<p>标准自动编码器学会生成紧凑的表示和重建他们的输入，但除了能用于一些应用程序，如去噪自动编码器，他们是相当有限的。自动编码器的基本问题在于，它们将其输入转换成其编码矢量，其所在的潜在空间可能不连续，或者允许简单的插值。</p>
<p>例如，在MNIST数据集上训练一个自编码器，并从2D潜在空间中可视化编码，可以看到不同簇的形成。 这是有道理的，因为每种图像类型的不同编码使得解码器对它们进行解码变得更容易。如果你只是复制相同的图像，这是不错的。但是当你建立一个生成模型时，你不想准备复制你输入的相同图像。你想从潜在的空间随机抽样，或者从一个连续的潜在空间中产生输入图像的变化。</p>
<p><img src="3.png" /></p>
<h2 id="vae">VAE</h2>
<p>变分自动编码器（VAEs）具有一个独特的性质，可以将它们与vanilla自动编码器分离开来，正是这种特性使其在生成建模时非常有用：它们的潜在空间在设计上是连续的，允许随机采样和插值。它通过做一些约束来达到这个目的：使编码器不输出大小为n的编码矢量，而是输出两个大小为n的矢量：平均矢量<span class="math inline">\(\mu\)</span>和另一个标准偏差矢量<span class="math inline">\(\sigma\)</span>。</p>
<p><img src="4.png" /> <img src="5.png" /> 这种随机生成意味着，即使对于相同的输入，虽然平均值和标准偏差保持不变，但是实际编码会在采样过程中发生些许变化。</p>
<p>首先我们有一批数据样本 <span class="math inline">\(\{X_1,\dots,X_n\}\)</span>，其整体用 <span class="math inline">\(X\)</span> 表示，我们本想根据<span class="math inline">\(\{X_1,\dots,X_n\}\)</span>来得到<span class="math inline">\(X\)</span>的分布<span class="math inline">\(P(x)\)</span>，如果能得到的话，那我直接根据<span class="math inline">\(P(x)\)</span>来采样，就可以得到所有可能的<span class="math inline">\(X\)</span>了。这是一个终极理想的生成模型了。当然，这个理想很难实现，于是我们将分布改一改。</p>
<p><span class="math display">\[p(X)=\sum_Z p(X|Z)p(Z)\tag{1}\]</span></p>
<p>此时<span class="math inline">\(p(X|Z)\)</span>就描述了一个由<span class="math inline">\(Z\)</span>来生成<span class="math inline">\(X\)</span>的模型，而我们假设<span class="math inline">\(Z\)</span>服从标准正态分布，也就是<span class="math inline">\(p(Z) \sim \mathcal{N}(0,I)\)</span>，如果这个理想能实现，那么我们就可以先从标准正态分布中采样一个<span class="math inline">\(Z\)</span>，然后根据<span class="math inline">\(Z\)</span>来算一个<span class="math inline">\(X\)</span>，也是一个很棒的生成模型。接下来就是结合自编码器来实现重构，保证有效信息没有丢失，再加上一系列的推导，最后把模型实现。框架的示意图如下：</p>
<p><img src="1.png" /></p>
<p>看出了什么问题了吗？如果像这个图的话，我们其实完全不清楚：究竟经过重新采样出来的<span class="math inline">\(Z_k\)</span>是不是还对应着原来的<span class="math inline">\(X_k\)</span>， 所以我们如果直接最小化<span class="math inline">\(L(\hat{X_k},X_k)\)</span>是很不科学的。</p>
<p><strong>其实，在整个VAE模型中，我们并没有去使用<span class="math inline">\(P(z)\)</span>（隐变量空间的分布）是正态分布的假设，我们用的是假设<span class="math inline">\(p(Z|X)\)</span>后验分布）是正态分布</strong>具体来说，给定一个特征<span class="math inline">\(X_K\)</span>，我们假设存在一个专属于<span class="math inline">\(X_K\)</span>的后验分布<span class="math inline">\(p(Z|X_k)\)</span>，并进一步假设这个分布是（独立的、多元的）正态分布。为什么要强调“专属”呢？因为我们后面要训练一个生成器<span class="math inline">\(X = g(Z)\)</span>，希望能够把从分布p(Z|x_k)采样出来的一个<span class="math inline">\(Z_k\)</span>还原为<span class="math inline">\(X_k\)</span>。这时候每一个<span class="math inline">\(X_k\)</span>都配上了一个专属的正态分布，才方便后面的生成器做还原。但这样有多少个<span class="math inline">\(X\)</span>就有多少个正态分布了。我们知道正态分布有两组参数：均值<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\sigma^{2}\)</span>（多元的话，它们都是向量），那我怎么找出专属于<span class="math inline">\(X_k\)</span>的正态分布<span class="math inline">\(p(Z|X_k)\)</span>的均值和方差呢？好像并没有什么直接的思路。那好吧，那我就用神经网络来拟合出来吧！这就是神经网络时代的哲学：难算的我们都用神经网络来拟合。</p>
<p><img src="2.png" /></p>
<p>通过这种方法，我们现在将给定输入的每个潜在特征表示为概率分布。当从潜在状态解码时，我们将从每个潜在状态分布中随机采样，生成一个向量作为解码器模型的输入。</p>
<p><img src="6.png" /></p>
<h3 id="reparameterization-trick">Reparameterization trick</h3>
<p>其实很简单，就是我们要从<span class="math inline">\(p(Z|X_k)\)</span>中采样一个<span class="math inline">\(z_k\)</span>，出来，尽管我们知道了<span class="math inline">\(p(Z|X_k)\)</span>是正态分布，但是均值方差都是靠模型算出来的，我们要靠这个过程反过来优化均值方差的模型，但是<strong>采样</strong>这个操作是不可导的，而采样的结果是可导的。我们利用 <span class="math display">\[\begin{aligned}&amp;\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(z-\mu)^2}{2\sigma^2}\right)dz \\ 
=&amp; \frac{1}{\sqrt{2\pi}}\exp\left[-\frac{1}{2}\left(\frac{z-\mu}{\sigma}\right)^2\right]d\left(\frac{z-\mu}{\sigma}\right)\end{aligned}\tag{6}\]</span></p>
<p><span class="math inline">\((z-\mu)/\sigma=\varepsilon\)</span>是服从均值为0、方差为1的标准正态分布的，要同时把<span class="math inline">\(dz\)</span>考虑进去，因为乘上<span class="math inline">\(dz\)</span>才是概率，不乘是概率密度。</p>
<blockquote>
<p>从 <span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span>中采样一个，相当于从<span class="math inline">\(\mathcal{N}(0,I)\)</span>中采样一个<span class="math inline">\(\varepsilon\)</span>，再让<span class="math inline">\(Z=\mu + \varepsilon \times \sigma\)</span>.</p>
</blockquote>
<p>于是，我们将从<span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span>采样，变化为从<span class="math inline">\(\mathcal{N}(0,I)\)</span>中采样，然后通过参数变换得到从<span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span>采样的结果。这样一来，“采样”这个操作就不用参与梯度下降了，改为采样的结果参与，使得整个模型可训练了。</p>
<h2 id="project">Project</h2>
<h2 id="data">Data</h2>
<p>For data let’s use MNIST dataset. Pytorch vision module has an easy way to create training and test dataset for MNIST</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets,transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">use_cuda = torch.cuda.is_available()</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">100</span></span><br><span class="line">trainset = datasets.MNIST(<span class="string">&#x27;./data/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>,</span><br><span class="line">                   transform=transforms.ToTensor())</span><br><span class="line">train_generator = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,</span><br><span class="line">                                          shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line"><span class="comment"># test</span></span><br><span class="line">testset = datasets.MNIST(<span class="string">&#x27;./data/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">False</span>,</span><br><span class="line">                   transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">test_generator = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,</span><br><span class="line">                                          shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h2 id="visualization">Visualization</h2>
<p>Before proceeding, let’s visualize some data. For that I am using torchvision.utils.make_grid which creates a grid from multiple images:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_images</span>(<span class="params">images</span>):</span></span><br><span class="line">    images = make_grid(images)</span><br><span class="line">    show_image(images[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_image</span>(<span class="params">img</span>):</span></span><br><span class="line">    plt.imshow(img, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">dataiter = <span class="built_in">iter</span>(trainloader)</span><br><span class="line">images, labels = dataiter.<span class="built_in">next</span>()</span><br><span class="line">show_images(images)</span><br></pre></td></tr></table></figure>
<figure>
<img src="output_5_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<h2 id="network-architecture">Network Architecture</h2>
<p>Similar to deniosing auto encoder, VAE has an encoder and decoder.</p>
<h3 id="encoder">Encoder</h3>
<p>The encoder encodes an image to a varibale z with normal distribution. For normal distribution we just need to approximate mean m and standard deviation s. Therefore, the role of neural network is to learn a funcion from image to m and s. This implicitly means we are learning a function from image to a probability distribution for z. We implement that function approximator using linear matrix and RELU nonlinearity:</p>
<h3 id="decoder">Decoder</h3>
<p>The decoder gets the encoded value z, which in theory is reffered to as latent variable, and decodes that value to an image. Therefore, the role of decoder is to learn a function that maps a value of z to a vector of 782 real values. Note that z is in fact a random variable but here we just work with a realization (a.k.a a sampled value) of that random variable</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shape = images[<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VAE</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,latent_variable_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(VAE,self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">## encoder</span></span><br><span class="line">        self.fc1 = nn.Linear(np.prod(shape),<span class="number">400</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">## mean and std</span></span><br><span class="line">        self.fc_mean = nn.Linear(<span class="number">400</span>,latent_variable_dim)</span><br><span class="line">        self.fc_std = nn.Linear(<span class="number">400</span>, latent_variable_dim)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">## decoder</span></span><br><span class="line">        </span><br><span class="line">        self.fc3 = nn.Linear(latent_variable_dim, <span class="number">400</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">400</span>, np.prod(shape))</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reparameterize</span>(<span class="params">self, log_var, mu</span>):</span></span><br><span class="line">        s = torch.exp(<span class="number">0.5</span>*log_var)</span><br><span class="line">        eps = torch.rand_like(s) <span class="comment"># generate a iid standard normal same shape as s</span></span><br><span class="line">        <span class="keyword">return</span> eps.mul(s).add_(mu)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = functional.relu(self.fc3(x))</span><br><span class="line">        x = torch.sigmoid(self.fc4(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span></span><br><span class="line">        x = <span class="built_in">input</span>.view(-<span class="number">1</span>,<span class="number">784</span>)</span><br><span class="line">        x = functional.relu(self.fc1(x))</span><br><span class="line">        log_s = self.fc_std(x)</span><br><span class="line">        m = self.fc_mean(x)</span><br><span class="line">        z = self.reparameterize(log_s,m)</span><br><span class="line">        x = self.decode(z)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x,m,log_s</span><br></pre></td></tr></table></figure>
<h2 id="loss">Loss</h2>
<p>For doing training we need a loss function. VAE combines two type of losses - A loss from reconstructing the image. This is simply a Cross Entropy (CE) or Mean Square Error (MSE) between decoded image and original image - KL divergence: this loss function is for latent variable <span class="math inline">\(Z\)</span>,What we like to do is to make <span class="math inline">\(P(z | input)\)</span>,as close as possible to standard normal (with mean zero and variance 1). Since <span class="math inline">\(z\)</span> has normal distribution with mean m and variance s. <span class="math inline">\(z ~ N(m, s)\)</span> we can use this simple formula to calculate the loss function of z.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">input_image, recon_image, mu, log_var</span>):</span></span><br><span class="line">    CE = functional.binary_cross_entropy(recon_image, input_image.view(-<span class="number">1</span>, np.prod(shape)), reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">    KLD = -<span class="number">0.5</span> * torch.<span class="built_in">sum</span>(<span class="number">1</span> + log_var - mu.<span class="built_in">pow</span>(<span class="number">2</span>) - log_var.exp())</span><br><span class="line">    <span class="keyword">return</span> KLD + CE</span><br></pre></td></tr></table></figure>
<h2 id="train">Train</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">vae = VAE(<span class="number">40</span>)</span><br><span class="line">optimizer = Adam(vae.parameters())</span><br><span class="line">train_loss = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_generator):</span><br><span class="line">        images,labels = data</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        recon_image, s, mu = vae(images)</span><br><span class="line">        l = loss(images, recon_image, mu, s)</span><br><span class="line">        l.backward()</span><br><span class="line">        train_loss.append(l.item() / <span class="built_in">len</span>(images))</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchviz <span class="keyword">import</span> make_dot</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>blue boxes</strong>: these correspond to the tensors we use as parameters, the ones we’re asking PyTorch to compute gradients for;</li>
<li><strong>gray box</strong>: a Python operation that involves a gradient-computing tensor or its dependencies;</li>
<li><strong>green box</strong>: the same as the gray box, except it is the starting point for the computation of gradients (assuming the backward()method is called from the variable used to visualize the graph)— they are computed from the bottom-up in a graph.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make_dot(l)</span><br></pre></td></tr></table></figure>
<figure>
<img src="output_15_0.svg" alt="svg" /><figcaption aria-hidden="true">svg</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(train_loss)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x11afb1d50&gt;]</code></pre>
<figure>
<img src="output_16_1.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(testloader, <span class="number">0</span>):</span><br><span class="line">        images, labels = data</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        recon_image, s, mu = vae(images)</span><br><span class="line">        recon_image_ = recon_image.view(BATCH_SIZE, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            show_images(recon_image_)</span><br></pre></td></tr></table></figure>
<figure>
<img src="output_17_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure>
<img src="output_17_1.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure>
<img src="output_17_2.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure>
<img src="output_17_3.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure>
<img src="output_17_4.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure>
<img src="output_17_5.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<blockquote>
<p>reference https://kexue.fm/archives/5253 http://sofasofa.io/forum_main_post.php?postid=1002963</p>
</blockquote>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Neural-Network-Training/2019/10/05/" rel="prev" title="Neural Network Training">
      <i class="fa fa-chevron-left"></i> Neural Network Training
    </a></div>
      <div class="post-nav-item">
    <a href="/BackPropagation-through-time/2019/10/12/" rel="next" title="Back Propagation Through Time">
      Back Propagation Through Time <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#pre-knowledge"><span class="nav-number">1.</span> <span class="nav-text">Pre-knowledge</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83i.i.d"><span class="nav-number">1.1.</span> <span class="nav-text">什么是独立同分布(i.i.d)？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%88%86%E5%B8%83"><span class="nav-number">1.2.</span> <span class="nav-text">什么是数据集的分布？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E5%8F%98%E9%87%8Flatent-variable"><span class="nav-number">1.3.</span> <span class="nav-text">隐变量（latent variable）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%8F%98%E6%8D%A2"><span class="nav-number">2.</span> <span class="nav-text">分布变换</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#auto-encoder"><span class="nav-number">3.</span> <span class="nav-text">Auto-Encoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#vae"><span class="nav-number">4.</span> <span class="nav-text">VAE</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#reparameterization-trick"><span class="nav-number">4.1.</span> <span class="nav-text">Reparameterization trick</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#project"><span class="nav-number">5.</span> <span class="nav-text">Project</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#data"><span class="nav-number">6.</span> <span class="nav-text">Data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#visualization"><span class="nav-number">7.</span> <span class="nav-text">Visualization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#network-architecture"><span class="nav-number">8.</span> <span class="nav-text">Network Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#encoder"><span class="nav-number">8.1.</span> <span class="nav-text">Encoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decoder"><span class="nav-number">8.2.</span> <span class="nav-text">Decoder</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#loss"><span class="nav-number">9.</span> <span class="nav-text">Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#train"><span class="nav-number">10.</span> <span class="nav-text">Train</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">223</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail → mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'en' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
