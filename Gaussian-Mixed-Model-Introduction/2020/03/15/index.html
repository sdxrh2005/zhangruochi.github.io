<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Gaussian Mixture Models Introduction">
<meta property="og:type" content="article">
<meta property="og:title" content="Gaussian Mixed Model Introduction">
<meta property="og:url" content="https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:description" content="Gaussian Mixture Models Introduction">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/1.png">
<meta property="og:image" content="https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/2.png">
<meta property="article:published_time" content="2020-03-14T16:44:46.000Z">
<meta property="article:modified_time" content="2020-03-15T07:08:58.000Z">
<meta property="article:author" content="Ruochi Zhang">
<meta property="article:tag" content="Machine Learning Algorithm">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/1.png">

<link rel="canonical" href="https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Gaussian Mixed Model Introduction | RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Gaussian-Mixed-Model-Introduction/2020/03/15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Gaussian Mixed Model Introduction
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-15 00:44:46 / Modified: 15:08:58" itemprop="dateCreated datePublished" datetime="2020-03-15T00:44:46+08:00">2020-03-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Gaussian-Mixed-Model-Introduction/2020/03/15/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Gaussian-Mixed-Model-Introduction/2020/03/15/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <div class="post-description">Gaussian Mixture Models Introduction</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="gaussian-mixture-models高斯混合模型">Gaussian Mixture Models(高斯混合模型)</h2>
<p>高斯模型即正态分布，高斯混合模型就是几个正态分布的叠加，每一个正态分布代表一个类别，所以和K-means很像，高斯混合模型也可以用来做无监督的聚类分析。</p>
<h2 id="math">Math</h2>
<h3 id="jensens-inequality">Jensen’s inequality</h3>
<p>For any concave function, we have</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="1.png" width = "50%" height="50%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
Property of concave function
</div>
</center>
<p><span class="math display">\[f(\alpha a + (1-\alpha)b) \geq \alpha f(a) + (1 - \alpha) f(b)\]</span></p>
<p>Then, we have:</p>
<p><span class="math display">\[f(\mathbb{E}_{p(t)}t) \geq \mathbb{E}_{p(t)}f(t)\]</span></p>
<h3 id="kullbackleibler-divergence">Kullback–Leibler divergence</h3>
<p><span class="math display">\[\mathcal K \mathcal L (q || p) = \int q(x) log\frac{q(x)}{p(x)}dx\]</span></p>
<h3 id="training">Training</h3>
<p><span class="math display">\[max_{\theta} \prod_{i=1}^{N} p(x_i | \theta) = \prod_{i = 1}^{N} (\pi_1 \mathcal{N} (x_i | \mu_1, \mathbb{E_1}) + \cdots )\]</span></p>
<p><span class="math display">\[\text{subject to} \qquad \pi_1 + \pi_2 + \pi_3 = 1; \pi_k \geq 0; k = 1,2,3\]</span></p>
<h3 id="introducing-latent-variable">Introducing latent variable</h3>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="2.png" width = "50%" height="50%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
Latent Variable
</div>
</center>
<p><span class="math display">\[p(t=c| \theta) = \pi_c \]</span> <span class="math display">\[p(x | t = c , \theta) = \mathcal N ( x | \mu_c,\mathbb{E_c} )\]</span></p>
<h3 id="general-form-of-expectation-maximization">General form of Expectation Maximization</h3>
<p><span class="math display">\[p(x_i | \theta) = \sum_{c=1}^{3}p(x_i | t_i = c , \theta) p(t_i = c | \theta)\]</span></p>
<h2 id="基本步骤">基本步骤</h2>
<h3 id="概率角度">概率角度</h3>
<ol type="1">
<li>初始化<span class="math inline">\(\theta^{old}\)</span></li>
<li>E step: 用 <span class="math inline">\(\theta^{old}\)</span>计算样本对应隐变量的概率分布，即求后验概率：<span class="math inline">\(p(Z|X,\theta^{old})\)</span>。然后计算完全数据的对数似然对后验概率的期望，它是变量<span class="math inline">\(\theta\)</span>的函数: <span class="math display">\[Q(\theta, \theta^{old}) = \sum_{Z}p(Z|X, \theta^{old})ln p(X,Z|\theta)\]</span></li>
<li>M step: 极大化Q函数,得到<span class="math inline">\(\theta^{new}\)</span></li>
<li>若不收敛、持续迭代。</li>
</ol>
<h3 id="程序角度">程序角度</h3>
<ol type="1">
<li>猜测有几个类别，既有几个高斯分布;</li>
<li>针对每一个高斯分布，随机给其均值和方差进行赋值;</li>
<li>针对每一个样本，计算其在各个高斯分布下的概率; <span class="math display">\[f(x)=\frac{1 }{\sqrt\times\sigma}e^{\frac{1}{2}(\frac{x-\mu}{\sigma})^2}\]</span></li>
<li>针对每一个高斯分布，每一个样本对该高斯分布的贡献可以由其下的概率表示，如概率大则表示贡献大，反之亦然。这样把样本对该高斯分布的贡献作为权重来计算加权的均值和方差。之后替代其原本的均值和方差;</li>
<li>重复3~4直到每一个高斯分布的均值和方差收敛;</li>
<li>当高斯混合模型的特征值维数大于一维时，在计算加权的时候还要计算协方差，即要考虑不同维度之间的相互关联.</li>
</ol>
<blockquote>
<p>即通过模型来计算数据的期望值。通过更新参数μ和σ来让期望值最大化。这个过程可以不断迭代直到两次迭代中生成的参数变化非常小为止。该过程和k-means的算法训练过程很相似（k-means不断更新类中心来让结果最大化），只不过在这里的高斯模型中，我们需要同时更新两个参数：分布的均值和标准差。</p>
</blockquote>
<h2 id="gmm-vs-kmeans">GMM VS KMeans</h2>
<p>KMeans 将样本分到离其最近的聚类中心所在的簇，也就是每个样本数据属于某簇的概率非零即1。对比KMeans，高斯混合的不同之处在于，样本点属于某簇的概率不是非零即1的，而是属于不同簇有不同的概率值。高斯混合模型假设所有样本点是由K个高斯分布混合而成的。</p>
<h2 id="implementing-the-emexpectation-maximization-algorithm-for-gaussian-mixture-models">Implementing the EM(Expectation Maximization) algorithm for Gaussian mixture models</h2>
<h3 id="log-likelihood">Log likelihood</h3>
<p>We provide a function to calculate log likelihood for mixture of Gaussians. The log likelihood quantifies the probability of observing a given set of data under a particular setting of the parameters in our model. We will use this to assess convergence of our EM algorithm; specifically, we will keep looping through EM update steps until the log likehood ceases to increase at a certain rate.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_sum_exp</span>(<span class="params">Z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Compute log(\sum_i exp(Z_i)) for some array Z.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">max</span>(Z) + np.log(np.<span class="built_in">sum</span>(np.exp(Z - np.<span class="built_in">max</span>(Z))))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loglikelihood</span>(<span class="params">data, weights, means, covs</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Compute the loglikelihood of the data for a Gaussian mixture model with the given parameters. &quot;&quot;&quot;</span></span><br><span class="line">    num_clusters = <span class="built_in">len</span>(means)</span><br><span class="line">    num_dim = <span class="built_in">len</span>(data[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    ll = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">        </span><br><span class="line">        Z = np.zeros(num_clusters)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(num_clusters):</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Compute (x-mu)^T * Sigma^&#123;-1&#125; * (x-mu)</span></span><br><span class="line">            delta = np.array(d) - means[k]</span><br><span class="line">            exponent_term = np.dot(delta.T, np.dot(np.linalg.inv(covs[k]), delta))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Compute loglikelihood contribution for this data point and this cluster</span></span><br><span class="line">            Z[k] += np.log(weights[k])</span><br><span class="line">            Z[k] -= <span class="number">1</span>/<span class="number">2.</span> * (num_dim * np.log(<span class="number">2</span>*np.pi) + np.log(np.linalg.det(covs[k])) + exponent_term)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Increment loglikelihood contribution of this data point across all clusters</span></span><br><span class="line">        ll += log_sum_exp(Z)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> ll</span><br></pre></td></tr></table></figure>
<h3 id="e-step-assign-cluster-responsibilities-given-current-parameters">E-step: assign cluster responsibilities, given current parameters</h3>
<p>The first step in the EM algorithm is to compute cluster responsibilities. Let <span class="math inline">\(r_{ik}\)</span> denote the responsibility of cluster <span class="math inline">\(k\)</span> for data point <span class="math inline">\(i\)</span>. Note that cluster responsibilities are fractional parts: Cluster responsibilities for a single data point <span class="math inline">\(i\)</span> should sum to 1. <span class="math display">\[
r_{i1} + r_{i2} + \ldots + r_{iK} = 1
\]</span></p>
<p>To figure how much a cluster is responsible for a given data point, we compute the likelihood of the data point under the particular cluster assignment, multiplied by the weight of the cluster. For data point <span class="math inline">\(i\)</span> and cluster <span class="math inline">\(k\)</span>, this quantity is <span class="math display">\[
r_{ik} \propto \pi_k N(x_i | \mu_k, \Sigma_k)
\]</span> where <span class="math inline">\(N(x_i | \mu_k, \Sigma_k)\)</span> is the Gaussian distribution for cluster <span class="math inline">\(k\)</span> (with mean <span class="math inline">\(\mu_k\)</span> and covariance <span class="math inline">\(\Sigma_k\)</span>).</p>
<p>We used <span class="math inline">\(\propto\)</span> because the quantity <span class="math inline">\(N(x_i | \mu_k, \Sigma_k)\)</span> is not yet the responsibility we want. To ensure that all responsibilities over each data point add up to 1, we add the normalization constant in the denominator: <span class="math display">\[
r_{ik} = \frac{\pi_k N(x_i | \mu_k, \Sigma_k)}{\sum_{k=1}^{K} \pi_k N(x_i | \mu_k, \Sigma_k)}.
\]</span></p>
<p>Complete the following function that computes <span class="math inline">\(r_{ik}\)</span> for all data points <span class="math inline">\(i\)</span> and clusters <span class="math inline">\(k\)</span>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_responsibilities</span>(<span class="params">data, weights, means, covariances</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;E-step: compute responsibilities, given the current parameters&#x27;&#x27;&#x27;</span></span><br><span class="line">    num_data = <span class="built_in">len</span>(data)</span><br><span class="line">    num_clusters = <span class="built_in">len</span>(means)</span><br><span class="line">    resp = np.zeros((num_data, num_clusters))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update resp matrix so that resp[i,k] is the responsibility of cluster k for data point i.</span></span><br><span class="line">    <span class="comment"># Hint: To compute likelihood of seeing data point i given cluster k, use multivariate_normal.pdf.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_data):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(num_clusters):</span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            resp[i, k] = weights[k]*multivariate_normal.pdf(data[i], mean=means[k], cov=covariances[k])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add up responsibilities over each data point and normalize</span></span><br><span class="line">    row_sums = resp.<span class="built_in">sum</span>(axis=<span class="number">1</span>)[:, np.newaxis]</span><br><span class="line">    resp = resp / row_sums</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> resp</span><br></pre></td></tr></table></figure>
<h3 id="m-step-update-parameters-given-current-cluster-responsibilities">M-step: Update parameters, given current cluster responsibilities</h3>
<p>Once the cluster responsibilities are computed, we update the parameters (weights, means, and covariances) associated with the clusters.</p>
<p><strong>Computing soft counts</strong>. Before updating the parameters, we first compute what is known as "soft counts". The soft count of a cluster is the sum of all cluster responsibilities for that cluster: <span class="math display">\[
N^{\text{soft}}_k = r_{1k} + r_{2k} + \ldots + r_{Nk} = \sum_{i=1}^{N} r_{ik}
\]</span></p>
<p>where we loop over data points. Note that, unlike k-means, we must loop over every single data point in the dataset. This is because all clusters are represented in all data points, to a varying degree.</p>
<p>We provide the function for computing the soft counts:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_soft_counts</span>(<span class="params">resp</span>):</span></span><br><span class="line">    <span class="comment"># Compute the total responsibility assigned to each cluster, which will be useful when </span></span><br><span class="line">    <span class="comment"># implementing M-steps below. In the lectures this is called N^&#123;soft&#125;</span></span><br><span class="line">    counts = np.<span class="built_in">sum</span>(resp, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> counts</span><br></pre></td></tr></table></figure>
<p><strong>Updating weights.</strong> The cluster weights show us how much each cluster is represented over all data points. The weight of cluster <span class="math inline">\(k\)</span> is given by the ratio of the soft count <span class="math inline">\(N^{\text{soft}}_{k}\)</span> to the total number of data points <span class="math inline">\(N\)</span>: <span class="math display">\[
\hat{\pi}_k = \frac{N^{\text{soft}}_{k}}{N}
\]</span></p>
<p>Notice that <span class="math inline">\(N\)</span> is equal to the sum over the soft counts <span class="math inline">\(N^{\text{soft}}_{k}\)</span> of all clusters.</p>
<p>Complete the following function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_weights</span>(<span class="params">counts</span>):</span></span><br><span class="line">    num_clusters = <span class="built_in">len</span>(counts)</span><br><span class="line">    weights = [<span class="number">0.</span>] * num_clusters</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(num_clusters):</span><br><span class="line">        <span class="comment"># Update the weight for cluster k using the M-step update rule for the cluster weight, \hat&#123;\pi&#125;_k.</span></span><br><span class="line">        <span class="comment"># HINT: compute # of data points by summing soft counts.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        weights[k] = counts[k] / np.<span class="built_in">sum</span>(counts)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
<p><strong>Updating means</strong>. The mean of each cluster is set to the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Weighted_arithmetic_mean">weighted average</a> of all data points, weighted by the cluster responsibilities: <span class="math display">\[
\hat{\mu}_k = \frac{1}{N_k^{\text{soft}}} \sum_{i=1}^N r_{ik}x_i
\]</span></p>
<p>Complete the following function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_means</span>(<span class="params">data, resp, counts</span>):</span></span><br><span class="line">    num_clusters = <span class="built_in">len</span>(counts)</span><br><span class="line">    num_data = <span class="built_in">len</span>(data)</span><br><span class="line">    means = [np.zeros(<span class="built_in">len</span>(data[<span class="number">0</span>]))] * num_clusters</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(num_clusters):</span><br><span class="line">        <span class="comment"># Update means for cluster k using the M-step update rule for the mean variables.</span></span><br><span class="line">        <span class="comment"># This will assign the variable means[k] to be our estimate for \hat&#123;\mu&#125;_k.</span></span><br><span class="line">        weighted_sum = <span class="number">0.</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_data):</span><br><span class="line">            <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">            weighted_sum += data[i] * resp[i][k]</span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        means[k] = weighted_sum / counts[k]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> means</span><br></pre></td></tr></table></figure>
<p><strong>Updating covariances</strong>. The covariance of each cluster is set to the weighted average of all <a target="_blank" rel="noopener" href="https://people.duke.edu/~ccc14/sta-663/LinearAlgebraReview.html">outer products</a>, weighted by the cluster responsibilities: <span class="math display">\[
\hat{\Sigma}_k = \frac{1}{N^{\text{soft}}_k}\sum_{i=1}^N r_{ik} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T
\]</span></p>
<p>The "outer product" in this context refers to the matrix product <span class="math display">\[
(x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T.
\]</span> Letting <span class="math inline">\((x_i - \hat{\mu}_k)\)</span> to be <span class="math inline">\(d \times 1\)</span> column vector, this product is a <span class="math inline">\(d \times d\)</span> matrix. Taking the weighted average of all outer products gives us the covariance matrix, which is also <span class="math inline">\(d \times d\)</span>.</p>
<p>Complete the following function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_covariances</span>(<span class="params">data, resp, counts, means</span>):</span></span><br><span class="line">    num_clusters = <span class="built_in">len</span>(counts)</span><br><span class="line">    num_dim = <span class="built_in">len</span>(data[<span class="number">0</span>])</span><br><span class="line">    num_data = <span class="built_in">len</span>(data)</span><br><span class="line">    covariances = [np.zeros((num_dim,num_dim))] * num_clusters</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(num_clusters):</span><br><span class="line">        <span class="comment"># Update covariances for cluster k using the M-step update rule for covariance variables.</span></span><br><span class="line">        <span class="comment"># This will assign the variable covariances[k] to be the estimate for \hat&#123;\Sigma&#125;_k.</span></span><br><span class="line">        weighted_sum = np.zeros((num_dim, num_dim))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_data):</span><br><span class="line">            <span class="comment"># YOUR CODE HERE (Hint: Use np.outer on the data[i] and this cluster&#x27;s mean)</span></span><br><span class="line">            weighted_sum += resp[i][k]*np.outer(data[i] - means[k], data[i] - means[k])</span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        covariances[k] = weighted_sum / counts[k]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> covariances</span><br></pre></td></tr></table></figure>
<h3 id="the-em-algorithm">The EM algorithm</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SOLUTION</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">EM</span>(<span class="params">data, init_means, init_covariances, init_weights, maxiter=<span class="number">1000</span>, thresh=<span class="number">1e-4</span></span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Make copies of initial parameters, which we will update during each iteration</span></span><br><span class="line">    means = init_means[:]</span><br><span class="line">    covariances = init_covariances[:]</span><br><span class="line">    weights = init_weights[:]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Infer dimensions of dataset and the number of clusters</span></span><br><span class="line">    num_data = <span class="built_in">len</span>(data)</span><br><span class="line">    num_dim = <span class="built_in">len</span>(data[<span class="number">0</span>])</span><br><span class="line">    num_clusters = <span class="built_in">len</span>(means)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize some useful variables</span></span><br><span class="line">    resp = np.zeros((num_data, num_clusters))</span><br><span class="line">    ll = loglikelihood(data, weights, means, covariances)</span><br><span class="line">    ll_trace = [ll]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(maxiter):</span><br><span class="line">        <span class="keyword">if</span> it % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Iteration %s&quot;</span> % it)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># E-step: compute responsibilities</span></span><br><span class="line">        resp = compute_responsibilities(data, weights, means, covariances)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># M-step</span></span><br><span class="line">        <span class="comment"># Compute the total responsibility assigned to each cluster, which will be useful when </span></span><br><span class="line">        <span class="comment"># implementing M-steps below. In the lectures this is called N^&#123;soft&#125;</span></span><br><span class="line">        counts = compute_soft_counts(resp)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the weight for cluster k using the M-step update rule for the cluster weight, \hat&#123;\pi&#125;_k.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        weights = compute_weights(counts)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update means for cluster k using the M-step update rule for the mean variables.</span></span><br><span class="line">        <span class="comment"># This will assign the variable means[k] to be our estimate for \hat&#123;\mu&#125;_k.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        means = compute_means(data, resp, counts)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update covariances for cluster k using the M-step update rule for covariance variables.</span></span><br><span class="line">        <span class="comment"># This will assign the variable covariances[k] to be the estimate for \hat&#123;\Sigma&#125;_k.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        covariances = compute_covariances(data, resp, counts, means)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the loglikelihood at this iteration</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        ll_latest = loglikelihood(data, weights, means, covariances)</span><br><span class="line">        ll_trace.append(ll_latest)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check for convergence in log-likelihood and store</span></span><br><span class="line">        <span class="keyword">if</span> (ll_latest - ll) &lt; thresh <span class="keyword">and</span> ll_latest &gt; -np.inf:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        ll = ll_latest</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> it % <span class="number">5</span> != <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Iteration %s&quot;</span> % it)</span><br><span class="line">    </span><br><span class="line">    out = &#123;<span class="string">&#x27;weights&#x27;</span>: weights, <span class="string">&#x27;means&#x27;</span>: means, <span class="string">&#x27;covs&#x27;</span>: covariances, <span class="string">&#x27;loglik&#x27;</span>: ll_trace, <span class="string">&#x27;resp&#x27;</span>: resp&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p><strong>Reference from <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29538307">https://zhuanlan.zhihu.com/p/29538307</a></strong><br />
<strong>Reference from <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31103654">https://zhuanlan.zhihu.com/p/31103654</a></strong><br />
<strong>Reference from coursera course Machine Learning Foundation from University of Washington</strong></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning-Algorithm/" rel="tag"># Machine Learning Algorithm</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Lifelong-Learning/2020/03/14/" rel="prev" title="Lifelong Learning">
      <i class="fa fa-chevron-left"></i> Lifelong Learning
    </a></div>
      <div class="post-nav-item">
    <a href="/Fucking-distributions/2020/03/22/" rel="next" title="Fucking distributions">
      Fucking distributions <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#gaussian-mixture-models%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.</span> <span class="nav-text">Gaussian Mixture Models(高斯混合模型)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#math"><span class="nav-number">2.</span> <span class="nav-text">Math</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#jensens-inequality"><span class="nav-number">2.1.</span> <span class="nav-text">Jensen’s inequality</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kullbackleibler-divergence"><span class="nav-number">2.2.</span> <span class="nav-text">Kullback–Leibler divergence</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#training"><span class="nav-number">2.3.</span> <span class="nav-text">Training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#introducing-latent-variable"><span class="nav-number">2.4.</span> <span class="nav-text">Introducing latent variable</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#general-form-of-expectation-maximization"><span class="nav-number">2.5.</span> <span class="nav-text">General form of Expectation Maximization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4"><span class="nav-number">3.</span> <span class="nav-text">基本步骤</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E8%A7%92%E5%BA%A6"><span class="nav-number">3.1.</span> <span class="nav-text">概率角度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A8%8B%E5%BA%8F%E8%A7%92%E5%BA%A6"><span class="nav-number">3.2.</span> <span class="nav-text">程序角度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gmm-vs-kmeans"><span class="nav-number">4.</span> <span class="nav-text">GMM VS KMeans</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#implementing-the-emexpectation-maximization-algorithm-for-gaussian-mixture-models"><span class="nav-number">5.</span> <span class="nav-text">Implementing the EM(Expectation Maximization) algorithm for Gaussian mixture models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#log-likelihood"><span class="nav-number">5.1.</span> <span class="nav-text">Log likelihood</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#e-step-assign-cluster-responsibilities-given-current-parameters"><span class="nav-number">5.2.</span> <span class="nav-text">E-step: assign cluster responsibilities, given current parameters</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#m-step-update-parameters-given-current-cluster-responsibilities"><span class="nav-number">5.3.</span> <span class="nav-text">M-step: Update parameters, given current cluster responsibilities</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-em-algorithm"><span class="nav-number">5.4.</span> <span class="nav-text">The EM algorithm</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">211</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">42</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail → mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'en' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
