<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="RUOCHI.AI">
<meta property="og:url" content="https://zhangruochi.com/page/4/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Ruochi Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhangruochi.com/page/4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Understanding-Deepfakes-with-Keras/2020/07/30/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Understanding-Deepfakes-with-Keras/2020/07/30/" class="post-title-link" itemprop="url">Understanding Deepfakes with Keras</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-07-30 15:00:32 / Modified: 15:03:40" itemprop="dateCreated datePublished" datetime="2020-07-30T15:00:32+08:00">2020-07-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Understanding-Deepfakes-with-Keras/2020/07/30/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Understanding-Deepfakes-with-Keras/2020/07/30/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="understanding-deepfakes-with-keras">Understanding Deepfakes with Keras</h1>
<figure>
<img src="DCGAN.png" alt="DCGAN" /><figcaption aria-hidden="true">DCGAN</figcaption>
</figure>
<h1 id="task-1-importing-libraries-and-helper-functions">Task 1: Importing Libraries and Helper Functions</h1>
<p>Please note: If you haven't already, please install the required packages by executing the code cell below.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip3 install tensorflow==2.1.0 pillow matplotlib</span></span><br><span class="line"><span class="comment"># !pip3 install git+https://github.com/am1tyadav/tfutils.git</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tfutils</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, Flatten, Conv2D, BatchNormalization</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Conv2DTranspose, Reshape, LeakyReLU</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model, Sequential</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;TensorFlow version:&#x27;</span>, tf.__version__)</span><br></pre></td></tr></table></figure>
<pre><code>TensorFlow version: 2.1.0</code></pre>
<h1 id="task-2-importing-and-plotting-the-data">Task 2: Importing and Plotting the Data</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(x_train, y_train), (x_test, y_test) = tfutils.datasets.mnist.load_data(one_hot=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">x_train = tfutils.datasets.mnist.load_subset([<span class="number">0</span>], x_train, y_train)</span><br><span class="line">x_test = tfutils.datasets.mnist.load_subset([<span class="number">0</span>], x_test, y_test)</span><br><span class="line"></span><br><span class="line">x = np.concatenate([x_train, x_test], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure>
<pre><code>(6903, 784)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tfutils.datasets.mnist.plot_ten_random_examples(plt, x, np.zeros((x.shape[<span class="number">0</span>], <span class="number">1</span>))).show()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;IPython.core.display.Javascript object&gt;</code></pre>
<p><img src="data:image/png;base64,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" width="1000"></p>
<h1 id="task-3-discriminator">Task 3: Discriminator</h1>
<figure>
<img src="artist_critic.png" alt="Artist and Critic" /><figcaption aria-hidden="true">Artist and Critic</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">size = <span class="number">28</span></span><br><span class="line">noise_dim = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">discriminator = Sequential([</span><br><span class="line">    Conv2D(<span class="number">64</span>, <span class="number">3</span>, strides=<span class="number">2</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)),</span><br><span class="line">    LeakyReLU(),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    Conv2D(<span class="number">128</span>, <span class="number">5</span>, strides=<span class="number">2</span>),</span><br><span class="line">    LeakyReLU(),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    Conv2D(<span class="number">256</span>, <span class="number">5</span>, strides=<span class="number">2</span>),</span><br><span class="line">    LeakyReLU(),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    Flatten(),</span><br><span class="line">    Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">opt = tf.keras.optimizers.Adam(lr=<span class="number">2e-4</span>, beta_1=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">discriminator.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=opt, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">discriminator.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 13, 13, 64)        640       
_________________________________________________________________
leaky_re_lu (LeakyReLU)      (None, 13, 13, 64)        0         
_________________________________________________________________
batch_normalization (BatchNo (None, 13, 13, 64)        256       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 5, 5, 128)         204928    
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 5, 5, 128)         0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 5, 5, 128)         512       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 1, 1, 256)         819456    
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 1, 1, 256)         0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 1, 1, 256)         1024      
_________________________________________________________________
flatten (Flatten)            (None, 256)               0         
_________________________________________________________________
dense (Dense)                (None, 1)                 257       
=================================================================
Total params: 1,027,073
Trainable params: 1,026,177
Non-trainable params: 896
_________________________________________________________________</code></pre>
<h1 id="task-4-generator">Task 4: Generator</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">generator = Sequential([</span><br><span class="line">    Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(noise_dim,)),</span><br><span class="line">    Reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>)),</span><br><span class="line">    </span><br><span class="line">    Conv2DTranspose(<span class="number">256</span>, <span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    Conv2DTranspose(<span class="number">128</span>, <span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line"></span><br><span class="line">    Conv2DTranspose(<span class="number">64</span>, <span class="number">5</span>, strides=<span class="number">2</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    Conv2DTranspose(<span class="number">32</span>, <span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line"></span><br><span class="line">    Conv2DTranspose(<span class="number">1</span>, <span class="number">4</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line"></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">generator.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 256)               512       
_________________________________________________________________
reshape (Reshape)            (None, 1, 1, 256)         0         
_________________________________________________________________
conv2d_transpose (Conv2DTran (None, 5, 5, 256)         1638656   
_________________________________________________________________
batch_normalization_3 (Batch (None, 5, 5, 256)         1024      
_________________________________________________________________
conv2d_transpose_1 (Conv2DTr (None, 9, 9, 128)         819328    
_________________________________________________________________
batch_normalization_4 (Batch (None, 9, 9, 128)         512       
_________________________________________________________________
conv2d_transpose_2 (Conv2DTr (None, 21, 21, 64)        204864    
_________________________________________________________________
batch_normalization_5 (Batch (None, 21, 21, 64)        256       
_________________________________________________________________
conv2d_transpose_3 (Conv2DTr (None, 25, 25, 32)        51232     
_________________________________________________________________
batch_normalization_6 (Batch (None, 25, 25, 32)        128       
_________________________________________________________________
conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         513       
=================================================================
Total params: 2,717,025
Trainable params: 2,716,065
Non-trainable params: 960
_________________________________________________________________</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">noise = np.random.randn(<span class="number">1</span>, noise_dim)</span><br><span class="line">gen_image = generator.predict(noise)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(np.reshape(gen_image, (<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">&#x27;binary&#x27;</span>);</span><br></pre></td></tr></table></figure>
<pre><code>&lt;IPython.core.display.Javascript object&gt;</code></pre>
<p><img src="data:image/png;base64,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" width="640"></p>
<h1 id="task-5-generative-adversarial-network-gan">Task 5: Generative Adversarial Network (GAN)</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input_layer = tf.keras.layers.Input(shape=(noise_dim,))</span><br><span class="line">gen_out = generator(input_layer)</span><br><span class="line">disc_out = discriminator(gen_out)</span><br><span class="line"></span><br><span class="line">gan = Model(</span><br><span class="line">    input_layer,</span><br><span class="line">    disc_out</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">discriminator.trainable = <span class="literal">False</span></span><br><span class="line">gan.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=opt, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">gan.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
sequential_1 (Sequential)    (None, 28, 28, 1)         2717025   
_________________________________________________________________
sequential (Sequential)      (None, 1)                 1027073   
=================================================================
Total params: 3,744,098
Trainable params: 2,716,065
Non-trainable params: 1,028,033
_________________________________________________________________</code></pre>
<h1 id="tasks-6-and-7-training-the-gan">Tasks 6 and 7: Training the GAN</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">25</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">steps_per_epoch = <span class="built_in">int</span>(<span class="number">2</span> * x.shape[<span class="number">0</span>]/batch_size)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Steps per epoch=&#x27;</span>, steps_per_epoch)</span><br><span class="line"></span><br><span class="line">dp = tfutils.plotting.DynamicPlot(plt, <span class="number">5</span>, <span class="number">5</span>, (<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, epochs):</span><br><span class="line">    </span><br><span class="line">    dp.start_of_epoch(e)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, steps_per_epoch):</span><br><span class="line">        true_examples = x[<span class="built_in">int</span>(batch_size/<span class="number">2</span>)*step: <span class="built_in">int</span>(batch_size/<span class="number">2</span>)*(step + <span class="number">1</span>)]</span><br><span class="line">        true_examples = np.reshape(true_examples, (true_examples.shape[<span class="number">0</span>], <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        noise = np.random.randn(<span class="built_in">int</span>(batch_size/<span class="number">2</span>), noise_dim)</span><br><span class="line">        generated_examples = generator.predict(noise)</span><br><span class="line"></span><br><span class="line">        x_batch = np.concatenate([generated_examples, true_examples], axis=<span class="number">0</span>)</span><br><span class="line">        y_batch = np.array([<span class="number">0</span>] * <span class="built_in">int</span>(batch_size/<span class="number">2</span>) + [<span class="number">1</span>] * <span class="built_in">int</span>(batch_size/<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        indices = np.random.choice(<span class="built_in">range</span>(batch_size), batch_size, replace=<span class="literal">False</span>)</span><br><span class="line">        x_batch = x_batch[indices]</span><br><span class="line">        y_batch = y_batch[indices]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># train the discriminator</span></span><br><span class="line">        discriminator.trainable = <span class="literal">True</span></span><br><span class="line">        discriminator.train_on_batch(x_batch, y_batch)</span><br><span class="line">        discriminator.trainable = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># train the generator</span></span><br><span class="line">        loss, _ = gan.train_on_batch(noise, np.ones((<span class="built_in">int</span>(batch_size/<span class="number">2</span>), <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">        _, acc = discriminator.evaluate(x_batch, y_batch, verbose=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    noise = np.random.randn(<span class="number">1</span>, noise_dim)</span><br><span class="line">    generated_example = generator.predict(noise)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    dp.end_of_epoch(np.reshape(generated_example, (<span class="number">28</span>, <span class="number">28</span>)), <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;DiscAcc:&#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(acc), <span class="string">&#x27;GANLoss:&#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(loss))</span><br></pre></td></tr></table></figure>
<pre><code>Steps per epoch= 107



&lt;IPython.core.display.Javascript object&gt;</code></pre>
<p><img src="data:image/png;base64,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" width="800"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Auto-Complete/2020/07/19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Auto-Complete/2020/07/19/" class="post-title-link" itemprop="url">Auto-Complete</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-07-19 17:45:54 / Modified: 17:46:20" itemprop="dateCreated datePublished" datetime="2020-07-19T17:45:54+08:00">2020-07-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Auto-Complete/2020/07/19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Auto-Complete/2020/07/19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="language-models-auto-complete">Language Models: Auto-Complete</h1>
<p>In this assignment, you will build an auto-complete system. Auto-complete system is something you may see every day - When you google something, you often have suggestions to help you complete your search. - When you are writing an email, you get suggestions telling you possible endings to your sentence.</p>
<p>By the end of this assignment, you will develop a prototype of such a system.</p>
<p><img src = "stanford.png" style="width:700px;height:300px;"/></p>
<h2 id="outline">Outline</h2>
<ul>
<li><a href="#1">1 Load and Preprocess Data</a></li>
<li><a href="#1.1">1.1: Load the data</a></li>
<li><a href="#1.2">1.2 Pre-process the data</a>
<ul>
<li><a href="#ex-01">Exercise 01</a></li>
<li><a href="#ex-02">Exercise 02</a></li>
<li><a href="#ex-03">Exercise 03</a></li>
<li><a href="#ex-04">Exercise 04</a></li>
<li><a href="#ex-05">Exercise 05</a></li>
<li><a href="#ex-06">Exercise 06</a></li>
<li><a href="#ex-07">Exercise 07</a></li>
</ul></li>
<li><a href="#2">2 Develop n-gram based language models</a>
<ul>
<li><a href="#ex-08">Exercise 08</a></li>
<li><a href="#ex-09">Exercise 09</a><br />
</li>
</ul></li>
<li><a href="#3">3 Perplexity</a>
<ul>
<li><a href="#ex-10">Exercise 10</a></li>
</ul></li>
<li><a href="#4">4 Build an auto-complete system</a>
<ul>
<li><a href="#ex-11">Exercise 11</a></li>
</ul></li>
</ul>
<p>A key building block for an auto-complete system is a language model. A language model assigns the probability to a sequence of words, in a way that more "likely" sequences receive higher scores. For example, &gt;"I have a pen" is expected to have a higher probability than &gt;"I am a pen" since the first one seems to be a more natural sentence in the real world.</p>
<p>You can take advantage of this probability calculation to develop an auto-complete system.<br />
Suppose the user typed &gt;"I eat scrambled" Then you can find a word <code>x</code> such that "I eat scrambled x" receives the highest probability. If x = "eggs", the sentence would be &gt;"I eat scrambled eggs"</p>
<p>While a variety of language models have been developed, this assignment uses <strong>N-grams</strong>, a simple but powerful method for language modeling. - N-grams are also used in machine translation and speech recognition.</p>
<p>Here are the steps of this assignment:</p>
<ol type="1">
<li>Load and preprocess data
<ul>
<li>Load and tokenize data.</li>
<li>Split the sentences into train and test sets.</li>
<li>Replace words with a low frequency by an unknown marker <code>&lt;unk&gt;</code>.</li>
</ul></li>
<li>Develop N-gram based language models
<ul>
<li>Compute the count of n-grams from a given data set.</li>
<li>Estimate the conditional probability of a next word with k-smoothing.</li>
</ul></li>
<li>Evaluate the N-gram models by computing the perplexity score.</li>
<li>Use your own model to suggest an upcoming word given your sentence.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.data.path.append(<span class="string">&#x27;.&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><a name='1'></a> ## Part 1: Load and Preprocess Data</p>
<p><a name='1.1'></a> ### Part 1.1: Load the data You will use twitter data. Load the data and view the first few sentences by running the next cell.</p>
<p>Notice that data is a long string that contains many many tweets. Observe that there is a line break "" between tweets.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;en_US.twitter.txt&quot;</span>, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = f.read()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Data type:&quot;</span>, <span class="built_in">type</span>(data))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of letters:&quot;</span>, <span class="built_in">len</span>(data))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First 300 letters of the data&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-------&quot;</span>)</span><br><span class="line">display(data[<span class="number">0</span>:<span class="number">300</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Last 300 letters of the data&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-------&quot;</span>)</span><br><span class="line">display(data[-<span class="number">300</span>:])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-------&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Data type: &lt;class &#39;str&#39;&gt;
Number of letters: 3335477
First 300 letters of the data
-------



&quot;How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\nWhen you meet someone special... you&#39;ll know. Your heart will beat more rapidly and you&#39;ll smile for no reason.\nthey&#39;ve decided its more fun if I don&#39;t.\nSo Tired D; Played Lazer Tag &amp; Ran A &quot;


-------
Last 300 letters of the data
-------



&quot;ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\nColombia is with an &#39;o&#39;...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\n#GutsiestMovesYouCanMake Giving a cat a bath.\nCoffee after 5 was a TERRIBLE idea.\n&quot;


-------</code></pre>
<p><a name='1.2'></a> ### Part 1.2 Pre-process the data</p>
<p>Preprocess this data with the following steps:</p>
<ol type="1">
<li>Split data into sentences using "" as the delimiter.</li>
<li>Split each sentence into tokens. Note that in this assignment we use "token" and "words" interchangeably.</li>
<li>Assign sentences into train or test sets.</li>
<li>Find tokens that appear at least N times in the training data.</li>
<li>Replace tokens that appear less than N times by <code>&lt;unk&gt;</code></li>
</ol>
<p>Note: we omit validation data in this exercise. - In real applications, we should hold a part of data as a validation set and use it to tune our training. - We skip this process for simplicity.</p>
<p><a name='ex-01'></a> ### Exercise 01</p>
<p>Split data into sentences.</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
<li>
Use <a target="_blank" rel="noopener" href="https://docs.python.org/3/library/stdtypes.html?highlight=split#str.split" >str.split</a>
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: split_to_sentences ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_to_sentences</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Split data by linebreak &quot;\n&quot;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data: str</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A list of sentences</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    sentences = data.split(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Additional clearning (This part is already implemented)</span></span><br><span class="line">    <span class="comment"># - Remove leading and trailing spaces from each sentence</span></span><br><span class="line">    <span class="comment"># - Drop sentences if they are empty strings.</span></span><br><span class="line">    sentences = [s.strip() <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">    sentences = [s <span class="keyword">for</span> s <span class="keyword">in</span> sentences <span class="keyword">if</span> <span class="built_in">len</span>(s) &gt; <span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> sentences    </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">x = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">I have a pen.\nI have an apple. \nAh\nApple pen.\n</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">split_to_sentences(x)</span><br></pre></td></tr></table></figure>
<pre><code>I have a pen.
I have an apple. 
Ah
Apple pen.







[&#39;I have a pen.&#39;, &#39;I have an apple.&#39;, &#39;Ah&#39;, &#39;Apple pen.&#39;]</code></pre>
<p>Expected answer: <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;I have a pen.&#x27;</span>, <span class="string">&#x27;I have an apple.&#x27;</span>, <span class="string">&#x27;Ah&#x27;</span>, <span class="string">&#x27;Apple pen.&#x27;</span>]</span><br></pre></td></tr></table></figure></p>
<p><a name='ex-02'></a> ### Exercise 02 The next step is to tokenize sentences (split a sentence into a list of words). - Convert all tokens into lower case so that words which are capitalized (for example, at the start of a sentence) in the original text are treated the same as the lowercase versions of the words. - Append each tokenized list of words into a list of tokenized sentences.</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
<li>
Use <a target="_blank" rel="noopener" href="https://docs.python.org/3/library/stdtypes.html?highlight=split#str.lower" >str.lower</a> to convert strings to lowercase.
</li>
<li>
Please use <a target="_blank" rel="noopener" href="https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize" >nltk.word_tokenize</a> to split sentences into tokens.
</li>
<li>
If you used str.split insteaad of nltk.word_tokenize, there are additional edge cases to handle, such as the punctuation (comma, period) that follows a word.
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: tokenize_sentences ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_sentences</span>(<span class="params">sentences</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Tokenize sentences into tokens (words)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        sentences: List of strings</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of lists of tokens</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the list of lists of tokenized sentences</span></span><br><span class="line">    tokenized_sentences = []</span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each sentence</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert to lowercase letters</span></span><br><span class="line">        sentence = sentence.lower()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert into a list of words</span></span><br><span class="line">        tokenized = nltk.word_tokenize(sentence)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># append the list of words to the list of lists</span></span><br><span class="line">        tokenized_sentences.append(tokenized)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tokenized_sentences</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [<span class="string">&quot;Sky is blue.&quot;</span>, <span class="string">&quot;Leaves are green.&quot;</span>, <span class="string">&quot;Roses are red.&quot;</span>]</span><br><span class="line">tokenize_sentences(sentences)</span><br></pre></td></tr></table></figure>
<pre><code>[[&#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;],
 [&#39;leaves&#39;, &#39;are&#39;, &#39;green&#39;, &#39;.&#39;],
 [&#39;roses&#39;, &#39;are&#39;, &#39;red&#39;, &#39;.&#39;]]</code></pre>
<h3 id="expected-output">Expected output</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="string">&#x27;sky&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;leaves&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;green&#x27;</span>, <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;roses&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]]</span><br></pre></td></tr></table></figure>
<p><a name='ex-03'></a> ### Exercise 03</p>
<p>Use the two functions that you have just implemented to get the tokenized data. - split the data into sentences - tokenize those sentences</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: get_tokenized_data ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tokenized_data</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Make a list of tokenized sentences</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data: String</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of lists of tokens</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the sentences by splitting up the data</span></span><br><span class="line">    sentences = split_to_sentences(data)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the list of lists of tokens by tokenizing the sentences</span></span><br><span class="line">    tokenized_sentences = tokenize_sentences(sentences)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tokenized_sentences</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your function</span></span><br><span class="line">x = <span class="string">&quot;Sky is blue.\nLeaves are green\nRoses are red.&quot;</span></span><br><span class="line">get_tokenized_data(x)</span><br></pre></td></tr></table></figure>
<pre><code>[[&#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;],
 [&#39;leaves&#39;, &#39;are&#39;, &#39;green&#39;],
 [&#39;roses&#39;, &#39;are&#39;, &#39;red&#39;, &#39;.&#39;]]</code></pre>
<h5 id="expected-outcome">Expected outcome</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="string">&#x27;sky&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;leaves&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;green&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;roses&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="split-into-train-and-test-sets">Split into train and test sets</h3>
<p>Now run the cell below to split data into training and test sets.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenized_data = get_tokenized_data(data)</span><br><span class="line">random.seed(<span class="number">87</span>)</span><br><span class="line">random.shuffle(tokenized_data)</span><br><span class="line"></span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="built_in">len</span>(tokenized_data) * <span class="number">0.8</span>)</span><br><span class="line">train_data = tokenized_data[<span class="number">0</span>:train_size]</span><br><span class="line">test_data = tokenized_data[train_size:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; data are split into &#123;&#125; train and &#123;&#125; test set&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">    <span class="built_in">len</span>(tokenized_data), <span class="built_in">len</span>(train_data), <span class="built_in">len</span>(test_data)))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First training sample:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(train_data[<span class="number">0</span>])</span><br><span class="line">      </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First test sample&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(test_data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>47961 data are split into 38368 train and 9593 test set
First training sample:
[&#39;i&#39;, &#39;personally&#39;, &#39;would&#39;, &#39;like&#39;, &#39;as&#39;, &#39;our&#39;, &#39;official&#39;, &#39;glove&#39;, &#39;of&#39;, &#39;the&#39;, &#39;team&#39;, &#39;local&#39;, &#39;company&#39;, &#39;and&#39;, &#39;quality&#39;, &#39;production&#39;]
First test sample
[&#39;that&#39;, &#39;picture&#39;, &#39;i&#39;, &#39;just&#39;, &#39;seen&#39;, &#39;whoa&#39;, &#39;dere&#39;, &#39;!&#39;, &#39;!&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;]</code></pre>
<h5 id="expected-output-1">Expected output</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">47961</span> data are split into <span class="number">38368</span> train <span class="keyword">and</span> <span class="number">9593</span> test set</span><br><span class="line">First training sample:</span><br><span class="line">[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;personally&#x27;</span>, <span class="string">&#x27;would&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;as&#x27;</span>, <span class="string">&#x27;our&#x27;</span>, <span class="string">&#x27;official&#x27;</span>, <span class="string">&#x27;glove&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;team&#x27;</span>, <span class="string">&#x27;local&#x27;</span>, <span class="string">&#x27;company&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;quality&#x27;</span>, <span class="string">&#x27;production&#x27;</span>]</span><br><span class="line">First test sample</span><br><span class="line">[<span class="string">&#x27;that&#x27;</span>, <span class="string">&#x27;picture&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;just&#x27;</span>, <span class="string">&#x27;seen&#x27;</span>, <span class="string">&#x27;whoa&#x27;</span>, <span class="string">&#x27;dere&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p><a name='ex-04'></a> ### Exercise 04</p>
<p>You won't use all the tokens (words) appearing in the data for training. Instead, you will use the more frequently used words.<br />
- You will focus on the words that appear at least N times in the data. - First count how many times each word appears in the data.</p>
<p>You will need a double for-loop, one for sentences and the other for tokens within a sentence.</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
<li>
If you decide to import and use defaultdict, remember to cast the dictionary back to a regular 'dict' before returning it.
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: count_words ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_words</span>(<span class="params">tokenized_sentences</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Count the number of word appearence in the tokenized sentences</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tokenized_sentences: List of lists of strings</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dict that maps word (str) to the frequency (int)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">    word_counts = &#123;&#125;</span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop through each sentence</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> tokenized_sentences: <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Go through each token in the sentence</span></span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> sentence: <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># If the token is not in the dictionary yet, set the count to 1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> token <span class="keyword">in</span> word_counts: <span class="comment"># complete this line</span></span><br><span class="line">                word_counts[token] = <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If the token is already in the dictionary, increment the count by 1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                word_counts[token] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> word_counts</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">tokenized_sentences = [[<span class="string">&#x27;sky&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line">                       [<span class="string">&#x27;leaves&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;green&#x27;</span>, <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line">                       [<span class="string">&#x27;roses&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]]</span><br><span class="line">count_words(tokenized_sentences)</span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;sky&#39;: 1,
 &#39;is&#39;: 1,
 &#39;blue&#39;: 1,
 &#39;.&#39;: 3,
 &#39;leaves&#39;: 1,
 &#39;are&#39;: 2,
 &#39;green&#39;: 1,
 &#39;roses&#39;: 1,
 &#39;red&#39;: 1&#125;</code></pre>
<h5 id="expected-output-2">Expected output</h5>
<p>Note that the order may differ.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;sky&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&#x27;is&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&#x27;blue&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&#x27;.&#x27;</span>: <span class="number">3</span>,</span><br><span class="line"> <span class="string">&#x27;leaves&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&#x27;are&#x27;</span>: <span class="number">2</span>,</span><br><span class="line"> <span class="string">&#x27;green&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&#x27;roses&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&#x27;red&#x27;</span>: <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>
<h3 id="handling-out-of-vocabulary-words">Handling 'Out of Vocabulary' words</h3>
<p>If your model is performing autocomplete, but encounters a word that it never saw during training, it won't have an input word to help it determine the next word to suggest. The model will not be able to predict the next word because there are no counts for the current word. - This 'new' word is called an 'unknown word', or <b>out of vocabulary (OOV)</b> words. - The percentage of unknown words in the test set is called the <b> OOV </b> rate.</p>
<p>To handle unknown words during prediction, use a special token to represent all unknown words 'unk'. - Modify the training data so that it has some 'unknown' words to train on. - Words to convert into "unknown" words are those that do not occur very frequently in the training set. - Create a list of the most frequent words in the training set, called the <b> closed vocabulary </b>. - Convert all the other words that are not part of the closed vocabulary to the token 'unk'.</p>
<p><a name='ex-05'></a> ### Exercise 05</p>
<p>You will now create a function that takes in a text document and a threshold 'count_threshold'. - Any word whose count is greater than or equal to the threshold 'count_threshold' is kept in the closed vocabulary. - used that you want to keep, returns the document containing only the word closed vocabulary and the word unk.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: get_words_with_nplus_frequency ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_words_with_nplus_frequency</span>(<span class="params">tokenized_sentences, count_threshold</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Find the words that appear N times or more</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tokenized_sentences: List of lists of sentences</span></span><br><span class="line"><span class="string">        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of words that appear N times or more</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Initialize an empty list to contain the words that</span></span><br><span class="line">    <span class="comment"># appear at least &#x27;minimum_freq&#x27; times.</span></span><br><span class="line">    closed_vocab = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the word couts of the tokenized sentences</span></span><br><span class="line">    <span class="comment"># Use the function that you defined earlier to count the words</span></span><br><span class="line">    word_counts = count_words(tokenized_sentences)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># for each word and its count</span></span><br><span class="line">    <span class="keyword">for</span> word, cnt <span class="keyword">in</span> word_counts.items(): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># check that the word&#x27;s count</span></span><br><span class="line">        <span class="comment"># is at least as great as the minimum count</span></span><br><span class="line">        <span class="keyword">if</span> cnt &gt;= count_threshold:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># append the word to the list</span></span><br><span class="line">            closed_vocab.append(word)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> closed_vocab</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">tokenized_sentences = [[<span class="string">&#x27;sky&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line">                       [<span class="string">&#x27;leaves&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;green&#x27;</span>, <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line">                       [<span class="string">&#x27;roses&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]]</span><br><span class="line">tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Closed vocabulary:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tmp_closed_vocab)</span><br></pre></td></tr></table></figure>
<pre><code>Closed vocabulary:
[&#39;.&#39;, &#39;are&#39;]</code></pre>
<h5 id="expected-output-3">Expected output</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Closed vocabulary:</span><br><span class="line">[<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;are&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p><a name='ex-06'></a> ### Exercise 06</p>
<p>The words that appear 'count_threshold' times or more are in the 'closed vocabulary. - All other words are regarded as 'unknown'. - Replace words not in the closed vocabulary with the token "&lt;unk&gt;".</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: replace_oov_words_by_unk ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replace_oov_words_by_unk</span>(<span class="params">tokenized_sentences, vocabulary, unknown_token=<span class="string">&quot;&lt;unk&gt;&quot;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Replace words not in the given vocabulary with &#x27;&lt;unk&gt;&#x27; token.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tokenized_sentences: List of lists of strings</span></span><br><span class="line"><span class="string">        vocabulary: List of strings that we will use</span></span><br><span class="line"><span class="string">        unknown_token: A string representing unknown (out-of-vocabulary) words</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of lists of strings, with words not in the vocabulary replaced</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Place vocabulary into a set for faster search</span></span><br><span class="line">    vocabulary = <span class="built_in">set</span>(vocabulary)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a list that will hold the sentences</span></span><br><span class="line">    <span class="comment"># after less frequent words are replaced by the unknown token</span></span><br><span class="line">    replaced_tokenized_sentences = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each sentence</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> tokenized_sentences:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize the list that will contain</span></span><br><span class="line">        <span class="comment"># a single sentence with &quot;unknown_token&quot; replacements</span></span><br><span class="line">        replaced_sentence = []</span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># for each token in the sentence</span></span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> sentence: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Check if the token is in the closed vocabulary</span></span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">in</span> vocabulary: <span class="comment"># complete this line</span></span><br><span class="line">                <span class="comment"># If so, append the word to the replaced_sentence</span></span><br><span class="line">                replaced_sentence.append(token)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># otherwise, append the unknown token instead</span></span><br><span class="line">                replaced_sentence.append(unknown_token)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Append the list of tokens to the list of lists</span></span><br><span class="line">        replaced_tokenized_sentences.append(replaced_sentence)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> replaced_tokenized_sentences</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenized_sentences = [[<span class="string">&quot;dogs&quot;</span>, <span class="string">&quot;run&quot;</span>], [<span class="string">&quot;cats&quot;</span>, <span class="string">&quot;sleep&quot;</span>]]</span><br><span class="line">vocabulary = [<span class="string">&quot;dogs&quot;</span>, <span class="string">&quot;sleep&quot;</span>]</span><br><span class="line">tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Original sentence:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenized_sentences)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;tokenized_sentences with less frequent words converted to &#x27;&lt;unk&gt;&#x27;:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tmp_replaced_tokenized_sentences)</span><br></pre></td></tr></table></figure>
<pre><code>Original sentence:
[[&#39;dogs&#39;, &#39;run&#39;], [&#39;cats&#39;, &#39;sleep&#39;]]
tokenized_sentences with less frequent words converted to &#39;&lt;unk&gt;&#39;:
[[&#39;dogs&#39;, &#39;&lt;unk&gt;&#39;], [&#39;&lt;unk&gt;&#39;, &#39;sleep&#39;]]</code></pre>
<h3 id="expected-answer">Expected answer</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Original sentence:</span><br><span class="line">[[<span class="string">&#x27;dogs&#x27;</span>, <span class="string">&#x27;run&#x27;</span>], [<span class="string">&#x27;cats&#x27;</span>, <span class="string">&#x27;sleep&#x27;</span>]]</span><br><span class="line">tokenized_sentences with less frequent words converted to <span class="string">&#x27;&lt;unk&gt;&#x27;</span>:</span><br><span class="line">[[<span class="string">&#x27;dogs&#x27;</span>, <span class="string">&#x27;&lt;unk&gt;&#x27;</span>], [<span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;sleep&#x27;</span>]]</span><br></pre></td></tr></table></figure>
<p><a name='ex-07'></a> ### Exercise 07</p>
<p>Now we are ready to process our data by combining the functions that you just implemented.</p>
<ol type="1">
<li>Find tokens that appear at least count_threshold times in the training data.</li>
<li>Replace tokens that appear less than count_threshold times by "&lt;unk&gt;" both for training and test data.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: preprocess_data ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_data</span>(<span class="params">train_data, test_data, count_threshold</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Preprocess data, i.e.,</span></span><br><span class="line"><span class="string">        - Find tokens that appear at least N times in the training data.</span></span><br><span class="line"><span class="string">        - Replace tokens that appear less than N times by &quot;&lt;unk&gt;&quot; both for training and test data.        </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        train_data, test_data: List of lists of strings.</span></span><br><span class="line"><span class="string">        count_threshold: Words whose count is less than this are </span></span><br><span class="line"><span class="string">                      treated as unknown.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Tuple of</span></span><br><span class="line"><span class="string">        - training data with low frequent words replaced by &quot;&lt;unk&gt;&quot;</span></span><br><span class="line"><span class="string">        - test data with low frequent words replaced by &quot;&lt;unk&gt;&quot;</span></span><br><span class="line"><span class="string">        - vocabulary of words that appear n times or more in the training data</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get the closed vocabulary using the train data</span></span><br><span class="line">    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># For the train data, replace less common words with &quot;&lt;unk&gt;&quot;</span></span><br><span class="line">    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary, unknown_token=<span class="string">&quot;&lt;unk&gt;&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># For the test data, replace less common words with &quot;&lt;unk&gt;&quot;</span></span><br><span class="line">    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary, unknown_token=<span class="string">&quot;&lt;unk&gt;&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> train_data_replaced, test_data_replaced, vocabulary</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">tmp_train = [[<span class="string">&#x27;sky&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line">     [<span class="string">&#x27;leaves&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;green&#x27;</span>]]</span><br><span class="line">tmp_test = [[<span class="string">&#x27;roses&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train, </span><br><span class="line">                                                           tmp_test, </span><br><span class="line">                                                           count_threshold = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tmp_train_repl&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tmp_train_repl)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tmp_test_repl&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tmp_test_repl)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tmp_vocab&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tmp_vocab)</span><br></pre></td></tr></table></figure>
<pre><code>tmp_train_repl
[[&#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;], [&#39;leaves&#39;, &#39;are&#39;, &#39;green&#39;]]

tmp_test_repl
[[&#39;&lt;unk&gt;&#39;, &#39;are&#39;, &#39;&lt;unk&gt;&#39;, &#39;.&#39;]]

tmp_vocab
[&#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;, &#39;leaves&#39;, &#39;are&#39;, &#39;green&#39;]</code></pre>
<h5 id="expected-outcome-1">Expected outcome</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tmp_train_repl</span><br><span class="line">[[<span class="string">&#x27;sky&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;.&#x27;</span>], [<span class="string">&#x27;leaves&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;green&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">tmp_test_repl</span><br><span class="line">[[<span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">tmp_vocab</span><br><span class="line">[<span class="string">&#x27;sky&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;leaves&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;green&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h3 id="preprocess-the-train-and-test-data">Preprocess the train and test data</h3>
<p>Run the cell below to complete the preprocessing both for training and test sets.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">minimum_freq = <span class="number">2</span></span><br><span class="line">train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, </span><br><span class="line">                                                                        test_data, </span><br><span class="line">                                                                        minimum_freq)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First preprocessed training sample:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(train_data_processed[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First preprocessed test sample:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(test_data_processed[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First 10 vocabulary:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(vocabulary[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Size of vocabulary:&quot;</span>, <span class="built_in">len</span>(vocabulary))</span><br></pre></td></tr></table></figure>
<pre><code>First preprocessed training sample:
[&#39;i&#39;, &#39;personally&#39;, &#39;would&#39;, &#39;like&#39;, &#39;as&#39;, &#39;our&#39;, &#39;official&#39;, &#39;glove&#39;, &#39;of&#39;, &#39;the&#39;, &#39;team&#39;, &#39;local&#39;, &#39;company&#39;, &#39;and&#39;, &#39;quality&#39;, &#39;production&#39;]

First preprocessed test sample:
[&#39;that&#39;, &#39;picture&#39;, &#39;i&#39;, &#39;just&#39;, &#39;seen&#39;, &#39;whoa&#39;, &#39;dere&#39;, &#39;!&#39;, &#39;!&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;]

First 10 vocabulary:
[&#39;i&#39;, &#39;personally&#39;, &#39;would&#39;, &#39;like&#39;, &#39;as&#39;, &#39;our&#39;, &#39;official&#39;, &#39;glove&#39;, &#39;of&#39;, &#39;the&#39;]

Size of vocabulary: 14821</code></pre>
<h5 id="expected-output-4">Expected output</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">First preprocessed training sample:</span><br><span class="line">[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;personally&#x27;</span>, <span class="string">&#x27;would&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;as&#x27;</span>, <span class="string">&#x27;our&#x27;</span>, <span class="string">&#x27;official&#x27;</span>, <span class="string">&#x27;glove&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;team&#x27;</span>, <span class="string">&#x27;local&#x27;</span>, <span class="string">&#x27;company&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;quality&#x27;</span>, <span class="string">&#x27;production&#x27;</span>]</span><br><span class="line"></span><br><span class="line">First preprocessed test sample:</span><br><span class="line">[<span class="string">&#x27;that&#x27;</span>, <span class="string">&#x27;picture&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;just&#x27;</span>, <span class="string">&#x27;seen&#x27;</span>, <span class="string">&#x27;whoa&#x27;</span>, <span class="string">&#x27;dere&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>]</span><br><span class="line"></span><br><span class="line">First <span class="number">10</span> vocabulary:</span><br><span class="line">[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;personally&#x27;</span>, <span class="string">&#x27;would&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;as&#x27;</span>, <span class="string">&#x27;our&#x27;</span>, <span class="string">&#x27;official&#x27;</span>, <span class="string">&#x27;glove&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;the&#x27;</span>]</span><br><span class="line"></span><br><span class="line">Size of vocabulary: <span class="number">14821</span></span><br></pre></td></tr></table></figure>
<p>You are done with the preprocessing section of the assignment. Objects <code>train_data_processed</code>, <code>test_data_processed</code>, and <code>vocabulary</code> will be used in the rest of the exercises.</p>
<p><a name='2'></a> ## Part 2: Develop n-gram based language models</p>
<p>In this section, you will develop the n-grams language model. - Assume the probability of the next word depends only on the previous n-gram. - The previous n-gram is the series of the previous 'n' words.</p>
<p>The conditional probability for the word at position 't' in the sentence, given that the words preceding it are <span class="math inline">\(w_{t-1}, w_{t-2} \cdots w_{t-n}\)</span> is:</p>
<p><span class="math display">\[ P(w_t | w_{t-1}\dots w_{t-n}) \tag{1}\]</span></p>
<p>You can estimate this probability by counting the occurrences of these series of words in the training data. - The probability can be estimated as a ratio, where - The numerator is the number of times word 't' appears after words t-1 through t-n appear in the training data. - The denominator is the number of times word t-1 through t-n appears in the training data.</p>
<p><span class="math display">\[ \hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n)}{C(w_{t-1}\dots w_{t-n})} \tag{2} \]</span></p>
<ul>
<li>The function <span class="math inline">\(C(\cdots)\)</span> denotes the number of occurence of the given sequence.</li>
<li><span class="math inline">\(\hat{P}\)</span> means the estimation of <span class="math inline">\(P\)</span>.</li>
<li>Notice that denominator of the equation (2) is the number of occurence of the previous <span class="math inline">\(n\)</span> words, and the numerator is the same sequence followed by the word <span class="math inline">\(w_t\)</span>.</li>
</ul>
<p>Later, you will modify the equation (2) by adding k-smoothing, which avoids errors when any counts are zero.</p>
<p>The equation (2) tells us that to estimate probabilities based on n-grams, you need the counts of n-grams (for denominator) and (n+1)-grams (for numerator).</p>
<p><a name='ex-08'></a> ### Exercise 08 Next, you will implement a function that computes the counts of n-grams for an arbitrary number <span class="math inline">\(n\)</span>.</p>
<p>When computing the counts for n-grams, prepare the sentence beforehand by prepending <span class="math inline">\(n-1\)</span> starting markers "&lt;s&gt;" to indicate the beginning of the sentence.<br />
- For example, in the bi-gram model (N=2), a sequence with two start tokens "&lt;s&gt;&lt;s&gt;" should predict the first word of a sentence. - So, if the sentence is "I like food", modify it to be "&lt;s&gt;&lt;s&gt; I like food". - Also prepare the sentence for counting by appending an end token "&lt;e&gt;" so that the model can predict when to finish a sentence.</p>
<p>Technical note: In this implementation, you will store the counts as a dictionary. - The key of each key-value pair in the dictionary is a <strong>tuple</strong> of n words (and not a list) - The value in the key-value pair is the number of occurrences.<br />
- The reason for using a tuple as a key instead of a list is because a list in Python is a mutable object (it can be changed after it is first created). A tuple is "immutable", so it cannot be altered after it is first created. This makes a tuple suitable as a data type for the key in a dictionary.</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
<li>
To prepend or append, you can create lists and concatenate them using the + operator
</li>
<li>
To create a list of a repeated value, you can follow this syntax: <code>['a'] * 3</code> to get <code>['a','a','a']</code>
</li>
<li>
To set the range for index 'i', think of this example: An n-gram where n=2 (bigram), and the sentence is length N=5 (including two start tokens and one end token). So the index positions are <code>[0,1,2,3,4]</code>. The largest index 'i' where a bigram can start is at position i=3, because the word tokens at position 3 and 4 will form the bigram.
</li>
<li>
Remember that the <code>range()</code> function excludes the value that is used for the maximum of the range. <code> range(3) </code> produces (0,1,2) but excludes 3.
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED FUNCTION: count_n_grams ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_n_grams</span>(<span class="params">data, n, start_token=<span class="string">&#x27;&lt;s&gt;&#x27;</span>, end_token = <span class="string">&#x27;&lt;e&gt;&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Count all n-grams in the data</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data: List of lists of words</span></span><br><span class="line"><span class="string">        n: number of words in a sequence</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A dictionary that maps a tuple of n-words to its frequency</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize dictionary of n-grams and their counts</span></span><br><span class="line">    n_grams = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each sentence in the data</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> data: <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># prepend start token n times, and  append &lt;e&gt; one time</span></span><br><span class="line">        sentence = [start_token] * n + sentence + [end_token]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># convert list to tuple</span></span><br><span class="line">        <span class="comment"># So that the sequence of words can be used as</span></span><br><span class="line">        <span class="comment"># a key in the dictionary</span></span><br><span class="line">        sentence = <span class="built_in">tuple</span>(sentence)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use &#x27;i&#x27; to indicate the start of the n-gram</span></span><br><span class="line">        <span class="comment"># from index 0</span></span><br><span class="line">        <span class="comment"># to the last index where the end of the n-gram</span></span><br><span class="line">        <span class="comment"># is within the sentence.</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentence)-n+<span class="number">1</span>): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Get the n-gram from i to i+n</span></span><br><span class="line">            n_gram = sentence[i:i+n]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># check if the n-gram is in the dictionary</span></span><br><span class="line">            <span class="keyword">if</span> n_gram <span class="keyword">in</span> n_grams: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">                <span class="comment"># Increment the count for this n-gram</span></span><br><span class="line">                n_grams[n_gram] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># Initialize this n-gram count to 1</span></span><br><span class="line">                n_grams[n_gram] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">            <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> n_grams</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line"><span class="comment"># CODE REVIEW COMMENT: Outcome does not match expected outcome</span></span><br><span class="line">sentences = [[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>],</span><br><span class="line">             [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Uni-gram:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(count_n_grams(sentences, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bi-gram:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(count_n_grams(sentences, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Uni-gram:
&#123;(&#39;&lt;s&gt;&#39;,): 2, (&#39;i&#39;,): 1, (&#39;like&#39;,): 2, (&#39;a&#39;,): 2, (&#39;cat&#39;,): 2, (&#39;&lt;e&gt;&#39;,): 2, (&#39;this&#39;,): 1, (&#39;dog&#39;,): 1, (&#39;is&#39;,): 1&#125;
Bi-gram:
&#123;(&#39;&lt;s&gt;&#39;, &#39;&lt;s&gt;&#39;): 2, (&#39;&lt;s&gt;&#39;, &#39;i&#39;): 1, (&#39;i&#39;, &#39;like&#39;): 1, (&#39;like&#39;, &#39;a&#39;): 2, (&#39;a&#39;, &#39;cat&#39;): 2, (&#39;cat&#39;, &#39;&lt;e&gt;&#39;): 2, (&#39;&lt;s&gt;&#39;, &#39;this&#39;): 1, (&#39;this&#39;, &#39;dog&#39;): 1, (&#39;dog&#39;, &#39;is&#39;): 1, (&#39;is&#39;, &#39;like&#39;): 1&#125;</code></pre>
<p>Expected outcome:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Uni-gram:</span><br><span class="line">&#123;(<span class="string">&#x27;&lt;s&gt;&#x27;</span>,): <span class="number">2</span>, (<span class="string">&#x27;i&#x27;</span>,): <span class="number">1</span>, (<span class="string">&#x27;like&#x27;</span>,): <span class="number">2</span>, (<span class="string">&#x27;a&#x27;</span>,): <span class="number">2</span>, (<span class="string">&#x27;cat&#x27;</span>,): <span class="number">2</span>, (<span class="string">&#x27;&lt;e&gt;&#x27;</span>,): <span class="number">2</span>, (<span class="string">&#x27;this&#x27;</span>,): <span class="number">1</span>, (<span class="string">&#x27;dog&#x27;</span>,): <span class="number">1</span>, (<span class="string">&#x27;is&#x27;</span>,): <span class="number">1</span>&#125;</span><br><span class="line">Bi-gram:</span><br><span class="line">&#123;(<span class="string">&#x27;&lt;s&gt;&#x27;</span>, <span class="string">&#x27;&lt;s&gt;&#x27;</span>): <span class="number">2</span>, (<span class="string">&#x27;&lt;s&gt;&#x27;</span>, <span class="string">&#x27;i&#x27;</span>): <span class="number">1</span>, (<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>): <span class="number">1</span>, (<span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>): <span class="number">2</span>, (<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>): <span class="number">2</span>, (<span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;&lt;e&gt;&#x27;</span>): <span class="number">2</span>, (<span class="string">&#x27;&lt;s&gt;&#x27;</span>, <span class="string">&#x27;this&#x27;</span>): <span class="number">1</span>, (<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>): <span class="number">1</span>, (<span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>): <span class="number">1</span>, (<span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>): <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>
<p><a name='ex-09'></a> ### Exercise 09</p>
<p>Next, estimate the probability of a word given the prior 'n' words using the n-gram counts.</p>
<p><span class="math display">\[ \hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n)}{C(w_{t-1}\dots w_{t-n})} \tag{2} \]</span></p>
<p>This formula doesn't work when a count of an n-gram is zero.. - Suppose we encounter an n-gram that did not occur in the training data.<br />
- Then, the equation (2) cannot be evaluated (it becomes zero divided by zero).</p>
<p>A way to handle zero counts is to add k-smoothing.<br />
- K-smoothing adds a positive constant <span class="math inline">\(k\)</span> to each numerator and <span class="math inline">\(k \times |V|\)</span> in the denominator, where <span class="math inline">\(|V|\)</span> is the number of words in the vocabulary.</p>
<p><span class="math display">\[ \hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n) + k}{C(w_{t-1}\dots w_{t-n}) + k|V|} \tag{3} \]</span></p>
<p>For n-grams that have a zero count, the equation (3) becomes <span class="math inline">\(\frac{1}{|V|}\)</span>. - This means that any n-gram with zero count has the same probability of <span class="math inline">\(\frac{1}{|V|}\)</span>.</p>
<p>Define a function that computes the probability estimate (3) from n-gram counts and a constant <span class="math inline">\(k\)</span>.</p>
<ul>
<li>The function takes in a dictionary 'n_gram_counts', where the key is the n-gram and the value is the count of that n-gram.</li>
<li>The function also takes another dictionary n_plus1_gram_counts, which you'll use to find the count for the previous n-gram plus the current word.</li>
</ul>
<details>
<summary>
<font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
<li>
To define a tuple containing a single value, add a comma after that value. For example: <code>('apple',)</code> is a tuple containing a single string 'apple'
</li>
<li>
To concatenate two tuples, use the '+' operator
</li>
<li>
<a href="" > words </a>
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED FUNCTION: estimate_probabilityy ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">estimate_probability</span>(<span class="params">word, previous_n_gram, </span></span></span><br><span class="line"><span class="params"><span class="function">                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Estimate the probabilities of a next word using the n-gram counts with k-smoothing</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        word: next word</span></span><br><span class="line"><span class="string">        previous_n_gram: A sequence of words of length n</span></span><br><span class="line"><span class="string">        n_gram_counts: Dictionary of counts of n-grams</span></span><br><span class="line"><span class="string">        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        vocabulary_size: number of words in the vocabulary</span></span><br><span class="line"><span class="string">        k: positive constant, smoothing parameter</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A probability</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># convert list to tuple to use it as a dictionary key</span></span><br><span class="line">    previous_n_gram = <span class="built_in">tuple</span>(previous_n_gram)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the denominator</span></span><br><span class="line">    <span class="comment"># If the previous n-gram exists in the dictionary of n-gram counts,</span></span><br><span class="line">    <span class="comment"># Get its count.  Otherwise set the count to zero</span></span><br><span class="line">    <span class="comment"># Use the dictionary that has counts for n-grams</span></span><br><span class="line">    previous_n_gram_count = n_gram_counts.get(previous_n_gram, <span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Calculate the denominator using the count of the previous n gram</span></span><br><span class="line">    <span class="comment"># and apply k-smoothing</span></span><br><span class="line">    denominator = previous_n_gram_count + k * vocabulary_size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define n plus 1 gram as the previous n-gram plus the current word as a tuple</span></span><br><span class="line">    n_plus1_gram = n_gram_counts.get(previous_n_gram, <span class="number">0</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Set the count to the count in the dictionary,</span></span><br><span class="line">    <span class="comment"># otherwise 0 if not in the dictionary</span></span><br><span class="line">    <span class="comment"># use the dictionary that has counts for the n-gram plus current word</span></span><br><span class="line">    n_plus1_gram_count = n_plus1_gram_counts.get(previous_n_gram + (word, ) ,<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Define the numerator use the count of the n-gram plus current word,</span></span><br><span class="line">    <span class="comment"># and apply smoothing</span></span><br><span class="line">    numerator = n_plus1_gram_count + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the probability as the numerator divided by denominator</span></span><br><span class="line">    probability = numerator / denominator</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> probability</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>],</span><br><span class="line">             [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]]</span><br><span class="line">unique_words = <span class="built_in">list</span>(<span class="built_in">set</span>(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line">tmp_prob = estimate_probability(<span class="string">&quot;cat&quot;</span>, <span class="string">&quot;a&quot;</span>, unigram_counts, bigram_counts, <span class="built_in">len</span>(unique_words), k=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The estimated probability of word &#x27;cat&#x27; given the previous n-gram &#x27;a&#x27; is: <span class="subst">&#123;tmp_prob:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>The estimated probability of word &#39;cat&#39; given the previous n-gram &#39;a&#39; is: 0.3333</code></pre>
<h5 id="expected-output-5">Expected output</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The estimated probability of word <span class="string">&#x27;cat&#x27;</span> given the previous n-gram <span class="string">&#x27;a&#x27;</span> is: <span class="number">0.3333</span></span><br></pre></td></tr></table></figure>
<h3 id="estimate-probabilities-for-all-words">Estimate probabilities for all words</h3>
<p>The function defined below loops over all words in vocabulary to calculate probabilities for all possible words. - This function is provided for you.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">estimate_probabilities</span>(<span class="params">previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Estimate the probabilities of next words using the n-gram counts with k-smoothing</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        previous_n_gram: A sequence of words of length n</span></span><br><span class="line"><span class="string">        n_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        vocabulary: List of words</span></span><br><span class="line"><span class="string">        k: positive constant, smoothing parameter</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A dictionary mapping from next words to the probability.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># convert list to tuple to use it as a dictionary key</span></span><br><span class="line">    previous_n_gram = <span class="built_in">tuple</span>(previous_n_gram)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># add &lt;e&gt; &lt;unk&gt; to the vocabulary</span></span><br><span class="line">    <span class="comment"># &lt;s&gt; is not needed since it should not appear as the next word</span></span><br><span class="line">    vocabulary = vocabulary + [<span class="string">&quot;&lt;e&gt;&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>]</span><br><span class="line">    vocabulary_size = <span class="built_in">len</span>(vocabulary)</span><br><span class="line">    </span><br><span class="line">    probabilities = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> vocabulary:</span><br><span class="line">        probability = estimate_probability(word, previous_n_gram, </span><br><span class="line">                                           n_gram_counts, n_plus1_gram_counts, </span><br><span class="line">                                           vocabulary_size, k=k)</span><br><span class="line">        probabilities[word] = probability</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> probabilities</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>],</span><br><span class="line">             [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]]</span><br><span class="line">unique_words = <span class="built_in">list</span>(<span class="built_in">set</span>(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line">estimate_probabilities(<span class="string">&quot;a&quot;</span>, unigram_counts, bigram_counts, unique_words, k=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;dog&#39;: 0.09090909090909091,
 &#39;like&#39;: 0.09090909090909091,
 &#39;cat&#39;: 0.2727272727272727,
 &#39;i&#39;: 0.09090909090909091,
 &#39;is&#39;: 0.09090909090909091,
 &#39;this&#39;: 0.09090909090909091,
 &#39;a&#39;: 0.09090909090909091,
 &#39;&lt;e&gt;&#39;: 0.09090909090909091,
 &#39;&lt;unk&gt;&#39;: 0.09090909090909091&#125;</code></pre>
<h5 id="expected-output-6">Expected output</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;cat&#x27;</span>: <span class="number">0.2727272727272727</span>,</span><br><span class="line"> <span class="string">&#x27;i&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;this&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;a&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;is&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;like&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;dog&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;&lt;e&gt;&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;&lt;unk&gt;&#x27;</span>: <span class="number">0.09090909090909091</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Additional test</span></span><br><span class="line">trigram_counts = count_n_grams(sentences, <span class="number">3</span>)</span><br><span class="line">estimate_probabilities([<span class="string">&quot;&lt;s&gt;&quot;</span>, <span class="string">&quot;&lt;s&gt;&quot;</span>], bigram_counts, trigram_counts, unique_words, k=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;dog&#39;: 0.09090909090909091,
 &#39;like&#39;: 0.09090909090909091,
 &#39;cat&#39;: 0.09090909090909091,
 &#39;i&#39;: 0.18181818181818182,
 &#39;is&#39;: 0.09090909090909091,
 &#39;this&#39;: 0.18181818181818182,
 &#39;a&#39;: 0.09090909090909091,
 &#39;&lt;e&gt;&#39;: 0.09090909090909091,
 &#39;&lt;unk&gt;&#39;: 0.09090909090909091&#125;</code></pre>
<h5 id="expected-output-7">Expected output</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;cat&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;i&#x27;</span>: <span class="number">0.18181818181818182</span>,</span><br><span class="line"> <span class="string">&#x27;this&#x27;</span>: <span class="number">0.18181818181818182</span>,</span><br><span class="line"> <span class="string">&#x27;a&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;is&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;like&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;dog&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;&lt;e&gt;&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;&lt;unk&gt;&#x27;</span>: <span class="number">0.09090909090909091</span>&#125;</span><br></pre></td></tr></table></figure>
<h3 id="count-and-probability-matrices">Count and probability matrices</h3>
<p>As we have seen so far, the n-gram counts computed above are sufficient for computing the probabilities of the next word.<br />
- It can be more intuitive to present them as count or probability matrices. - The functions defined in the next cells return count or probability matrices. - This function is provided for you.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_count_matrix</span>(<span class="params">n_plus1_gram_counts, vocabulary</span>):</span></span><br><span class="line">    <span class="comment"># add &lt;e&gt; &lt;unk&gt; to the vocabulary</span></span><br><span class="line">    <span class="comment"># &lt;s&gt; is omitted since it should not appear as the next word</span></span><br><span class="line">    vocabulary = vocabulary + [<span class="string">&quot;&lt;e&gt;&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># obtain unique n-grams</span></span><br><span class="line">    n_grams = []</span><br><span class="line">    <span class="keyword">for</span> n_plus1_gram <span class="keyword">in</span> n_plus1_gram_counts.keys():</span><br><span class="line">        n_gram = n_plus1_gram[<span class="number">0</span>:-<span class="number">1</span>]</span><br><span class="line">        n_grams.append(n_gram)</span><br><span class="line">    n_grams = <span class="built_in">list</span>(<span class="built_in">set</span>(n_grams))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># mapping from n-gram to row</span></span><br><span class="line">    row_index = &#123;n_gram:i <span class="keyword">for</span> i, n_gram <span class="keyword">in</span> <span class="built_in">enumerate</span>(n_grams)&#125;</span><br><span class="line">    <span class="comment"># mapping from next word to column</span></span><br><span class="line">    col_index = &#123;word:j <span class="keyword">for</span> j, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocabulary)&#125;</span><br><span class="line">    </span><br><span class="line">    nrow = <span class="built_in">len</span>(n_grams)</span><br><span class="line">    ncol = <span class="built_in">len</span>(vocabulary)</span><br><span class="line">    count_matrix = np.zeros((nrow, ncol))</span><br><span class="line">    <span class="keyword">for</span> n_plus1_gram, count <span class="keyword">in</span> n_plus1_gram_counts.items():</span><br><span class="line">        n_gram = n_plus1_gram[<span class="number">0</span>:-<span class="number">1</span>]</span><br><span class="line">        word = n_plus1_gram[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocabulary:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        i = row_index[n_gram]</span><br><span class="line">        j = col_index[word]</span><br><span class="line">        count_matrix[i, j] = count</span><br><span class="line">    </span><br><span class="line">    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)</span><br><span class="line">    <span class="keyword">return</span> count_matrix</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sentences = [[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]]</span><br><span class="line">unique_words = <span class="built_in">list</span>(<span class="built_in">set</span>(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;bigram counts&#x27;</span>)</span><br><span class="line">display(make_count_matrix(bigram_counts, unique_words))</span><br></pre></td></tr></table></figure>
<pre><code>bigram counts</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
dog
</th>
<th>
like
</th>
<th>
cat
</th>
<th>
i
</th>
<th>
is
</th>
<th>
this
</th>
<th>
a
</th>
<th>
&lt;e&gt;
</th>
<th>
&lt;unk&gt;
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
(dog,)
</th>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
(like,)
</th>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
2.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
(&lt;s&gt;,)
</th>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
(i,)
</th>
<td>
0.0
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
(is,)
</th>
<td>
0.0
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
(this,)
</th>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
(cat,)
</th>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
2.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
(a,)
</th>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
2.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
</tbody>
</table>
</div>
<h5 id="expected-output-8">Expected output</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">bigram counts</span><br><span class="line">          cat    i   <span class="keyword">this</span>   a  is   like  dog  &lt;e&gt;   &lt;unk&gt;</span><br><span class="line">(&lt;s&gt;,)    <span class="number">0.0</span>   <span class="number">1.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(a,)      <span class="number">2.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(<span class="keyword">this</span>,)   <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">1.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(like,)   <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">2.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(dog,)    <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(cat,)    <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">2.0</span>    <span class="number">0.0</span></span><br><span class="line">(is,)     <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(i,)      <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Show trigram counts</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\ntrigram counts&#x27;</span>)</span><br><span class="line">trigram_counts = count_n_grams(sentences, <span class="number">3</span>)</span><br><span class="line">display(make_count_matrix(trigram_counts, unique_words))</span><br></pre></td></tr></table></figure>
<pre><code>trigram counts</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
dog
</th>
<th>
like
</th>
<th>
cat
</th>
<th>
i
</th>
<th>
is
</th>
<th>
this
</th>
<th>
a
</th>
<th>
&lt;e&gt;
</th>
<th>
&lt;unk&gt;
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
(&lt;s&gt;, &lt;s&gt;)
</th>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
(&lt;s&gt;, i)
</th>
<td>
0.0
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
(is, like)
</th>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
(i, like)
</th>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
(&lt;s&gt;, this)
</th>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
(a, cat)
</th>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
2.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
(like, a)
</th>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
2.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
(dog, is)
</th>
<td>
0.0
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<th>
(this, dog)
</th>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
1.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
</tbody>
</table>
</div>
<h5 id="expected-output-9">Expected output</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">trigram counts</span><br><span class="line">              cat    i   <span class="keyword">this</span>   a  is   like  dog  &lt;e&gt;   &lt;unk&gt;</span><br><span class="line">(dog, is)     <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(<span class="keyword">this</span>, dog)   <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(a, cat)      <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">2.0</span>    <span class="number">0.0</span></span><br><span class="line">(like, a)     <span class="number">2.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(is, like)    <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(&lt;s&gt;, i)      <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(i, like)     <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(&lt;s&gt;, &lt;s&gt;)    <span class="number">0.0</span>   <span class="number">1.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(&lt;s&gt;, <span class="keyword">this</span>)   <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">1.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<p>The following function calculates the probabilities of each word given the previous n-gram, and stores this in matrix form. - This function is provided for you.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_probability_matrix</span>(<span class="params">n_plus1_gram_counts, vocabulary, k</span>):</span></span><br><span class="line">    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)</span><br><span class="line">    count_matrix += k</span><br><span class="line">    prob_matrix = count_matrix.div(count_matrix.<span class="built_in">sum</span>(axis=<span class="number">1</span>), axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> prob_matrix</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sentences = [[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]]</span><br><span class="line">unique_words = <span class="built_in">list</span>(<span class="built_in">set</span>(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;bigram probabilities&quot;</span>)</span><br><span class="line">display(make_probability_matrix(bigram_counts, unique_words, k=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<pre><code>bigram probabilities</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
dog
</th>
<th>
like
</th>
<th>
cat
</th>
<th>
i
</th>
<th>
is
</th>
<th>
this
</th>
<th>
a
</th>
<th>
&lt;e&gt;
</th>
<th>
&lt;unk&gt;
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
(dog,)
</th>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.200000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
</tr>
<tr>
<th>
(like,)
</th>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.272727
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
</tr>
<tr>
<th>
(&lt;s&gt;,)
</th>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.181818
</td>
<td>
0.090909
</td>
<td>
0.181818
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
</tr>
<tr>
<th>
(i,)
</th>
<td>
0.100000
</td>
<td>
0.200000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
</tr>
<tr>
<th>
(is,)
</th>
<td>
0.100000
</td>
<td>
0.200000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
</tr>
<tr>
<th>
(this,)
</th>
<td>
0.200000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
</tr>
<tr>
<th>
(cat,)
</th>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.272727
</td>
<td>
0.090909
</td>
</tr>
<tr>
<th>
(a,)
</th>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.272727
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;trigram probabilities&quot;</span>)</span><br><span class="line">trigram_counts = count_n_grams(sentences, <span class="number">3</span>)</span><br><span class="line">display(make_probability_matrix(trigram_counts, unique_words, k=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<pre><code>trigram probabilities</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
dog
</th>
<th>
like
</th>
<th>
cat
</th>
<th>
i
</th>
<th>
is
</th>
<th>
this
</th>
<th>
a
</th>
<th>
&lt;e&gt;
</th>
<th>
&lt;unk&gt;
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
(&lt;s&gt;, &lt;s&gt;)
</th>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.181818
</td>
<td>
0.090909
</td>
<td>
0.181818
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
</tr>
<tr>
<th>
(&lt;s&gt;, i)
</th>
<td>
0.100000
</td>
<td>
0.200000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
</tr>
<tr>
<th>
(is, like)
</th>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.200000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
</tr>
<tr>
<th>
(i, like)
</th>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.200000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
</tr>
<tr>
<th>
(&lt;s&gt;, this)
</th>
<td>
0.200000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
</tr>
<tr>
<th>
(a, cat)
</th>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.272727
</td>
<td>
0.090909
</td>
</tr>
<tr>
<th>
(like, a)
</th>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.272727
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
<td>
0.090909
</td>
</tr>
<tr>
<th>
(dog, is)
</th>
<td>
0.100000
</td>
<td>
0.200000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
</tr>
<tr>
<th>
(this, dog)
</th>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.200000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
<td>
0.100000
</td>
</tr>
</tbody>
</table>
</div>
<p>Confirm that you obtain the same results as for the <code>estimate_probabilities</code> function that you implemented.</p>
<p><a name='3'></a> ## Part 3: Perplexity</p>
<p>In this section, you will generate the perplexity score to evaluate your model on the test set. - You will also use back-off when needed. - Perplexity is used as an evaluation metric of your language model. - To calculate the the perplexity score of the test set on an n-gram model, use:</p>
<p><span class="math display">\[ PP(W) =\sqrt[N]{ \prod_{t=n+1}^N \frac{1}{P(w_t | w_{t-n} \cdots w_{t-1})} } \tag{4}\]</span></p>
<ul>
<li>where <span class="math inline">\(N\)</span> is the length of the sentence.</li>
<li><span class="math inline">\(n\)</span> is the number of words in the n-gram (e.g. 2 for a bigram).</li>
<li>In math, the numbering starts at one and not zero.</li>
</ul>
<p>In code, array indexing starts at zero, so the code will use ranges for <span class="math inline">\(t\)</span> according to this formula:</p>
<p><span class="math display">\[ PP(W) =\sqrt[N]{ \prod_{t=n}^{N-1} \frac{1}{P(w_t | w_{t-n} \cdots w_{t-1})} } \tag{4.1}\]</span></p>
<p>The higher the probabilities are, the lower the perplexity will be. - The more the n-grams tell us about the sentence, the lower the perplexity score will be.</p>
<p><a name='ex-10'></a> ### Exercise 10 Compute the perplexity score given an N-gram count matrix and a sentence.</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
<li>
Remember that <code>range(2,4)</code> produces the integers [2, 3] (and excludes 4).
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: calculate_perplexity</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_perplexity</span>(<span class="params">sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculate perplexity for a list of sentences</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        sentence: List of strings</span></span><br><span class="line"><span class="string">        n_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        vocabulary_size: number of unique words in the vocabulary</span></span><br><span class="line"><span class="string">        k: Positive smoothing constant</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Perplexity score</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># length of previous words</span></span><br><span class="line">    n = <span class="built_in">len</span>(<span class="built_in">list</span>(n_gram_counts.keys())[<span class="number">0</span>]) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># prepend &lt;s&gt; and append &lt;e&gt;</span></span><br><span class="line">    sentence = [<span class="string">&quot;&lt;s&gt;&quot;</span>] * n + sentence + [<span class="string">&quot;&lt;e&gt;&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Cast the sentence from a list to a tuple</span></span><br><span class="line">    sentence = <span class="built_in">tuple</span>(sentence)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># length of sentence (after adding &lt;s&gt; and &lt;e&gt; tokens)</span></span><br><span class="line">    N = <span class="built_in">len</span>(sentence)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The variable p will hold the product</span></span><br><span class="line">    <span class="comment"># that is calculated inside the n-root</span></span><br><span class="line">    <span class="comment"># Update this in the code below</span></span><br><span class="line">    product_pi = <span class="number">1.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Index t ranges from n to N - 1, inclusive on both ends</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(n, N): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get the n-gram preceding the word at position t</span></span><br><span class="line">        n_gram = sentence[t-n:t]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get the word at position t</span></span><br><span class="line">        word = sentence[t]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Estimate the probability of the word given the n-gram</span></span><br><span class="line">        <span class="comment"># using the n-gram counts, n-plus1-gram counts,</span></span><br><span class="line">        <span class="comment"># vocabulary size, and smoothing constant</span></span><br><span class="line">        probability = estimate_probability(word, n_gram, </span><br><span class="line">                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the product of the probabilities</span></span><br><span class="line">        <span class="comment"># This &#x27;product_pi&#x27; is a cumulative product </span></span><br><span class="line">        <span class="comment"># of the (1/P) factors that are calculated in the loop</span></span><br><span class="line">        product_pi *= (<span class="number">1</span>/probability)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Take the Nth root of the product</span></span><br><span class="line">    perplexity = product_pi ** (<span class="number">1</span>/N)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> perplexity</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line"></span><br><span class="line">sentences = [[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]]</span><br><span class="line">unique_words = <span class="built_in">list</span>(<span class="built_in">set</span>(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">perplexity_train1 = calculate_perplexity(sentences[<span class="number">0</span>],</span><br><span class="line">                                         unigram_counts, bigram_counts,</span><br><span class="line">                                         <span class="built_in">len</span>(unique_words), k=<span class="number">1.0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Perplexity for first train sample: <span class="subst">&#123;perplexity_train1:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">test_sentence = [<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>]</span><br><span class="line">perplexity_test = calculate_perplexity(test_sentence,</span><br><span class="line">                                       unigram_counts, bigram_counts,</span><br><span class="line">                                       <span class="built_in">len</span>(unique_words), k=<span class="number">1.0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Perplexity for test sample: <span class="subst">&#123;perplexity_test:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Perplexity for first train sample: 2.8040
Perplexity for test sample: 3.9654</code></pre>
<h3 id="expected-output-10">Expected Output</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Perplexity <span class="keyword">for</span> first train sample: <span class="number">2.8040</span></span><br><span class="line">Perplexity <span class="keyword">for</span> test sample: <span class="number">3.9654</span></span><br></pre></td></tr></table></figure>
<p><b> Note: </b> If your sentence is really long, there will be underflow when multiplying many fractions. - To handle longer sentences, modify your implementation to take the sum of the log of the probabilities.</p>
<p><a name='4'></a> ## Part 4: Build an auto-complete system</p>
<p>In this section, you will combine the language models developed so far to implement an auto-complete system.</p>
<p><a name='ex-11'></a> ### Exercise 11 Compute probabilities for all possible next words and suggest the most likely one. - This function also take an optional argument <code>start_with</code>, which specifies the first few letters of the next words.</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
<li>
<code>estimate_probabilities</code> returns a dictionary where the key is a word and the value is the word's probability.
</li>
<li>
Use <code>str1.startswith(str2)</code> to determine if a string starts with the letters of another string. For example, <code>'learning'.startswith('lea')</code> returns True, whereas <code>'learning'.startswith('ear')</code> returns False. There are two additional parameters in <code>str.startswith()</code>, but you can use the default values for those parameters in this case.
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: suggest_a_word</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">suggest_a_word</span>(<span class="params">previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=<span class="number">1.0</span>, start_with=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Get suggestion for the next word</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        previous_tokens: The sentence you input where each token is a word. Must have length &gt; n </span></span><br><span class="line"><span class="string">        n_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        vocabulary: List of words</span></span><br><span class="line"><span class="string">        k: positive constant, smoothing parameter</span></span><br><span class="line"><span class="string">        start_with: If not None, specifies the first few letters of the next word</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A tuple of </span></span><br><span class="line"><span class="string">          - string of the most likely next word</span></span><br><span class="line"><span class="string">          - corresponding probability</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># length of previous words</span></span><br><span class="line">    n = <span class="built_in">len</span>(<span class="built_in">list</span>(n_gram_counts.keys())[<span class="number">0</span>]) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># From the words that the user already typed</span></span><br><span class="line">    <span class="comment"># get the most recent &#x27;n&#x27; words as the previous n-gram</span></span><br><span class="line">    previous_n_gram = previous_tokens[-n:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Estimate the probabilities that each word in the vocabulary</span></span><br><span class="line">    <span class="comment"># is the next word,</span></span><br><span class="line">    <span class="comment"># given the previous n-gram, the dictionary of n-gram counts,</span></span><br><span class="line">    <span class="comment"># the dictionary of n plus 1 gram counts, and the smoothing constant</span></span><br><span class="line">    probabilities = estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize suggested word to None</span></span><br><span class="line">    <span class="comment"># This will be set to the word with highest probability</span></span><br><span class="line">    suggestion = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the highest word probability to 0</span></span><br><span class="line">    <span class="comment"># this will be set to the highest probability </span></span><br><span class="line">    <span class="comment"># of all words to be suggested</span></span><br><span class="line">    max_prob = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># For each word and its probability in the probabilities dictionary:</span></span><br><span class="line">    <span class="keyword">for</span> word, prob <span class="keyword">in</span> probabilities.items(): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If the optional start_with string is set</span></span><br><span class="line">        <span class="keyword">if</span> start_with: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Check if the beginning of word does not match with the letters in &#x27;start_with&#x27;</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> word.startswith(start_with): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># if they don&#x27;t match, skip this word (move onto the next word)</span></span><br><span class="line">                 <span class="keyword">continue</span><span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check if this word&#x27;s probability</span></span><br><span class="line">        <span class="comment"># is greater than the current maximum probability</span></span><br><span class="line">        <span class="keyword">if</span> prob &gt; max_prob: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If so, save this word as the best suggestion (so far)</span></span><br><span class="line">            suggestion = word</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Save the new maximum probability</span></span><br><span class="line">            max_prob = prob</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> suggestion, max_prob</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>],</span><br><span class="line">             [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]]</span><br><span class="line">unique_words = <span class="built_in">list</span>(<span class="built_in">set</span>(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">previous_tokens = [<span class="string">&quot;i&quot;</span>, <span class="string">&quot;like&quot;</span>]</span><br><span class="line">tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=<span class="number">1.0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The previous words are &#x27;i like&#x27;,\n\tand the suggested word is `<span class="subst">&#123;tmp_suggest1[<span class="number">0</span>]&#125;</span>` with a probability of <span class="subst">&#123;tmp_suggest1[<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="comment"># test your code when setting the starts_with</span></span><br><span class="line">tmp_starts_with = <span class="string">&#x27;c&#x27;</span></span><br><span class="line">tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=<span class="number">1.0</span>, start_with=tmp_starts_with)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The previous words are &#x27;i like&#x27;, the suggestion must start with `<span class="subst">&#123;tmp_starts_with&#125;</span>`\n\tand the suggested word is `<span class="subst">&#123;tmp_suggest2[<span class="number">0</span>]&#125;</span>` with a probability of <span class="subst">&#123;tmp_suggest2[<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>The previous words are &#39;i like&#39;,
    and the suggested word is `a` with a probability of 0.2727

The previous words are &#39;i like&#39;, the suggestion must start with `c`
    and the suggested word is `cat` with a probability of 0.0909</code></pre>
<h3 id="expected-output-11">Expected output</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">The previous words are <span class="string">&#x27;i like&#x27;</span>,</span><br><span class="line">    <span class="keyword">and</span> the suggested word is `a` with a probability of <span class="number">0.2727</span></span><br><span class="line"></span><br><span class="line">The previous words are <span class="string">&#x27;i like&#x27;</span>, the suggestion must start with `c`</span><br><span class="line">    <span class="keyword">and</span> the suggested word is `cat` with a probability of <span class="number">0.0909</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="get-multiple-suggestions">Get multiple suggestions</h3>
<p>The function defined below loop over varioud n-gram models to get multiple suggestions.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_suggestions</span>(<span class="params">previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>, start_with=<span class="literal">None</span></span>):</span></span><br><span class="line">    model_counts = <span class="built_in">len</span>(n_gram_counts_list)</span><br><span class="line">    suggestions = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(model_counts-<span class="number">1</span>):</span><br><span class="line">        n_gram_counts = n_gram_counts_list[i]</span><br><span class="line">        n_plus1_gram_counts = n_gram_counts_list[i+<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        suggestion = suggest_a_word(previous_tokens, n_gram_counts,</span><br><span class="line">                                    n_plus1_gram_counts, vocabulary,</span><br><span class="line">                                    k=k, start_with=start_with)</span><br><span class="line">        suggestions.append(suggestion)</span><br><span class="line">    <span class="keyword">return</span> suggestions</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>],</span><br><span class="line">             [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]]</span><br><span class="line">unique_words = <span class="built_in">list</span>(<span class="built_in">set</span>(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line">trigram_counts = count_n_grams(sentences, <span class="number">3</span>)</span><br><span class="line">quadgram_counts = count_n_grams(sentences, <span class="number">4</span>)</span><br><span class="line">qintgram_counts = count_n_grams(sentences, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]</span><br><span class="line">previous_tokens = [<span class="string">&quot;i&quot;</span>, <span class="string">&quot;like&quot;</span>]</span><br><span class="line">tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The previous words are &#x27;i like&#x27;, the suggestions are:&quot;</span>)</span><br><span class="line">display(tmp_suggest3)</span><br></pre></td></tr></table></figure>
<pre><code>The previous words are &#39;i like&#39;, the suggestions are:



[(&#39;a&#39;, 0.2727272727272727),
 (&#39;a&#39;, 0.2),
 (&#39;dog&#39;, 0.1111111111111111),
 (&#39;dog&#39;, 0.1111111111111111)]</code></pre>
<h3 id="suggest-multiple-words-using-n-grams-of-varying-length">Suggest multiple words using n-grams of varying length</h3>
<p>Congratulations! You have developed all building blocks for implementing your own auto-complete systems.</p>
<p>Let's see this with n-grams of varying lengths (unigrams, bigrams, trigrams, 4-grams...6-grams).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">n_gram_counts_list = []</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Computing n-gram counts with n =&quot;</span>, n, <span class="string">&quot;...&quot;</span>)</span><br><span class="line">    n_model_counts = count_n_grams(train_data_processed, n)</span><br><span class="line">    n_gram_counts_list.append(n_model_counts)</span><br></pre></td></tr></table></figure>
<pre><code>Computing n-gram counts with n = 1 ...
Computing n-gram counts with n = 2 ...
Computing n-gram counts with n = 3 ...
Computing n-gram counts with n = 4 ...
Computing n-gram counts with n = 5 ...</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">&quot;i&quot;</span>, <span class="string">&quot;am&quot;</span>, <span class="string">&quot;to&quot;</span>]</span><br><span class="line">tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:&quot;</span>)</span><br><span class="line">display(tmp_suggest4)</span><br></pre></td></tr></table></figure>
<pre><code>The previous words are [&#39;i&#39;, &#39;am&#39;, &#39;to&#39;], the suggestions are:



[(&#39;be&#39;, 0.027665685098338604),
 (&#39;have&#39;, 0.00013487086115044844),
 (&#39;have&#39;, 0.00013490725126475548),
 (&#39;i&#39;, 6.746272684341901e-05)]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">&quot;i&quot;</span>, <span class="string">&quot;want&quot;</span>, <span class="string">&quot;to&quot;</span>, <span class="string">&quot;go&quot;</span>]</span><br><span class="line">tmp_suggest5 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:&quot;</span>)</span><br><span class="line">display(tmp_suggest5)</span><br></pre></td></tr></table></figure>
<pre><code>The previous words are [&#39;i&#39;, &#39;want&#39;, &#39;to&#39;, &#39;go&#39;], the suggestions are:



[(&#39;to&#39;, 0.014051961029228078),
 (&#39;to&#39;, 0.004697942168993581),
 (&#39;to&#39;, 0.0009424436216762033),
 (&#39;to&#39;, 0.0004044489383215369)]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">&quot;hey&quot;</span>, <span class="string">&quot;how&quot;</span>, <span class="string">&quot;are&quot;</span>]</span><br><span class="line">tmp_suggest6 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:&quot;</span>)</span><br><span class="line">display(tmp_suggest6)</span><br></pre></td></tr></table></figure>
<pre><code>The previous words are [&#39;hey&#39;, &#39;how&#39;, &#39;are&#39;], the suggestions are:



[(&#39;you&#39;, 0.023426812585499317),
 (&#39;you&#39;, 0.003559435862995299),
 (&#39;you&#39;, 0.00013491635186184566),
 (&#39;i&#39;, 6.746272684341901e-05)]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">&quot;hey&quot;</span>, <span class="string">&quot;how&quot;</span>, <span class="string">&quot;are&quot;</span>, <span class="string">&quot;you&quot;</span>]</span><br><span class="line">tmp_suggest7 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:&quot;</span>)</span><br><span class="line">display(tmp_suggest7)</span><br></pre></td></tr></table></figure>
<pre><code>The previous words are [&#39;hey&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;], the suggestions are:



[(&quot;&#39;re&quot;, 0.023973994311255586),
 (&#39;?&#39;, 0.002888465830762161),
 (&#39;?&#39;, 0.0016134453781512605),
 (&#39;&lt;e&gt;&#39;, 0.00013491635186184566)]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">&quot;hey&quot;</span>, <span class="string">&quot;how&quot;</span>, <span class="string">&quot;are&quot;</span>, <span class="string">&quot;you&quot;</span>]</span><br><span class="line">tmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>, start_with=<span class="string">&quot;d&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:&quot;</span>)</span><br><span class="line">display(tmp_suggest8)</span><br></pre></td></tr></table></figure>
<pre><code>The previous words are [&#39;hey&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;], the suggestions are:



[(&#39;do&#39;, 0.009020723283218204),
 (&#39;doing&#39;, 0.0016411737674785006),
 (&#39;doing&#39;, 0.00047058823529411766),
 (&#39;dvd&#39;, 6.745817593092283e-05)]</code></pre>
<h1 id="congratulations">Congratulations!</h1>
<p>You've completed this assignment by building an autocomplete model using an n-gram language model!</p>
<p>Please continue onto the fourth and final week of this course!</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Auto-Correct/2020/07/19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Auto-Correct/2020/07/19/" class="post-title-link" itemprop="url">Auto Correct</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-07-19 16:27:04" itemprop="dateCreated datePublished" datetime="2020-07-19T16:27:04+08:00">2020-07-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-06 23:32:06" itemprop="dateModified" datetime="2020-09-06T23:32:06+08:00">2020-09-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Auto-Correct/2020/07/19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Auto-Correct/2020/07/19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="assignment-1-auto-correct">Assignment 1: Auto Correct</h1>
<p>Welcome to the first assignment of Course 2. This assignment will give you a chance to brush up on your python and probability skills. In doing so, you will implement an auto-correct system that is very effective and useful.</p>
<h2 id="outline">Outline</h2>
<ul>
<li><a href="#0">0. Overview</a>
<ul>
<li><a href="#0-1">0.1 Edit Distance</a></li>
</ul></li>
<li><a href="#1">1. Data Preprocessing</a>
<ul>
<li><a href="#ex-1">1.1 Exercise 1</a></li>
<li><a href="#ex-2">1.2 Exercise 2</a></li>
<li><a href="#ex-3">1.3 Exercise 3</a></li>
</ul></li>
<li><a href="#2">2. String Manipulation</a>
<ul>
<li><a href="#ex-4">2.1 Exercise 4</a></li>
<li><a href="#ex-5">2.2 Exercise 5</a></li>
<li><a href="#ex-6">2.3 Exercise 6</a></li>
<li><a href="#ex-7">2.4 Exercise 7</a></li>
</ul></li>
<li><a href="#3">3. Combining the edits</a>
<ul>
<li><a href="#ex-8">3.1 Exercise 8</a></li>
<li><a href="#ex-9">3.2 Exercise 9</a></li>
<li><a href="#ex-10">3.3 Exercise 10</a></li>
</ul></li>
<li><a href="#4">4. Minimum Edit Distance</a>
<ul>
<li><a href="#ex-11">4.1 Exercise 11</a></li>
</ul></li>
<li><a href="#5">5. Backtrace (Optional)</a></li>
</ul>
<p><a name='0'></a> ## 0. Overview</p>
<p>You use autocorrect every day on your cell phone and computer. In this assignment, you will explore what really goes on behind the scenes. Of course, the model you are about to implement is not identical to the one used in your phone, but it is still quite good.</p>
<p>By completing this assignment you will learn how to:</p>
<ul>
<li>Get a word count given a corpus</li>
<li>Get a word probability in the corpus</li>
<li>Manipulate strings</li>
<li>Filter strings</li>
<li>Implement Minimum edit distance to compare strings and to help find the optimal path for the edits.</li>
<li>Understand how dynamic programming works</li>
</ul>
<p>Similar systems are used everywhere. - For example, if you type in the word <strong>"I am lerningg"</strong>, chances are very high that you meant to write <strong>"learning"</strong>, as shown in <strong>Figure 1</strong>.</p>
<div style="width:image width px; font-size:100%; text-align:center;">
<img src='auto-correct.png' alt="alternate text" width="width" height="height" style="width:300px;height:250px;" /> Figure 1
</div>
<p><a name='0-1'></a> #### 0.1 Edit Distance</p>
<p>In this assignment, you will implement models that correct words that are 1 and 2 edit distances away. - We say two words are n edit distance away from each other when we need n edits to change one word into another.</p>
<p>An edit could consist of one of the following options:</p>
<ul>
<li>Delete (remove a letter): ‘hat’ =&gt; ‘at, ha, ht’</li>
<li>Switch (swap 2 adjacent letters): ‘eta’ =&gt; ‘eat, tea,...’</li>
<li>Replace (change 1 letter to another): ‘jat’ =&gt; ‘hat, rat, cat, mat, ...’</li>
<li>Insert (add a letter): ‘te’ =&gt; ‘the, ten, ate, ...’</li>
</ul>
<p>You will be using the four methods above to implement an Auto-correct. - To do so, you will need to compute probabilities that a certain word is correct given an input.</p>
<p>This auto-correct you are about to implement was first created by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Peter_Norvig">Peter Norvig</a> in 2007. - His <a target="_blank" rel="noopener" href="https://norvig.com/spell-correct.html">original article</a> may be a useful reference for this assignment.</p>
<p>The goal of our spell check model is to compute the following probability:</p>
<p><span class="math display">\[P(c|w) = \frac{P(w|c)\times P(c)}{P(w)} \tag{Eqn-1}\]</span></p>
<p>The equation above is <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes Rule</a>. - Equation 1 says that the probability of a word being correct $P(c|w) $is equal to the probability of having a certain word <span class="math inline">\(w\)</span>, given that it is correct <span class="math inline">\(P(w|c)\)</span>, multiplied by the probability of being correct in general <span class="math inline">\(P(C)\)</span> divided by the probability of that word <span class="math inline">\(w\)</span> appearing <span class="math inline">\(P(w)\)</span> in general. - To compute equation 1, you will first import a data set and then create all the probabilities that you need using that data set.</p>
<p><a name='1'></a> # Part 1: Data Preprocessing</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>
<p>As in any other machine learning task, the first thing you have to do is process your data set. - Many courses load in pre-processed data for you. - However, in the real world, when you build these NLP systems, you load the datasets and process them. - So let's get some real world practice in pre-processing the data!</p>
<p>Your first task is to read in a file called <strong>'shakespeare.txt'</strong> which is found in your file directory. To look at this file you can go to <code>File ==&gt; Open</code>.</p>
<p><a name='ex-1'></a> ### Exercise 1 Implement the function <code>process_data</code> which</p>
<ol type="1">
<li><p>Reads in a corpus (text file)</p></li>
<li><p>Changes everything to lowercase</p></li>
<li><p>Returns a list of words.</p></li>
</ol>
<h4 id="options-and-hints">Options and Hints</h4>
<ul>
<li>If you would like more of a real-life practice, don't open the 'Hints' below (yet) and try searching the web to derive your answer.</li>
<li>If you want a little help, click on the green "General Hints" section by clicking on it with your mouse.</li>
<li>If you get stuck or are not getting the expected results, click on the green 'Detailed Hints' section to get hints for each step that you'll take to complete this function.</li>
</ul>
<details>
<summary>
<font size="3" color="darkgreen"><b>General Hints</b></font>
</summary>
<p>
General Hints to get started
<ul>
<li>
Python <a target="_blank" rel="noopener" href="https://docs.python.org/3/tutorial/inputoutput.html">input and output<a>
</li>
<li>
Python <a target="_blank" rel="noopener" href="https://docs.python.org/3/library/re.html" >'re' documentation </a>
</li>
</ul>
</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Detailed Hints</b></font>
</summary>
<p>
Detailed hints if you're stuck
<ul>
<li>
Use 'with' syntax to read a file
</li>
<li>
Decide whether to use 'read()' or 'readline(). What's the difference?
</li>
<li>
Choose whether to use either str.lower() or str.lowercase(). What is the difference?
</li>
<li>
Use re.findall(pattern, string)
</li>
<li>
Look for the "Raw String Notation" section in the Python 're' documentation to understand the difference between r'', r'' and '\W'.
</li>
<li>
For the pattern, decide between using '', '', '+' or '+'. What do you think are the differences?
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: process_data</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_data</span>(<span class="params">file_name</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        A file_name which is found in your current directory. You just have to read it in. </span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        words: a list containing all the words in the corpus (text file you read) in lower case. </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    words = [] <span class="comment"># return this variable correctly</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_name, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        content = f.read().lower()</span><br><span class="line">    </span><br><span class="line">    words = re.findall(<span class="string">&#x27;\w+&#x27;</span>,content)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> words</span><br></pre></td></tr></table></figure>
<p>Note, in the following cell, 'words' is converted to a python <code>set</code>. This eliminates any duplicate entries.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DO NOT MODIFY THIS CELL</span></span><br><span class="line">word_l = process_data(<span class="string">&#x27;shakespeare.txt&#x27;</span>)</span><br><span class="line">vocab = <span class="built_in">set</span>(word_l)  <span class="comment"># this will be your new vocabulary</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The first ten words in the text are: \n<span class="subst">&#123;word_l[<span class="number">0</span>:<span class="number">10</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;There are <span class="subst">&#123;<span class="built_in">len</span>(vocab)&#125;</span> unique words in the vocabulary.&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>The first ten words in the text are: 
[&#39;o&#39;, &#39;for&#39;, &#39;a&#39;, &#39;muse&#39;, &#39;of&#39;, &#39;fire&#39;, &#39;that&#39;, &#39;would&#39;, &#39;ascend&#39;, &#39;the&#39;]
There are 6116 unique words in the vocabulary.</code></pre>
<h4 id="expected-output">Expected Output</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The first ten words <span class="keyword">in</span> the text are: </span><br><span class="line">[<span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;for&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;muse&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;fire&#x27;</span>, <span class="string">&#x27;that&#x27;</span>, <span class="string">&#x27;would&#x27;</span>, <span class="string">&#x27;ascend&#x27;</span>, <span class="string">&#x27;the&#x27;</span>]</span><br><span class="line">There are <span class="number">6116</span> unique words <span class="keyword">in</span> the vocabulary.</span><br></pre></td></tr></table></figure>
<p><a name='ex-2'></a> ### Exercise 2</p>
<p>Implement a <code>get_count</code> function that returns a dictionary - The dictionary's keys are words - The value for each word is the number of times that word appears in the corpus.</p>
For example, given the following sentence: <strong>"I am happy because I am learning"</strong>, your dictionary should return the following:
<table style="width:20%">
<tr>
<td>
<b>Key </b>
</td>
<td>
<b>Value </b>
</td>
</tr>
<tr>
<td>
I
</td>
<td>
2
</td>
</tr>
<tr>
<td>
am
</td>
<td>
2
</td>
</tr>
<tr>
<td>
happy
</td>
<td>
1
</td>
</tr>
<tr>
<td>
because
</td>
<td>
1
</td>
</tr>
<tr>
<td>
learning
</td>
<td>
1
</td>
</tr>
</table>
<p><strong>Instructions</strong>: Implement a <code>get_count</code> which returns a dictionary where the key is a word and the value is the number of times the word appears in the list.</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
<li>
Try implementing this using a for loop and a regular dictionary. This may be good practice for similar coding interview questions
</li>
<li>
You can also use defaultdict instead of a regualr dictionary, along with the for loop
</li>
<li>
Otherwise, to skip using a for loop, you can use Python's <a target="_blank" rel="noopener" href="https://docs.python.org/3.7/library/collections.html#collections.Counter" > Counter class</a>
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_count</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_count</span>(<span class="params">word_l</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word_l: a set of words representing the corpus. </span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    word_count_dict = &#123;&#125;  <span class="comment"># fill this with word counts</span></span><br><span class="line">    <span class="comment">### START CODE HERE </span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word_l:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_count_dict:</span><br><span class="line">            word_count_dict[word] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            word_count_dict[word] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> word_count_dict</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DO NOT MODIFY THIS CELL</span></span><br><span class="line">word_count_dict = get_count(word_l)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;There are <span class="subst">&#123;<span class="built_in">len</span>(word_count_dict)&#125;</span> key values pairs&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The count for the word &#x27;thee&#x27; is <span class="subst">&#123;word_count_dict.get(<span class="string">&#x27;thee&#x27;</span>,<span class="number">0</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>There are 6116 key values pairs
The count for the word &#39;thee&#39; is 240</code></pre>
<h4 id="expected-output-1">Expected Output</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">There are <span class="number">6116</span> key values pairs</span><br><span class="line">The count <span class="keyword">for</span> the word <span class="string">&#x27;thee&#x27;</span> <span class="keyword">is</span> <span class="number">240</span></span><br></pre></td></tr></table></figure>
<p><a name='ex-3'></a> ### Exercise 3 Given the dictionary of word counts, compute the probability that each word will appear if randomly selected from the corpus of words.</p>
<p><span class="math display">\[P(w_i) = \frac{C(w_i)}{M} \tag{Eqn-2}\]</span> where</p>
<p><span class="math inline">\(C(w_i)\)</span> is the total number of times <span class="math inline">\(w_i\)</span> appears in the corpus.</p>
<p><span class="math inline">\(M\)</span> is the total number of words in the corpus.</p>
<p>For example, the probability of the word 'am' in the sentence <strong>'I am happy because I am learning'</strong> is:</p>
<p><span class="math display">\[P(am) = \frac{C(w_i)}{M} = \frac {2}{7} \tag{Eqn-3}.\]</span></p>
<p><strong>Instructions:</strong> Implement <code>get_probs</code> function which gives you the probability that a word occurs in a sample. This returns a dictionary where the keys are words, and the value for each word is its probability in the corpus of words.</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
General advice
<ul>
<li>
Use dictionary.values()
</li>
<li>
Use sum()
</li>
<li>
The cardinality (number of words in the corpus should be equal to len(word_l). You will calculate this same number, but using the word count dictionary.
</li>
</ul>
If you're using a for loop:
<ul>
<li>
Use dictionary.keys()
</li>
</ul>
If you're using a dictionary comprehension:
<ul>
<li>
Use dictionary.items()
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_probs</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_probs</span>(<span class="params">word_count_dict</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        probs: A dictionary where keys are the words and the values are the probability that a word will occur. </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    probs = &#123;&#125;  <span class="comment"># return this variable correctly</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    M = np.<span class="built_in">sum</span>(<span class="built_in">list</span>(word_count_dict.values()))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> word, C <span class="keyword">in</span> word_count_dict.items():</span><br><span class="line">        probs[word] =  <span class="built_in">float</span>(C) / M</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> probs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DO NOT MODIFY THIS CELL</span></span><br><span class="line">probs = get_probs(word_count_dict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Length of probs is <span class="subst">&#123;<span class="built_in">len</span>(probs)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;P(&#x27;thee&#x27;) is <span class="subst">&#123;probs[<span class="string">&#x27;thee&#x27;</span>]:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Length of probs is 6116
P(&#39;thee&#39;) is 0.0045</code></pre>
<h4 id="expected-output-2">Expected Output</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Length of probs <span class="keyword">is</span> <span class="number">6116</span></span><br><span class="line">P(<span class="string">&#x27;thee&#x27;</span>) <span class="keyword">is</span> <span class="number">0.0045</span></span><br></pre></td></tr></table></figure>
<p><a name='2'></a> # Part 2: String Manipulations</p>
<p>Now, that you have computed <span class="math inline">\(P(w_i)\)</span> for all the words in the corpus, you will write a few functions to manipulate strings so that you can edit the erroneous strings and return the right spellings of the words. In this section, you will implement four functions:</p>
<ul>
<li><code>delete_letter</code>: given a word, it returns all the possible strings that have <strong>one character removed</strong>.</li>
<li><code>switch_letter</code>: given a word, it returns all the possible strings that have <strong>two adjacent letters switched</strong>.</li>
<li><code>replace_letter</code>: given a word, it returns all the possible strings that have <strong>one character replaced by another different letter</strong>.</li>
<li><code>insert_letter</code>: given a word, it returns all the possible strings that have an <strong>additional character inserted</strong>.</li>
</ul>
<h4 id="list-comprehensions">List comprehensions</h4>
<p>String and list manipulation in python will often make use of a python feature called <a target="_blank" rel="noopener" href="https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions">list comprehensions</a>. The routines below will be described as using list comprehensions, but if you would rather implement them in another way, you are free to do so as long as the result is the same. Further, the following section will provide detailed instructions on how to use list comprehensions and how to implement the desired functions. If you are a python expert, feel free to skip the python hints and move to implementing the routines directly.</p>
<p>Python List Comprehensions embed a looping structure inside of a list declaration, collapsing many lines of code into a single line. If you are not familiar with them, they seem slightly out of order relative to for loops.</p>
<div style="width:image width px; font-size:100%; text-align:center;">
<img src='GenericListComp3.png' alt="alternate text" width="width" height="height"  style="width:800px;height:400px;"/> Figure 2
</div>
<p>The diagram above shows that the components of a list comprehension are the same components you would find in a typical for loop that appends to a list, but in a different order. With that in mind, we'll continue the specifics of this assignment. We will be very descriptive for the first function, <code>deletes()</code>, and less so in later functions as you become familiar with list comprehensions.</p>
<p><a name='ex-4'></a> ### Exercise 4</p>
<p><strong>Instructions for delete_letter():</strong> Implement a <code>delete_letter()</code> function that, given a word, returns a list of strings with one character deleted.</p>
<p>For example, given the word <strong>nice</strong>, it would return the set: {'ice', 'nce', 'nic', 'nie'}.</p>
<p><strong>Step 1:</strong> Create a list of 'splits'. This is all the ways you can split a word into Left and Right: For example,<br />
'nice is split into : <code>[('', 'nice'), ('n', 'ice'), ('ni', 'ce'), ('nic', 'e'), ('nice', '')]</code> This is common to all four functions (delete, replace, switch, insert).</p>
<div style="width:image width px; font-size:100%; text-align:center;">
<img src='Splits1.png' alt="alternate text" width="width" height="height" style="width:650px;height:200px;" /> Figure 3
</div>
<p><strong>Step 2:</strong> This is specific to <code>delete_letter</code>. Here, we are generating all words that result from deleting one character.<br />
This can be done in a single line with a list comprehension. You can makes use of this type of syntax:<br />
<code>[f(a,b) for a, b in splits if condition]</code></p>
<p>For our 'nice' example you get: ['ice', 'nce', 'nie', 'nic']</p>
<div style="width:image width px; font-size:100%; text-align:center;">
<img src='ListComp2.png' alt="alternate text" width="width" height="height" style="width:550px;height:300px;" /> Figure 4
</div>
<h4 id="levels-of-assistance">Levels of assistance</h4>
<p>Try this exercise with these levels of assistance.<br />
- We hope that this will make it both a meaningful experience but also not a frustrating experience. - Start with level 1, then move onto level 2, and 3 as needed.</p>
<pre><code>- Level 1. Try to think this through and implement this yourself.
- Level 2. Click on the &quot;Level 2 Hints&quot; section for some hints to get started.
- Level 3. If you would prefer more guidance, please click on the &quot;Level 3 Hints&quot; cell for step by step instructions.</code></pre>
<ul>
<li>If you are still stuck, look at the images in the "list comprehensions" section above.</li>
</ul>
<details>
<summary>
<font size="3" color="darkgreen"><b>Level 2 Hints</b></font>
</summary>
<p>
<ul>
<li>
<a href="" > Use array slicing like my_string[0:2] </a>
</li>
<li>
<a href="" > Use list comprehensions or for loops </a>
</li>
</ul>
</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Level 3 Hints</b></font>
</summary>
<p>
<ul>
<li>
splits: Use array slicing, like my_str[0:2], to separate a string into two pieces.
</li>
<li>
Do this in a loop or list comprehension, so that you have a list of tuples.
<li>
For example, "cake" can get split into "ca" and "ke". They're stored in a tuple ("ca","ke"), and the tuple is appended to a list. We'll refer to these as L and R, so the tuple is (L,R)
</li>
<pre><code>&lt;li&gt;When choosing the range for your loop, if you input the word &quot;cans&quot; and generate the tuple  (&#39;cans&#39;,&#39;&#39;), make sure to include an if statement to check the length of that right-side string (R) in the tuple (L,R) &lt;/li&gt;
&lt;li&gt;deletes: Go through the list of tuples and combine the two strings together. You can use the + operator to combine two strings&lt;/li&gt;
&lt;li&gt;When combining the tuples, make sure that you leave out a middle character.&lt;/li&gt;
&lt;li&gt;Use array slicing to leave out the first character of the right substring.&lt;/li&gt;</code></pre>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: deletes</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">delete_letter</span>(<span class="params">word, verbose=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: the string/word for which you will generate all possible words </span></span><br><span class="line"><span class="string">                in the vocabulary which have 1 missing character</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        delete_l: a list of all possible strings obtained by deleting 1 character from word</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    delete_l = []</span><br><span class="line">    split_l = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    split_l = [(word[:i],word[i:]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word) + <span class="number">1</span>)]</span><br><span class="line">    delete_l = [L + R[<span class="number">1</span>:] <span class="keyword">for</span> L,R <span class="keyword">in</span> split_l <span class="keyword">if</span> R]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> verbose: <span class="built_in">print</span>(<span class="string">f&quot;input word <span class="subst">&#123;word&#125;</span>, \nsplit_l = <span class="subst">&#123;split_l&#125;</span>, \ndelete_l = <span class="subst">&#123;delete_l&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> delete_l</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">delete_word_l = delete_letter(word=<span class="string">&quot;cans&quot;</span>,</span><br><span class="line">                        verbose=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>input word cans, 
split_l = [(&#39;&#39;, &#39;cans&#39;), (&#39;c&#39;, &#39;ans&#39;), (&#39;ca&#39;, &#39;ns&#39;), (&#39;can&#39;, &#39;s&#39;), (&#39;cans&#39;, &#39;&#39;)], 
delete_l = [&#39;ans&#39;, &#39;cns&#39;, &#39;cas&#39;, &#39;can&#39;]</code></pre>
<h4 id="expected-output-3">Expected Output</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> word cans, </span><br><span class="line">split_l = [(<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;cans&#x27;</span>), (<span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;ans&#x27;</span>), (<span class="string">&#x27;ca&#x27;</span>, <span class="string">&#x27;ns&#x27;</span>), (<span class="string">&#x27;can&#x27;</span>, <span class="string">&#x27;s&#x27;</span>)], </span><br><span class="line">delete_l = [<span class="string">&#x27;ans&#x27;</span>, <span class="string">&#x27;cns&#x27;</span>, <span class="string">&#x27;cas&#x27;</span>, <span class="string">&#x27;can&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h4 id="note-1">Note 1</h4>
<p>You might get a slightly different result with split_l.<br />
- Notice how it has the extra tuple <code>('cans', '')</code>. - This will be fine as long as you have checked the size of the right-side substring in tuple (L,R). - Can you explain why this will give you the same result for the list of deletion strings (delete_l)?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> word cans, </span><br><span class="line">split_l = [(<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;cans&#x27;</span>), (<span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;ans&#x27;</span>), (<span class="string">&#x27;ca&#x27;</span>, <span class="string">&#x27;ns&#x27;</span>), (<span class="string">&#x27;can&#x27;</span>, <span class="string">&#x27;s&#x27;</span>), (<span class="string">&#x27;cans&#x27;</span>, <span class="string">&#x27;&#x27;</span>)], </span><br><span class="line">delete_l = [<span class="string">&#x27;ans&#x27;</span>, <span class="string">&#x27;cns&#x27;</span>, <span class="string">&#x27;cas&#x27;</span>, <span class="string">&#x27;can&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h4 id="note-2">Note 2</h4>
<p>If you end up getting the same word as your input word, like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> word cans, </span><br><span class="line">split_l = [(<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;cans&#x27;</span>), (<span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;ans&#x27;</span>), (<span class="string">&#x27;ca&#x27;</span>, <span class="string">&#x27;ns&#x27;</span>), (<span class="string">&#x27;can&#x27;</span>, <span class="string">&#x27;s&#x27;</span>), (<span class="string">&#x27;cans&#x27;</span>, <span class="string">&#x27;&#x27;</span>)], </span><br><span class="line">delete_l = [<span class="string">&#x27;ans&#x27;</span>, <span class="string">&#x27;cns&#x27;</span>, <span class="string">&#x27;cas&#x27;</span>, <span class="string">&#x27;can&#x27;</span>, <span class="string">&#x27;cans&#x27;</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li>Check how you set the <code>range</code>.</li>
<li>See if you check the length of the string on the right-side of the split.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test # 2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of outputs of delete_letter(&#x27;at&#x27;) is <span class="subst">&#123;<span class="built_in">len</span>(delete_letter(<span class="string">&#x27;at&#x27;</span>))&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Number of outputs of delete_letter(&#39;at&#39;) is 2</code></pre>
<h4 id="expected-output-4">Expected output</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Number of outputs of <span class="title">delete_letter</span><span class="params">(<span class="string">&#x27;at&#x27;</span>)</span> is 2</span></span><br></pre></td></tr></table></figure>
<p><a name='ex-5'></a> ### Exercise 5</p>
<p><strong>Instructions for switch_letter()</strong>: Now implement a function that switches two letters in a word. It takes in a word and returns a list of all the possible switches of two letters <strong>that are adjacent to each other</strong>. - For example, given the word 'eta', it returns {'eat', 'tea'}, but does not return 'ate'.</p>
<p><strong>Step 1:</strong> is the same as in delete_letter()<br />
<strong>Step 2:</strong> A list comprehension or for loop which forms strings by swapping adjacent letters. This is of the form:<br />
<code>[f(L,R) for L, R in splits if condition]</code> where 'condition' will test the length of R in a given iteration. See below.</p>
<div style="width:image width px; font-size:100%; text-align:center;">
<img src='Switches1.png' alt="alternate text" width="width" height="height" style="width:600px;height:200px;"/> Figure 5
</div>
<h4 id="levels-of-difficulty">Levels of difficulty</h4>
<p>Try this exercise with these levels of difficulty.<br />
- Level 1. Try to think this through and implement this yourself. - Level 2. Click on the "Level 2 Hints" section for some hints to get started. - Level 3. If you would prefer more guidance, please click on the "Level 3 Hints" cell for step by step instructions.</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Level 2 Hints</b></font>
</summary>
<p>
<ul>
<li>
<a href="" > Use array slicing like my_string[0:2] </a>
</li>
<li>
<a href="" > Use list comprehensions or for loops </a>
</li>
<li>
To do a switch, think of the whole word as divided into 4 distinct parts. Write out 'cupcakes' on a piece of paper and see how you can split it into ('cupc', 'k', 'a', 'es')
</li>
</ul>
</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Level 3 Hints</b></font>
</summary>
<p>
<ul>
<li>
splits: Use array slicing, like my_str[0:2], to separate a string into two pieces.
</li>
<li>
Splitting is the same as for delete_letter
</li>
<li>
To perform the switch, go through the list of tuples and combine four strings together. You can use the + operator to combine strings
</li>
<li>
The four strings will be the left substring from the split tuple, followed by the first (index 1) character of the right substring, then the zero-th character (index 0) of the right substring, and then the remaining part of the right substring.
</li>
<li>
Unlike delete_letter, you will want to check that your right substring is at least a minimum length. To see why, review the previous hint bullet point (directly before this one).
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: switches</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">switch_letter</span>(<span class="params">word, verbose=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: input string</span></span><br><span class="line"><span class="string">     Output:</span></span><br><span class="line"><span class="string">        switches: a list of all possible strings with one adjacent charater switched</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span> </span><br><span class="line">    </span><br><span class="line">    switch_l = []</span><br><span class="line">    split_l = []</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">L,R</span>):</span></span><br><span class="line">        <span class="keyword">return</span> L[:-<span class="number">1</span>] + R[<span class="number">0</span>] + L[-<span class="number">1</span>] + R[<span class="number">1</span>:]</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    split_l = [(word[:i],word[i:]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word) + <span class="number">1</span>)]</span><br><span class="line">    switch_l = [f(L,R) <span class="keyword">for</span> L,R <span class="keyword">in</span> split_l <span class="keyword">if</span> L <span class="keyword">and</span> R]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> verbose: <span class="built_in">print</span>(<span class="string">f&quot;Input word = <span class="subst">&#123;word&#125;</span> \nsplit_l = <span class="subst">&#123;split_l&#125;</span> \nswitch_l = <span class="subst">&#123;switch_l&#125;</span>&quot;</span>) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> switch_l</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">switch_word_l = switch_letter(word=<span class="string">&quot;eta&quot;</span>,</span><br><span class="line">                         verbose=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Input word = eta 
split_l = [(&#39;&#39;, &#39;eta&#39;), (&#39;e&#39;, &#39;ta&#39;), (&#39;et&#39;, &#39;a&#39;), (&#39;eta&#39;, &#39;&#39;)] 
switch_l = [&#39;tea&#39;, &#39;eat&#39;]</code></pre>
<h4 id="expected-output-5">Expected output</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input word = eta </span><br><span class="line">split_l = [(<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;eta&#x27;</span>), (<span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;ta&#x27;</span>), (<span class="string">&#x27;et&#x27;</span>, <span class="string">&#x27;a&#x27;</span>)] </span><br><span class="line">switch_l = [<span class="string">&#x27;tea&#x27;</span>, <span class="string">&#x27;eat&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h4 id="note-1-1">Note 1</h4>
<p>You may get this: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input word = eta </span><br><span class="line">split_l = [(<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;eta&#x27;</span>), (<span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;ta&#x27;</span>), (<span class="string">&#x27;et&#x27;</span>, <span class="string">&#x27;a&#x27;</span>), (<span class="string">&#x27;eta&#x27;</span>, <span class="string">&#x27;&#x27;</span>)] </span><br><span class="line">switch_l = [<span class="string">&#x27;tea&#x27;</span>, <span class="string">&#x27;eat&#x27;</span>]</span><br></pre></td></tr></table></figure> - Notice how it has the extra tuple <code>('eta', '')</code>. - This is also correct. - Can you think of why this is the case?</p>
<h4 id="note-2-1">Note 2</h4>
<p>If you get an error <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IndexError: string index out of <span class="built_in">range</span></span><br></pre></td></tr></table></figure> - Please see if you have checked the length of the strings when switching characters.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test # 2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of outputs of switch_letter(&#x27;at&#x27;) is <span class="subst">&#123;<span class="built_in">len</span>(switch_letter(<span class="string">&#x27;at&#x27;</span>))&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Number of outputs of switch_letter(&#39;at&#39;) is 1</code></pre>
<h4 id="expected-output-6">Expected output</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Number of outputs of <span class="title">switch_letter</span><span class="params">(<span class="string">&#x27;at&#x27;</span>)</span> is 1</span></span><br></pre></td></tr></table></figure>
<p><a name='ex-6'></a> ### Exercise 6 <strong>Instructions for replace_letter()</strong>: Now implement a function that takes in a word and returns a list of strings with one <strong>replaced letter</strong> from the original word.</p>
<p><strong>Step 1:</strong> is the same as in <code>delete_letter()</code></p>
<p><strong>Step 2:</strong> A list comprehension or for loop which form strings by replacing letters. This can be of the form:<br />
<code>[f(a,b,c) for a, b in splits if condition for c in string]</code> Note the use of the second for loop.<br />
It is expected in this routine that one or more of the replacements will include the original word. For example, replacing the first letter of 'ear' with 'e' will return 'ear'.</p>
<p><strong>Step 3:</strong> Remove the original input letter from the output.</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
<li>
To remove a word from a list, first store its contents inside a set()
</li>
<li>
Use set.discard('the_word') to remove a word in a set (if the word does not exist in the set, then it will not throw a KeyError. Using set.remove('the_word') throws a KeyError if the word does not exist in the set.
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: replaces</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replace_letter</span>(<span class="params">word, verbose=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: the input string/word </span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        replaces: a list of all possible strings where we replaced one letter from the original word. </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span> </span><br><span class="line">    </span><br><span class="line">    letters = <span class="string">&#x27;abcdefghijklmnopqrstuvwxyz&#x27;</span></span><br><span class="line">    replace_l = []</span><br><span class="line">    split_l = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    split_l = [(word[:i],word[i:]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word) + <span class="number">1</span>)]</span><br><span class="line">    replace_set = [L + C + R[<span class="number">1</span>:] <span class="keyword">for</span> L,R <span class="keyword">in</span> split_l <span class="keyword">if</span> R <span class="keyword">for</span> C <span class="keyword">in</span> letters <span class="keyword">if</span> C <span class="keyword">is</span> <span class="keyword">not</span> R[<span class="number">0</span>]]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># turn the set back into a list and sort it, for easier viewing</span></span><br><span class="line">    replace_l = <span class="built_in">sorted</span>(<span class="built_in">list</span>(replace_set))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> verbose: <span class="built_in">print</span>(<span class="string">f&quot;Input word = <span class="subst">&#123;word&#125;</span> \nsplit_l = <span class="subst">&#123;split_l&#125;</span> \nreplace_l <span class="subst">&#123;replace_l&#125;</span>&quot;</span>)   </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> replace_l</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">replace_l = replace_letter(word=<span class="string">&#x27;can&#x27;</span>,</span><br><span class="line">                              verbose=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Input word = can 
split_l = [(&#39;&#39;, &#39;can&#39;), (&#39;c&#39;, &#39;an&#39;), (&#39;ca&#39;, &#39;n&#39;), (&#39;can&#39;, &#39;&#39;)] 
replace_l [&#39;aan&#39;, &#39;ban&#39;, &#39;caa&#39;, &#39;cab&#39;, &#39;cac&#39;, &#39;cad&#39;, &#39;cae&#39;, &#39;caf&#39;, &#39;cag&#39;, &#39;cah&#39;, &#39;cai&#39;, &#39;caj&#39;, &#39;cak&#39;, &#39;cal&#39;, &#39;cam&#39;, &#39;cao&#39;, &#39;cap&#39;, &#39;caq&#39;, &#39;car&#39;, &#39;cas&#39;, &#39;cat&#39;, &#39;cau&#39;, &#39;cav&#39;, &#39;caw&#39;, &#39;cax&#39;, &#39;cay&#39;, &#39;caz&#39;, &#39;cbn&#39;, &#39;ccn&#39;, &#39;cdn&#39;, &#39;cen&#39;, &#39;cfn&#39;, &#39;cgn&#39;, &#39;chn&#39;, &#39;cin&#39;, &#39;cjn&#39;, &#39;ckn&#39;, &#39;cln&#39;, &#39;cmn&#39;, &#39;cnn&#39;, &#39;con&#39;, &#39;cpn&#39;, &#39;cqn&#39;, &#39;crn&#39;, &#39;csn&#39;, &#39;ctn&#39;, &#39;cun&#39;, &#39;cvn&#39;, &#39;cwn&#39;, &#39;cxn&#39;, &#39;cyn&#39;, &#39;czn&#39;, &#39;dan&#39;, &#39;ean&#39;, &#39;fan&#39;, &#39;gan&#39;, &#39;han&#39;, &#39;ian&#39;, &#39;jan&#39;, &#39;kan&#39;, &#39;lan&#39;, &#39;man&#39;, &#39;nan&#39;, &#39;oan&#39;, &#39;pan&#39;, &#39;qan&#39;, &#39;ran&#39;, &#39;san&#39;, &#39;tan&#39;, &#39;uan&#39;, &#39;van&#39;, &#39;wan&#39;, &#39;xan&#39;, &#39;yan&#39;, &#39;zan&#39;]</code></pre>
<h4 id="expected-output-7">Expected Output**:</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input word = can </span><br><span class="line">split_l = [(<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;can&#x27;</span>), (<span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;an&#x27;</span>), (<span class="string">&#x27;ca&#x27;</span>, <span class="string">&#x27;n&#x27;</span>)] </span><br><span class="line">replace_l [<span class="string">&#x27;aan&#x27;</span>, <span class="string">&#x27;ban&#x27;</span>, <span class="string">&#x27;caa&#x27;</span>, <span class="string">&#x27;cab&#x27;</span>, <span class="string">&#x27;cac&#x27;</span>, <span class="string">&#x27;cad&#x27;</span>, <span class="string">&#x27;cae&#x27;</span>, <span class="string">&#x27;caf&#x27;</span>, <span class="string">&#x27;cag&#x27;</span>, <span class="string">&#x27;cah&#x27;</span>, <span class="string">&#x27;cai&#x27;</span>, <span class="string">&#x27;caj&#x27;</span>, <span class="string">&#x27;cak&#x27;</span>, <span class="string">&#x27;cal&#x27;</span>, <span class="string">&#x27;cam&#x27;</span>, <span class="string">&#x27;cao&#x27;</span>, <span class="string">&#x27;cap&#x27;</span>, <span class="string">&#x27;caq&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;cas&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;cau&#x27;</span>, <span class="string">&#x27;cav&#x27;</span>, <span class="string">&#x27;caw&#x27;</span>, <span class="string">&#x27;cax&#x27;</span>, <span class="string">&#x27;cay&#x27;</span>, <span class="string">&#x27;caz&#x27;</span>, <span class="string">&#x27;cbn&#x27;</span>, <span class="string">&#x27;ccn&#x27;</span>, <span class="string">&#x27;cdn&#x27;</span>, <span class="string">&#x27;cen&#x27;</span>, <span class="string">&#x27;cfn&#x27;</span>, <span class="string">&#x27;cgn&#x27;</span>, <span class="string">&#x27;chn&#x27;</span>, <span class="string">&#x27;cin&#x27;</span>, <span class="string">&#x27;cjn&#x27;</span>, <span class="string">&#x27;ckn&#x27;</span>, <span class="string">&#x27;cln&#x27;</span>, <span class="string">&#x27;cmn&#x27;</span>, <span class="string">&#x27;cnn&#x27;</span>, <span class="string">&#x27;con&#x27;</span>, <span class="string">&#x27;cpn&#x27;</span>, <span class="string">&#x27;cqn&#x27;</span>, <span class="string">&#x27;crn&#x27;</span>, <span class="string">&#x27;csn&#x27;</span>, <span class="string">&#x27;ctn&#x27;</span>, <span class="string">&#x27;cun&#x27;</span>, <span class="string">&#x27;cvn&#x27;</span>, <span class="string">&#x27;cwn&#x27;</span>, <span class="string">&#x27;cxn&#x27;</span>, <span class="string">&#x27;cyn&#x27;</span>, <span class="string">&#x27;czn&#x27;</span>, <span class="string">&#x27;dan&#x27;</span>, <span class="string">&#x27;ean&#x27;</span>, <span class="string">&#x27;fan&#x27;</span>, <span class="string">&#x27;gan&#x27;</span>, <span class="string">&#x27;han&#x27;</span>, <span class="string">&#x27;ian&#x27;</span>, <span class="string">&#x27;jan&#x27;</span>, <span class="string">&#x27;kan&#x27;</span>, <span class="string">&#x27;lan&#x27;</span>, <span class="string">&#x27;man&#x27;</span>, <span class="string">&#x27;nan&#x27;</span>, <span class="string">&#x27;oan&#x27;</span>, <span class="string">&#x27;pan&#x27;</span>, <span class="string">&#x27;qan&#x27;</span>, <span class="string">&#x27;ran&#x27;</span>, <span class="string">&#x27;san&#x27;</span>, <span class="string">&#x27;tan&#x27;</span>, <span class="string">&#x27;uan&#x27;</span>, <span class="string">&#x27;van&#x27;</span>, <span class="string">&#x27;wan&#x27;</span>, <span class="string">&#x27;xan&#x27;</span>, <span class="string">&#x27;yan&#x27;</span>, <span class="string">&#x27;zan&#x27;</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li>Note how the input word 'can' should not be one of the output words.</li>
</ul>
<h4 id="note-1-2">Note 1</h4>
<p>If you get something like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input word = can </span><br><span class="line">split_l = [(<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;can&#x27;</span>), (<span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;an&#x27;</span>), (<span class="string">&#x27;ca&#x27;</span>, <span class="string">&#x27;n&#x27;</span>), (<span class="string">&#x27;can&#x27;</span>, <span class="string">&#x27;&#x27;</span>)] </span><br><span class="line">replace_l [<span class="string">&#x27;aan&#x27;</span>, <span class="string">&#x27;ban&#x27;</span>, <span class="string">&#x27;caa&#x27;</span>, <span class="string">&#x27;cab&#x27;</span>, <span class="string">&#x27;cac&#x27;</span>, <span class="string">&#x27;cad&#x27;</span>, <span class="string">&#x27;cae&#x27;</span>, <span class="string">&#x27;caf&#x27;</span>, <span class="string">&#x27;cag&#x27;</span>, <span class="string">&#x27;cah&#x27;</span>, <span class="string">&#x27;cai&#x27;</span>, <span class="string">&#x27;caj&#x27;</span>, <span class="string">&#x27;cak&#x27;</span>, <span class="string">&#x27;cal&#x27;</span>, <span class="string">&#x27;cam&#x27;</span>, <span class="string">&#x27;cao&#x27;</span>, <span class="string">&#x27;cap&#x27;</span>, <span class="string">&#x27;caq&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;cas&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;cau&#x27;</span>, <span class="string">&#x27;cav&#x27;</span>, <span class="string">&#x27;caw&#x27;</span>, <span class="string">&#x27;cax&#x27;</span>, <span class="string">&#x27;cay&#x27;</span>, <span class="string">&#x27;caz&#x27;</span>, <span class="string">&#x27;cbn&#x27;</span>, <span class="string">&#x27;ccn&#x27;</span>, <span class="string">&#x27;cdn&#x27;</span>, <span class="string">&#x27;cen&#x27;</span>, <span class="string">&#x27;cfn&#x27;</span>, <span class="string">&#x27;cgn&#x27;</span>, <span class="string">&#x27;chn&#x27;</span>, <span class="string">&#x27;cin&#x27;</span>, <span class="string">&#x27;cjn&#x27;</span>, <span class="string">&#x27;ckn&#x27;</span>, <span class="string">&#x27;cln&#x27;</span>, <span class="string">&#x27;cmn&#x27;</span>, <span class="string">&#x27;cnn&#x27;</span>, <span class="string">&#x27;con&#x27;</span>, <span class="string">&#x27;cpn&#x27;</span>, <span class="string">&#x27;cqn&#x27;</span>, <span class="string">&#x27;crn&#x27;</span>, <span class="string">&#x27;csn&#x27;</span>, <span class="string">&#x27;ctn&#x27;</span>, <span class="string">&#x27;cun&#x27;</span>, <span class="string">&#x27;cvn&#x27;</span>, <span class="string">&#x27;cwn&#x27;</span>, <span class="string">&#x27;cxn&#x27;</span>, <span class="string">&#x27;cyn&#x27;</span>, <span class="string">&#x27;czn&#x27;</span>, <span class="string">&#x27;dan&#x27;</span>, <span class="string">&#x27;ean&#x27;</span>, <span class="string">&#x27;fan&#x27;</span>, <span class="string">&#x27;gan&#x27;</span>, <span class="string">&#x27;han&#x27;</span>, <span class="string">&#x27;ian&#x27;</span>, <span class="string">&#x27;jan&#x27;</span>, <span class="string">&#x27;kan&#x27;</span>, <span class="string">&#x27;lan&#x27;</span>, <span class="string">&#x27;man&#x27;</span>, <span class="string">&#x27;nan&#x27;</span>, <span class="string">&#x27;oan&#x27;</span>, <span class="string">&#x27;pan&#x27;</span>, <span class="string">&#x27;qan&#x27;</span>, <span class="string">&#x27;ran&#x27;</span>, <span class="string">&#x27;san&#x27;</span>, <span class="string">&#x27;tan&#x27;</span>, <span class="string">&#x27;uan&#x27;</span>, <span class="string">&#x27;van&#x27;</span>, <span class="string">&#x27;wan&#x27;</span>, <span class="string">&#x27;xan&#x27;</span>, <span class="string">&#x27;yan&#x27;</span>, <span class="string">&#x27;zan&#x27;</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li>Notice how split_l has an extra tuple <code>('can', '')</code>, but the output is still the same, so this is okay.</li>
</ul>
<h4 id="note-2-2">Note 2</h4>
<p>If you get something like this: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input word = can </span><br><span class="line">split_l = [(<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;can&#x27;</span>), (<span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;an&#x27;</span>), (<span class="string">&#x27;ca&#x27;</span>, <span class="string">&#x27;n&#x27;</span>), (<span class="string">&#x27;can&#x27;</span>, <span class="string">&#x27;&#x27;</span>)] </span><br><span class="line">replace_l [<span class="string">&#x27;aan&#x27;</span>, <span class="string">&#x27;ban&#x27;</span>, <span class="string">&#x27;caa&#x27;</span>, <span class="string">&#x27;cab&#x27;</span>, <span class="string">&#x27;cac&#x27;</span>, <span class="string">&#x27;cad&#x27;</span>, <span class="string">&#x27;cae&#x27;</span>, <span class="string">&#x27;caf&#x27;</span>, <span class="string">&#x27;cag&#x27;</span>, <span class="string">&#x27;cah&#x27;</span>, <span class="string">&#x27;cai&#x27;</span>, <span class="string">&#x27;caj&#x27;</span>, <span class="string">&#x27;cak&#x27;</span>, <span class="string">&#x27;cal&#x27;</span>, <span class="string">&#x27;cam&#x27;</span>, <span class="string">&#x27;cana&#x27;</span>, <span class="string">&#x27;canb&#x27;</span>, <span class="string">&#x27;canc&#x27;</span>, <span class="string">&#x27;cand&#x27;</span>, <span class="string">&#x27;cane&#x27;</span>, <span class="string">&#x27;canf&#x27;</span>, <span class="string">&#x27;cang&#x27;</span>, <span class="string">&#x27;canh&#x27;</span>, <span class="string">&#x27;cani&#x27;</span>, <span class="string">&#x27;canj&#x27;</span>, <span class="string">&#x27;cank&#x27;</span>, <span class="string">&#x27;canl&#x27;</span>, <span class="string">&#x27;canm&#x27;</span>, <span class="string">&#x27;cann&#x27;</span>, <span class="string">&#x27;cano&#x27;</span>, <span class="string">&#x27;canp&#x27;</span>, <span class="string">&#x27;canq&#x27;</span>, <span class="string">&#x27;canr&#x27;</span>, <span class="string">&#x27;cans&#x27;</span>, <span class="string">&#x27;cant&#x27;</span>, <span class="string">&#x27;canu&#x27;</span>, <span class="string">&#x27;canv&#x27;</span>, <span class="string">&#x27;canw&#x27;</span>, <span class="string">&#x27;canx&#x27;</span>, <span class="string">&#x27;cany&#x27;</span>, <span class="string">&#x27;canz&#x27;</span>, <span class="string">&#x27;cao&#x27;</span>, <span class="string">&#x27;cap&#x27;</span>, <span class="string">&#x27;caq&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;cas&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;cau&#x27;</span>, <span class="string">&#x27;cav&#x27;</span>, <span class="string">&#x27;caw&#x27;</span>, <span class="string">&#x27;cax&#x27;</span>, <span class="string">&#x27;cay&#x27;</span>, <span class="string">&#x27;caz&#x27;</span>, <span class="string">&#x27;cbn&#x27;</span>, <span class="string">&#x27;ccn&#x27;</span>, <span class="string">&#x27;cdn&#x27;</span>, <span class="string">&#x27;cen&#x27;</span>, <span class="string">&#x27;cfn&#x27;</span>, <span class="string">&#x27;cgn&#x27;</span>, <span class="string">&#x27;chn&#x27;</span>, <span class="string">&#x27;cin&#x27;</span>, <span class="string">&#x27;cjn&#x27;</span>, <span class="string">&#x27;ckn&#x27;</span>, <span class="string">&#x27;cln&#x27;</span>, <span class="string">&#x27;cmn&#x27;</span>, <span class="string">&#x27;cnn&#x27;</span>, <span class="string">&#x27;con&#x27;</span>, <span class="string">&#x27;cpn&#x27;</span>, <span class="string">&#x27;cqn&#x27;</span>, <span class="string">&#x27;crn&#x27;</span>, <span class="string">&#x27;csn&#x27;</span>, <span class="string">&#x27;ctn&#x27;</span>, <span class="string">&#x27;cun&#x27;</span>, <span class="string">&#x27;cvn&#x27;</span>, <span class="string">&#x27;cwn&#x27;</span>, <span class="string">&#x27;cxn&#x27;</span>, <span class="string">&#x27;cyn&#x27;</span>, <span class="string">&#x27;czn&#x27;</span>, <span class="string">&#x27;dan&#x27;</span>, <span class="string">&#x27;ean&#x27;</span>, <span class="string">&#x27;fan&#x27;</span>, <span class="string">&#x27;gan&#x27;</span>, <span class="string">&#x27;han&#x27;</span>, <span class="string">&#x27;ian&#x27;</span>, <span class="string">&#x27;jan&#x27;</span>, <span class="string">&#x27;kan&#x27;</span>, <span class="string">&#x27;lan&#x27;</span>, <span class="string">&#x27;man&#x27;</span>, <span class="string">&#x27;nan&#x27;</span>, <span class="string">&#x27;oan&#x27;</span>, <span class="string">&#x27;pan&#x27;</span>, <span class="string">&#x27;qan&#x27;</span>, <span class="string">&#x27;ran&#x27;</span>, <span class="string">&#x27;san&#x27;</span>, <span class="string">&#x27;tan&#x27;</span>, <span class="string">&#x27;uan&#x27;</span>, <span class="string">&#x27;van&#x27;</span>, <span class="string">&#x27;wan&#x27;</span>, <span class="string">&#x27;xan&#x27;</span>, <span class="string">&#x27;yan&#x27;</span>, <span class="string">&#x27;zan&#x27;</span>]</span><br></pre></td></tr></table></figure> - Notice how there are strings that are 1 letter longer than the original word, such as <code>cana</code>. - Please check for the case when there is an empty string <code>''</code>, and if so, do not use that empty string when setting replace_l.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test # 2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of outputs of switch_letter(&#x27;at&#x27;) is <span class="subst">&#123;<span class="built_in">len</span>(switch_letter(<span class="string">&#x27;at&#x27;</span>))&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Number of outputs of switch_letter(&#39;at&#39;) is 1</code></pre>
<h4 id="expected-output-8">Expected output</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Number of outputs of <span class="title">switch_letter</span><span class="params">(<span class="string">&#x27;at&#x27;</span>)</span> is 1</span></span><br></pre></td></tr></table></figure>
<p><a name='ex-7'></a> ### Exercise 7</p>
<p><strong>Instructions for insert_letter()</strong>: Now implement a function that takes in a word and returns a list with a letter inserted at every offset.</p>
<p><strong>Step 1:</strong> is the same as in <code>delete_letter()</code></p>
<p><strong>Step 2:</strong> This can be a list comprehension of the form:<br />
<code>[f(a,b,c) for a, b in splits if condition for c in string]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: inserts</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_letter</span>(<span class="params">word, verbose=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: the input string/word </span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        inserts: a set of all possible strings with one new letter inserted at every offset</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span> </span><br><span class="line">    letters = <span class="string">&#x27;abcdefghijklmnopqrstuvwxyz&#x27;</span></span><br><span class="line">    insert_l = []</span><br><span class="line">    split_l = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    split_l = [(word[:i],word[i:]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word) + <span class="number">1</span>)]</span><br><span class="line">    insert_l = [L + C + R <span class="keyword">for</span> L, R <span class="keyword">in</span> split_l <span class="keyword">for</span> C <span class="keyword">in</span> letters]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> verbose: <span class="built_in">print</span>(<span class="string">f&quot;Input word <span class="subst">&#123;word&#125;</span> \nsplit_l = <span class="subst">&#123;split_l&#125;</span> \ninsert_l = <span class="subst">&#123;insert_l&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> insert_l</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">insert_l = insert_letter(<span class="string">&#x27;at&#x27;</span>, <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of strings output by insert_letter(&#x27;at&#x27;) is <span class="subst">&#123;<span class="built_in">len</span>(insert_l)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Input word at 
split_l = [(&#39;&#39;, &#39;at&#39;), (&#39;a&#39;, &#39;t&#39;), (&#39;at&#39;, &#39;&#39;)] 
insert_l = [&#39;aat&#39;, &#39;bat&#39;, &#39;cat&#39;, &#39;dat&#39;, &#39;eat&#39;, &#39;fat&#39;, &#39;gat&#39;, &#39;hat&#39;, &#39;iat&#39;, &#39;jat&#39;, &#39;kat&#39;, &#39;lat&#39;, &#39;mat&#39;, &#39;nat&#39;, &#39;oat&#39;, &#39;pat&#39;, &#39;qat&#39;, &#39;rat&#39;, &#39;sat&#39;, &#39;tat&#39;, &#39;uat&#39;, &#39;vat&#39;, &#39;wat&#39;, &#39;xat&#39;, &#39;yat&#39;, &#39;zat&#39;, &#39;aat&#39;, &#39;abt&#39;, &#39;act&#39;, &#39;adt&#39;, &#39;aet&#39;, &#39;aft&#39;, &#39;agt&#39;, &#39;aht&#39;, &#39;ait&#39;, &#39;ajt&#39;, &#39;akt&#39;, &#39;alt&#39;, &#39;amt&#39;, &#39;ant&#39;, &#39;aot&#39;, &#39;apt&#39;, &#39;aqt&#39;, &#39;art&#39;, &#39;ast&#39;, &#39;att&#39;, &#39;aut&#39;, &#39;avt&#39;, &#39;awt&#39;, &#39;axt&#39;, &#39;ayt&#39;, &#39;azt&#39;, &#39;ata&#39;, &#39;atb&#39;, &#39;atc&#39;, &#39;atd&#39;, &#39;ate&#39;, &#39;atf&#39;, &#39;atg&#39;, &#39;ath&#39;, &#39;ati&#39;, &#39;atj&#39;, &#39;atk&#39;, &#39;atl&#39;, &#39;atm&#39;, &#39;atn&#39;, &#39;ato&#39;, &#39;atp&#39;, &#39;atq&#39;, &#39;atr&#39;, &#39;ats&#39;, &#39;att&#39;, &#39;atu&#39;, &#39;atv&#39;, &#39;atw&#39;, &#39;atx&#39;, &#39;aty&#39;, &#39;atz&#39;]
Number of strings output by insert_letter(&#39;at&#39;) is 78</code></pre>
<h4 id="expected-output-9">Expected output</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input word at </span><br><span class="line">split_l = [(<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;at&#x27;</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;t&#x27;</span>), (<span class="string">&#x27;at&#x27;</span>, <span class="string">&#x27;&#x27;</span>)] </span><br><span class="line">insert_l = [<span class="string">&#x27;aat&#x27;</span>, <span class="string">&#x27;bat&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dat&#x27;</span>, <span class="string">&#x27;eat&#x27;</span>, <span class="string">&#x27;fat&#x27;</span>, <span class="string">&#x27;gat&#x27;</span>, <span class="string">&#x27;hat&#x27;</span>, <span class="string">&#x27;iat&#x27;</span>, <span class="string">&#x27;jat&#x27;</span>, <span class="string">&#x27;kat&#x27;</span>, <span class="string">&#x27;lat&#x27;</span>, <span class="string">&#x27;mat&#x27;</span>, <span class="string">&#x27;nat&#x27;</span>, <span class="string">&#x27;oat&#x27;</span>, <span class="string">&#x27;pat&#x27;</span>, <span class="string">&#x27;qat&#x27;</span>, <span class="string">&#x27;rat&#x27;</span>, <span class="string">&#x27;sat&#x27;</span>, <span class="string">&#x27;tat&#x27;</span>, <span class="string">&#x27;uat&#x27;</span>, <span class="string">&#x27;vat&#x27;</span>, <span class="string">&#x27;wat&#x27;</span>, <span class="string">&#x27;xat&#x27;</span>, <span class="string">&#x27;yat&#x27;</span>, <span class="string">&#x27;zat&#x27;</span>, <span class="string">&#x27;aat&#x27;</span>, <span class="string">&#x27;abt&#x27;</span>, <span class="string">&#x27;act&#x27;</span>, <span class="string">&#x27;adt&#x27;</span>, <span class="string">&#x27;aet&#x27;</span>, <span class="string">&#x27;aft&#x27;</span>, <span class="string">&#x27;agt&#x27;</span>, <span class="string">&#x27;aht&#x27;</span>, <span class="string">&#x27;ait&#x27;</span>, <span class="string">&#x27;ajt&#x27;</span>, <span class="string">&#x27;akt&#x27;</span>, <span class="string">&#x27;alt&#x27;</span>, <span class="string">&#x27;amt&#x27;</span>, <span class="string">&#x27;ant&#x27;</span>, <span class="string">&#x27;aot&#x27;</span>, <span class="string">&#x27;apt&#x27;</span>, <span class="string">&#x27;aqt&#x27;</span>, <span class="string">&#x27;art&#x27;</span>, <span class="string">&#x27;ast&#x27;</span>, <span class="string">&#x27;att&#x27;</span>, <span class="string">&#x27;aut&#x27;</span>, <span class="string">&#x27;avt&#x27;</span>, <span class="string">&#x27;awt&#x27;</span>, <span class="string">&#x27;axt&#x27;</span>, <span class="string">&#x27;ayt&#x27;</span>, <span class="string">&#x27;azt&#x27;</span>, <span class="string">&#x27;ata&#x27;</span>, <span class="string">&#x27;atb&#x27;</span>, <span class="string">&#x27;atc&#x27;</span>, <span class="string">&#x27;atd&#x27;</span>, <span class="string">&#x27;ate&#x27;</span>, <span class="string">&#x27;atf&#x27;</span>, <span class="string">&#x27;atg&#x27;</span>, <span class="string">&#x27;ath&#x27;</span>, <span class="string">&#x27;ati&#x27;</span>, <span class="string">&#x27;atj&#x27;</span>, <span class="string">&#x27;atk&#x27;</span>, <span class="string">&#x27;atl&#x27;</span>, <span class="string">&#x27;atm&#x27;</span>, <span class="string">&#x27;atn&#x27;</span>, <span class="string">&#x27;ato&#x27;</span>, <span class="string">&#x27;atp&#x27;</span>, <span class="string">&#x27;atq&#x27;</span>, <span class="string">&#x27;atr&#x27;</span>, <span class="string">&#x27;ats&#x27;</span>, <span class="string">&#x27;att&#x27;</span>, <span class="string">&#x27;atu&#x27;</span>, <span class="string">&#x27;atv&#x27;</span>, <span class="string">&#x27;atw&#x27;</span>, <span class="string">&#x27;atx&#x27;</span>, <span class="string">&#x27;aty&#x27;</span>, <span class="string">&#x27;atz&#x27;</span>]</span><br><span class="line">Number of strings output by insert_letter(<span class="string">&#x27;at&#x27;</span>) <span class="keyword">is</span> <span class="number">78</span></span><br></pre></td></tr></table></figure>
<h4 id="note-1-3">Note 1</h4>
<p>If you get a split_l like this: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input word at </span><br><span class="line">split_l = [(<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;at&#x27;</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;t&#x27;</span>)] </span><br><span class="line">insert_l = [<span class="string">&#x27;aat&#x27;</span>, <span class="string">&#x27;bat&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dat&#x27;</span>, <span class="string">&#x27;eat&#x27;</span>, <span class="string">&#x27;fat&#x27;</span>, <span class="string">&#x27;gat&#x27;</span>, <span class="string">&#x27;hat&#x27;</span>, <span class="string">&#x27;iat&#x27;</span>, <span class="string">&#x27;jat&#x27;</span>, <span class="string">&#x27;kat&#x27;</span>, <span class="string">&#x27;lat&#x27;</span>, <span class="string">&#x27;mat&#x27;</span>, <span class="string">&#x27;nat&#x27;</span>, <span class="string">&#x27;oat&#x27;</span>, <span class="string">&#x27;pat&#x27;</span>, <span class="string">&#x27;qat&#x27;</span>, <span class="string">&#x27;rat&#x27;</span>, <span class="string">&#x27;sat&#x27;</span>, <span class="string">&#x27;tat&#x27;</span>, <span class="string">&#x27;uat&#x27;</span>, <span class="string">&#x27;vat&#x27;</span>, <span class="string">&#x27;wat&#x27;</span>, <span class="string">&#x27;xat&#x27;</span>, <span class="string">&#x27;yat&#x27;</span>, <span class="string">&#x27;zat&#x27;</span>, <span class="string">&#x27;aat&#x27;</span>, <span class="string">&#x27;abt&#x27;</span>, <span class="string">&#x27;act&#x27;</span>, <span class="string">&#x27;adt&#x27;</span>, <span class="string">&#x27;aet&#x27;</span>, <span class="string">&#x27;aft&#x27;</span>, <span class="string">&#x27;agt&#x27;</span>, <span class="string">&#x27;aht&#x27;</span>, <span class="string">&#x27;ait&#x27;</span>, <span class="string">&#x27;ajt&#x27;</span>, <span class="string">&#x27;akt&#x27;</span>, <span class="string">&#x27;alt&#x27;</span>, <span class="string">&#x27;amt&#x27;</span>, <span class="string">&#x27;ant&#x27;</span>, <span class="string">&#x27;aot&#x27;</span>, <span class="string">&#x27;apt&#x27;</span>, <span class="string">&#x27;aqt&#x27;</span>, <span class="string">&#x27;art&#x27;</span>, <span class="string">&#x27;ast&#x27;</span>, <span class="string">&#x27;att&#x27;</span>, <span class="string">&#x27;aut&#x27;</span>, <span class="string">&#x27;avt&#x27;</span>, <span class="string">&#x27;awt&#x27;</span>, <span class="string">&#x27;axt&#x27;</span>, <span class="string">&#x27;ayt&#x27;</span>, <span class="string">&#x27;azt&#x27;</span>]</span><br><span class="line">Number of strings output by insert_letter(<span class="string">&#x27;at&#x27;</span>) <span class="keyword">is</span> <span class="number">52</span></span><br></pre></td></tr></table></figure> - Notice that split_l is missing the extra tuple ('at', ''). For insertion, we actually <strong>WANT</strong> this tuple. - The function is not creating all the desired output strings. - Check the range that you use for the for loop.</p>
<h4 id="note-2-3">Note 2</h4>
<p>If you see this: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input word at </span><br><span class="line">split_l = [(<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;at&#x27;</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;t&#x27;</span>), (<span class="string">&#x27;at&#x27;</span>, <span class="string">&#x27;&#x27;</span>)] </span><br><span class="line">insert_l = [<span class="string">&#x27;aat&#x27;</span>, <span class="string">&#x27;bat&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dat&#x27;</span>, <span class="string">&#x27;eat&#x27;</span>, <span class="string">&#x27;fat&#x27;</span>, <span class="string">&#x27;gat&#x27;</span>, <span class="string">&#x27;hat&#x27;</span>, <span class="string">&#x27;iat&#x27;</span>, <span class="string">&#x27;jat&#x27;</span>, <span class="string">&#x27;kat&#x27;</span>, <span class="string">&#x27;lat&#x27;</span>, <span class="string">&#x27;mat&#x27;</span>, <span class="string">&#x27;nat&#x27;</span>, <span class="string">&#x27;oat&#x27;</span>, <span class="string">&#x27;pat&#x27;</span>, <span class="string">&#x27;qat&#x27;</span>, <span class="string">&#x27;rat&#x27;</span>, <span class="string">&#x27;sat&#x27;</span>, <span class="string">&#x27;tat&#x27;</span>, <span class="string">&#x27;uat&#x27;</span>, <span class="string">&#x27;vat&#x27;</span>, <span class="string">&#x27;wat&#x27;</span>, <span class="string">&#x27;xat&#x27;</span>, <span class="string">&#x27;yat&#x27;</span>, <span class="string">&#x27;zat&#x27;</span>, <span class="string">&#x27;aat&#x27;</span>, <span class="string">&#x27;abt&#x27;</span>, <span class="string">&#x27;act&#x27;</span>, <span class="string">&#x27;adt&#x27;</span>, <span class="string">&#x27;aet&#x27;</span>, <span class="string">&#x27;aft&#x27;</span>, <span class="string">&#x27;agt&#x27;</span>, <span class="string">&#x27;aht&#x27;</span>, <span class="string">&#x27;ait&#x27;</span>, <span class="string">&#x27;ajt&#x27;</span>, <span class="string">&#x27;akt&#x27;</span>, <span class="string">&#x27;alt&#x27;</span>, <span class="string">&#x27;amt&#x27;</span>, <span class="string">&#x27;ant&#x27;</span>, <span class="string">&#x27;aot&#x27;</span>, <span class="string">&#x27;apt&#x27;</span>, <span class="string">&#x27;aqt&#x27;</span>, <span class="string">&#x27;art&#x27;</span>, <span class="string">&#x27;ast&#x27;</span>, <span class="string">&#x27;att&#x27;</span>, <span class="string">&#x27;aut&#x27;</span>, <span class="string">&#x27;avt&#x27;</span>, <span class="string">&#x27;awt&#x27;</span>, <span class="string">&#x27;axt&#x27;</span>, <span class="string">&#x27;ayt&#x27;</span>, <span class="string">&#x27;azt&#x27;</span>]</span><br><span class="line">Number of strings output by insert_letter(<span class="string">&#x27;at&#x27;</span>) <span class="keyword">is</span> <span class="number">52</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li>Even though you may have fixed the split_l so that it contains the tuple <code>('at', '')</code>, notice that you're still missing some output strings.
<ul>
<li>Notice that it's missing strings such as 'ata', 'atb', 'atc' all the way to 'atz'.</li>
</ul></li>
<li>To fix this, make sure that when you set insert_l, you allow the use of the empty string <code>''</code>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test # 2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of outputs of insert_letter(&#x27;at&#x27;) is <span class="subst">&#123;<span class="built_in">len</span>(insert_letter(<span class="string">&#x27;at&#x27;</span>))&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Number of outputs of insert_letter(&#39;at&#39;) is 78</code></pre>
<h4 id="expected-output-10">Expected output</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Number of outputs of <span class="title">insert_letter</span><span class="params">(<span class="string">&#x27;at&#x27;</span>)</span> is 78</span></span><br></pre></td></tr></table></figure>
<p><a name='3'></a></p>
<h1 id="part-3-combining-the-edits">Part 3: Combining the edits</h1>
<p>Now that you have implemented the string manipulations, you will create two functions that, given a string, will return all the possible single and double edits on that string. These will be <code>edit_one_letter()</code> and <code>edit_two_letters()</code>.</p>
<p><a name='3-1'></a> ## 3.1 Edit one letter</p>
<p><a name='ex-8'></a> ### Exercise 8</p>
<p><strong>Instructions</strong>: Implement the <code>edit_one_letter</code> function to get all the possible edits that are one edit away from a word. The edits consist of the replace, insert, delete, and optionally the switch operation. You should use the previous functions you have already implemented to complete this function. The 'switch' function is a less common edit function, so its use will be selected by an "allow_switches" input argument.</p>
<p>Note that those functions return <em>lists</em> while this function should return a <em>python set</em>. Utilizing a set eliminates any duplicate entries.</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
<li>
Each of the functions returns a list. You can combine lists using the <code>+</code> operator.
</li>
<li>
To get unique strings (avoid duplicates), you can use the set() function.
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: edit_one_letter</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edit_one_letter</span>(<span class="params">word, allow_switches = <span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: the string/word for which we will generate all possible wordsthat are one edit away.</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        edit_one_set: a set of words with one possible edit. Please return a set. and not a list.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    edit_one_set = <span class="built_in">set</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    edit_one_set = edit_one_set | <span class="built_in">set</span>(delete_letter(word)) | <span class="built_in">set</span>(insert_letter(word)) | <span class="built_in">set</span>(replace_letter(word))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> allow_switches:</span><br><span class="line">        edit_one_set |= <span class="built_in">set</span>(switch_letter(word))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> edit_one_set</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tmp_word = <span class="string">&quot;at&quot;</span></span><br><span class="line">tmp_edit_one_set = edit_one_letter(tmp_word)</span><br><span class="line"><span class="comment"># turn this into a list to sort it, in order to view it</span></span><br><span class="line">tmp_edit_one_l = <span class="built_in">sorted</span>(<span class="built_in">list</span>(tmp_edit_one_set))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;input word <span class="subst">&#123;tmp_word&#125;</span> \nedit_one_l \n<span class="subst">&#123;tmp_edit_one_l&#125;</span>\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The type of the returned object should be a set <span class="subst">&#123;<span class="built_in">type</span>(tmp_edit_one_set)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of outputs from edit_one_letter(&#x27;at&#x27;) is <span class="subst">&#123;<span class="built_in">len</span>(edit_one_letter(<span class="string">&#x27;at&#x27;</span>))&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>input word at 
edit_one_l 
[&#39;a&#39;, &#39;aa&#39;, &#39;aat&#39;, &#39;ab&#39;, &#39;abt&#39;, &#39;ac&#39;, &#39;act&#39;, &#39;ad&#39;, &#39;adt&#39;, &#39;ae&#39;, &#39;aet&#39;, &#39;af&#39;, &#39;aft&#39;, &#39;ag&#39;, &#39;agt&#39;, &#39;ah&#39;, &#39;aht&#39;, &#39;ai&#39;, &#39;ait&#39;, &#39;aj&#39;, &#39;ajt&#39;, &#39;ak&#39;, &#39;akt&#39;, &#39;al&#39;, &#39;alt&#39;, &#39;am&#39;, &#39;amt&#39;, &#39;an&#39;, &#39;ant&#39;, &#39;ao&#39;, &#39;aot&#39;, &#39;ap&#39;, &#39;apt&#39;, &#39;aq&#39;, &#39;aqt&#39;, &#39;ar&#39;, &#39;art&#39;, &#39;as&#39;, &#39;ast&#39;, &#39;ata&#39;, &#39;atb&#39;, &#39;atc&#39;, &#39;atd&#39;, &#39;ate&#39;, &#39;atf&#39;, &#39;atg&#39;, &#39;ath&#39;, &#39;ati&#39;, &#39;atj&#39;, &#39;atk&#39;, &#39;atl&#39;, &#39;atm&#39;, &#39;atn&#39;, &#39;ato&#39;, &#39;atp&#39;, &#39;atq&#39;, &#39;atr&#39;, &#39;ats&#39;, &#39;att&#39;, &#39;atu&#39;, &#39;atv&#39;, &#39;atw&#39;, &#39;atx&#39;, &#39;aty&#39;, &#39;atz&#39;, &#39;au&#39;, &#39;aut&#39;, &#39;av&#39;, &#39;avt&#39;, &#39;aw&#39;, &#39;awt&#39;, &#39;ax&#39;, &#39;axt&#39;, &#39;ay&#39;, &#39;ayt&#39;, &#39;az&#39;, &#39;azt&#39;, &#39;bat&#39;, &#39;bt&#39;, &#39;cat&#39;, &#39;ct&#39;, &#39;dat&#39;, &#39;dt&#39;, &#39;eat&#39;, &#39;et&#39;, &#39;fat&#39;, &#39;ft&#39;, &#39;gat&#39;, &#39;gt&#39;, &#39;hat&#39;, &#39;ht&#39;, &#39;iat&#39;, &#39;it&#39;, &#39;jat&#39;, &#39;jt&#39;, &#39;kat&#39;, &#39;kt&#39;, &#39;lat&#39;, &#39;lt&#39;, &#39;mat&#39;, &#39;mt&#39;, &#39;nat&#39;, &#39;nt&#39;, &#39;oat&#39;, &#39;ot&#39;, &#39;pat&#39;, &#39;pt&#39;, &#39;qat&#39;, &#39;qt&#39;, &#39;rat&#39;, &#39;rt&#39;, &#39;sat&#39;, &#39;st&#39;, &#39;t&#39;, &#39;ta&#39;, &#39;tat&#39;, &#39;tt&#39;, &#39;uat&#39;, &#39;ut&#39;, &#39;vat&#39;, &#39;vt&#39;, &#39;wat&#39;, &#39;wt&#39;, &#39;xat&#39;, &#39;xt&#39;, &#39;yat&#39;, &#39;yt&#39;, &#39;zat&#39;, &#39;zt&#39;]

The type of the returned object should be a set &lt;class &#39;set&#39;&gt;
Number of outputs from edit_one_letter(&#39;at&#39;) is 129</code></pre>
<h4 id="expected-output-11">Expected Output</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input word at </span><br><span class="line">edit_one_l </span><br><span class="line">[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;aa&#x27;</span>, <span class="string">&#x27;aat&#x27;</span>, <span class="string">&#x27;ab&#x27;</span>, <span class="string">&#x27;abt&#x27;</span>, <span class="string">&#x27;ac&#x27;</span>, <span class="string">&#x27;act&#x27;</span>, <span class="string">&#x27;ad&#x27;</span>, <span class="string">&#x27;adt&#x27;</span>, <span class="string">&#x27;ae&#x27;</span>, <span class="string">&#x27;aet&#x27;</span>, <span class="string">&#x27;af&#x27;</span>, <span class="string">&#x27;aft&#x27;</span>, <span class="string">&#x27;ag&#x27;</span>, <span class="string">&#x27;agt&#x27;</span>, <span class="string">&#x27;ah&#x27;</span>, <span class="string">&#x27;aht&#x27;</span>, <span class="string">&#x27;ai&#x27;</span>, <span class="string">&#x27;ait&#x27;</span>, <span class="string">&#x27;aj&#x27;</span>, <span class="string">&#x27;ajt&#x27;</span>, <span class="string">&#x27;ak&#x27;</span>, <span class="string">&#x27;akt&#x27;</span>, <span class="string">&#x27;al&#x27;</span>, <span class="string">&#x27;alt&#x27;</span>, <span class="string">&#x27;am&#x27;</span>, <span class="string">&#x27;amt&#x27;</span>, <span class="string">&#x27;an&#x27;</span>, <span class="string">&#x27;ant&#x27;</span>, <span class="string">&#x27;ao&#x27;</span>, <span class="string">&#x27;aot&#x27;</span>, <span class="string">&#x27;ap&#x27;</span>, <span class="string">&#x27;apt&#x27;</span>, <span class="string">&#x27;aq&#x27;</span>, <span class="string">&#x27;aqt&#x27;</span>, <span class="string">&#x27;ar&#x27;</span>, <span class="string">&#x27;art&#x27;</span>, <span class="string">&#x27;as&#x27;</span>, <span class="string">&#x27;ast&#x27;</span>, <span class="string">&#x27;ata&#x27;</span>, <span class="string">&#x27;atb&#x27;</span>, <span class="string">&#x27;atc&#x27;</span>, <span class="string">&#x27;atd&#x27;</span>, <span class="string">&#x27;ate&#x27;</span>, <span class="string">&#x27;atf&#x27;</span>, <span class="string">&#x27;atg&#x27;</span>, <span class="string">&#x27;ath&#x27;</span>, <span class="string">&#x27;ati&#x27;</span>, <span class="string">&#x27;atj&#x27;</span>, <span class="string">&#x27;atk&#x27;</span>, <span class="string">&#x27;atl&#x27;</span>, <span class="string">&#x27;atm&#x27;</span>, <span class="string">&#x27;atn&#x27;</span>, <span class="string">&#x27;ato&#x27;</span>, <span class="string">&#x27;atp&#x27;</span>, <span class="string">&#x27;atq&#x27;</span>, <span class="string">&#x27;atr&#x27;</span>, <span class="string">&#x27;ats&#x27;</span>, <span class="string">&#x27;att&#x27;</span>, <span class="string">&#x27;atu&#x27;</span>, <span class="string">&#x27;atv&#x27;</span>, <span class="string">&#x27;atw&#x27;</span>, <span class="string">&#x27;atx&#x27;</span>, <span class="string">&#x27;aty&#x27;</span>, <span class="string">&#x27;atz&#x27;</span>, <span class="string">&#x27;au&#x27;</span>, <span class="string">&#x27;aut&#x27;</span>, <span class="string">&#x27;av&#x27;</span>, <span class="string">&#x27;avt&#x27;</span>, <span class="string">&#x27;aw&#x27;</span>, <span class="string">&#x27;awt&#x27;</span>, <span class="string">&#x27;ax&#x27;</span>, <span class="string">&#x27;axt&#x27;</span>, <span class="string">&#x27;ay&#x27;</span>, <span class="string">&#x27;ayt&#x27;</span>, <span class="string">&#x27;az&#x27;</span>, <span class="string">&#x27;azt&#x27;</span>, <span class="string">&#x27;bat&#x27;</span>, <span class="string">&#x27;bt&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;ct&#x27;</span>, <span class="string">&#x27;dat&#x27;</span>, <span class="string">&#x27;dt&#x27;</span>, <span class="string">&#x27;eat&#x27;</span>, <span class="string">&#x27;et&#x27;</span>, <span class="string">&#x27;fat&#x27;</span>, <span class="string">&#x27;ft&#x27;</span>, <span class="string">&#x27;gat&#x27;</span>, <span class="string">&#x27;gt&#x27;</span>, <span class="string">&#x27;hat&#x27;</span>, <span class="string">&#x27;ht&#x27;</span>, <span class="string">&#x27;iat&#x27;</span>, <span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;jat&#x27;</span>, <span class="string">&#x27;jt&#x27;</span>, <span class="string">&#x27;kat&#x27;</span>, <span class="string">&#x27;kt&#x27;</span>, <span class="string">&#x27;lat&#x27;</span>, <span class="string">&#x27;lt&#x27;</span>, <span class="string">&#x27;mat&#x27;</span>, <span class="string">&#x27;mt&#x27;</span>, <span class="string">&#x27;nat&#x27;</span>, <span class="string">&#x27;nt&#x27;</span>, <span class="string">&#x27;oat&#x27;</span>, <span class="string">&#x27;ot&#x27;</span>, <span class="string">&#x27;pat&#x27;</span>, <span class="string">&#x27;pt&#x27;</span>, <span class="string">&#x27;qat&#x27;</span>, <span class="string">&#x27;qt&#x27;</span>, <span class="string">&#x27;rat&#x27;</span>, <span class="string">&#x27;rt&#x27;</span>, <span class="string">&#x27;sat&#x27;</span>, <span class="string">&#x27;st&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;ta&#x27;</span>, <span class="string">&#x27;tat&#x27;</span>, <span class="string">&#x27;tt&#x27;</span>, <span class="string">&#x27;uat&#x27;</span>, <span class="string">&#x27;ut&#x27;</span>, <span class="string">&#x27;vat&#x27;</span>, <span class="string">&#x27;vt&#x27;</span>, <span class="string">&#x27;wat&#x27;</span>, <span class="string">&#x27;wt&#x27;</span>, <span class="string">&#x27;xat&#x27;</span>, <span class="string">&#x27;xt&#x27;</span>, <span class="string">&#x27;yat&#x27;</span>, <span class="string">&#x27;yt&#x27;</span>, <span class="string">&#x27;zat&#x27;</span>, <span class="string">&#x27;zt&#x27;</span>]</span><br><span class="line"></span><br><span class="line">The type of the returned object should be a set &lt;class &#x27;set&#x27;&gt;</span><br><span class="line"><span class="function">Number of outputs from <span class="title">edit_one_letter</span><span class="params">(<span class="string">&#x27;at&#x27;</span>)</span> is 129</span></span><br></pre></td></tr></table></figure>
<p><a name='3-2'></a> ## Part 3.2 Edit two letters</p>
<p><a name='ex-9'></a> ### Exercise 9</p>
<p>Now you can generalize this to implement to get two edits on a word. To do so, you would have to get all the possible edits on a single word and then for each modified word, you would have to modify it again.</p>
<p><strong>Instructions</strong>: Implement the <code>edit_two_letters</code> function that returns a set of words that are two edits away. Note that creating additional edits based on the <code>edit_one_letter</code> function may 'restore' some one_edits to zero or one edits. That is allowed here. This accounted for in get_corrections.</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
<li>
You will likely want to take the union of two sets.
</li>
<li>
You can either use set.union() or use the '|' (or operator) to union two sets
</li>
<li>
See the documentation <a target="_blank" rel="noopener" href="https://docs.python.org/2/library/sets.html" > Python sets </a> for examples of using operators or functions of the Python set.
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: edit_two_letters</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edit_two_letters</span>(<span class="params">word, allow_switches = <span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        word: the input string/word </span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        edit_two_set: a set of strings with all possible two edits</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    edit_two_set = <span class="built_in">set</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    one_letter =  edit_one_letter(word,allow_switches)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> one_letter:</span><br><span class="line">        edit_two_set |= edit_one_letter(word,allow_switches)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> edit_two_set</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tmp_edit_two_set = edit_two_letters(<span class="string">&quot;a&quot;</span>)</span><br><span class="line">tmp_edit_two_l = <span class="built_in">sorted</span>(<span class="built_in">list</span>(tmp_edit_two_set))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of strings with edit distance of two: <span class="subst">&#123;<span class="built_in">len</span>(tmp_edit_two_l)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;First 10 strings <span class="subst">&#123;tmp_edit_two_l[:<span class="number">10</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Last 10 strings <span class="subst">&#123;tmp_edit_two_l[-<span class="number">10</span>:]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The data type of the returned object should be a set <span class="subst">&#123;<span class="built_in">type</span>(tmp_edit_two_set)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of strings that are 2 edit distances from &#x27;at&#x27; is <span class="subst">&#123;<span class="built_in">len</span>(edit_two_letters(<span class="string">&#x27;at&#x27;</span>))&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Number of strings with edit distance of two: 2654
First 10 strings [&#39;&#39;, &#39;a&#39;, &#39;aa&#39;, &#39;aaa&#39;, &#39;aab&#39;, &#39;aac&#39;, &#39;aad&#39;, &#39;aae&#39;, &#39;aaf&#39;, &#39;aag&#39;]
Last 10 strings [&#39;zv&#39;, &#39;zva&#39;, &#39;zw&#39;, &#39;zwa&#39;, &#39;zx&#39;, &#39;zxa&#39;, &#39;zy&#39;, &#39;zya&#39;, &#39;zz&#39;, &#39;zza&#39;]
The data type of the returned object should be a set &lt;class &#39;set&#39;&gt;
Number of strings that are 2 edit distances from &#39;at&#39; is 7154</code></pre>
<h4 id="expected-output-12">Expected Output</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Number of strings with edit distance of two: <span class="number">2654</span></span><br><span class="line">First <span class="number">10</span> strings [<span class="string">&#x27;&#x27;, &#x27;</span>a<span class="number">&#x27;</span>, <span class="string">&#x27;aa&#x27;</span>, <span class="string">&#x27;aaa&#x27;</span>, <span class="string">&#x27;aab&#x27;</span>, <span class="string">&#x27;aac&#x27;</span>, <span class="string">&#x27;aad&#x27;</span>, <span class="string">&#x27;aae&#x27;</span>, <span class="string">&#x27;aaf&#x27;</span>, <span class="string">&#x27;aag&#x27;</span>]</span><br><span class="line">Last <span class="number">10</span> strings [<span class="string">&#x27;zv&#x27;</span>, <span class="string">&#x27;zva&#x27;</span>, <span class="string">&#x27;zw&#x27;</span>, <span class="string">&#x27;zwa&#x27;</span>, <span class="string">&#x27;zx&#x27;</span>, <span class="string">&#x27;zxa&#x27;</span>, <span class="string">&#x27;zy&#x27;</span>, <span class="string">&#x27;zya&#x27;</span>, <span class="string">&#x27;zz&#x27;</span>, <span class="string">&#x27;zza&#x27;</span>]</span><br><span class="line">The data type of the returned object should be a set &lt;class &#x27;set&#x27;&gt;</span><br><span class="line">Number of strings that are <span class="number">2</span> edit distances from <span class="string">&#x27;at&#x27;</span> is <span class="number">7154</span></span><br></pre></td></tr></table></figure>
<p><a name='3-3'></a> ## Part 3-3: suggest spelling suggestions</p>
<p>Now you will use your <code>edit_two_letters</code> function to get a set of all the possible 2 edits on your word. You will then use those strings to get the most probable word you meant to type aka your typing suggestion.</p>
<p><a name='ex-10'></a> ### Exercise 10 <strong>Instructions</strong>: Implement <code>get_corrections</code>, which returns a list of zero to n possible suggestion tuples of the form (word, probability_of_word).</p>
<p><strong>Step 1:</strong> Generate suggestions for a supplied word: You'll use the edit functions you have developed. The 'suggestion algorithm' should follow this logic: * If the word is in the vocabulary, suggest the word. * Otherwise, if there are suggestions from <code>edit_one_letter</code> that are in the vocabulary, use those. * Otherwise, if there are suggestions from <code>edit_two_letters</code> that are in the vocabulary, use those. * Otherwise, suggest the input word.*<br />
* The idea is that words generated from fewer edits are more likely than words with more edits.</p>
<p>Note: - Edits of one or two letters may 'restore' strings to either zero or one edit. This algorithm accounts for this by preferentially selecting lower distance edits first.</p>
<h4 id="short-circuit">Short circuit</h4>
<p>In Python, logical operations such as <code>and</code> and <code>or</code> have two useful properties. They can operate on lists and they have <a target="_blank" rel="noopener" href="https://docs.python.org/3/library/stdtypes.html">'short-circuit' behavior</a>. Try these:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example of logical operation on lists or sets</span></span><br><span class="line"><span class="built_in">print</span>( [] <span class="keyword">and</span> [<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>] )</span><br><span class="line"><span class="built_in">print</span>( [] <span class="keyword">or</span> [<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>] )</span><br><span class="line"><span class="comment">#example of Short circuit behavior</span></span><br><span class="line">val1 =  [<span class="string">&quot;Most&quot;</span>,<span class="string">&quot;Likely&quot;</span>] <span class="keyword">or</span> [<span class="string">&quot;Less&quot;</span>,<span class="string">&quot;so&quot;</span>] <span class="keyword">or</span> [<span class="string">&quot;least&quot;</span>,<span class="string">&quot;of&quot;</span>,<span class="string">&quot;all&quot;</span>]  <span class="comment"># selects first, does not evalute remainder</span></span><br><span class="line"><span class="built_in">print</span>(val1)</span><br><span class="line">val2 =  [] <span class="keyword">or</span> [] <span class="keyword">or</span> [<span class="string">&quot;least&quot;</span>,<span class="string">&quot;of&quot;</span>,<span class="string">&quot;all&quot;</span>] <span class="comment"># continues evaluation until there is a non-empty list</span></span><br><span class="line"><span class="built_in">print</span>(val2)</span><br></pre></td></tr></table></figure>
<pre><code>[]
[&#39;a&#39;, &#39;b&#39;]
[&#39;Most&#39;, &#39;Likely&#39;]
[&#39;least&#39;, &#39;of&#39;, &#39;all&#39;]</code></pre>
<p>The logical <code>or</code> could be used to implement the suggestion algorithm very compactly. Alternately, if/then constructs could be used.</p>
<p><strong>Step 2</strong>: Create a 'best_words' dictionary where the 'key' is a suggestion and the 'value' is the probability of that word in your vocabulary. If the word is not in the vocabulary, assign it a probability of 0.</p>
<p><strong>Step 3</strong>: Select the n best suggestions. There may be fewer than n.</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
<li>
edit_one_letter and edit_two_letters return <em>python sets</em>.
</li>
<li>
Sets have a handy <a target="_blank" rel="noopener" href="https://docs.python.org/2/library/sets.html" > set.intersection </a> feature
</li>
<li>
To find the keys that have the highest values in a dictionary, you can use the Counter dictionary to create a Counter object from a regular dictionary. Then you can use Counter.most_common(n) to get the n most common keys.
</li>
<li>
To find the intersection of two sets, you can use set.intersection or the &amp; operator.
</li>
<li>
If you are not as familiar with short circuit syntax (as shown above), feel free to use if else statements instead.
</li>
<li>
To use an if statement to check of a set is empty, use 'if not x:' syntax
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># UNIT TEST COMMENT: Candidate for Table Driven Tests</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_corrections</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_corrections</span>(<span class="params">word, probs, vocab, n=<span class="number">2</span>, verbose = <span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        word: a user entered string to check for suggestions</span></span><br><span class="line"><span class="string">        probs: a dictionary that maps each word to its probability in the corpus</span></span><br><span class="line"><span class="string">        vocab: a set containing all the vocabulary</span></span><br><span class="line"><span class="string">        n: number of possible word corrections you want returned in the dictionary</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        n_best: a list of tuples with the most probable n corrected words and their probabilities.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    suggestions = []</span><br><span class="line">    n_best = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">        suggestions = [(word, probs[word])]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        suggestions = [(_, probs[_]) <span class="keyword">for</span> _ <span class="keyword">in</span> edit_one_letter(word) <span class="keyword">if</span> _ <span class="keyword">in</span> vocab] <span class="keyword">or</span>  \</span><br><span class="line">                    [(_, probs[_]) <span class="keyword">for</span> _ <span class="keyword">in</span> edit_two_letter(word) <span class="keyword">if</span> _ <span class="keyword">in</span> vocab] <span class="keyword">or</span> \</span><br><span class="line">                    [(word,<span class="number">0</span>)]</span><br><span class="line">    </span><br><span class="line">    n_best = <span class="built_in">sorted</span>(suggestions, key = <span class="keyword">lambda</span> x : x[-<span class="number">1</span>], reverse = <span class="literal">True</span>)[:n]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> verbose: <span class="built_in">print</span>(<span class="string">&quot;suggestions = &quot;</span>, suggestions)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> n_best</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test your implementation - feel free to try other words in my word</span></span><br><span class="line">my_word = <span class="string">&#x27;dys&#x27;</span> </span><br><span class="line">tmp_corrections = get_corrections(my_word, probs, vocab, <span class="number">2</span>, verbose=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> i, word_prob <span class="keyword">in</span> <span class="built_in">enumerate</span>(tmp_corrections):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;word <span class="subst">&#123;i&#125;</span>: <span class="subst">&#123;word_prob[<span class="number">0</span>]&#125;</span>, probability <span class="subst">&#123;word_prob[<span class="number">1</span>]:<span class="number">.6</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># CODE REVIEW COMMENT: using &quot;tmp_corrections&quot; insteads of &quot;cors&quot;. &quot;cors&quot; is not defined</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;data type of corrections <span class="subst">&#123;<span class="built_in">type</span>(tmp_corrections)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>suggestions =  [(&#39;dye&#39;, 1.865184466743761e-05), (&#39;days&#39;, 0.0004103405826836274)]
word 0: days, probability 0.000410
word 1: dye, probability 0.000019
data type of corrections &lt;class &#39;list&#39;&gt;</code></pre>
<h4 id="expected-output-13">Expected Output</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">word <span class="number">0</span>: days, probability <span class="number">0.000410</span></span><br><span class="line">word <span class="number">1</span>: dye, probability <span class="number">0.000019</span></span><br><span class="line">data type of corrections &lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">list</span>&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<p><a name='4'></a> # Part 4: Minimum Edit distance</p>
<p>Now that you have implemented your auto-correct, how do you evaluate the similarity between two strings? For example: 'waht' and 'what'</p>
<p>Also how do you efficiently find the shortest path to go from the word, 'waht' to the word 'what'?</p>
<p>You will implement a dynamic programming system that will tell you the minimum number of edits required to convert a string into another string.</p>
<p><a name='4-1'></a> ### Part 4.1 Dynamic Programming</p>
<p>Dynamic Programming breaks a problem down into subproblems which can be combined to form the final solution. Here, given a string source[0..i] and a string target[0..j], we will compute all the combinations of substrings[i, j] and calculate their edit distance. To do this efficiently, we will use a table to maintain the previously computed substrings and use those to calculate larger substrings.</p>
<p>You have to create a matrix and update each element in the matrix as follows:</p>
<p><span class="math display">\[\text{Initialization}\]</span></p>
<p><span class="math display">\[\begin{align}
D[0,0] &amp;= 0 \\
D[i,0] &amp;= D[i-1,0] + del\_cost(source[i]) \tag{4}\\
D[0,j] &amp;= D[0,j-1] + ins\_cost(target[j]) \\
\end{align}\]</span></p>
<p><span class="math display">\[\text{Per Cell Operations}\]</span> <span class="math display">\[\begin{align}
 \\
D[i,j] =min
\begin{cases}
D[i-1,j] + del\_cost\\
D[i,j-1] + ins\_cost\\
D[i-1,j-1] + \left\{\begin{matrix}
rep\_cost; &amp; if src[i]\neq tar[j]\\
0 ; &amp; if src[i]=tar[j]
\end{matrix}\right.
\end{cases}
\tag{5}
\end{align}\]</span></p>
So converting the source word <strong>play</strong> to the target word <strong>stay</strong>, using an input cost of one, a delete cost of 1, and replace cost of 2 would give you the following table:
<table style="width:20%">
<tr>
<td>
<b> </b>
</td>
<td>
<b># </b>
</td>
<td>
<b>s </b>
</td>
<td>
<b>t </b>
</td>
<td>
<b>a </b>
</td>
<td>
<b>y </b>
</td>
</tr>
<tr>
<td>
<b> # </b>
</td>
<td>
0
</td>
<td>
1
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
</tr>
<tr>
<td>
<b> p </b>
</td>
<td>
1
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
5
</td>
</tr>
<tr>
<td>
<b> l </b>
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
5
</td>
<td>
6
</td>
</tr>
<tr>
<td>
<b> a </b>
</td>
<td>
3
</td>
<td>
4
</td>
<td>
5
</td>
<td>
4
</td>
<td>
5
</td>
</tr>
<tr>
<td>
<b> y </b>
</td>
<td>
4
</td>
<td>
5
</td>
<td>
6
</td>
<td>
5
</td>
<td>
4
</td>
</tr>
</table>
<p>The operations used in this algorithm are 'insert', 'delete', and 'replace'. These correspond to the functions that you defined earlier: insert_letter(), delete_letter() and replace_letter(). switch_letter() is not used here.</p>
<p>The diagram below describes how to initialize the table. Each entry in D[i,j] represents the minimum cost of converting string source[0:i] to string target[0:j]. The first column is initialized to represent the cumulative cost of deleting the source characters to convert string "EER" to "". The first row is initialized to represent the cumulative cost of inserting the target characters to convert from "" to "NEAR".</p>
<div style="width:image width px; font-size:100%; text-align:center;">
<img src='EditDistInit4.png' alt="alternate text" width="width" height="height" style="width:1000px;height:400px;"/> Figure 6 Initializing Distance Matrix
</div>
<p>Filling in the remainder of the table utilizes the 'Per Cell Operations' in the equation (5) above. Note, the diagram below includes in the table some of the 3 sub-calculations shown in light grey. Only 'min' of those operations is stored in the table in the <code>min_edit_distance()</code> function.</p>
<div style="width:image width px; font-size:100%; text-align:center;">
<img src='EditDistFill2.png' alt="alternate text" width="width" height="height" style="width:800px;height:400px;"/> Figure 7 Filling Distance Matrix
</div>
<p>Note that the formula for <span class="math inline">\(D[i,j]\)</span> shown in the image is equivalent to:</p>
<p><span class="math display">\[\begin{align}
 \\
D[i,j] =min
\begin{cases}
D[i-1,j] + del\_cost\\
D[i,j-1] + ins\_cost\\
D[i-1,j-1] + \left\{\begin{matrix}
rep\_cost; &amp; if src[i]\neq tar[j]\\
0 ; &amp; if src[i]=tar[j]
\end{matrix}\right.
\end{cases}
\tag{5}
\end{align}\]</span></p>
<p>The variable <code>sub_cost</code> (for substitution cost) is the same as <code>rep_cost</code>; replacement cost. We will stick with the term "replace" whenever possible.</p>
<p>Below are some examples of cells where replacement is used. This also shows the minimum path from the lower right final position where "EER" has been replaced by "NEAR" back to the start. This provides a starting point for the optional 'backtrace' algorithm below.</p>
<div style="width:image width px; font-size:100%; text-align:center;">
<img src='EditDistExample1.png' alt="alternate text" width="width" height="height" style="width:1200px;height:400px;"/> Figure 8 Examples Distance Matrix
</div>
<p><a name='ex-11'></a> ### Exercise 11</p>
<p>Again, the word "substitution" appears in the figure, but think of this as "replacement".</p>
<p><strong>Instructions</strong>: Implement the function below to get the minimum amount of edits required given a source string and a target string.</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
<li>
The range(start, stop, step) function excludes 'stop' from its output
</li>
<li>
<a href="" > words </a>
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: min_edit_distance</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min_edit_distance</span>(<span class="params">source, target, ins_cost = <span class="number">1</span>, del_cost = <span class="number">1</span>, rep_cost = <span class="number">2</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        source: a string corresponding to the string you are starting with</span></span><br><span class="line"><span class="string">        target: a string corresponding to the string you want to end with</span></span><br><span class="line"><span class="string">        ins_cost: an integer setting the insert cost</span></span><br><span class="line"><span class="string">        del_cost: an integer setting the delete cost</span></span><br><span class="line"><span class="string">        rep_cost: an integer setting the replace cost</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances</span></span><br><span class="line"><span class="string">        med: the minimum edit distance (med) required to convert the source string to the target</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># use deletion and insert cost as  1</span></span><br><span class="line">    m = <span class="built_in">len</span>(source) </span><br><span class="line">    n = <span class="built_in">len</span>(target) </span><br><span class="line">    <span class="comment">#initialize cost matrix with zeros and dimensions (m+1,n+1) </span></span><br><span class="line">    D = np.zeros((m+<span class="number">1</span>, n+<span class="number">1</span>), dtype=<span class="built_in">int</span>) </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fill in column 0, from row 1 to row m, both inclusive</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,m+<span class="number">1</span>): <span class="comment"># Replace None with the proper range</span></span><br><span class="line">        D[row,<span class="number">0</span>] = row</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Fill in row 0, for all columns from 1 to n, both inclusive</span></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,n+<span class="number">1</span>): <span class="comment"># Replace None with the proper range</span></span><br><span class="line">        D[<span class="number">0</span>,col] = col</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Loop through row 1 to row m, both inclusive</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,m+<span class="number">1</span>): </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loop through column 1 to column n, both inclusive</span></span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,n+<span class="number">1</span>):</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Intialize r_cost to the &#x27;replace&#x27; cost that is passed into this function</span></span><br><span class="line">            r_cost = rep_cost</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Check to see if source character at the previous row</span></span><br><span class="line">            <span class="comment"># matches the target character at the previous column, </span></span><br><span class="line">            <span class="keyword">if</span> source[row-<span class="number">1</span>] == target[col-<span class="number">1</span>]:</span><br><span class="line">                <span class="comment"># Update the replacement cost to 0 if source and target are the same</span></span><br><span class="line">                r_cost = <span class="number">0</span></span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Update the cost at row, col based on previous entries in the cost matrix</span></span><br><span class="line">            <span class="comment"># Refer to the equation calculate for D[i,j] (the minimum of three calculated costs)</span></span><br><span class="line">            D[row,col] = <span class="built_in">min</span>([ D[row-<span class="number">1</span>,col] + del_cost, D[row, col-<span class="number">1</span>] + ins_cost , D[row-<span class="number">1</span>, col-<span class="number">1</span>] + r_cost])</span><br><span class="line">          </span><br><span class="line">    <span class="comment"># Set the minimum edit distance with the cost found at row m, column n</span></span><br><span class="line">    med = D[m,n]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> D, med</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DO NOT MODIFY THIS CELL</span></span><br><span class="line"><span class="comment"># testing your implementation </span></span><br><span class="line">source =  <span class="string">&#x27;play&#x27;</span></span><br><span class="line">target = <span class="string">&#x27;stay&#x27;</span></span><br><span class="line">matrix, min_edits = min_edit_distance(source, target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;minimum edits: &quot;</span>,min_edits, <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">idx = <span class="built_in">list</span>(<span class="string">&#x27;#&#x27;</span> + source)</span><br><span class="line">cols = <span class="built_in">list</span>(<span class="string">&#x27;#&#x27;</span> + target)</span><br><span class="line">df = pd.DataFrame(matrix, index=idx, columns= cols)</span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure>
<pre><code>minimum edits:  4 

   #  s  t  a  y
#  0  1  2  3  4
p  1  2  3  4  5
l  2  3  4  5  6
a  3  4  5  4  5
y  4  5  6  5  4</code></pre>
<p><strong>Expected Results:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   <span class="meta">#  s  t  a  y</span></span><br><span class="line">#  <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span></span><br><span class="line">p  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span></span><br><span class="line">l  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span></span><br><span class="line">a  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">4</span>  <span class="number">5</span></span><br><span class="line">y  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">5</span>  <span class="number">4</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DO NOT MODIFY THIS CELL</span></span><br><span class="line"><span class="comment"># testing your implementation </span></span><br><span class="line">source =  <span class="string">&#x27;eer&#x27;</span></span><br><span class="line">target = <span class="string">&#x27;near&#x27;</span></span><br><span class="line">matrix, min_edits = min_edit_distance(source, target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;minimum edits: &quot;</span>,min_edits, <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">idx = <span class="built_in">list</span>(source)</span><br><span class="line">idx.insert(<span class="number">0</span>, <span class="string">&#x27;#&#x27;</span>)</span><br><span class="line">cols = <span class="built_in">list</span>(target)</span><br><span class="line">cols.insert(<span class="number">0</span>, <span class="string">&#x27;#&#x27;</span>)</span><br><span class="line">df = pd.DataFrame(matrix, index=idx, columns= cols)</span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure>
<pre><code>minimum edits:  3 

   #  n  e  a  r
#  0  1  2  3  4
e  1  2  1  2  3
e  2  3  2  3  4
r  3  4  3  4  3</code></pre>
<p><strong>Expected Results</strong><br />
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">minimum edits:  <span class="number">3</span> </span><br><span class="line"></span><br><span class="line">   <span class="meta">#  n  e  a  r</span></span><br><span class="line">#  <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span></span><br><span class="line">e  <span class="number">1</span>  <span class="number">2</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span></span><br><span class="line">e  <span class="number">2</span>  <span class="number">3</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span></span><br><span class="line">r  <span class="number">3</span>  <span class="number">4</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">3</span></span><br></pre></td></tr></table></figure></p>
<p>We can now test several of our routines at once:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">source = <span class="string">&quot;eer&quot;</span></span><br><span class="line">targets = edit_one_letter(source,allow_switches = <span class="literal">False</span>)  <span class="comment">#disable switches since min_edit_distance does not include them</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> targets:</span><br><span class="line">    _, min_edits = min_edit_distance(source, t,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)  <span class="comment"># set ins, del, sub costs all to one</span></span><br><span class="line">    <span class="keyword">if</span> min_edits != <span class="number">1</span>: <span class="built_in">print</span>(source, t, min_edits)</span><br></pre></td></tr></table></figure>
<p><strong>Expected Results:</strong> (empty)</p>
<p>The 'replace()' routine utilizes all letters a-z one of which returns the original word.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">source = <span class="string">&quot;eer&quot;</span></span><br><span class="line">targets = edit_two_letters(source,allow_switches = <span class="literal">False</span>) <span class="comment">#disable switches since min_edit_distance does not include them</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> targets:</span><br><span class="line">    _, min_edits = min_edit_distance(source, t,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)  <span class="comment"># set ins, del, sub costs all to one</span></span><br><span class="line">    <span class="keyword">if</span> min_edits != <span class="number">2</span> <span class="keyword">and</span> min_edits != <span class="number">1</span>: <span class="built_in">print</span>(source, t, min_edits)</span><br></pre></td></tr></table></figure>
<pre><code>eer eer 0</code></pre>
<p><strong>Expected Results:</strong> eer eer 0<br />
We have to allow single edits here because some two_edits will restore a single edit.</p>
<h1 id="submission">Submission</h1>
<p>Make sure you submit your assignment before you modify anything below</p>
<p><a name='5'></a></p>
<h1 id="part-5-optional---backtrace">Part 5: Optional - Backtrace</h1>
<p>Once you have computed your matrix using minimum edit distance, how would find the shortest path from the top left corner to the bottom right corner?</p>
<p>Note that you could use backtrace algorithm. Try to find the shortest path given the matrix that your <code>min_edit_distance</code> function returned.</p>
<p>You can use these <a target="_blank" rel="noopener" href="https://web.stanford.edu/class/cs124/lec/med.pdf">lecture slides on minimum edit distance</a> by Dan Jurafsky to learn about the algorithm for backtrace.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Experiment with back trace - insert your code here</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="references">References</h4>
<ul>
<li>Dan Jurafsky - Speech and Language Processing - Textbook</li>
<li>This auto-correct explanation was first done by Peter Norvig in 2007</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Parts-of-Speech-Tagging/2020/07/19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Parts-of-Speech-Tagging/2020/07/19/" class="post-title-link" itemprop="url">Parts-of-Speech Tagging</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-07-19 01:39:21 / Modified: 01:50:42" itemprop="dateCreated datePublished" datetime="2020-07-19T01:39:21+08:00">2020-07-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Parts-of-Speech-Tagging/2020/07/19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Parts-of-Speech-Tagging/2020/07/19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="assignment-2-parts-of-speech-tagging-pos">Assignment 2: Parts-of-Speech Tagging (POS)</h1>
<p>Welcome to the second assignment of Course 2 in the Natural Language Processing specialization. This assignment will develop skills in part-of-speech (POS) tagging, the process of assigning a part-of-speech tag (Noun, Verb, Adjective...) to each word in an input text. Tagging is difficult because some words can represent more than one part of speech at different times. They are <strong>Ambiguous</strong>. Let's look at the following example:</p>
<ul>
<li>The whole team played <strong>well</strong>. [adverb]</li>
<li>You are doing <strong>well</strong> for yourself. [adjective]</li>
<li><strong>Well</strong>, this assignment took me forever to complete. [interjection]</li>
<li>The <strong>well</strong> is dry. [noun]</li>
<li>Tears were beginning to <strong>well</strong> in her eyes. [verb]</li>
</ul>
<p>Distinguishing the parts-of-speech of a word in a sentence will help you better understand the meaning of a sentence. This would be critically important in search queries. Identifying the proper noun, the organization, the stock symbol, or anything similar would greatly improve everything ranging from speech recognition to search. By completing this assignment, you will:</p>
<ul>
<li>Learn how parts-of-speech tagging works</li>
<li>Compute the transition matrix A in a Hidden Markov Model</li>
<li>Compute the transition matrix B in a Hidden Markov Model</li>
<li>Compute the Viterbi algorithm</li>
<li>Compute the accuracy of your own model</li>
</ul>
<h2 id="outline">Outline</h2>
<ul>
<li><a href="#0">0 Data Sources</a></li>
<li><a href="#1">1 POS Tagging</a>
<ul>
<li><a href="#1.1">1.1 Training</a>
<ul>
<li><a href="#ex-01">Exercise 01</a></li>
</ul></li>
<li><a href="#1.2">1.2 Testing</a>
<ul>
<li><a href="#ex-02">Exercise 02</a></li>
</ul></li>
</ul></li>
<li><a href="#2">2 Hidden Markov Models</a>
<ul>
<li><a href="#2.1">2.1 Generating Matrices</a>
<ul>
<li><a href="#ex-03">Exercise 03</a></li>
<li><a href="#ex-04">Exercise 04</a></li>
</ul></li>
</ul></li>
<li><a href="#3">3 Viterbi Algorithm</a>
<ul>
<li><a href="#3.1">3.1 Initialization</a>
<ul>
<li><a href="#ex-05">Exercise 05</a></li>
</ul></li>
<li><a href="#3.2">3.2 Viterbi Forward</a>
<ul>
<li><a href="#ex-06">Exercise 06</a></li>
</ul></li>
<li><a href="#3.3">3.3 Viterbi Backward</a>
<ul>
<li><a href="#ex-07">Exercise 07</a></li>
</ul></li>
</ul></li>
<li><a href="#4">4 Predicting on a data set</a>
<ul>
<li><a href="#ex-08">Exercise 08</a></li>
</ul></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Importing packages and loading in the data set </span></span><br><span class="line"><span class="keyword">from</span> utils_pos <span class="keyword">import</span> get_word_tag, preprocess  </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<p><a name='0'></a> ## Part 0: Data Sources This assignment will use two tagged data sets collected from the <strong>Wall Street Journal (WSJ)</strong>.</p>
<p><a target="_blank" rel="noopener" href="http://relearn.be/2015/training-common-sense/sources/software/pattern-2.6-critical-fork/docs/html/mbsp-tags.html">Here</a> is an example 'tag-set' or Part of Speech designation describing the two or three letter tag and their meaning. - One data set (<strong>WSJ-2_21.pos</strong>) will be used for <strong>training</strong>. - The other (<strong>WSJ-24.pos</strong>) for <strong>testing</strong>. - The tagged training data has been preprocessed to form a vocabulary (<strong>hmm_vocab.txt</strong>). - The words in the vocabulary are words from the training set that were used two or more times. - The vocabulary is augmented with a set of 'unknown word tokens', described below.</p>
<p>The training set will be used to create the emission, transmission and tag counts.</p>
<p>The test set (WSJ-24.pos) is read in to create <code>y</code>. - This contains both the test text and the true tag. - The test set has also been preprocessed to remove the tags to form <strong>test_words.txt</strong>. - This is read in and further processed to identify the end of sentences and handle words not in the vocabulary using functions provided in <strong>utils_pos.py</strong>. - This forms the list <code>prep</code>, the preprocessed text used to test our POS taggers.</p>
<p>A POS tagger will necessarily encounter words that are not in its datasets. - To improve accuracy, these words are further analyzed during preprocessing to extract available hints as to their appropriate tag. - For example, the suffix 'ize' is a hint that the word is a verb, as in 'final-ize' or 'character-ize'. - A set of unknown-tokens, such as '--unk-verb--' or '--unk-noun--' will replace the unknown words in both the training and test corpus and will appear in the emission, transmission and tag data structures.</p>
<p><img src = "DataSources1.png" /></p>
<p>Implementation note:</p>
<ul>
<li>For python 3.6 and beyond, dictionaries retain the insertion order.</li>
<li>Furthermore, their hash-based lookup makes them suitable for rapid membership tests.
<ul>
<li>If <em>di</em> is a dictionary, <code>key in di</code> will return <code>True</code> if <em>di</em> has a key <em>key</em>, else <code>False</code>.</li>
</ul></li>
</ul>
<p>The dictionary <code>vocab</code> will utilize these features.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load in the training corpus</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;WSJ_02-21.pos&quot;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    training_corpus = f.readlines()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;A few items of the training corpus list&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(training_corpus[<span class="number">0</span>:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<pre><code>A few items of the training corpus list
[&#39;In\tIN\n&#39;, &#39;an\tDT\n&#39;, &#39;Oct.\tNNP\n&#39;, &#39;19\tCD\n&#39;, &#39;review\tNN\n&#39;]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># read the vocabulary data, split by each line of text, and save the list</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;hmm_vocab.txt&quot;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    voc_l = f.read().split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A few items of the vocabulary list&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(voc_l[<span class="number">0</span>:<span class="number">50</span>])</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A few items at the end of the vocabulary list&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(voc_l[-<span class="number">50</span>:])</span><br></pre></td></tr></table></figure>
<pre><code>A few items of the vocabulary list
[&#39;!&#39;, &#39;#&#39;, &#39;$&#39;, &#39;%&#39;, &#39;&amp;&#39;, &quot;&#39;&quot;, &quot;&#39;&#39;&quot;, &quot;&#39;40s&quot;, &quot;&#39;60s&quot;, &quot;&#39;70s&quot;, &quot;&#39;80s&quot;, &quot;&#39;86&quot;, &quot;&#39;90s&quot;, &quot;&#39;N&quot;, &quot;&#39;S&quot;, &quot;&#39;d&quot;, &quot;&#39;em&quot;, &quot;&#39;ll&quot;, &quot;&#39;m&quot;, &quot;&#39;n&#39;&quot;, &quot;&#39;re&quot;, &quot;&#39;s&quot;, &quot;&#39;til&quot;, &quot;&#39;ve&quot;, &#39;(&#39;, &#39;)&#39;, &#39;,&#39;, &#39;-&#39;, &#39;--&#39;, &#39;--n--&#39;, &#39;--unk--&#39;, &#39;--unk_adj--&#39;, &#39;--unk_adv--&#39;, &#39;--unk_digit--&#39;, &#39;--unk_noun--&#39;, &#39;--unk_punct--&#39;, &#39;--unk_upper--&#39;, &#39;--unk_verb--&#39;, &#39;.&#39;, &#39;...&#39;, &#39;0.01&#39;, &#39;0.0108&#39;, &#39;0.02&#39;, &#39;0.03&#39;, &#39;0.05&#39;, &#39;0.1&#39;, &#39;0.10&#39;, &#39;0.12&#39;, &#39;0.13&#39;, &#39;0.15&#39;]

A few items at the end of the vocabulary list
[&#39;yards&#39;, &#39;yardstick&#39;, &#39;year&#39;, &#39;year-ago&#39;, &#39;year-before&#39;, &#39;year-earlier&#39;, &#39;year-end&#39;, &#39;year-on-year&#39;, &#39;year-round&#39;, &#39;year-to-date&#39;, &#39;year-to-year&#39;, &#39;yearlong&#39;, &#39;yearly&#39;, &#39;years&#39;, &#39;yeast&#39;, &#39;yelled&#39;, &#39;yelling&#39;, &#39;yellow&#39;, &#39;yen&#39;, &#39;yes&#39;, &#39;yesterday&#39;, &#39;yet&#39;, &#39;yield&#39;, &#39;yielded&#39;, &#39;yielding&#39;, &#39;yields&#39;, &#39;you&#39;, &#39;young&#39;, &#39;younger&#39;, &#39;youngest&#39;, &#39;youngsters&#39;, &#39;your&#39;, &#39;yourself&#39;, &#39;youth&#39;, &#39;youthful&#39;, &#39;yuppie&#39;, &#39;yuppies&#39;, &#39;zero&#39;, &#39;zero-coupon&#39;, &#39;zeroing&#39;, &#39;zeros&#39;, &#39;zinc&#39;, &#39;zip&#39;, &#39;zombie&#39;, &#39;zone&#39;, &#39;zones&#39;, &#39;zoning&#39;, &#39;&#123;&#39;, &#39;&#125;&#39;, &#39;&#39;]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vocab: dictionary that has the index of the corresponding words</span></span><br><span class="line">vocab = &#123;&#125; </span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the index of the corresponding words. </span></span><br><span class="line"><span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">sorted</span>(voc_l)): </span><br><span class="line">    vocab[word] = i       </span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Vocabulary dictionary, key is the word, value is a unique integer&quot;</span>)</span><br><span class="line">cnt = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> vocab.items():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;k&#125;</span>:<span class="subst">&#123;v&#125;</span>&quot;</span>)</span><br><span class="line">    cnt += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> cnt &gt; <span class="number">20</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<pre><code>Vocabulary dictionary, key is the word, value is a unique integer
:0
!:1
#:2
$:3
%:4
&amp;:5
&#39;:6
&#39;&#39;:7
&#39;40s:8
&#39;60s:9
&#39;70s:10
&#39;80s:11
&#39;86:12
&#39;90s:13
&#39;N:14
&#39;S:15
&#39;d:16
&#39;em:17
&#39;ll:18
&#39;m:19
&#39;n&#39;:20</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load in the test corpus</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;WSJ_24.pos&quot;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    y = f.readlines()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A sample of the test corpus&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(y[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<pre><code>A sample of the test corpus
[&#39;The\tDT\n&#39;, &#39;economy\tNN\n&#39;, &quot;&#39;s\tPOS\n&quot;, &#39;temperature\tNN\n&#39;, &#39;will\tMD\n&#39;, &#39;be\tVB\n&#39;, &#39;taken\tVBN\n&#39;, &#39;from\tIN\n&#39;, &#39;several\tJJ\n&#39;, &#39;vantage\tNN\n&#39;]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#corpus without tags, preprocessed</span></span><br><span class="line">_, prep = preprocess(vocab, <span class="string">&quot;test.words&quot;</span>)     </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The length of the preprocessed test corpus: &#x27;</span>, <span class="built_in">len</span>(prep))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;This is a sample of the test_corpus: &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(prep[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<pre><code>The length of the preprocessed test corpus:  34199
This is a sample of the test_corpus: 
[&#39;The&#39;, &#39;economy&#39;, &quot;&#39;s&quot;, &#39;temperature&#39;, &#39;will&#39;, &#39;be&#39;, &#39;taken&#39;, &#39;from&#39;, &#39;several&#39;, &#39;--unk--&#39;]</code></pre>
<p><a name='1'></a> # Part 1: Parts-of-speech tagging</p>
<p><a name='1.1'></a> ## Part 1.1 - Training You will start with the simplest possible parts-of-speech tagger and we will build up to the state of the art.</p>
<p>In this section, you will find the words that are not ambiguous. - For example, the word <code>is</code> is a verb and it is not ambiguous. - In the <code>WSJ</code> corpus, <span class="math inline">\(86\)</span>% of the token are unambiguous (meaning they have only one tag) - About <span class="math inline">\(14\%\)</span> are ambiguous (meaning that they have more than one tag)</p>
<p><img src = "pos.png" style="width:400px;height:250px;"/></p>
<p>Before you start predicting the tags of each word, you will need to compute a few dictionaries that will help you to generate the tables.</p>
<h4 id="transition-counts">Transition counts</h4>
<ul>
<li>The first dictionary is the <code>transition_counts</code> dictionary which computes the number of times each tag happened next to another tag.</li>
</ul>
<p>This dictionary will be used to compute: <span class="math display">\[P(t_i |t_{i-1}) \tag{1}\]</span></p>
<p>This is the probability of a tag at position <span class="math inline">\(i\)</span> given the tag at position <span class="math inline">\(i-1\)</span>.</p>
<p>In order for you to compute equation 1, you will create a <code>transition_counts</code> dictionary where - The keys are <code>(prev_tag, tag)</code> - The values are the number of times those two tags appeared in that order.</p>
<h4 id="emission-counts">Emission counts</h4>
<p>The second dictionary you will compute is the <code>emission_counts</code> dictionary. This dictionary will be used to compute:</p>
<p><span class="math display">\[P(w_i|t_i)\tag{2}\]</span></p>
<p>In other words, you will use it to compute the probability of a word given its tag.</p>
<p>In order for you to compute equation 2, you will create an <code>emission_counts</code> dictionary where - The keys are <code>(tag, word)</code> - The values are the number of times that pair showed up in your training set.</p>
<h4 id="tag-counts">Tag counts</h4>
<p>The last dictionary you will compute is the <code>tag_counts</code> dictionary. - The key is the tag - The value is the number of times each tag appeared.</p>
<p><a name='ex-01'></a> ### Exercise 01</p>
<p><strong>Instructions:</strong> Write a program that takes in the <code>training_corpus</code> and returns the three dictionaries mentioned above <code>transition_counts</code>, <code>emission_counts</code>, and <code>tag_counts</code>. - <code>emission_counts</code>: maps (tag, word) to the number of times it happened. - <code>transition_counts</code>: maps (prev_tag, tag) to the number of times it has appeared. - <code>tag_counts</code>: maps (tag) to the number of times it has occured.</p>
<p>Implementation note: This routine utilises <em>defaultdict</em>, which is a subclass of <em>dict</em>. - A standard Python dictionary throws a <em>KeyError</em> if you try to access an item with a key that is not currently in the dictionary. - In contrast, the <em>defaultdict</em> will create an item of the type of the argument, in this case an integer with the default value of 0. - See <a target="_blank" rel="noopener" href="https://docs.python.org/3.3/library/collections.html#defaultdict-objects">defaultdict</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: create_dictionaries</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dictionaries</span>(<span class="params">training_corpus, vocab</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        training_corpus: a corpus where each line has a word followed by its tag.</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts</span></span><br><span class="line"><span class="string">        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts</span></span><br><span class="line"><span class="string">        tag_counts: a dictionary where the keys are the tags and the values are the counts</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the dictionaries using defaultdict</span></span><br><span class="line">    emission_counts = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    transition_counts = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    tag_counts = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize &quot;prev_tag&quot; (previous tag) with the start state, denoted by &#x27;--s--&#x27;</span></span><br><span class="line">    prev_tag = <span class="string">&#x27;--s--&#x27;</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># use &#x27;i&#x27; to track the line number in the corpus</span></span><br><span class="line">    i = <span class="number">0</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Each item in the training corpus contains a word and its POS tag</span></span><br><span class="line">    <span class="comment"># Go through each word and its tag in the training corpus</span></span><br><span class="line">    <span class="keyword">for</span> word_tag <span class="keyword">in</span> training_corpus:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Increment the word_tag count</span></span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Every 50,000 words, print the word count</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;word count = <span class="subst">&#123;i&#125;</span>&quot;</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">        <span class="comment"># get the word and tag using the get_word_tag helper function (imported from utils_pos.py)</span></span><br><span class="line">        word, tag = get_word_tag(word_tag,vocab)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Increment the transition count for the previous word and tag</span></span><br><span class="line">        transition_counts[(prev_tag, tag)] += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Increment the emission count for the tag and word</span></span><br><span class="line">        emission_counts[(tag, word)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Increment the tag count</span></span><br><span class="line">        tag_counts[tag] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set the previous tag to this tag (for the next iteration of the loop)</span></span><br><span class="line">        prev_tag = tag</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> emission_counts, transition_counts, tag_counts</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)</span><br></pre></td></tr></table></figure>
<pre><code>word count = 50000
word count = 100000
word count = 150000
word count = 200000
word count = 250000
word count = 300000
word count = 350000
word count = 400000
word count = 450000
word count = 500000
word count = 550000
word count = 600000
word count = 650000
word count = 700000
word count = 750000
word count = 800000
word count = 850000
word count = 900000
word count = 950000</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get all the POS states</span></span><br><span class="line">states = <span class="built_in">sorted</span>(tag_counts.keys())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of POS tags (number of &#x27;states&#x27;): <span class="subst">&#123;<span class="built_in">len</span>(states)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;View these POS tags (states)&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(states)</span><br></pre></td></tr></table></figure>
<pre><code>Number of POS tags (number of &#39;states&#39;): 46
View these POS tags (states)
[&#39;#&#39;, &#39;$&#39;, &quot;&#39;&#39;&quot;, &#39;(&#39;, &#39;)&#39;, &#39;,&#39;, &#39;--s--&#39;, &#39;.&#39;, &#39;:&#39;, &#39;CC&#39;, &#39;CD&#39;, &#39;DT&#39;, &#39;EX&#39;, &#39;FW&#39;, &#39;IN&#39;, &#39;JJ&#39;, &#39;JJR&#39;, &#39;JJS&#39;, &#39;LS&#39;, &#39;MD&#39;, &#39;NN&#39;, &#39;NNP&#39;, &#39;NNPS&#39;, &#39;NNS&#39;, &#39;PDT&#39;, &#39;POS&#39;, &#39;PRP&#39;, &#39;PRP$&#39;, &#39;RB&#39;, &#39;RBR&#39;, &#39;RBS&#39;, &#39;RP&#39;, &#39;SYM&#39;, &#39;TO&#39;, &#39;UH&#39;, &#39;VB&#39;, &#39;VBD&#39;, &#39;VBG&#39;, &#39;VBN&#39;, &#39;VBP&#39;, &#39;VBZ&#39;, &#39;WDT&#39;, &#39;WP&#39;, &#39;WP$&#39;, &#39;WRB&#39;, &#39;``&#39;]</code></pre>
<h5 id="expected-output">Expected Output</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Number of POS <span class="title">tags</span> <span class="params">(number of <span class="string">&#x27;states&#x27;</span><span class="number">46</span></span></span></span><br><span class="line"><span class="params"><span class="function">View these states</span></span></span><br><span class="line"><span class="params"><span class="function">[<span class="string">&#x27;#&#x27;</span>, <span class="string">&#x27;$&#x27;</span>, <span class="string">&quot;&#x27;&#x27;&quot;</span>, <span class="string">&#x27;(&#x27;</span>, <span class="string">&#x27;)&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;--s--&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;:&#x27;</span>, <span class="string">&#x27;CC&#x27;</span>, <span class="string">&#x27;CD&#x27;</span>, <span class="string">&#x27;DT&#x27;</span>, <span class="string">&#x27;EX&#x27;</span>, <span class="string">&#x27;FW&#x27;</span>, <span class="string">&#x27;IN&#x27;</span>, <span class="string">&#x27;JJ&#x27;</span>, <span class="string">&#x27;JJR&#x27;</span>, <span class="string">&#x27;JJS&#x27;</span>, <span class="string">&#x27;LS&#x27;</span>, <span class="string">&#x27;MD&#x27;</span>, <span class="string">&#x27;NN&#x27;</span>, <span class="string">&#x27;NNP&#x27;</span>, <span class="string">&#x27;NNPS&#x27;</span>, <span class="string">&#x27;NNS&#x27;</span>, <span class="string">&#x27;PDT&#x27;</span>, <span class="string">&#x27;POS&#x27;</span>, <span class="string">&#x27;PRP&#x27;</span>, <span class="string">&#x27;PRP$&#x27;</span>, <span class="string">&#x27;RB&#x27;</span>, <span class="string">&#x27;RBR&#x27;</span>, <span class="string">&#x27;RBS&#x27;</span>, <span class="string">&#x27;RP&#x27;</span>, <span class="string">&#x27;SYM&#x27;</span>, <span class="string">&#x27;TO&#x27;</span>, <span class="string">&#x27;UH&#x27;</span>, <span class="string">&#x27;VB&#x27;</span>, <span class="string">&#x27;VBD&#x27;</span>, <span class="string">&#x27;VBG&#x27;</span>, <span class="string">&#x27;VBN&#x27;</span>, <span class="string">&#x27;VBP&#x27;</span>, <span class="string">&#x27;VBZ&#x27;</span>, <span class="string">&#x27;WDT&#x27;</span>, <span class="string">&#x27;WP&#x27;</span>, <span class="string">&#x27;WP$&#x27;</span>, <span class="string">&#x27;WRB&#x27;</span>, <span class="string">&#x27;``&#x27;</span>]</span></span></span><br></pre></td></tr></table></figure>
<p>The 'states' are the Parts-of-speech designations found in the training data. They will also be referred to as 'tags' or POS in this assignment.</p>
<ul>
<li>"NN" is noun, singular,</li>
<li>'NNS' is noun, plural.</li>
<li>In addition, there are helpful tags like '--s--' which indicate a start of a sentence.</li>
<li>You can get a more complete description at <a target="_blank" rel="noopener" href="https://www.clips.uantwerpen.be/pages/mbsp-tags">Penn Treebank II tag set</a>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;transition examples: &quot;</span>)</span><br><span class="line"><span class="keyword">for</span> ex <span class="keyword">in</span> <span class="built_in">list</span>(transition_counts.items())[:<span class="number">3</span>]:</span><br><span class="line">    <span class="built_in">print</span>(ex)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;emission examples: &quot;</span>)</span><br><span class="line"><span class="keyword">for</span> ex <span class="keyword">in</span> <span class="built_in">list</span>(emission_counts.items())[<span class="number">200</span>:<span class="number">203</span>]:</span><br><span class="line">    <span class="built_in">print</span> (ex)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;ambiguous word example: &quot;</span>)</span><br><span class="line"><span class="keyword">for</span> tup,cnt <span class="keyword">in</span> emission_counts.items():</span><br><span class="line">    <span class="keyword">if</span> tup[<span class="number">1</span>] == <span class="string">&#x27;back&#x27;</span>: <span class="built_in">print</span> (tup, cnt) </span><br></pre></td></tr></table></figure>
<pre><code>transition examples: 
((&#39;--s--&#39;, &#39;IN&#39;), 5050)
((&#39;IN&#39;, &#39;DT&#39;), 32364)
((&#39;DT&#39;, &#39;NNP&#39;), 9044)

emission examples: 
((&#39;DT&#39;, &#39;any&#39;), 721)
((&#39;NN&#39;, &#39;decrease&#39;), 7)
((&#39;NN&#39;, &#39;insider-trading&#39;), 5)

ambiguous word example: 
(&#39;RB&#39;, &#39;back&#39;) 304
(&#39;VB&#39;, &#39;back&#39;) 20
(&#39;RP&#39;, &#39;back&#39;) 84
(&#39;JJ&#39;, &#39;back&#39;) 25
(&#39;NN&#39;, &#39;back&#39;) 29
(&#39;VBP&#39;, &#39;back&#39;) 4</code></pre>
<h5 id="expected-output-1">Expected Output</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">transition examples: </span><br><span class="line">((<span class="string">&#x27;--s--&#x27;</span>, <span class="string">&#x27;IN&#x27;</span>), <span class="number">5050</span>)</span><br><span class="line">((<span class="string">&#x27;IN&#x27;</span>, <span class="string">&#x27;DT&#x27;</span>), <span class="number">32364</span>)</span><br><span class="line">((<span class="string">&#x27;DT&#x27;</span>, <span class="string">&#x27;NNP&#x27;</span>), <span class="number">9044</span>)</span><br><span class="line"></span><br><span class="line">emission examples: </span><br><span class="line">((<span class="string">&#x27;DT&#x27;</span>, <span class="string">&#x27;any&#x27;</span>), <span class="number">721</span>)</span><br><span class="line">((<span class="string">&#x27;NN&#x27;</span>, <span class="string">&#x27;decrease&#x27;</span>), <span class="number">7</span>)</span><br><span class="line">((<span class="string">&#x27;NN&#x27;</span>, <span class="string">&#x27;insider-trading&#x27;</span>), <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">ambiguous word example: </span><br><span class="line">(<span class="string">&#x27;RB&#x27;</span>, <span class="string">&#x27;back&#x27;</span>) <span class="number">304</span></span><br><span class="line">(<span class="string">&#x27;VB&#x27;</span>, <span class="string">&#x27;back&#x27;</span>) <span class="number">20</span></span><br><span class="line">(<span class="string">&#x27;RP&#x27;</span>, <span class="string">&#x27;back&#x27;</span>) <span class="number">84</span></span><br><span class="line">(<span class="string">&#x27;JJ&#x27;</span>, <span class="string">&#x27;back&#x27;</span>) <span class="number">25</span></span><br><span class="line">(<span class="string">&#x27;NN&#x27;</span>, <span class="string">&#x27;back&#x27;</span>) <span class="number">29</span></span><br><span class="line">(<span class="string">&#x27;VBP&#x27;</span>, <span class="string">&#x27;back&#x27;</span>) <span class="number">4</span></span><br></pre></td></tr></table></figure>
<p><a name='1.2'></a> ### Part 1.2 - Testing</p>
<p>Now you will test the accuracy of your parts-of-speech tagger using your <code>emission_counts</code> dictionary. - Given your preprocessed test corpus <code>prep</code>, you will assign a parts-of-speech tag to every word in that corpus. - Using the original tagged test corpus <code>y</code>, you will then compute what percent of the tags you got correct.</p>
<p><a name='ex-02'></a> ### Exercise 02</p>
<p><strong>Instructions:</strong> Implement <code>predict_pos</code> that computes the accuracy of your model.</p>
<ul>
<li>This is a warm up exercise.</li>
<li>To assign a part of speech to a word, assign the most frequent POS for that word in the training set.</li>
<li>Then evaluate how well this approach works. Each time you predict based on the most frequent POS for the given word, check whether the actual POS of that word is the same. If so, the prediction was correct!</li>
<li>Calculate the accuracy as the number of correct predictions divided by the total number of words for which you predicted the POS tag.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: predict_pos</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_pos</span>(<span class="params">prep, y, emission_counts, vocab, states</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        prep: a preprocessed version of &#x27;y&#x27;. A list with the &#x27;word&#x27; component of the tuples.</span></span><br><span class="line"><span class="string">        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)</span></span><br><span class="line"><span class="string">        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">        states: a sorted list of all possible tags for this assignment</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        accuracy: Number of times you classified a word correctly</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the number of correct predictions to zero</span></span><br><span class="line">    num_correct = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the (tag, word) tuples, stored as a set</span></span><br><span class="line">    all_words = <span class="built_in">set</span>(emission_counts.keys())  <span class="comment"># (tag, word)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the number of (word, POS) tuples in the corpus &#x27;y&#x27;</span></span><br><span class="line">    total = <span class="built_in">len</span>(y)</span><br><span class="line">    <span class="keyword">for</span> word, y_tup <span class="keyword">in</span> <span class="built_in">zip</span>(prep, y): </span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split the (word, POS) string into a list of two items</span></span><br><span class="line">        y_tup_l = y_tup.split()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Verify that y_tup contain both word and POS</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(y_tup_l) == <span class="number">2</span>:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Set the true POS label for this word</span></span><br><span class="line">            true_label = y_tup_l[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># If the y_tup didn&#x27;t contain word and POS, go to next word</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line">        count_final = <span class="number">0</span></span><br><span class="line">        pos_final = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If the word is in the vocabulary...</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">            <span class="keyword">for</span> pos <span class="keyword">in</span> states:</span><br><span class="line"></span><br><span class="line">            <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">                        </span><br><span class="line">                <span class="comment"># define the key as the tuple containing the POS and word</span></span><br><span class="line">                key = (pos, word)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># check if the (pos, word) key exists in the emission_counts dictionary</span></span><br><span class="line">                <span class="keyword">if</span> key <span class="keyword">in</span> emission_counts: <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># get the emission count of the (pos,word) tuple </span></span><br><span class="line">                    count = emission_counts[key]</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># keep track of the POS with the largest count</span></span><br><span class="line">                    <span class="keyword">if</span> count &gt; count_final: <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">                        <span class="comment"># update the final count (largest count)</span></span><br><span class="line">                        count_final = count</span><br><span class="line"></span><br><span class="line">                        <span class="comment"># update the final POS</span></span><br><span class="line">                        pos_final = pos</span><br><span class="line"></span><br><span class="line">            <span class="comment"># If the final POS (with the largest count) matches the true POS:</span></span><br><span class="line">            <span class="keyword">if</span> pos_final == true_label: <span class="comment"># complete this line</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Update the number of correct predictions</span></span><br><span class="line">                num_correct += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    accuracy = num_correct / total</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Accuracy of prediction using predict_pos is <span class="subst">&#123;accuracy_predict_pos:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy of prediction using predict_pos is 0.8889</code></pre>
<h5 id="expected-output-2">Expected Output</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of prediction <span class="keyword">using</span> predict_pos is <span class="number">0.8889</span></span><br></pre></td></tr></table></figure>
<p>88.9% is really good for this warm up exercise. With hidden markov models, you should be able to get <strong>95% accuracy.</strong></p>
<p><a name='2'></a> # Part 2: Hidden Markov Models for POS</p>
<p>Now you will build something more context specific. Concretely, you will be implementing a Hidden Markov Model (HMM) with a Viterbi decoder - The HMM is one of the most commonly used algorithms in Natural Language Processing, and is a foundation to many deep learning techniques you will see in this specialization. - In addition to parts-of-speech tagging, HMM is used in speech recognition, speech synthesis, etc. - By completing this part of the assignment you will get a 95% accuracy on the same dataset you used in Part 1.</p>
<p>The Markov Model contains a number of states and the probability of transition between those states. - In this case, the states are the parts-of-speech. - A Markov Model utilizes a transition matrix, <code>A</code>. - A Hidden Markov Model adds an observation or emission matrix <code>B</code> which describes the probability of a visible observation when we are in a particular state. - In this case, the emissions are the words in the corpus - The state, which is hidden, is the POS tag of that word.</p>
<p><a name='2.1'></a> ## Part 2.1 Generating Matrices</p>
<h3 id="creating-the-a-transition-probabilities-matrix">Creating the 'A' transition probabilities matrix</h3>
<p>Now that you have your <code>emission_counts</code>, <code>transition_counts</code>, and <code>tag_counts</code>, you will start implementing the Hidden Markov Model.</p>
<p>This will allow you to quickly construct the - <code>A</code> transition probabilities matrix. - and the <code>B</code> emission probabilities matrix.</p>
<p>You will also use some smoothing when computing these matrices.</p>
<p>Here is an example of what the <code>A</code> transition matrix would look like (it is simplified to 5 tags for viewing. It is 46x46 in this assignment.):</p>
<p>|<strong>A</strong> |...| RBS | RP | SYM | TO | UH|... | --- ||---:-------------| ------------ | ------------ | -------- | ---------- |---- |<strong>RBS</strong> |...|2.217069e-06 |2.217069e-06 |2.217069e-06 |0.008870 |2.217069e-06|... |<strong>RP</strong> |...|3.756509e-07 |7.516775e-04 |3.756509e-07 |0.051089 |3.756509e-07|... |<strong>SYM</strong> |...|1.722772e-05 |1.722772e-05 |1.722772e-05 |0.000017 |1.722772e-05|... |<strong>TO</strong> |...|4.477336e-05 |4.472863e-08 |4.472863e-08 |0.000090 |4.477336e-05|... |<strong>UH</strong> |...|1.030439e-05 |1.030439e-05 |1.030439e-05 |0.061837 |3.092348e-02|... | ... |...| ... | ... | ... | ... | ... | ...</p>
<p>Note that the matrix above was computed with smoothing.</p>
<p>Each cell gives you the probability to go from one part of speech to another. - In other words, there is a 4.47e-8 chance of going from parts-of-speech <code>TO</code> to <code>RP</code>. - The sum of each row has to equal 1, because we assume that the next POS tag must be one of the available columns in the table.</p>
<p>The smoothing was done as follows:</p>
<p><span class="math display">\[ P(t_i | t_{i-1}) = \frac{C(t_{i-1}, t_{i}) + \alpha }{C(t_{i-1}) +\alpha * N}\tag{3}\]</span></p>
<ul>
<li><span class="math inline">\(N\)</span> is the total number of tags</li>
<li><span class="math inline">\(C(t_{i-1}, t_{i})\)</span> is the count of the tuple (previous POS, current POS) in <code>transition_counts</code> dictionary.</li>
<li><span class="math inline">\(C(t_{i-1})\)</span> is the count of the previous POS in the <code>tag_counts</code> dictionary.</li>
<li><span class="math inline">\(\alpha\)</span> is a smoothing parameter.</li>
</ul>
<p><a name='ex-03'></a> ### Exercise 03</p>
<p><strong>Instructions:</strong> Implement the <code>create_transition_matrix</code> below for all tags. Your task is to output a matrix that computes equation 3 for each cell in matrix <code>A</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: create_transition_matrix</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_transition_matrix</span>(<span class="params">alpha, tag_counts, transition_counts</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; </span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        alpha: number used for smoothing</span></span><br><span class="line"><span class="string">        tag_counts: a dictionary mapping each tag to its respective count</span></span><br><span class="line"><span class="string">        transition_counts: transition count for the previous word and tag</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        A: matrix of dimension (num_tags,num_tags)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># Get a sorted list of unique POS tags</span></span><br><span class="line">    all_tags = <span class="built_in">sorted</span>(tag_counts.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Count the number of unique POS tags</span></span><br><span class="line">    num_tags = <span class="built_in">len</span>(all_tags)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the transition matrix &#x27;A&#x27;</span></span><br><span class="line">    A = np.zeros((num_tags,num_tags))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the unique transition tuples (previous POS, current POS)</span></span><br><span class="line">    trans_keys = <span class="built_in">set</span>(transition_counts.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Return instances of &#x27;None&#x27; with your code) ### </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each row of the transition matrix A</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_tags):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Go through each column of the transition matrix A</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_tags):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Initialize the count of the (prev POS, current POS) to zero</span></span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">            <span class="comment"># Define the tuple (prev POS, current POS)</span></span><br><span class="line">            <span class="comment"># Get the tag at position i and tag at position j (from the all_tags list)</span></span><br><span class="line">            key = (all_tags[i],all_tags[j])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Check if the (prev POS, current POS) tuple </span></span><br><span class="line">            <span class="comment"># exists in the transition counts dictionaory</span></span><br><span class="line">            <span class="keyword">if</span> key <span class="keyword">in</span> trans_keys: <span class="comment">#complete this line</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Get count from the transition_counts dictionary </span></span><br><span class="line">                <span class="comment"># for the (prev POS, current POS) tuple</span></span><br><span class="line">                count = transition_counts[key]</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Get the count of the previous tag (index position i) from tag_counts</span></span><br><span class="line">            count_prev_tag = tag_counts[all_tags[i]]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Apply smoothing using count of the tuple, alpha, </span></span><br><span class="line">            <span class="comment"># count of previous tag, alpha, and number of total tags</span></span><br><span class="line">            A[i,j] = (count + alpha ) / ( count_prev_tag + alpha * num_tags)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.001</span></span><br><span class="line">A = create_transition_matrix(alpha, tag_counts, transition_counts)</span><br><span class="line"><span class="comment"># Testing your function</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;A at row 0, col 0: <span class="subst">&#123;A[<span class="number">0</span>,<span class="number">0</span>]:<span class="number">.9</span>f&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;A at row 3, col 1: <span class="subst">&#123;A[<span class="number">3</span>,<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;View a subset of transition matrix A&quot;</span>)</span><br><span class="line">A_sub = pd.DataFrame(A[<span class="number">30</span>:<span class="number">35</span>,<span class="number">30</span>:<span class="number">35</span>], index=states[<span class="number">30</span>:<span class="number">35</span>], columns = states[<span class="number">30</span>:<span class="number">35</span>] )</span><br><span class="line"><span class="built_in">print</span>(A_sub)</span><br></pre></td></tr></table></figure>
<pre><code>A at row 0, col 0: 0.000007040
A at row 3, col 1: 0.1691
View a subset of transition matrix A
              RBS            RP           SYM        TO            UH
RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06
RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07
SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05
TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05
UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02</code></pre>
<h5 id="expected-output-3">Expected Output</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">A at row <span class="number">0</span>, col <span class="number">0</span>: <span class="number">0.000007040</span></span><br><span class="line">A at row <span class="number">3</span>, col <span class="number">1</span>: <span class="number">0.1691</span></span><br><span class="line">View a subset of transition matrix A</span><br><span class="line">              RBS            RP           SYM        TO            UH</span><br><span class="line">RBS  <span class="number">2.217069e-06</span>  <span class="number">2.217069e-06</span>  <span class="number">2.217069e-06</span>  <span class="number">0.008870</span>  <span class="number">2.217069e-06</span></span><br><span class="line">RP   <span class="number">3.756509e-07</span>  <span class="number">7.516775e-04</span>  <span class="number">3.756509e-07</span>  <span class="number">0.051089</span>  <span class="number">3.756509e-07</span></span><br><span class="line">SYM  <span class="number">1.722772e-05</span>  <span class="number">1.722772e-05</span>  <span class="number">1.722772e-05</span>  <span class="number">0.000017</span>  <span class="number">1.722772e-05</span></span><br><span class="line">TO   <span class="number">4.477336e-05</span>  <span class="number">4.472863e-08</span>  <span class="number">4.472863e-08</span>  <span class="number">0.000090</span>  <span class="number">4.477336e-05</span></span><br><span class="line">UH   <span class="number">1.030439e-05</span>  <span class="number">1.030439e-05</span>  <span class="number">1.030439e-05</span>  <span class="number">0.061837</span>  <span class="number">3.092348e-02</span></span><br></pre></td></tr></table></figure>
<h3 id="create-the-b-emission-probabilities-matrix">Create the 'B' emission probabilities matrix</h3>
<p>Now you will create the <code>B</code> transition matrix which computes the emission probability.</p>
<p>You will use smoothing as defined below:</p>
<p><span class="math display">\[P(w_i | t_i) = \frac{C(t_i, word_i)+ \alpha}{C(t_{i}) +\alpha * N}\tag{4}\]</span></p>
<ul>
<li><span class="math inline">\(C(t_i, word_i)\)</span> is the number of times <span class="math inline">\(word_i\)</span> was associated with <span class="math inline">\(tag_i\)</span> in the training data (stored in <code>emission_counts</code> dictionary).</li>
<li><span class="math inline">\(C(t_i)\)</span> is the number of times <span class="math inline">\(tag_i\)</span> was in the training data (stored in <code>tag_counts</code> dictionary).</li>
<li><span class="math inline">\(N\)</span> is the number of words in the vocabulary</li>
<li><span class="math inline">\(\alpha\)</span> is a smoothing parameter.</li>
</ul>
<p>The matrix <code>B</code> is of dimension (num_tags, N), where num_tags is the number of possible parts-of-speech tags.</p>
Here is an example of the matrix, only a subset of tags and words are shown:
<p style="text-align: center;">
<b>B Emissions Probability Matrix (subset)</b>
</p>
<table>
<thead>
<tr class="header">
<th><strong>B</strong></th>
<th>...</th>
<th>725</th>
<th>adroitly</th>
<th>engineers</th>
<th>promoted</th>
<th>synergy</th>
<th>...</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>CD</strong></td>
<td>...</td>
<td><strong>8.201296e-05</strong></td>
<td>2.732854e-08</td>
<td>2.732854e-08</td>
<td>2.732854e-08</td>
<td>2.732854e-08</td>
<td>...</td>
</tr>
<tr class="even">
<td><strong>NN</strong></td>
<td>...</td>
<td>7.521128e-09</td>
<td>7.521128e-09</td>
<td>7.521128e-09</td>
<td>7.521128e-09</td>
<td><strong>2.257091e-05</strong></td>
<td>...</td>
</tr>
<tr class="odd">
<td><strong>NNS</strong></td>
<td>...</td>
<td>1.670013e-08</td>
<td>1.670013e-08</td>
<td><strong>4.676203e-04</strong></td>
<td>1.670013e-08</td>
<td>1.670013e-08</td>
<td>...</td>
</tr>
<tr class="even">
<td><strong>VB</strong></td>
<td>...</td>
<td>3.779036e-08</td>
<td>3.779036e-08</td>
<td>3.779036e-08</td>
<td>3.779036e-08</td>
<td>3.779036e-08</td>
<td>...</td>
</tr>
<tr class="odd">
<td><strong>RB</strong></td>
<td>...</td>
<td>3.226454e-08</td>
<td><strong>6.456135e-05</strong></td>
<td>3.226454e-08</td>
<td>3.226454e-08</td>
<td>3.226454e-08</td>
<td>...</td>
</tr>
<tr class="even">
<td><strong>RP</strong></td>
<td>...</td>
<td>3.723317e-07</td>
<td>3.723317e-07</td>
<td>3.723317e-07</td>
<td><strong>3.723317e-07</strong></td>
<td>3.723317e-07</td>
<td>...</td>
</tr>
<tr class="odd">
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table>
<p><a name='ex-04'></a> ### Exercise 04 <strong>Instructions:</strong> Implement the <code>create_emission_matrix</code> below that computes the <code>B</code> emission probabilities matrix. Your function takes in <span class="math inline">\(\alpha\)</span>, the smoothing parameter, <code>tag_counts</code>, which is a dictionary mapping each tag to its respective count, the <code>emission_counts</code> dictionary where the keys are (tag, word) and the values are the counts. Your task is to output a matrix that computes equation 4 for each cell in matrix <code>B</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: create_emission_matrix</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_emission_matrix</span>(<span class="params">alpha, tag_counts, emission_counts, vocab</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        alpha: tuning parameter used in smoothing </span></span><br><span class="line"><span class="string">        tag_counts: a dictionary mapping each tag to its respective count</span></span><br><span class="line"><span class="string">        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        B: a matrix of dimension (num_tags, len(vocab))</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get the number of POS tag</span></span><br><span class="line">    num_tags = <span class="built_in">len</span>(tag_counts)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get a list of all POS tags</span></span><br><span class="line">    all_tags = <span class="built_in">sorted</span>(tag_counts.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the total number of unique words in the vocabulary</span></span><br><span class="line">    num_words = <span class="built_in">len</span>(vocab)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the emission matrix B with places for</span></span><br><span class="line">    <span class="comment"># tags in the rows and words in the columns</span></span><br><span class="line">    B = np.zeros((num_tags, num_words))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get a set of all (POS, word) tuples </span></span><br><span class="line">    <span class="comment"># from the keys of the emission_counts dictionary</span></span><br><span class="line">    emis_keys = <span class="built_in">set</span>(<span class="built_in">list</span>(emission_counts.keys()))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each row (POS tags)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_tags): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Go through each column (words)</span></span><br><span class="line">        <span class="keyword">for</span> j,word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Initialize the emission count for the (POS tag, word) to zero</span></span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">                    </span><br><span class="line">            <span class="comment"># Define the (POS tag, word) tuple for this row and column</span></span><br><span class="line">            key =  (all_tags[i], word)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># check if the (POS tag, word) tuple exists as a key in emission counts</span></span><br><span class="line">            <span class="keyword">if</span> key <span class="keyword">in</span> emis_keys: <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">                <span class="comment"># Get the count of (POS tag, word) from the emission_counts d</span></span><br><span class="line">                count = emission_counts[key]</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Get the count of the POS tag</span></span><br><span class="line">            count_tag = tag_counts[all_tags[i]]</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># Apply smoothing and store the smoothed value </span></span><br><span class="line">            <span class="comment"># into the emission matrix B for this row and column</span></span><br><span class="line">            B[i,j] = (count + alpha) / (count_tag + alpha * num_words)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> B</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># creating your emission probability matrix. this takes a few minutes to run. </span></span><br><span class="line">B = create_emission_matrix(alpha, tag_counts, emission_counts, <span class="built_in">list</span>(vocab))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;View Matrix position at row 0, column 0: <span class="subst">&#123;B[<span class="number">0</span>,<span class="number">0</span>]:<span class="number">.9</span>f&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;View Matrix position at row 3, column 1: <span class="subst">&#123;B[<span class="number">3</span>,<span class="number">1</span>]:<span class="number">.9</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Try viewing emissions for a few words in a sample dataframe</span></span><br><span class="line">cidx  = [<span class="string">&#x27;725&#x27;</span>,<span class="string">&#x27;adroitly&#x27;</span>,<span class="string">&#x27;engineers&#x27;</span>, <span class="string">&#x27;promoted&#x27;</span>, <span class="string">&#x27;synergy&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the integer ID for each word</span></span><br><span class="line">cols = [vocab[a] <span class="keyword">for</span> a <span class="keyword">in</span> cidx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Choose POS tags to show in a sample dataframe</span></span><br><span class="line">rvals =[<span class="string">&#x27;CD&#x27;</span>,<span class="string">&#x27;NN&#x27;</span>,<span class="string">&#x27;NNS&#x27;</span>, <span class="string">&#x27;VB&#x27;</span>,<span class="string">&#x27;RB&#x27;</span>,<span class="string">&#x27;RP&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># For each POS tag, get the row number from the &#x27;states&#x27; list</span></span><br><span class="line">rows = [states.index(a) <span class="keyword">for</span> a <span class="keyword">in</span> rvals]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the emissions for the sample of words, and the sample of POS tags</span></span><br><span class="line">B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )</span><br><span class="line"><span class="built_in">print</span>(B_sub)</span><br></pre></td></tr></table></figure>
<pre><code>View Matrix position at row 0, column 0: 0.000006032
View Matrix position at row 3, column 1: 0.000000720
              725      adroitly     engineers      promoted       synergy
CD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08
NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05
NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08
VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08
RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08
RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07</code></pre>
<h5 id="expected-output-4">Expected Output</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">View Matrix position at row <span class="number">0</span>, column <span class="number">0</span>: <span class="number">0.000006032</span></span><br><span class="line">View Matrix position at row <span class="number">3</span>, column <span class="number">1</span>: <span class="number">0.000000720</span></span><br><span class="line">              <span class="number">725</span>      adroitly     engineers      promoted       synergy</span><br><span class="line">CD   <span class="number">8.201296e-05</span>  <span class="number">2.732854e-08</span>  <span class="number">2.732854e-08</span>  <span class="number">2.732854e-08</span>  <span class="number">2.732854e-08</span></span><br><span class="line">NN   <span class="number">7.521128e-09</span>  <span class="number">7.521128e-09</span>  <span class="number">7.521128e-09</span>  <span class="number">7.521128e-09</span>  <span class="number">2.257091e-05</span></span><br><span class="line">NNS  <span class="number">1.670013e-08</span>  <span class="number">1.670013e-08</span>  <span class="number">4.676203e-04</span>  <span class="number">1.670013e-08</span>  <span class="number">1.670013e-08</span></span><br><span class="line">VB   <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span>  <span class="number">3.779036e-08</span></span><br><span class="line">RB   <span class="number">3.226454e-08</span>  <span class="number">6.456135e-05</span>  <span class="number">3.226454e-08</span>  <span class="number">3.226454e-08</span>  <span class="number">3.226454e-08</span></span><br><span class="line">RP   <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span>  <span class="number">3.723317e-07</span></span><br></pre></td></tr></table></figure>
<p><a name='3'></a> # Part 3: Viterbi Algorithm and Dynamic Programming</p>
<p>In this part of the assignment you will implement the Viterbi algorithm which makes use of dynamic programming. Specifically, you will use your two matrices, <code>A</code> and <code>B</code> to compute the Viterbi algorithm. We have decomposed this process into three main steps for you.</p>
<ul>
<li><strong>Initialization</strong> - In this part you initialize the <code>best_paths</code> and <code>best_probabilities</code> matrices that you will be populating in <code>feed_forward</code>.</li>
<li><strong>Feed forward</strong> - At each step, you calculate the probability of each path happening and the best paths up to that point.</li>
<li><strong>Feed backward</strong>: This allows you to find the best path with the highest probabilities.</li>
</ul>
<p><a name='3.1'></a> ## Part 3.1: Initialization</p>
<p>You will start by initializing two matrices of the same dimension.</p>
<ul>
<li><p>best_probs: Each cell contains the probability of going from one POS tag to a word in the corpus.</p></li>
<li><p>best_paths: A matrix that helps you trace through the best possible path in the corpus.</p></li>
</ul>
<p><a name='ex-05'></a> ### Exercise 05 <strong>Instructions</strong>: Write a program below that initializes the <code>best_probs</code> and the <code>best_paths</code> matrix.</p>
<p>Both matrices will be initialized to zero except for column zero of <code>best_probs</code>.<br />
- Column zero of <code>best_probs</code> is initialized with the assumption that the first word of the corpus was preceded by a start token ("--s--"). - This allows you to reference the <strong>A</strong> matrix for the transition probability</p>
<p>Here is how to initialize column 0 of <code>best_probs</code>: - The probability of the best path going from the start index to a given POS tag indexed by integer <span class="math inline">\(i\)</span> is denoted by <span class="math inline">\(\textrm{best_probs}[s_{idx}, i]\)</span>. - This is estimated as the probability that the start tag transitions to the POS denoted by index <span class="math inline">\(i\)</span>: <span class="math inline">\(\mathbf{A}[s_{idx}, i]\)</span> AND that the POS tag denoted by <span class="math inline">\(i\)</span> emits the first word of the given corpus, which is <span class="math inline">\(\mathbf{B}[i, vocab[corpus[0]]]\)</span>. - Note that vocab[corpus[0]] refers to the first word of the corpus (the word at position 0 of the corpus). - <strong>vocab</strong> is a dictionary that returns the unique integer that refers to that particular word.</p>
<p>Conceptually, it looks like this: <span class="math inline">\(\textrm{best_probs}[s_{idx}, i] = \mathbf{A}[s_{idx}, i] \times \mathbf{B}[i, corpus[0] ]\)</span></p>
<p>In order to avoid multiplying and storing small values on the computer, we'll take the log of the product, which becomes the sum of two logs:</p>
<p><span class="math inline">\(best\_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]\)</span></p>
<p>Also, to avoid taking the log of 0 (which is defined as negative infinity), the code itself will just set <span class="math inline">\(best\_probs[i,0] = float(&#39;-inf&#39;)\)</span> when <span class="math inline">\(A[s_{idx}, i] == 0\)</span></p>
<p>So the implementation to initialize <span class="math inline">\(best\_probs\)</span> looks like this:</p>
<p>$ if A[s_{idx}, i] &lt;&gt; 0 : best_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]])$</p>
<p>$ if A[s_{idx}, i] == 0 : best_probs[i,0] = float('-inf')$</p>
<p>Please use <a target="_blank" rel="noopener" href="https://docs.python.org/3/library/math.html">math.log</a> to compute the natural logarithm.</p>
<p>The example below shows the initialization assuming the corpus starts with the phrase "Loss tracks upward".</p>
<p><img src = "Initialize4.png"/></p>
<p>Represent infinity and negative infinity like this:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in"><span class="keyword">float</span></span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line"><span class="built_in"><span class="keyword">float</span></span>(<span class="string">&#x27;-inf&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: initialize</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span>(<span class="params">states, tag_counts, A, B, corpus, vocab</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        states: a list of all possible parts-of-speech</span></span><br><span class="line"><span class="string">        tag_counts: a dictionary mapping each tag to its respective count</span></span><br><span class="line"><span class="string">        A: Transition Matrix of dimension (num_tags, num_tags)</span></span><br><span class="line"><span class="string">        B: Emission Matrix of dimension (num_tags, len(vocab))</span></span><br><span class="line"><span class="string">        corpus: a sequence of words whose POS is to be identified in a list </span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        best_probs: matrix of dimension (num_tags, len(corpus)) of floats</span></span><br><span class="line"><span class="string">        best_paths: matrix of dimension (num_tags, len(corpus)) of integers</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># Get the total number of unique POS tags</span></span><br><span class="line">    num_tags = <span class="built_in">len</span>(tag_counts)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize best_probs matrix </span></span><br><span class="line">    <span class="comment"># POS tags in the rows, number of words in the corpus as the columns</span></span><br><span class="line">    best_probs = np.zeros((num_tags, <span class="built_in">len</span>(corpus)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize best_paths matrix</span></span><br><span class="line">    <span class="comment"># POS tags in the rows, number of words in the corpus as columns</span></span><br><span class="line">    best_paths = np.zeros((num_tags, <span class="built_in">len</span>(corpus)), dtype=<span class="built_in">int</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the start token</span></span><br><span class="line">    s_idx = states.index(<span class="string">&quot;--s--&quot;</span>)</span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each of the POS tags</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_tags) : <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Handle the special case when the transition from start token to POS tag i is zero</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>,i] == <span class="number">0</span>: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_probs at POS tag &#x27;i&#x27;, column 0, to negative infinity</span></span><br><span class="line">            best_probs[i,<span class="number">0</span>] = <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># For all other cases when transition from start token to POS tag i is non-zero:</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_probs at POS tag &#x27;i&#x27;, column 0</span></span><br><span class="line">            <span class="comment"># Check the formula in the instructions above</span></span><br><span class="line">            best_probs[i,<span class="number">0</span>] = math.log(A[s_idx,i])  +  math.log(B[i,vocab[corpus[<span class="number">0</span>]]])</span><br><span class="line">            </span><br><span class="line">                         </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> best_probs, best_paths</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the function</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;best_probs[0,0]: <span class="subst">&#123;best_probs[<span class="number">0</span>,<span class="number">0</span>]:<span class="number">.4</span>f&#125;</span>&quot;</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;best_paths[2,3]: <span class="subst">&#123;best_paths[<span class="number">2</span>,<span class="number">3</span>]:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>best_probs[0,0]: -22.6098
best_paths[2,3]: 0.0000</code></pre>
<h5 id="expected-output-5">Expected Output</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">best_probs[<span class="number">0</span>,<span class="number">0</span>]: <span class="number">-22.6098</span></span><br><span class="line">best_paths[<span class="number">2</span>,<span class="number">3</span>]: <span class="number">0.0000</span></span><br></pre></td></tr></table></figure>
<p><a name='3.2'></a> ## Part 3.2 Viterbi Forward</p>
<p>In this part of the assignment, you will implement the <code>viterbi_forward</code> segment. In other words, you will populate your <code>best_probs</code> and <code>best_paths</code> matrices. - Walk forward through the corpus. - For each word, compute a probability for each possible tag. - Unlike the previous algorithm <code>predict_pos</code> (the 'warm-up' exercise), this will include the path up to that (word,tag) combination.</p>
<p>Here is an example with a three-word corpus "Loss tracks upward": - Note, in this example, only a subset of states (POS tags) are shown in the diagram below, for easier reading. - In the diagram below, the first word "Loss" is already initialized. - The algorithm will compute a probability for each of the potential tags in the second and future words.</p>
<p>Compute the probability that the tag of the second work ('tracks') is a verb, 3rd person singular present (VBZ).<br />
- In the <code>best_probs</code> matrix, go to the column of the second word ('tracks'), and row 40 (VBZ), this cell is highlighted in light orange in the diagram below. - Examine each of the paths from the tags of the first word ('Loss') and choose the most likely path.<br />
- An example of the calculation for <strong>one</strong> of those paths is the path from ('Loss', NN) to ('tracks', VBZ). - The log of the probability of the path up to and including the first word 'Loss' having POS tag NN is <span class="math inline">\(-14.32\)</span>. The <code>best_probs</code> matrix contains this value -14.32 in the column for 'Loss' and row for 'NN'. - Find the probability that NN transitions to VBZ. To find this probability, go to the <code>A</code> transition matrix, and go to the row for 'NN' and the column for 'VBZ'. The value is <span class="math inline">\(4.37e-02\)</span>, which is circled in the diagram, so add <span class="math inline">\(-14.32 + log(4.37e-02)\)</span>. - Find the log of the probability that the tag VBS would 'emit' the word 'tracks'. To find this, look at the 'B' emission matrix in row 'VBZ' and the column for the word 'tracks'. The value <span class="math inline">\(4.61e-04\)</span> is circled in the diagram below. So add <span class="math inline">\(-14.32 + log(4.37e-02) + log(4.61e-04)\)</span>. - The sum of <span class="math inline">\(-14.32 + log(4.37e-02) + log(4.61e-04)\)</span> is <span class="math inline">\(-25.13\)</span>. Store <span class="math inline">\(-25.13\)</span> in the <code>best_probs</code> matrix at row 'VBZ' and column 'tracks' (as seen in the cell that is highlighted in light orange in the diagram). - All other paths in best_probs are calculated. Notice that <span class="math inline">\(-25.13\)</span> is greater than all of the other values in column 'tracks' of matrix <code>best_probs</code>, and so the most likely path to 'VBZ' is from 'NN'. 'NN' is in row 20 of the <code>best_probs</code> matrix, so <span class="math inline">\(20\)</span> is the most likely path. - Store the most likely path <span class="math inline">\(20\)</span> in the <code>best_paths</code> table. This is highlighted in light orange in the diagram below.</p>
<p>The formula to compute the probability and path for the <span class="math inline">\(i^{th}\)</span> word in the <span class="math inline">\(corpus\)</span>, the prior word <span class="math inline">\(i-1\)</span> in the corpus, current POS tag <span class="math inline">\(j\)</span>, and previous POS tag <span class="math inline">\(k\)</span> is:</p>
<p><span class="math inline">\(\mathrm{prob} = \mathbf{best\_prob}_{k, i-1} + \mathrm{log}(\mathbf{A}_{k, j}) + \mathrm{log}(\mathbf{B}_{j, vocab(corpus_{i})})\)</span></p>
<p>where <span class="math inline">\(corpus_{i}\)</span> is the word in the corpus at index <span class="math inline">\(i\)</span>, and <span class="math inline">\(vocab\)</span> is the dictionary that gets the unique integer that represents a given word.</p>
<p><span class="math inline">\(\mathrm{path} = k\)</span></p>
<p>where <span class="math inline">\(k\)</span> is the integer representing the previous POS tag.</p>
<p><a name='ex-06'></a></p>
<h3 id="exercise-06">Exercise 06</h3>
<p>Instructions: Implement the <code>viterbi_forward</code> algorithm and store the best_path and best_prob for every possible tag for each word in the matrices <code>best_probs</code> and <code>best_tags</code> using the pseudo code below.</p>
<p>`for each word in the corpus</p>
<pre><code>for each POS tag type that this word may be

    for POS tag type that the previous word could be
    
        compute the probability that the previous word had a given POS tag, that the current word has a given POS tag, and that the POS tag would emit this current word.
        
        retain the highest probability computed for the current word
        
        set best_probs to this highest probability
        
        set best_paths to the index &#39;k&#39;, representing the POS tag of the previous word which produced the highest probability `</code></pre>
<p>Please use <a target="_blank" rel="noopener" href="https://docs.python.org/3/library/math.html">math.log</a> to compute the natural logarithm.</p>
<p><img src = "Forward4.png"/></p>
##
<details>
<summary>
<font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
<li>
Remember that when accessing emission matrix B, the column index is the unique integer ID associated with the word. It can be accessed by using the 'vocab' dictionary, where the key is the word, and the value is the unique integer ID for that word.
</li>
</ul>
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: viterbi_forward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi_forward</span>(<span class="params">A, B, test_corpus, best_probs, best_paths, vocab</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        A, B: The transiton and emission matrices respectively</span></span><br><span class="line"><span class="string">        test_corpus: a list containing a preprocessed corpus</span></span><br><span class="line"><span class="string">        best_probs: an initilized matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">        best_paths: an initilized matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">        vocab: a dictionary where keys are words in vocabulary and value is an index </span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        best_probs: a completed matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">        best_paths: a completed matrix of dimension (num_tags, len(corpus))</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># Get the number of unique POS tags (which is the num of rows in best_probs)</span></span><br><span class="line">    num_tags = best_probs.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through every word in the corpus starting from word 1</span></span><br><span class="line">    <span class="comment"># Recall that word 0 was initialized in `initialize()`</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(test_corpus)): </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print number of words processed, every 5000 words</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">5000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Words processed: &#123;:&gt;8&#125;&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">            </span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code EXCEPT the first &#x27;best_path_i = None&#x27;) ###</span></span><br><span class="line">        <span class="comment"># For each unique POS tag that the current word can be</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_tags): <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_prob for word i to negative infinity</span></span><br><span class="line">            best_prob_i = <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Initialize best_path for current word i to None</span></span><br><span class="line">            best_path_i = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># For each POS tag that the previous word can be:</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(num_tags): <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">                <span class="comment"># Calculate the probability = </span></span><br><span class="line">                <span class="comment"># best probs of POS tag k, previous word i-1 + </span></span><br><span class="line">                <span class="comment"># log(prob of transition from POS k to POS j) + </span></span><br><span class="line">                <span class="comment"># log(prob that emission of POS j is word i)</span></span><br><span class="line">                prob = best_probs[k, i-<span class="number">1</span>] + math.log(A[k,j]) + math.log(B[j, vocab[test_corpus[i]]])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># check if this path&#x27;s probability is greater than</span></span><br><span class="line">                <span class="comment"># the best probability up to and before this point</span></span><br><span class="line">                <span class="keyword">if</span> prob &gt; best_prob_i: <span class="comment"># complete this line</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Keep track of the best probability</span></span><br><span class="line">                    best_prob_i = prob</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># keep track of the POS tag of the previous word</span></span><br><span class="line">                    <span class="comment"># that is part of the best path.  </span></span><br><span class="line">                    <span class="comment"># Save the index (integer) associated with </span></span><br><span class="line">                    <span class="comment"># that previous word&#x27;s POS tag</span></span><br><span class="line">                    best_path_i = k</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Save the best probability for the </span></span><br><span class="line">            <span class="comment"># given current word&#x27;s POS tag</span></span><br><span class="line">            <span class="comment"># and the position of the current word inside the corpus</span></span><br><span class="line">            best_probs[j,i] = best_prob_i</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Save the unique integer ID of the previous POS tag</span></span><br><span class="line">            <span class="comment"># into best_paths matrix, for the POS tag of the current word</span></span><br><span class="line">            <span class="comment"># and the position of the current word inside the corpus.</span></span><br><span class="line">            best_paths[j,i] = best_path_i</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> best_probs, best_paths</span><br></pre></td></tr></table></figure>
<p>Run the <code>viterbi_forward</code> function to fill in the <code>best_probs</code> and <code>best_paths</code> matrices.</p>
<p><strong>Note</strong> that this will take a few minutes to run. There are about 30,000 words to process.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this will take a few minutes to run =&gt; processes ~ 30,000 words</span></span><br><span class="line">best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)</span><br></pre></td></tr></table></figure>
<pre><code>Words processed:     5000
Words processed:    10000
Words processed:    15000
Words processed:    20000
Words processed:    25000
Words processed:    30000</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test this function </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;best_probs[0,1]: <span class="subst">&#123;best_probs[<span class="number">0</span>,<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>&quot;</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;best_probs[0,4]: <span class="subst">&#123;best_probs[<span class="number">0</span>,<span class="number">4</span>]:<span class="number">.4</span>f&#125;</span>&quot;</span>) </span><br></pre></td></tr></table></figure>
<pre><code>best_probs[0,1]: -24.7822
best_probs[0,4]: -49.5601</code></pre>
<h5 id="expected-output-6">Expected Output</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">best_probs[<span class="number">0</span>,<span class="number">1</span>]: <span class="number">-24.7822</span></span><br><span class="line">best_probs[<span class="number">0</span>,<span class="number">4</span>]: <span class="number">-49.5601</span></span><br></pre></td></tr></table></figure>
<p><a name='3.3'></a> ## Part 3.3 Viterbi backward</p>
<p>Now you will implement the Viterbi backward algorithm. - The Viterbi backward algorithm gets the predictions of the POS tags for each word in the corpus using the <code>best_paths</code> and the <code>best_probs</code> matrices.</p>
<p>The example below shows how to walk backwards through the best_paths matrix to get the POS tags of each word in the corpus. Recall that this example corpus has three words: "Loss tracks upward".</p>
<p>POS tag for 'upward' is <code>RB</code> - Select the the most likely POS tag for the last word in the corpus, 'upward' in the <code>best_prob</code> table. - Look for the row in the column for 'upward' that has the largest probability. - Notice that in row 28 of <code>best_probs</code>, the estimated probability is -34.99, which is larger than the other values in the column. So the most likely POS tag for 'upward' is <code>RB</code> an adverb, at row 28 of <code>best_prob</code>. - The variable <code>z</code> is an array that stores the unique integer ID of the predicted POS tags for each word in the corpus. In array z, at position 2, store the value 28 to indicate that the word 'upward' (at index 2 in the corpus), most likely has the POS tag associated with unique ID 28 (which is <code>RB</code>). - The variable <code>pred</code> contains the POS tags in string form. So <code>pred</code> at index 2 stores the string <code>RB</code>.</p>
<p>POS tag for 'tracks' is <code>VBZ</code> - The next step is to go backward one word in the corpus ('tracks'). Since the most likely POS tag for 'upward' is <code>RB</code>, which is uniquely identified by integer ID 28, go to the <code>best_paths</code> matrix in column 2, row 28. The value stored in <code>best_paths</code>, column 2, row 28 indicates the unique ID of the POS tag of the previous word. In this case, the value stored here is 40, which is the unique ID for POS tag <code>VBZ</code> (verb, 3rd person singular present). - So the previous word at index 1 of the corpus ('tracks'), most likely has the POS tag with unique ID 40, which is <code>VBZ</code>. - In array <code>z</code>, store the value 40 at position 1, and for array <code>pred</code>, store the string <code>VBZ</code> to indicate that the word 'tracks' most likely has POS tag <code>VBZ</code>.</p>
<p>POS tag for 'Loss' is <code>NN</code> - In <code>best_paths</code> at column 1, the unique ID stored at row 40 is 20. 20 is the unique ID for POS tag <code>NN</code>. - In array <code>z</code> at position 0, store 20. In array <code>pred</code> at position 0, store <code>NN</code>.</p>
<p><img src = "Backwards5.png"/></p>
<p><a name='ex-07'></a> ### Exercise 07 Implement the <code>viterbi_backward</code> algorithm, which returns a list of predicted POS tags for each word in the corpus.</p>
<ul>
<li>Note that the numbering of the index positions starts at 0 and not 1.</li>
<li><code>m</code> is the number of words in the corpus.
<ul>
<li>So the indexing into the corpus goes from <code>0</code> to <code>m - 1</code>.</li>
<li>Also, the columns in <code>best_probs</code> and <code>best_paths</code> are indexed from <code>0</code> to <code>m - 1</code></li>
</ul></li>
</ul>
<p><strong>In Step 1:</strong><br />
Loop through all the rows (POS tags) in the last entry of <code>best_probs</code> and find the row (POS tag) with the maximum value. Convert the unique integer ID to a tag (a string representation) using the dictionary <code>states</code>.</p>
<p>Referring to the three-word corpus described above: - <code>z[2] = 28</code>: For the word 'upward' at position 2 in the corpus, the POS tag ID is 28. Store 28 in <code>z</code> at position 2. - states(28) is 'RB': The POS tag ID 28 refers to the POS tag 'RB'. - <code>pred[2] = 'RB'</code>: In array <code>pred</code>, store the POS tag for the word 'upward'.</p>
<p><strong>In Step 2:</strong><br />
- Starting at the last column of best_paths, use <code>best_probs</code> to find the most likely POS tag for the last word in the corpus. - Then use <code>best_paths</code> to find the most likely POS tag for the previous word. - Update the POS tag for each word in <code>z</code> and in <code>preds</code>.</p>
<p>Referring to the three-word example from above, read best_paths at column 2 and fill in z at position 1.<br />
<code>z[1] = best_paths[z[2],2]</code></p>
<p>The small test following the routine prints the last few words of the corpus and their states to aid in debug.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: viterbi_backward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi_backward</span>(<span class="params">best_probs, best_paths, corpus, states</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    This function returns the best path.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># Get the number of words in the corpus</span></span><br><span class="line">    <span class="comment"># which is also the number of columns in best_probs, best_paths</span></span><br><span class="line">    m = best_paths.shape[<span class="number">1</span>] </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize array z, same length as the corpus</span></span><br><span class="line">    z = [<span class="literal">None</span>] * m</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the number of unique POS tags</span></span><br><span class="line">    num_tags = best_probs.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the best probability for the last word</span></span><br><span class="line">    best_prob_for_last_word = <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize pred array, same length as corpus</span></span><br><span class="line">    pred = [<span class="literal">None</span>] * m</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    <span class="comment">## Step 1 ##</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each POS tag for the last word (last column of best_probs)</span></span><br><span class="line">    <span class="comment"># in order to find the row (POS tag integer ID) </span></span><br><span class="line">    <span class="comment"># with highest probability for the last word</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(num_tags): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># If the probability of POS tag at row k </span></span><br><span class="line">        <span class="comment"># is better than the previosly best probability for the last word:</span></span><br><span class="line">        <span class="keyword">if</span> best_probs[k,-<span class="number">1</span>] &gt; best_prob_for_last_word: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Store the new best probability for the lsat word</span></span><br><span class="line">            best_prob_for_last_word = best_probs[k,-<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># Store the unique integer ID of the POS tag</span></span><br><span class="line">            <span class="comment"># which is also the row number in best_probs</span></span><br><span class="line">            z[m - <span class="number">1</span>] = k</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># Convert the last word&#x27;s predicted POS tag</span></span><br><span class="line">    <span class="comment"># from its unique integer ID into the string representation</span></span><br><span class="line">    <span class="comment"># using the &#x27;states&#x27; dictionary</span></span><br><span class="line">    <span class="comment"># store this in the &#x27;pred&#x27; array for the last word</span></span><br><span class="line">    pred[m - <span class="number">1</span>] = states[k]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## Step 2 ##</span></span><br><span class="line">    <span class="comment"># Find the best POS tags by walking backward through the best_paths</span></span><br><span class="line">    <span class="comment"># From the last word in the corpus to the 0th word in the corpus</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m-<span class="number">1</span>, <span class="number">0</span>, -<span class="number">1</span>): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve the unique integer ID of</span></span><br><span class="line">        <span class="comment"># the POS tag for the word at position &#x27;i&#x27; in the corpus</span></span><br><span class="line">        pos_tag_for_word_i = best_paths[z[i], i]</span><br><span class="line"><span class="comment">#         print(z[i])</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># In best_paths, go to the row representing the POS tag of word i</span></span><br><span class="line">        <span class="comment"># and the column representing the word&#x27;s position in the corpus</span></span><br><span class="line">        <span class="comment"># to retrieve the predicted POS for the word at position i-1 in the corpus</span></span><br><span class="line">        z[i - <span class="number">1</span>] = pos_tag_for_word_i</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the previous word&#x27;s POS tag in string form</span></span><br><span class="line">        <span class="comment"># Use the &#x27;states&#x27; dictionary, </span></span><br><span class="line">        <span class="comment"># where the key is the unique integer ID of the POS tag,</span></span><br><span class="line">        <span class="comment"># and the value is the string representation of that POS tag</span></span><br><span class="line">        pred[i - <span class="number">1</span>] = states[pos_tag_for_word_i]        </span><br><span class="line">     <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run and test your function</span></span><br><span class="line">pred = viterbi_backward(best_probs, best_paths, prep, states)</span><br><span class="line">m=<span class="built_in">len</span>(pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The prediction for pred[-7:m-1] is: \n&#x27;</span>, prep[-<span class="number">7</span>:m-<span class="number">1</span>], <span class="string">&quot;\n&quot;</span>, pred[-<span class="number">7</span>:m-<span class="number">1</span>], <span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The prediction for pred[0:8] is: \n&#x27;</span>, pred[<span class="number">0</span>:<span class="number">7</span>], <span class="string">&quot;\n&quot;</span>, prep[<span class="number">0</span>:<span class="number">7</span>])</span><br></pre></td></tr></table></figure>
<pre><code>The prediction for pred[-7:m-1] is: 
 [&#39;see&#39;, &#39;them&#39;, &#39;here&#39;, &#39;with&#39;, &#39;us&#39;, &#39;.&#39;] 
 [&#39;VB&#39;, &#39;PRP&#39;, &#39;RB&#39;, &#39;IN&#39;, &#39;PRP&#39;, &#39;.&#39;] 

The prediction for pred[0:8] is: 
 [&#39;DT&#39;, &#39;NN&#39;, &#39;POS&#39;, &#39;NN&#39;, &#39;MD&#39;, &#39;VB&#39;, &#39;VBN&#39;] 
 [&#39;The&#39;, &#39;economy&#39;, &quot;&#39;s&quot;, &#39;temperature&#39;, &#39;will&#39;, &#39;be&#39;, &#39;taken&#39;]</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">The prediction <span class="keyword">for</span> prep[<span class="number">-7</span>:m<span class="number">-1</span>] is:  </span><br><span class="line"> [<span class="string">&#x27;see&#x27;</span>, <span class="string">&#x27;them&#x27;</span>, <span class="string">&#x27;here&#x27;</span>, <span class="string">&#x27;with&#x27;</span>, <span class="string">&#x27;us&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]  </span><br><span class="line"> [<span class="string">&#x27;VB&#x27;</span>, <span class="string">&#x27;PRP&#x27;</span>, <span class="string">&#x27;RB&#x27;</span>, <span class="string">&#x27;IN&#x27;</span>, <span class="string">&#x27;PRP&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]   </span><br><span class="line">The prediction <span class="keyword">for</span> pred[<span class="number">0</span>:<span class="number">8</span>] is:    </span><br><span class="line"> [<span class="string">&#x27;DT&#x27;</span>, <span class="string">&#x27;NN&#x27;</span>, <span class="string">&#x27;POS&#x27;</span>, <span class="string">&#x27;NN&#x27;</span>, <span class="string">&#x27;MD&#x27;</span>, <span class="string">&#x27;VB&#x27;</span>, <span class="string">&#x27;VBN&#x27;</span>]   </span><br><span class="line"> [<span class="string">&#x27;The&#x27;</span>, <span class="string">&#x27;economy&#x27;</span>, <span class="string">&quot;&#x27;s&quot;</span>, <span class="string">&#x27;temperature&#x27;</span>, <span class="string">&#x27;will&#x27;</span>, <span class="string">&#x27;be&#x27;</span>, <span class="string">&#x27;taken&#x27;</span>] </span><br></pre></td></tr></table></figure>
<p>Now you just have to compare the predicted labels to the true labels to evaluate your model on the accuracy metric!</p>
<p><a name='4'></a> # Part 4: Predicting on a data set</p>
<p>Compute the accuracy of your prediction by comparing it with the true <code>y</code> labels. - <code>pred</code> is a list of predicted POS tags corresponding to the words of the <code>test_corpus</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The third word is:&#x27;</span>, prep[<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Your prediction is:&#x27;</span>, pred[<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Your corresponding label y is: &#x27;</span>, y[<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<pre><code>The third word is: temperature
Your prediction is: NN
Your corresponding label y is:  temperature NN</code></pre>
<p><a name='ex-08'></a> ### Exercise 08</p>
<p>Implement a function to compute the accuracy of the viterbi algorithm's POS tag predictions. - To split y into the word and its tag you can use <code>y.split()</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: compute_accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span>(<span class="params">pred, y</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        pred: a list of the predicted parts-of-speech </span></span><br><span class="line"><span class="string">        y: a list of lines where each word is separated by a &#x27;\t&#x27; (i.e. word \t tag)</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    num_correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Zip together the prediction and the labels</span></span><br><span class="line">    <span class="keyword">for</span> prediction, y <span class="keyword">in</span> <span class="built_in">zip</span>(pred, y):</span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">        <span class="comment"># Split the label into the word and the POS tag</span></span><br><span class="line">        word_tag_tuple = y.strip().split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check that there is actually a word and a tag</span></span><br><span class="line">        <span class="comment"># no more and no less than 2 items</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(word_tag_tuple) &lt; <span class="number">2</span>: <span class="comment"># complete this line</span></span><br><span class="line">            <span class="keyword">continue</span> </span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">        <span class="comment"># store the word and tag separately</span></span><br><span class="line">        word, tag = [item.strip() <span class="keyword">for</span> item <span class="keyword">in</span> word_tag_tuple]</span><br><span class="line">        </span><br><span class="line"><span class="comment">#         print(tag, prediction)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check if the POS tag label matches the prediction</span></span><br><span class="line">        <span class="keyword">if</span> tag == prediction: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># count the number of times that the prediction</span></span><br><span class="line">            <span class="comment"># and label match</span></span><br><span class="line">            num_correct += <span class="number">1.0</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment"># keep track of the total number of examples (that have valid labels)</span></span><br><span class="line">        total += <span class="number">1.0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> num_correct/total</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Accuracy of the Viterbi algorithm is <span class="subst">&#123;compute_accuracy(pred, y):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy of the Viterbi algorithm is 0.9531</code></pre>
<h5 id="expected-output-7">Expected Output</h5>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy of the Viterbi algorithm is <span class="number">0.9531</span></span><br></pre></td></tr></table></figure>
<p>Congratulations you were able to classify the parts-of-speech with 95% accuracy.</p>
<h3 id="key-points-and-overview">Key Points and overview</h3>
<p>In this assignment you learned about parts-of-speech tagging. - In this assignment, you predicted POS tags by walking forward through a corpus and knowing the previous word. - There are other implementations that use bidirectional POS tagging. - Bidirectional POS tagging requires knowing the previous word and the next word in the corpus when predicting the current word's POS tag. - Bidirectional POS tagging would tell you more about the POS instead of just knowing the previous word. - Since you have learned to implement the unidirectional approach, you have the foundation to implement other POS taggers used in industry.</p>
<h3 id="references">References</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://web.stanford.edu/~jurafsky/slp3/">"Speech and Language Processing", Dan Jurafsky and James H. Martin</a></li>
<li>We would like to thank Melanie Tosik for her help and inspiration</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/ML-Interview-Computer-Vision/2020/05/29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/ML-Interview-Computer-Vision/2020/05/29/" class="post-title-link" itemprop="url">ML-Interview-Computer-Vision</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-05-29 07:41:05 / Modified: 12:43:09" itemprop="dateCreated datePublished" datetime="2020-05-29T07:41:05+08:00">2020-05-29</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Interview/" itemprop="url" rel="index"><span itemprop="name">Interview</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Interview/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/ML-Interview-Computer-Vision/2020/05/29/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/ML-Interview-Computer-Vision/2020/05/29/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>FCN, Unet, Mask R-CNN, YOLO</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/ML-Interview-Computer-Vision/2020/05/29/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/ML-Interview-Natural-Language-Processing/2020/05/28/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/ML-Interview-Natural-Language-Processing/2020/05/28/" class="post-title-link" itemprop="url">ML-Interview-Natural-Language-Processing</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-28 15:14:07" itemprop="dateCreated datePublished" datetime="2020-05-28T15:14:07+08:00">2020-05-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-29 06:50:59" itemprop="dateModified" datetime="2020-05-29T06:50:59+08:00">2020-05-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Interview/" itemprop="url" rel="index"><span itemprop="name">Interview</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Interview/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/ML-Interview-Natural-Language-Processing/2020/05/28/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/ML-Interview-Natural-Language-Processing/2020/05/28/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Natural language processing</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/ML-Interview-Natural-Language-Processing/2020/05/28/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/ML-Interview-Ensemble/2020/05/28/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/ML-Interview-Ensemble/2020/05/28/" class="post-title-link" itemprop="url">ML-Interview-Ensemble</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-28 10:07:46" itemprop="dateCreated datePublished" datetime="2020-05-28T10:07:46+08:00">2020-05-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-29 06:45:39" itemprop="dateModified" datetime="2020-05-29T06:45:39+08:00">2020-05-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Interview/" itemprop="url" rel="index"><span itemprop="name">Interview</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Interview/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/ML-Interview-Ensemble/2020/05/28/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/ML-Interview-Ensemble/2020/05/28/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>集成学习,Bagging,Boosting, Bias,Variance</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/ML-Interview-Ensemble/2020/05/28/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/ML-Interview-Deep-Learning/2020/05/27/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/ML-Interview-Deep-Learning/2020/05/27/" class="post-title-link" itemprop="url">ML-Interview-Deep-Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-27 17:46:36" itemprop="dateCreated datePublished" datetime="2020-05-27T17:46:36+08:00">2020-05-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-29 07:09:19" itemprop="dateModified" datetime="2020-05-29T07:09:19+08:00">2020-05-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Interview/" itemprop="url" rel="index"><span itemprop="name">Interview</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Interview/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/ML-Interview-Deep-Learning/2020/05/27/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/ML-Interview-Deep-Learning/2020/05/27/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>MLP, 神经网络训练技巧, CNN, RNN, Seq2Seq</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/ML-Interview-Deep-Learning/2020/05/27/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/ML-Interview-Optimization/2020/05/27/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/ML-Interview-Optimization/2020/05/27/" class="post-title-link" itemprop="url">ML-Interview-Optimization</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-27 07:19:28" itemprop="dateCreated datePublished" datetime="2020-05-27T07:19:28+08:00">2020-05-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-29 07:04:38" itemprop="dateModified" datetime="2020-05-29T07:04:38+08:00">2020-05-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Interview/" itemprop="url" rel="index"><span itemprop="name">Interview</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Interview/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/ML-Interview-Optimization/2020/05/27/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/ML-Interview-Optimization/2020/05/27/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>损失函数，优化算法</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/ML-Interview-Optimization/2020/05/27/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/ML-Interview-Unsupervised-Learning/2020/05/26/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/ML-Interview-Unsupervised-Learning/2020/05/26/" class="post-title-link" itemprop="url">ML-Interview-Unsupervised-Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-26 09:14:23" itemprop="dateCreated datePublished" datetime="2020-05-26T09:14:23+08:00">2020-05-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-29 06:49:19" itemprop="dateModified" datetime="2020-05-29T06:49:19+08:00">2020-05-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Interview/" itemprop="url" rel="index"><span itemprop="name">Interview</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Interview/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/ML-Interview-Unsupervised-Learning/2020/05/26/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/ML-Interview-Unsupervised-Learning/2020/05/26/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>无监督学习，K-Means，DBSCANS，Birch，GMM</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/ML-Interview-Unsupervised-Learning/2020/05/26/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">268</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail → mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'en' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
