<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="RUOCHI.AI">
<meta property="og:url" content="https://zhangruochi.com/page/3/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Ruochi Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhangruochi.com/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Markov-Decision-Processes/2020/09/04/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Markov-Decision-Processes/2020/09/04/" class="post-title-link" itemprop="url">Markov Decision Processes I</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-09-04 11:37:10" itemprop="dateCreated datePublished" datetime="2020-09-04T11:37:10+08:00">2020-09-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-07 13:55:54" itemprop="dateModified" datetime="2020-09-07T13:55:54+08:00">2020-09-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Markov-Decision-Processes/2020/09/04/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Markov-Decision-Processes/2020/09/04/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Lesson-1-Introduction-to-Markov-Decision-Processes"><a href="#Lesson-1-Introduction-to-Markov-Decision-Processes" class="headerlink" title="Lesson 1: Introduction to Markov Decision Processes"></a>Lesson 1: Introduction to Markov Decision Processes</h2><h3 id="Understand-Markov-Decision-Processes-or-MDPs"><a href="#Understand-Markov-Decision-Processes-or-MDPs" class="headerlink" title="Understand Markov Decision Processes, or MDPs"></a>Understand Markov Decision Processes, or MDPs</h3><p>MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards.</p>
<h3 id="Understand-the-graphical-representation-of-a-Markov-Decision-Process"><a href="#Understand-the-graphical-representation-of-a-Markov-Decision-Process" class="headerlink" title="Understand the graphical representation of a Markov Decision Process"></a>Understand the graphical representation of a Markov Decision Process</h3><p>MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal. The learner and decision maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the environment. These interact continually, the agent selecting actions and the environment responding to these actions and presenting new situations to the agent.1 The environment also gives rise to rewards, special numerical values that the agent seeks to maximize over time<br>through its choice of actions.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="1.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">The agent–environment interaction in a Markov decision process.</div>
</center>

<h3 id="Describe-how-the-dynamics-of-an-MDP-are-defined"><a href="#Describe-how-the-dynamics-of-an-MDP-are-defined" class="headerlink" title="Describe how the dynamics of an MDP are defined"></a>Describe how the dynamics of an MDP are defined</h3><p>In a finite MDP, the sets of states, actions, and rewards (S, A and R) all  have a finite number of elements. In this case, the random variables $R_t$ and $S_t$ have well defined<br>discrete probability distributions dependent only on the preceding state and action. That is, for particular values of these random variables, $s\prime \in S$ and $r \in R$, there is a probability of those values occurring at time t, given particular values of the preceding state and action:</p>
<p>$$p(s\prime, r | s, a) = Pr{S_t = s\prime, R_t = r | S_{t-1} = s, A_{t-1} = a }$$</p>
<p>$$\sum_{s\prime \in s}\sum_{r \in R} p(s\prime, r | s, a) = 1, \text{for all} , s\in S, a \in A_{(s)}$$</p>
<p>for all $s\prime, s \in S, r \in R \text{and} a \in A_{(s)}$.  The function p defines the dynamics of the MDP. The dot over the equals sign in the equation reminds us that it is a definition (in this case of the function p) rather than a fact that follows from previous definitions. The dynamics function $p: S x R x S x A \rightarrow [0, 1]$ is an ordinary deterministic function of four arguments.</p>
<p>In a Markov decision process, the probabilities given by p completely characterize the environment’s dynamics. That is, the probability of each possible value for $S_t$ and $A_t$, and, given them, not at all on earlier states and actions. This is best viewed a restriction not on the decision process, but on the state.  The state must include information about all aspects of the past agent–environment interaction that make a difference for the future. If it does, then the state is said to have the Markov property.</p>
<p>From the four-argument dynamics function, p, one can compute anything else one might want to know about the environment, such as the <strong>state-transition probabilities</strong> (which we denote, with a slight abuse of notation, as a three-argument function<br>$p: S x S x A \rightarrow [0, 1]$)</p>
<p>$$p(s\prime | s, a) = Pr{S_t = s\prime | S_{t-1} = s, A_{t-1} = a } = \sum_{r \in R}p(s\prime, r | s, a)$$</p>
<p>We can also compute the expected rewards for state–action pairs as a two-argument function $r : S x A \rightarrow \mathbb{R}$</p>
<p>$$r(s,a) = \mathbb{R} [R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in R} r \sum_{s\prime \in S} p(s\prime, r | s, a)$$</p>
<p>and the expected rewards for <strong>state–action–next-state</strong> triples as a three-argument function $r: S x A x S \rightarrow \mathbb{R}$</p>
<p>$$r(s,a,s\prime) = \mathbb{E} [R_t | S_{t-1} = s, A_{t-1} = a, S_t = s\prime] = \sum_{r \in R} r \frac{ p(s\prime, r | s, a) }{p(s\prime | s, a)}$$</p>
<h3 id="Explain-how-many-diverse-processes-can-be-written-in-terms-of-the-MDP-framework"><a href="#Explain-how-many-diverse-processes-can-be-written-in-terms-of-the-MDP-framework" class="headerlink" title="Explain how many diverse processes can be written in terms of the MDP framework"></a>Explain how many diverse processes can be written in terms of the MDP framework</h3><p>The MDP framework is abstract and flexible and can be applied to many di↵erent problems in many di↵erent ways.  </p>
<ul>
<li>In general, actions can be any decisions we want to learn how to make, and<br>the states can be anything we can know that might be useful in making them. </li>
<li>The general rule we follow is that anything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment.  </li>
<li>The agent–environment boundary represents the limit of the agent’s absolute control, not of its knowledge.</li>
</ul>
<p>The MDP framework is a considerable abstraction of the problem of goal-directed learning from interaction. It proposes that whatever the details of the sensory, memory, and control apparatus, and whatever objective one is trying to achieve, any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment: one signal to represent the choices made by the agent (the <strong>actions</strong>), one signal to represent the basis on which the choices are made (the <strong>states</strong>), and one signal to define the agent’s goal (the <strong>rewards</strong>).</p>
<h2 id="Lesson-2-Goal-of-Reinforcement-Learning"><a href="#Lesson-2-Goal-of-Reinforcement-Learning" class="headerlink" title="Lesson 2: Goal of Reinforcement Learning"></a>Lesson 2: Goal of Reinforcement Learning</h2><h3 id="Describe-how-rewards-relate-to-the-goal-of-an-agent"><a href="#Describe-how-rewards-relate-to-the-goal-of-an-agent" class="headerlink" title="Describe how rewards relate to the goal of an agent"></a>Describe how rewards relate to the goal of an agent</h3><p>Informally, the agent’s goal is to maximize the total amount of reward it receives. This means maximizing not immediate reward, but cumulative reward in the long run. We can clearly state this informal idea as the reward hypothesis:</p>
<blockquote>
<p>That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).</p>
</blockquote>
<h3 id="Understand-episodes-and-identify-episodic-tasks"><a href="#Understand-episodes-and-identify-episodic-tasks" class="headerlink" title="Understand episodes and identify episodic tasks"></a>Understand episodes and identify episodic tasks</h3><p>when the agent–environment interaction breaks naturally into <strong>subsequences</strong>, which we call episodes,7 such as plays of a game, trips through a maze, or any sort of repeated interaction. Each episode ends in a special state called the <strong>terminal state</strong>, followed by a <strong>reset</strong> to a standard starting state or to a sample from a standard distribution of starting states.</p>
<p>In general, we seek to maximize the expected return, where the return, denoted $G_t$,  is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards:</p>
<p>$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \cdots + R_{T}$$</p>
<p>where T is a final time step.</p>
<h2 id="Lesson-3-Continuing-Tasks"><a href="#Lesson-3-Continuing-Tasks" class="headerlink" title="Lesson 3: Continuing Tasks"></a>Lesson 3: Continuing Tasks</h2><h3 id="Formulate-returns-for-continuing-tasks-using-discounting"><a href="#Formulate-returns-for-continuing-tasks-using-discounting" class="headerlink" title="Formulate returns for continuing tasks using discounting"></a>Formulate returns for continuing tasks using discounting</h3><p>In many cases the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit.  For example, this would be the natural way to formulate an on-going process-control task, or an application to a robot with a long life span. We call these continuing tasks. The return formulation is problematic for continuing tasks because the final time step would be $T = \infty$ and the return, which is what we are trying to maximize, could itself easily be infinite.</p>
<p>The additional concept that we need is that of <strong>discounting</strong>. According to this approach, the agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses $A_t$ to maximize the expected discounted return:</p>
<p>$$G_t = R_{t+1} + \eta R_{t+2} + \eta^2 R_{t+3} + \cdots  = \sum_{k=0}^{\infty} \eta^k R_{t+k+1}$$</p>
<p>where \eta is a parameter, $ 0 \leq \eta \leq 1$, called the discount rate.</p>
<p>The discount rate determines the present value of future rewards: a reward received k time steps in the future is worth only $\eta^{k-1}$ times what it would be worth if it were received immediately. If $\eta &lt; 1$, the infinite sum has a finite value as long as the reward sequence ${ R_k }$ is <strong>bounded</strong>.</p>
<p>Returns at successive time steps are related to each other in a way that is important for the theory and algorithms of reinforcement learning:</p>
<p>$$G_t =  R_{t+1} + \eta (R_{t+2} + \eta R_{t+3} + \eta^2 R_{t+4} + \cdots) = R_{t+1} + \eta G_{t+1}$$</p>
<h3 id="Describe-how-returns-at-successive-time-steps-are-related-to-each-other"><a href="#Describe-how-returns-at-successive-time-steps-are-related-to-each-other" class="headerlink" title="Describe how returns at successive time steps are related to each other"></a>Describe how returns at successive time steps are related to each other</h3><p>If $\eta = 0$, the agent is “myopic” in being concerned only with maximizing immediate rewards: its objective in this case is to learn how to choose At so as to maximize only $R_{t+1}$. If each of the agent’s actions happened to influence only the immediate reward, not future rewards as well, then a myopic agent could maximize by separately maximizing each immediate reward. But in general, acting to maximize immediate reward can reduce access to future rewards so that the return is reduced. As $\eta$ approaches 1, the return objective takes future rewards into account more strongly; the agent becomes more farsighted.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/The-K-Armed-Bandit-Problem/2020/09/03/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/The-K-Armed-Bandit-Problem/2020/09/03/" class="post-title-link" itemprop="url">The K-Armed Bandit Problem</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-09-03 18:55:55 / Modified: 19:10:47" itemprop="dateCreated datePublished" datetime="2020-09-03T18:55:55+08:00">2020-09-03</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/The-K-Armed-Bandit-Problem/2020/09/03/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/The-K-Armed-Bandit-Problem/2020/09/03/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Lesson-1-The-K-Armed-Bandit-Problem"><a href="#Lesson-1-The-K-Armed-Bandit-Problem" class="headerlink" title="Lesson 1: The K-Armed Bandit Problem"></a>Lesson 1: The K-Armed Bandit Problem</h2><h3 id="Define-reward"><a href="#Define-reward" class="headerlink" title="Define reward"></a>Define reward</h3><p>In the k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the <strong>value</strong> of that action. We denote the action selected on time step t as $A_t$, and the corresponding reward as $R_t$. The value then<br>of an arbitrary action a, denoted $q\ast(a)$, is the expected reward given that a is selected:</p>
<p>$$q\ast(a) = \mathbb{E} [R_t | A_t = a ]$$</p>
<p>We denote the estimated value of action a at time step t as $Q_t(a)$. We would like $Q_t(a)$ to be close<br>to $q\ast(a)$.</p>
<h3 id="Understand-the-temporal-nature-of-the-bandit-problem"><a href="#Understand-the-temporal-nature-of-the-bandit-problem" class="headerlink" title="Understand the temporal nature of the bandit problem"></a>Understand the temporal nature of the bandit problem</h3><h3 id="Define-k-armed-bandit"><a href="#Define-k-armed-bandit" class="headerlink" title="Define k-armed bandit"></a>Define k-armed bandit</h3><p>Consider the following learning problem. You are faced repeatedly with a choice among k di↵erent options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.</p>
<h3 id="Define-action-values"><a href="#Define-action-values" class="headerlink" title="Define action-values"></a>Define action-values</h3><p>We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods.</p>
<h2 id="Lesson-2-What-to-Learn-Estimating-Action-Values"><a href="#Lesson-2-What-to-Learn-Estimating-Action-Values" class="headerlink" title="Lesson 2: What to Learn? Estimating Action Values"></a>Lesson 2: What to Learn? Estimating Action Values</h2><h3 id="Define-action-value-estimation-methods"><a href="#Define-action-value-estimation-methods" class="headerlink" title="Define action-value estimation methods"></a>Define action-value estimation methods</h3><p>One natural way to estimate this is by averaging the rewards<br>actually received:</p>
<p>$$Q_t(a) = \frac{\text{sum of rewards when a taken prior to t} }{\text{ number of times a taken prior to t} }$$</p>
<h3 id="Define-exploration-and-exploitation"><a href="#Define-exploration-and-exploitation" class="headerlink" title="Define exploration and exploitation"></a>Define exploration and exploitation</h3><p>If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are <strong>exploiting</strong> your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are <strong>exploring</strong>, because this enables you to improve your estimate of the nongreedy action’s value.</p>
<p>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</p>
<h3 id="Select-actions-greedily-using-an-action-value-function"><a href="#Select-actions-greedily-using-an-action-value-function" class="headerlink" title="Select actions greedily using an action-value function"></a>Select actions greedily using an action-value function</h3><p>$$A_t = argmax_a Q_t(a)$$</p>
<h3 id="Define-online-learning"><a href="#Define-online-learning" class="headerlink" title="Define online learning"></a>Define online learning</h3><p>$$Q_n = \frac{ R_1 + R_2 + \cdots + R_{n-1} }{ n - 1 }$$</p>
<p>The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, if this is done, then the memory and computational requirements would grow over time as more rewards are seen. Each additional reward would require additional memory to store it and additional computation to compute the sum in the numerator.</p>
<p>As you might suspect, this is not really necessary. It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward.</p>
<p>$$Q_(n+1) = Q_n + \frac{1}{n}[ R_n - Q_n ]$$</p>
<h3 id="Define-the-general-online-update-equation"><a href="#Define-the-general-online-update-equation" class="headerlink" title="Define the general online update equation"></a>Define the general online update equation</h3><p>$$\text{NewEstimate} = \text{OldEstimate} + \text{StepSize} [ \text{target} - \text{OldEstimate} ]$$</p>
<h3 id="Understand-why-we-might-use-a-constant-stepsize-in-the-case-of-non-stationarity"><a href="#Understand-why-we-might-use-a-constant-stepsize-in-the-case-of-non-stationarity" class="headerlink" title="Understand why we might use a constant stepsize in the case of non-stationarity"></a>Understand why we might use a constant stepsize in the case of non-stationarity</h3><p>The averaging methods discussed so far are appropriate for stationary bandit problems, that is, for bandit problems in which the reward probabilities do not change over time. As noted earlier, we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to give more weight to recent rewards than to long-past rewards. One of the most popular ways of doing this is to use a constant step-size parameter.</p>
<p>$$Q_(n+1) = Q_n + \alpha [ R_n - Q_n ]$$</p>
<p>where the step-size parameter $ \alpha \in (0, 1]$ is constant.</p>
<p>$$Q_{n+1} = (1 - \alpha)^n Q_1 + \sum^{n}_{i=1}\alpha(1 - \alpha)^{n-i}R_i $$</p>
<p>Note that the weight, $\alpha(1 - \alpha)^{n-i}$ given the reward $R_i$ depends on how many rewards ago, $n-1$ it was observed. The quantity $1-\alpha$<br>is less than 1, and thus the weight given to $R_i$ decreases as the number of intervening rewards increases.</p>
<h2 id="Lesson-3-Exploration-vs-Exploitation-Tradeoff"><a href="#Lesson-3-Exploration-vs-Exploitation-Tradeoff" class="headerlink" title="Lesson 3: Exploration vs. Exploitation Tradeoff"></a>Lesson 3: Exploration vs. Exploitation Tradeoff</h2><h3 id="Define-epsilon-greedy"><a href="#Define-epsilon-greedy" class="headerlink" title="Define epsilon-greedy"></a>Define epsilon-greedy</h3><p>Greedy action selection always exploits current<br>knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability $\alpha$ instead select randomly from among all the actions with equal probability, independently of the action-value estimates.  We call methods using this near-greedy action selection rule $\alpha$-greedy methods.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">current_action = argmax(self.q_values) <span class="keyword">if</span> _ &gt;= self.epsilon <span class="keyword">else</span> np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(self.q_values))</span><br></pre></td></tr></table></figure>

<h3 id="Understand-optimistic-initial-values-Describe-the-benefits-of-optimistic-initial-values-for-early-exploration"><a href="#Understand-optimistic-initial-values-Describe-the-benefits-of-optimistic-initial-values-for-early-exploration" class="headerlink" title="Understand optimistic initial values, Describe the benefits of optimistic initial values for early exploration"></a>Understand optimistic initial values, Describe the benefits of optimistic initial values for early exploration</h3><p>Initial action values can also be used as a simple way to encourage exploration. Suppose that instead of setting the initial action values to zero, as we did in the 10-armed testbed,we set them all to +5. Recall that the $q\ast(a)$ in this problem are selected from a normal distribution with mean 0 and variance 1. An initial estimate of +5 is thus wildly optimistic. But this optimism encourages action-value methods to explore. Whichever actions are initially selected, the reward is less than the starting estimates; the learner switches to other actions, being <strong>disappointed</strong> with the rewards it is receiving. The result is that all actions are tried several times before the value estimates converge. The system does a fair amount of exploration even if greedy actions are selected all the time.</p>
<p><img src="initial_values.png"></p>
<h3 id="Explain-the-criticisms-of-optimistic-initial-values"><a href="#Explain-the-criticisms-of-optimistic-initial-values" class="headerlink" title="Explain the criticisms of optimistic initial values"></a>Explain the criticisms of optimistic initial values</h3><p>We call this technique for encouraging exploration optimistic initial values. We regard it as a simple trick that can be quite effective on stationary problems, but it is far from being a generally useful approach to encouraging exploration. For example, it is not well suited to nonstationary problems because its drive for exploration is inherently temporary. If the task changes, creating a renewed need for exploration, this method cannot help. Indeed, any method that focuses on the initial conditions in any special way is unlikely to help with the general nonstationary case. The beginning of time occurs only once, and thus we should not focus on it too much. This criticism applies as well to the sample-average methods, which also treat the beginning of time as a special event, averaging all subsequent rewards with equal weights.</p>
<h3 id="Describe-the-upper-confidence-bound-action-selection-method"><a href="#Describe-the-upper-confidence-bound-action-selection-method" class="headerlink" title="Describe the upper confidence bound action selection method"></a>Describe the upper confidence bound action selection method</h3><p>It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates.One effective way of doing this is to select actions according to</p>
<p>$$A_t = argmax_a [ Q_t(a) + c\sqrt{\frac{\ln t}{ N_t(a)}} ]$$</p>
<h3 id="Define-optimism-in-the-face-of-uncertainty"><a href="#Define-optimism-in-the-face-of-uncertainty" class="headerlink" title="Define optimism in the face of uncertainty"></a>Define optimism in the face of uncertainty</h3><p>The idea of this upper confidence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value  The quantity being max’ed over is thus a sort of upper bound on the possible true value of action $a$, with $c$ determining the confidence level. Each time $a$ is selected the uncertainty is presumably reduced: $N_t(a)$ increments, and, as it appears in the denominator, the uncertainty term decreases. On the other hand, each time an action other than a is selected, $t$ increases but $N_t(a)$ does not; because $t$ appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates,or that have already been selected frequently, will be selected with decreasing frequency over time.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Bandits-and-Exploration-Exploitation/2020/09/03/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Bandits-and-Exploration-Exploitation/2020/09/03/" class="post-title-link" itemprop="url">Bandits and Exploration/Exploitation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-09-03 17:59:35 / Modified: 18:00:48" itemprop="dateCreated datePublished" datetime="2020-09-03T17:59:35+08:00">2020-09-03</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Bandits-and-Exploration-Exploitation/2020/09/03/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Bandits-and-Exploration-Exploitation/2020/09/03/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Assignment-1-Bandits-and-Exploration-Exploitation"><a href="#Assignment-1-Bandits-and-Exploration-Exploitation" class="headerlink" title="Assignment 1: Bandits and Exploration/Exploitation"></a>Assignment 1: Bandits and Exploration/Exploitation</h1><p>Welcome to Assignment 1. This notebook will:</p>
<ul>
<li>Help you create your first bandit algorithm</li>
<li>Help you understand the effect of epsilon on exploration and learn about the exploration/exploitation tradeoff</li>
<li>Introduce you to some of the reinforcement learning software we are going to use for this specialization</li>
</ul>
<p>This class uses RL-Glue to implement most of our experiments. It was originally designed by Adam White, Brian Tanner, and Rich Sutton. This library will give you a solid framework to understand how reinforcement learning experiments work and how to run your own. If it feels a little confusing at first, don’t worry - we are going to walk you through it slowly and introduce you to more and more parts as you progress through the specialization.</p>
<p>We are assuming that you have used a Jupyter notebook before. But if not, it is quite simple. Simply press the run button, or shift+enter to run each of the cells. The places in the code that you need to fill in will be clearly marked for you.</p>
<h2 id="Section-0-Preliminaries"><a href="#Section-0-Preliminaries" class="headerlink" title="Section 0: Preliminaries"></a>Section 0: Preliminaries</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rlglue.rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">import</span> main_agent</span><br><span class="line"><span class="keyword">import</span> ten_arm_env</span><br><span class="line"><span class="keyword">import</span> test_env</span><br></pre></td></tr></table></figure>

<p>In the above cell, we import the libraries we need for this assignment. We use numpy throughout the course and occasionally provide hints for which methods to use in numpy. Other than that we mostly use vanilla python and the occasional other library, such as matplotlib for making plots.</p>
<p>You might have noticed that we import ten_arm_env. This is the <strong>10-armed Testbed</strong> introduced in <a target="_blank" rel="noopener" href="http://www.incompleteideas.net/book/RLbook2018.pdf">section 2.3</a> of the textbook. We use this throughout this notebook to test our bandit agents. It has 10 arms, which are the actions the agent can take. Pulling an arm generates a stochastic reward from a Gaussian distribution with unit-variance. For each action, the expected value of that action is randomly sampled from a normal distribution, at the start of each run. If you are unfamiliar with the 10-armed Testbed please review it in the textbook before continuing.</p>
<p><strong>DO NOT IMPORT OTHER LIBRARIES as this will break the autograder.</strong></p>
<p><strong>DO NOT SET A RANDOM SEED as this will break the autograder.</strong></p>
<h2 id="Section-1-Greedy-Agent"><a href="#Section-1-Greedy-Agent" class="headerlink" title="Section 1: Greedy Agent"></a>Section 1: Greedy Agent</h2><p>We want to create an agent that will find the action with the highest expected reward. One way an agent could operate is to always choose the action with  the highest value based on the agent’s current estimates. This is called a greedy agent as it greedily chooses the action that it thinks has the highest value. Let’s look at what happens in this case.</p>
<p>First we are going to implement the argmax function, which takes in a list of action values and returns an action with the highest value. Why are we implementing our own instead of using the argmax function that numpy uses? Numpy’s argmax function returns the first instance of the highest value. We do not want that to happen as it biases the agent to choose a specific action in the case of ties. Instead we want to break ties between the highest values randomly. So we are going to implement our own argmax function. You may want to look at <a target="_blank" rel="noopener" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html">np.random.choice</a> to randomly select from a list of values.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span>(<span class="params">q_values</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Takes in a list of q_values and returns the index of the item </span></span><br><span class="line"><span class="string">    with the highest value. Breaks ties randomly.</span></span><br><span class="line"><span class="string">    returns: int - the index of the highest value in q_values</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    top_value = <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>)</span><br><span class="line">    ties = []</span><br><span class="line">    </span><br><span class="line">    top_value = <span class="built_in">max</span>(q_values)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(q_values)):</span><br><span class="line">        <span class="comment"># if a value in q_values is greater than the highest value update top and reset ties to zero</span></span><br><span class="line">        <span class="comment"># if a value is equal to top value add the index to ties</span></span><br><span class="line">        <span class="comment"># return a random selection from ties.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        <span class="keyword">if</span> q_values[i] == top_value:</span><br><span class="line">            ties.append(i)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> np.random.choice(ties)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line">test_array = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">assert</span> argmax(test_array) == <span class="number">8</span>, <span class="string">&quot;Check your argmax implementation returns the index of the largest value&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure np.random.choice is called correctly</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">test_array = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> argmax(test_array) == <span class="number">0</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">test_array = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">assert</span> argmax(test_array) == <span class="number">8</span>, <span class="string">&quot;Check your argmax implementation returns the index of the largest value&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set random seed so results are deterministic</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">test_array = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">counts = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = argmax(test_array)</span><br><span class="line">    counts[a] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure argmax does not always choose first entry</span></span><br><span class="line"><span class="keyword">assert</span> counts[<span class="number">0</span>] != <span class="number">100</span>, <span class="string">&quot;Make sure your argmax implementation randomly choooses among the largest values.&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure argmax does not always choose last entry</span></span><br><span class="line"><span class="keyword">assert</span> counts[<span class="number">3</span>] != <span class="number">100</span>, <span class="string">&quot;Make sure your argmax implementation randomly choooses among the largest values.&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the random number generator is called exactly once whenver `argmax` is called</span></span><br><span class="line">expected = [<span class="number">44</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">56</span>] <span class="comment"># &lt;-- notice not perfectly uniform due to randomness</span></span><br><span class="line"><span class="keyword">assert</span> counts == expected</span><br></pre></td></tr></table></figure>

<p>Now we introduce the first part of an RL-Glue agent that you will implement. Here we are going to create a GreedyAgent and implement the agent_step method. This method gets called each time the agent takes a step. The method has to return the action selected by the agent. This method also ensures the agent’s estimates are updated based on the signals it gets from the environment.</p>
<p>Fill in the code below to implement a greedy agent.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GreedyAgent</span>(<span class="params">main_agent.Agent</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, observation</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Takes one step for the agent. It takes in a reward and observation and </span></span><br><span class="line"><span class="string">        returns the action the agent chooses at that time step.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">        reward -- float, the reward the agent recieved from the environment after taking the last action.</span></span><br><span class="line"><span class="string">        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it</span></span><br><span class="line"><span class="string">                              until future lessons</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        current_action -- int, the action chosen by the agent at the current time step.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">### Useful Class Variables ###</span></span><br><span class="line">        <span class="comment"># self.q_values : An array with what the agent believes each of the values of the arm are.</span></span><br><span class="line">        <span class="comment"># self.arm_count : An array with a count of the number of times each arm has been pulled.</span></span><br><span class="line">        <span class="comment"># self.last_action : The action that the agent took on the previous time step</span></span><br><span class="line">        <span class="comment">#######################</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update Q values Hint: Look at the algorithm in section 2.4 of the textbook.</span></span><br><span class="line">        <span class="comment"># increment the counter in self.arm_count for the action from the previous time step</span></span><br><span class="line">        <span class="comment"># update the step size using self.arm_count</span></span><br><span class="line">        <span class="comment"># update self.q_values for the action from the previous time step</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        </span><br><span class="line">        self.arm_count[self.last_action] += <span class="number">1</span></span><br><span class="line">        self.q_values[self.last_action] = self.q_values[self.last_action] + (<span class="number">1.0</span> / self.arm_count[self.last_action]) * (reward - self.q_values[self.last_action])</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># current action = ? # Use the argmax function you created above</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">        current_action = argmax(self.q_values)</span><br><span class="line">        self.last_action = current_action</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> current_action</span><br><span class="line">        </span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># build a fake agent for testing and set some initial conditions</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">greedy_agent = GreedyAgent()</span><br><span class="line">greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">greedy_agent.arm_count = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">greedy_agent.last_action = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">action = greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the q_values were updated correctly</span></span><br><span class="line"><span class="keyword">assert</span> greedy_agent.q_values == [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the agent is using the argmax that breaks ties randomly</span></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">2</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lock</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># build a fake agent for testing and set some initial conditions</span></span><br><span class="line">greedy_agent = GreedyAgent()</span><br><span class="line">greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">greedy_agent.arm_count = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">greedy_agent.last_action = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># take a fake agent step</span></span><br><span class="line">action = greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure agent took greedy action</span></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure q_values were updated correctly</span></span><br><span class="line"><span class="keyword">assert</span> greedy_agent.q_values == [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>Let’s visualize the result. Here we run an experiment using RL-Glue to test our agent. For now, we will set up the experiment code; in future lessons, we will walk you through running experiments so that you can create your own.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">num_runs = <span class="number">200</span>                    <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">num_steps = <span class="number">1000</span>                  <span class="comment"># The number of pulls of each arm the agent takes</span></span><br><span class="line">env = ten_arm_env.Environment     <span class="comment"># We set what environment we want to use to test</span></span><br><span class="line">agent = GreedyAgent               <span class="comment"># We choose what agent we want to use</span></span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">10</span>&#125;  <span class="comment"># We pass the agent the information it needs. Here how many arms there are.</span></span><br><span class="line">env_info = &#123;&#125;                     <span class="comment"># We pass the environment the information it needs. In this case nothing.</span></span><br><span class="line"></span><br><span class="line">all_averages = []</span><br><span class="line"></span><br><span class="line">average_best = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_runs)):           <span class="comment"># tqdm is what creates the progress bar below</span></span><br><span class="line">    np.random.seed(run)</span><br><span class="line">    </span><br><span class="line">    rl_glue = RLGlue(env, agent)          <span class="comment"># Creates a new RLGlue experiment with the env and agent we chose above</span></span><br><span class="line">    rl_glue.rl_init(agent_info, env_info) <span class="comment"># We pass RLGlue what it needs to initialize the agent and environment</span></span><br><span class="line">    rl_glue.rl_start()                    <span class="comment"># We start the experiment</span></span><br><span class="line"></span><br><span class="line">    average_best += np.<span class="built_in">max</span>(rl_glue.environment.arms)</span><br><span class="line">    </span><br><span class="line">    scores = [<span class="number">0</span>]</span><br><span class="line">    averages = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">        reward, _, action, _ = rl_glue.rl_step() <span class="comment"># The environment and agent take a step and return</span></span><br><span class="line">                                                 <span class="comment"># the reward, and action taken.</span></span><br><span class="line">        scores.append(scores[-<span class="number">1</span>] + reward)</span><br><span class="line">        averages.append(scores[-<span class="number">1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">    all_averages.append(averages)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">&#x27;w&#x27;</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.plot([average_best / num_runs <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_steps)], linestyle=<span class="string">&quot;--&quot;</span>)</span><br><span class="line">plt.plot(np.mean(all_averages, axis=<span class="number">0</span>))</span><br><span class="line">plt.legend([<span class="string">&quot;Best Possible&quot;</span>, <span class="string">&quot;Greedy&quot;</span>])</span><br><span class="line">plt.title(<span class="string">&quot;Average Reward of Greedy Agent&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Steps&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Average reward&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line">greedy_scores = np.mean(all_averages, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 200/200 [00:03&lt;00:00, 63.68it/s]
</code></pre>
<p><img src="output_15_1.png" alt="png"></p>
<p>How did our agent do? Is it possible for it to do better?</p>
<h2 id="Section-2-Epsilon-Greedy-Agent"><a href="#Section-2-Epsilon-Greedy-Agent" class="headerlink" title="Section 2: Epsilon-Greedy Agent"></a>Section 2: Epsilon-Greedy Agent</h2><p>We learned about <a target="_blank" rel="noopener" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/tHDck/what-is-the-trade-off">another way for an agent to operate</a>, where it does not always take the greedy action. Instead, sometimes it takes an exploratory action. It does this so that it can find out what the best action really is. If we always choose what we think is the current best action is, we may miss out on taking the true best action, because we haven’t explored enough times to find that best action.</p>
<p>Implement an epsilon-greedy agent below. Hint: we are implementing the algorithm from <a target="_blank" rel="noopener" href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=52">section 2.4</a> of the textbook. You may want to use your greedy code from above and look at <a target="_blank" rel="noopener" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.random.html">np.random.random</a>, as well as <a target="_blank" rel="noopener" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randint.html">np.random.randint</a>, to help you select random actions. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EpsilonGreedyAgent</span>(<span class="params">main_agent.Agent</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, observation</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Takes one step for the agent. It takes in a reward and observation and </span></span><br><span class="line"><span class="string">        returns the action the agent chooses at that time step.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">        reward -- float, the reward the agent recieved from the environment after taking the last action.</span></span><br><span class="line"><span class="string">        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it</span></span><br><span class="line"><span class="string">                              until future lessons</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        current_action -- int, the action chosen by the agent at the current time step.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### Useful Class Variables ###</span></span><br><span class="line">        <span class="comment"># self.q_values : An array with what the agent believes each of the values of the arm are.</span></span><br><span class="line">        <span class="comment"># self.arm_count : An array with a count of the number of times each arm has been pulled.</span></span><br><span class="line">        <span class="comment"># self.last_action : The action that the agent took on the previous time step</span></span><br><span class="line">        <span class="comment"># self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1)</span></span><br><span class="line">        <span class="comment">#######################</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update Q values - this should be the same update as your greedy agent above</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line"><span class="comment">#         raise NotImplementedError()</span></span><br><span class="line">        self.arm_count[self.last_action] += <span class="number">1</span></span><br><span class="line">        self.q_values[self.last_action] = self.q_values[self.last_action] + (<span class="number">1.0</span> / self.arm_count[self.last_action]) * (reward - self.q_values[self.last_action])</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy</span></span><br><span class="line">        <span class="comment"># Randomly choose a number between 0 and 1 and see if it&#x27;s less than self.epsilon</span></span><br><span class="line">        <span class="comment"># (hint: look at np.random.random()). If it is, set current_action to a random action.</span></span><br><span class="line">        <span class="comment"># otherwise choose current_action greedily as you did above.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line"><span class="comment">#         raise NotImplementedError()</span></span><br><span class="line"></span><br><span class="line">        _ = np.random.random()</span><br><span class="line">        </span><br><span class="line">        current_action = argmax(self.q_values) <span class="keyword">if</span> _ &gt;= self.epsilon <span class="keyword">else</span> np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(self.q_values))</span><br><span class="line">        </span><br><span class="line">        self.last_action = current_action</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> current_action</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># build a fake agent for testing and set some initial conditions</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">e_greedy_agent = EpsilonGreedyAgent()</span><br><span class="line">e_greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0.0</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.arm_count = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.num_actions = <span class="number">5</span></span><br><span class="line">e_greedy_agent.last_action = <span class="number">1</span></span><br><span class="line">e_greedy_agent.epsilon = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># given this random seed, we should see a greedy action (action 2) here</span></span><br><span class="line">action = e_greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------</span></span><br><span class="line"><span class="comment"># we&#x27;ll try to guess a few of the trickier places</span></span><br><span class="line"><span class="comment"># -----------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure to update for the *last_action* not the current action</span></span><br><span class="line"><span class="keyword">assert</span> e_greedy_agent.q_values != [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">&quot;A&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the stepsize is based on the *last_action* not the current action</span></span><br><span class="line"><span class="keyword">assert</span> e_greedy_agent.q_values != [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">&quot;B&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the agent is using the argmax that breaks ties randomly</span></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">2</span>, <span class="string">&quot;C&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># let&#x27;s see what happens for another action</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">e_greedy_agent = EpsilonGreedyAgent()</span><br><span class="line">e_greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.arm_count = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.num_actions = <span class="number">5</span></span><br><span class="line">e_greedy_agent.last_action = <span class="number">1</span></span><br><span class="line">e_greedy_agent.epsilon = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># given this random seed, we should see a random action (action 4) here</span></span><br><span class="line">action = e_greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The agent saw a reward of 1, so should increase the value for *last_action*</span></span><br><span class="line"><span class="keyword">assert</span> e_greedy_agent.q_values == [<span class="number">0</span>, <span class="number">0.75</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">&quot;D&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the agent should have picked a random action for this particular random seed</span></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">4</span>, <span class="string">&quot;E&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">e_greedy_agent = EpsilonGreedyAgent()</span><br><span class="line">e_greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.arm_count = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">e_greedy_agent.num_actions = <span class="number">5</span></span><br><span class="line">e_greedy_agent.last_action = <span class="number">1</span></span><br><span class="line">e_greedy_agent.epsilon = <span class="number">0.5</span></span><br><span class="line">action = e_greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> e_greedy_agent.q_values == [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># manipulate the random seed so the agent takes a random action</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">action = e_greedy_agent.agent_step(reward=<span class="number">0</span>, observation=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># check to make sure we update value for action 4</span></span><br><span class="line">action = e_greedy_agent.agent_step(reward=<span class="number">1</span>, observation=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> e_greedy_agent.q_values == [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">0.0</span>, <span class="number">0</span>, <span class="number">1.0</span>]</span><br></pre></td></tr></table></figure>

<p>Now that we have our epsilon greedy agent created. Let’s compare it against the greedy agent with epsilon of 0.1.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot Epsilon greedy results and greedy results</span></span><br><span class="line">num_runs = <span class="number">200</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line">epsilon = <span class="number">0.1</span></span><br><span class="line">agent = EpsilonGreedyAgent</span><br><span class="line">env = ten_arm_env.Environment</span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">10</span>, <span class="string">&quot;epsilon&quot;</span>: epsilon&#125;</span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line">all_averages = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_runs)):</span><br><span class="line">    np.random.seed(run)</span><br><span class="line">    </span><br><span class="line">    rl_glue = RLGlue(env, agent)</span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">    rl_glue.rl_start()</span><br><span class="line"></span><br><span class="line">    scores = [<span class="number">0</span>]</span><br><span class="line">    averages = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">        reward, _, action, _ = rl_glue.rl_step() <span class="comment"># The environment and agent take a step and return</span></span><br><span class="line">                                                 <span class="comment"># the reward, and action taken.</span></span><br><span class="line">        scores.append(scores[-<span class="number">1</span>] + reward)</span><br><span class="line">        averages.append(scores[-<span class="number">1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">    all_averages.append(averages)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">&#x27;w&#x27;</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.plot([<span class="number">1.55</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_steps)], linestyle=<span class="string">&quot;--&quot;</span>)</span><br><span class="line">plt.plot(greedy_scores)</span><br><span class="line">plt.title(<span class="string">&quot;Average Reward of Greedy Agent vs. E-Greedy Agent&quot;</span>)</span><br><span class="line">plt.plot(np.mean(all_averages, axis=<span class="number">0</span>))</span><br><span class="line">plt.legend((<span class="string">&quot;Best Possible&quot;</span>, <span class="string">&quot;Greedy&quot;</span>, <span class="string">&quot;Epsilon: 0.1&quot;</span>))</span><br><span class="line">plt.xlabel(<span class="string">&quot;Steps&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Average reward&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 200/200 [00:03&lt;00:00, 62.89it/s]
</code></pre>
<p><img src="output_23_1.png" alt="png"></p>
<p>Notice how much better the epsilon-greedy agent did. Because we occasionally choose a random action we were able to find a better long term policy. By acting greedily before our value estimates are accurate, we risk settling on a suboptimal action.</p>
<h2 id="Section-2-1-Averaging-Multiple-Runs"><a href="#Section-2-1-Averaging-Multiple-Runs" class="headerlink" title="Section 2.1 Averaging Multiple Runs"></a>Section 2.1 Averaging Multiple Runs</h2><p>Did you notice that we averaged over 2000 runs? Why did we do that?</p>
<p>To get some insight, let’s look at the results of two individual runs by the same agent.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot runs of e-greedy agent</span></span><br><span class="line">agent = EpsilonGreedyAgent</span><br><span class="line">env = ten_arm_env.Environment</span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">10</span>, <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>&#125;</span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line">all_averages = []</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">&#x27;w&#x27;</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> (<span class="number">0</span>, <span class="number">1</span>):</span><br><span class="line">    np.random.seed(run) <span class="comment"># Here we set the seed so that we can compare two different runs</span></span><br><span class="line">    averages = []</span><br><span class="line">    rl_glue = RLGlue(env, agent)</span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">    rl_glue.rl_start()</span><br><span class="line"></span><br><span class="line">    scores = [<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">        reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">        scores.append(scores[-<span class="number">1</span>] + reward)</span><br><span class="line">        averages.append(scores[-<span class="number">1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    plt.plot(averages)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;Comparing two independent runs&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Steps&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Average reward&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_27_0.png" alt="png"></p>
<p>Notice how the two runs were different? But, if this is the exact same algorithm, why does it behave differently in these two runs?</p>
<p>The answer is that it is due to randomness in the environment and in the agent. Depending on what action the agent randomly starts with, or when it randomly chooses to explore, it can change the results of the runs. And even if the agent chooses the same action, the reward from the environment is randomly sampled from a Gaussian. The agent could get lucky, and see larger rewards for the best action early on and so settle on the best action faster. Or, it could get unlucky and see smaller rewards for best action early on and so take longer to recognize that it is in fact the best action.</p>
<p>To be more concrete, let’s look at how many times an exploratory action is taken, for different seeds. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Random Seed 1&quot;</span>)</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">15</span>):</span><br><span class="line">    <span class="keyword">if</span> np.random.random() &lt; <span class="number">0.1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Exploratory Action&quot;</span>)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Random Seed 2&quot;</span>)</span><br><span class="line">np.random.seed(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">15</span>):</span><br><span class="line">    <span class="keyword">if</span> np.random.random() &lt; <span class="number">0.1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Exploratory Action&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Random Seed 1
Exploratory Action
Exploratory Action
Exploratory Action


Random Seed 2
Exploratory Action
</code></pre>
<p>With the first seed, we take an exploratory action three times out of 15, but with the second, we only take an exploratory action once. This can significantly affect the performance of our agent because the amount of exploration has changed significantly.</p>
<p>To compare algorithms, we therefore report performance averaged across many runs. We do this to ensure that we are not simply reporting a result that is due to stochasticity, as explained <a target="_blank" rel="noopener" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/PtVBs/sequential-decision-making-with-evaluative-feedback">in the lectures</a>. Rather, we want statistically significant outcomes. We will not use statistical significance tests in this course. Instead, because we have access to simulators for our experiments, we use the simpler strategy of running for a large number of runs and ensuring that the confidence intervals do not overlap. </p>
<h2 id="Section-3-Comparing-values-of-epsilon"><a href="#Section-3-Comparing-values-of-epsilon" class="headerlink" title="Section 3: Comparing values of epsilon"></a>Section 3: Comparing values of epsilon</h2><p>Can we do better than an epsilon of 0.1? Let’s try several different values for epsilon and see how they perform. We try different settings of key performance parameters to understand how the agent might perform under different conditions.</p>
<p>Below we run an experiment where we sweep over different values for epsilon:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment code for different e-greedy</span></span><br><span class="line">epsilons = [<span class="number">0.0</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.4</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">&#x27;w&#x27;</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.plot([<span class="number">1.55</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_steps)], linestyle=<span class="string">&quot;--&quot;</span>)</span><br><span class="line"></span><br><span class="line">n_q_values = []</span><br><span class="line">n_averages = []</span><br><span class="line">n_best_actions = []</span><br><span class="line"></span><br><span class="line">num_runs = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epsilon <span class="keyword">in</span> epsilons:</span><br><span class="line">    all_averages = []</span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_runs)):</span><br><span class="line">        agent = EpsilonGreedyAgent</span><br><span class="line">        agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">10</span>, <span class="string">&quot;epsilon&quot;</span>: epsilon&#125;</span><br><span class="line">        env_info = &#123;<span class="string">&quot;random_seed&quot;</span>: run&#125;</span><br><span class="line"></span><br><span class="line">        rl_glue = RLGlue(env, agent)</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">        rl_glue.rl_start()</span><br><span class="line">        </span><br><span class="line">        best_arm = np.argmax(rl_glue.environment.arms)</span><br><span class="line"></span><br><span class="line">        scores = [<span class="number">0</span>]</span><br><span class="line">        averages = []</span><br><span class="line">        best_action_chosen = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">            reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">            scores.append(scores[-<span class="number">1</span>] + reward)</span><br><span class="line">            averages.append(scores[-<span class="number">1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">if</span> action == best_arm:</span><br><span class="line">                best_action_chosen.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                best_action_chosen.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> epsilon == <span class="number">0.1</span> <span class="keyword">and</span> run == <span class="number">0</span>:</span><br><span class="line">                n_q_values.append(np.copy(rl_glue.agent.q_values))</span><br><span class="line">        <span class="keyword">if</span> epsilon == <span class="number">0.1</span>:</span><br><span class="line">            n_averages.append(averages)</span><br><span class="line">            n_best_actions.append(best_action_chosen)</span><br><span class="line">        all_averages.append(averages)</span><br><span class="line">        </span><br><span class="line">    plt.plot(np.mean(all_averages, axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">plt.legend([<span class="string">&quot;Best Possible&quot;</span>] + epsilons)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Steps&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Average reward&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 200/200 [00:03&lt;00:00, 58.80it/s]
100%|██████████| 200/200 [00:03&lt;00:00, 59.62it/s]
100%|██████████| 200/200 [00:03&lt;00:00, 61.00it/s]
100%|██████████| 200/200 [00:02&lt;00:00, 70.13it/s]
</code></pre>
<p><img src="output_33_1.png" alt="png"></p>
<p>Why did 0.1 perform better than 0.01?</p>
<p>If exploration helps why did 0.4 perform worse that 0.0 (the greedy agent)?</p>
<p>Think about these and how you would answer these questions. They are questions in the practice quiz. If you still have questions about it, retake the practice quiz.</p>
<h2 id="Section-4-The-Effect-of-Step-Size"><a href="#Section-4-The-Effect-of-Step-Size" class="headerlink" title="Section 4: The Effect of Step Size"></a>Section 4: The Effect of Step Size</h2><p>In Section 1 of this assignment, we decayed the step size over time based on action-selection counts. The step-size was 1/N(A), where N(A) is the number of times action A was selected. This is the same as computing a sample average. We could also set the step size to be a constant value, such as 0.1. What would be the effect of doing that? And is it better to use a constant or the sample average method? </p>
<p>To investigate this question, let’s start by creating a new agent that has a constant step size. This will be nearly identical to the agent created above. You will use the same code to select the epsilon-greedy action. You will change the update to have a constant step size instead of using the 1/N(A) update.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EpsilonGreedyAgentConstantStepsize</span>(<span class="params">main_agent.Agent</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, observation</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Takes one step for the agent. It takes in a reward and observation and </span></span><br><span class="line"><span class="string">        returns the action the agent chooses at that time step.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">        reward -- float, the reward the agent recieved from the environment after taking the last action.</span></span><br><span class="line"><span class="string">        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it</span></span><br><span class="line"><span class="string">                              until future lessons</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        current_action -- int, the action chosen by the agent at the current time step.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### Useful Class Variables ###</span></span><br><span class="line">        <span class="comment"># self.q_values : An array with what the agent believes each of the values of the arm are.</span></span><br><span class="line">        <span class="comment"># self.arm_count : An array with a count of the number of times each arm has been pulled.</span></span><br><span class="line">        <span class="comment"># self.last_action : An int of the action that the agent took on the previous time step.</span></span><br><span class="line">        <span class="comment"># self.step_size : A float which is the current step size for the agent.</span></span><br><span class="line">        <span class="comment"># self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1)</span></span><br><span class="line">        <span class="comment">#######################</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update q_values for action taken at previous time step </span></span><br><span class="line">        <span class="comment"># using self.step_size intead of using self.arm_count</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line"><span class="comment">#         raise NotImplementedError()</span></span><br><span class="line">        self.arm_count[self.last_action] += <span class="number">1</span></span><br><span class="line">        self.q_values[self.last_action] = self.q_values[self.last_action] +  self.step_size * (reward - self.q_values[self.last_action])</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy. This is the same as you implemented above.</span></span><br><span class="line">        <span class="comment"># YOUR CODE HERE</span></span><br><span class="line"><span class="comment">#         raise NotImplementedError()</span></span><br><span class="line">        _ = np.random.random()</span><br><span class="line">        current_action = argmax(self.q_values) <span class="keyword">if</span> _ &gt;= self.epsilon <span class="keyword">else</span> np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(self.q_values))</span><br><span class="line">        </span><br><span class="line">        self.last_action = current_action</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> current_action</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step_size <span class="keyword">in</span> [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">1.0</span>]:</span><br><span class="line">    e_greedy_agent = EpsilonGreedyAgentConstantStepsize()</span><br><span class="line">    e_greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    e_greedy_agent.num_actions = <span class="number">5</span></span><br><span class="line">    e_greedy_agent.last_action = <span class="number">1</span></span><br><span class="line">    e_greedy_agent.epsilon = <span class="number">0.0</span></span><br><span class="line">    e_greedy_agent.step_size = step_size</span><br><span class="line">    action = e_greedy_agent.agent_step(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span> e_greedy_agent.q_values == [<span class="number">0</span>, step_size, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">&quot;Check that you are updating q_values correctly using the stepsize.&quot;</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># Check Epsilon Greedy with Different Constant Stepsizes</span></span><br><span class="line"><span class="keyword">for</span> step_size <span class="keyword">in</span> [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">1.0</span>]:</span><br><span class="line">    e_greedy_agent = EpsilonGreedyAgentConstantStepsize()</span><br><span class="line">    e_greedy_agent.q_values = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    e_greedy_agent.num_actions = <span class="number">5</span></span><br><span class="line">    e_greedy_agent.last_action = <span class="number">1</span></span><br><span class="line">    e_greedy_agent.epsilon = <span class="number">0.0</span></span><br><span class="line">    e_greedy_agent.step_size = step_size</span><br><span class="line">    </span><br><span class="line">    action = e_greedy_agent.agent_step(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> e_greedy_agent.q_values == [<span class="number">0</span>, step_size, <span class="number">1.0</span>, <span class="number">0</span>, <span class="number">0</span>]    </span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment code for different step sizes</span></span><br><span class="line">step_sizes = [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">1.0</span>, <span class="string">&#x27;1/N(A)&#x27;</span>]</span><br><span class="line"></span><br><span class="line">epsilon = <span class="number">0.1</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line">num_runs = <span class="number">200</span></span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">&#x27;w&#x27;</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line"></span><br><span class="line">q_values = &#123;step_size: [] <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes&#125;</span><br><span class="line">true_values = &#123;step_size: <span class="literal">None</span> <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes&#125;</span><br><span class="line">best_actions = &#123;step_size: [] <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes:</span><br><span class="line">    all_averages = []</span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_runs)):</span><br><span class="line">        np.random.seed(run)</span><br><span class="line">        agent = EpsilonGreedyAgentConstantStepsize <span class="keyword">if</span> step_size != <span class="string">&#x27;1/N(A)&#x27;</span> <span class="keyword">else</span> EpsilonGreedyAgent</span><br><span class="line">        agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">10</span>, <span class="string">&quot;epsilon&quot;</span>: epsilon, <span class="string">&quot;step_size&quot;</span>: step_size, <span class="string">&quot;initial_value&quot;</span>: <span class="number">0.0</span>&#125;</span><br><span class="line">        env_info = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        rl_glue = RLGlue(env, agent)</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">        rl_glue.rl_start()</span><br><span class="line">        </span><br><span class="line">        best_arm = np.argmax(rl_glue.environment.arms)</span><br><span class="line"></span><br><span class="line">        scores = [<span class="number">0</span>]</span><br><span class="line">        averages = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> run == <span class="number">0</span>:</span><br><span class="line">            true_values[step_size] = np.copy(rl_glue.environment.arms)</span><br><span class="line">            </span><br><span class="line">        best_action_chosen = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">            reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">            scores.append(scores[-<span class="number">1</span>] + reward)</span><br><span class="line">            averages.append(scores[-<span class="number">1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">if</span> action == best_arm:</span><br><span class="line">                best_action_chosen.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                best_action_chosen.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> run == <span class="number">0</span>:</span><br><span class="line">                q_values[step_size].append(np.copy(rl_glue.agent.q_values))</span><br><span class="line">        best_actions[step_size].append(best_action_chosen)</span><br><span class="line">    ax.plot(np.mean(best_actions[step_size], axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">plt.legend(step_sizes)</span><br><span class="line">plt.title(<span class="string">&quot;% Best Arm Pulled&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Steps&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;% Best Arm Pulled&quot;</span>)</span><br><span class="line">vals = ax.get_yticks()</span><br><span class="line">ax.set_yticklabels([<span class="string">&#x27;&#123;:,.2%&#125;&#x27;</span>.<span class="built_in">format</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> vals])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 200/200 [00:03&lt;00:00, 63.14it/s]
100%|██████████| 200/200 [00:03&lt;00:00, 62.84it/s]
100%|██████████| 200/200 [00:03&lt;00:00, 62.57it/s]
100%|██████████| 200/200 [00:03&lt;00:00, 62.70it/s]
100%|██████████| 200/200 [00:03&lt;00:00, 61.82it/s]
</code></pre>
<p><img src="output_40_1.png" alt="png"></p>
<p>Notice first that we are now plotting the amount of time that the best action is taken rather than the average reward. To better  understand the performance of an agent, it can be useful to measure specific behaviors, beyond just how much reward is accumulated. This measure indicates how close the agent’s behaviour is to optimal.</p>
<p>It seems as though 1/N(A) performed better than the others, in that it reaches a solution where it takes the best action most frequently. Now why might this be? Why did a step size of 0.5 start out better but end up performing worse? Why did a step size of 0.01 perform so poorly?</p>
<p>Let’s dig into this further below. Let’s plot how well each agent tracks the true value, where each agent has a different step size method. You do not have to enter any code here, just follow along.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lock</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line">largest = <span class="number">0</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes:</span><br><span class="line">    plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">&#x27;w&#x27;</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">    largest = np.argmax(true_values[step_size])</span><br><span class="line">    plt.plot([true_values[step_size][largest] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_steps)], linestyle=<span class="string">&quot;--&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Step Size: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(step_size))</span><br><span class="line">    plt.plot(np.array(q_values[step_size])[:, largest])</span><br><span class="line">    plt.legend([<span class="string">&quot;True Expected Value&quot;</span>, <span class="string">&quot;Estimated Value&quot;</span>])</span><br><span class="line">    plt.xlabel(<span class="string">&quot;Steps&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;Value&quot;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_42_0.png" alt="png"></p>
<p><img src="output_42_1.png" alt="png"></p>
<p><img src="output_42_2.png" alt="png"></p>
<p><img src="output_42_3.png" alt="png"></p>
<p><img src="output_42_4.png" alt="png"></p>
<p>These plots help clarify the performance differences between the different step sizes. A step size of 0.01 makes such small updates that the agent’s value estimate of the best action does not get close to the actual value. Step sizes of 0.5 and 1.0 both get close to the true value quickly, but are very susceptible to stochasticity in the rewards. The updates overcorrect too much towards recent rewards, and so oscillate around the true value. This means that on many steps, the action that pulls the best arm may seem worse than it actually is.  A step size of 0.1 updates fairly quickly to the true value, and does not oscillate as widely around the true values as 0.5 and 1.0. This is one of the reasons that 0.1 performs quite well. Finally we see why 1/N(A) performed well. Early on while the step size is still reasonably high it moves quickly to the true expected value, but as it gets pulled more its step size is reduced which makes it less susceptible to the stochasticity of the rewards.</p>
<p>Does this mean that 1/N(A) is always the best? When might it not be? One possible setting where it might not be as effective is in non-stationary problems. You learned about non-stationarity in the lessons. Non-stationarity means that the environment may change over time. This could manifest itself as continual change over time of the environment, or a sudden change in the environment.</p>
<p>Let’s look at how a sudden change in the reward distributions affects a step size like 1/N(A). This time we will run the environment for 2000 steps, and after 1000 steps we will randomly change the expected value of all of the arms. We compare two agents, both using epsilon-greedy with epsilon = 0.1. One uses a constant step size of 0.1, the other a step size of 1/N(A) that reduces over time. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line">epsilon = <span class="number">0.1</span></span><br><span class="line">num_steps = <span class="number">2000</span></span><br><span class="line">num_runs = <span class="number">200</span></span><br><span class="line">step_size = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>), dpi= <span class="number">80</span>, facecolor=<span class="string">&#x27;w&#x27;</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.plot([<span class="number">1.55</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_steps)], linestyle=<span class="string">&quot;--&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> agent <span class="keyword">in</span> [EpsilonGreedyAgent, EpsilonGreedyAgentConstantStepsize]:</span><br><span class="line">    all_averages = []</span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_runs)):</span><br><span class="line">        agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">10</span>, <span class="string">&quot;epsilon&quot;</span>: epsilon, <span class="string">&quot;step_size&quot;</span>: step_size&#125;</span><br><span class="line">        np.random.seed(run)</span><br><span class="line">        </span><br><span class="line">        rl_glue = RLGlue(env, agent)</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">        rl_glue.rl_start()</span><br><span class="line"></span><br><span class="line">        scores = [<span class="number">0</span>]</span><br><span class="line">        averages = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">            reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">            scores.append(scores[-<span class="number">1</span>] + reward)</span><br><span class="line">            averages.append(scores[-<span class="number">1</span>] / (i + <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">1000</span>:</span><br><span class="line">                rl_glue.environment.arms = np.random.randn(<span class="number">10</span>)</span><br><span class="line">        all_averages.append(averages)</span><br><span class="line">        </span><br><span class="line">    plt.plot(np.mean(all_averages, axis=<span class="number">0</span>))</span><br><span class="line">plt.legend([<span class="string">&quot;Best Possible&quot;</span>, <span class="string">&quot;1/N(A)&quot;</span>, <span class="string">&quot;0.1&quot;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&quot;Steps&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Average reward&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 200/200 [00:06&lt;00:00, 29.43it/s]
100%|██████████| 200/200 [00:06&lt;00:00, 30.10it/s]
</code></pre>
<p><img src="output_44_1.png" alt="png"></p>
<p>Now the agent with a step size of 1/N(A) performed better at the start but then performed worse when the environment changed! What happened?</p>
<p>Think about what the step size would be after 1000 steps. Let’s say the best action gets chosen 500 times. That means the step size for that action is 1/500 or 0.002. At each step when we update the value of the action and the value is going to move only 0.002 * the error. That is a very tiny adjustment and it will take a long time for it to get to the true value.</p>
<p>The agent with step size 0.1, however, will always update in 1/10th of the direction of the error. This means that on average it will take ten steps for it to update its value to the sample mean.</p>
<p>These are the types of tradeoffs we have to think about in reinforcement learning. A larger step size moves us more quickly toward the true value, but can make our estimated values oscillate around the expected value. A step size that reduces over time can converge to close to the expected value, without oscillating. On the other hand, such a decaying stepsize is not able to adapt to changes in the environment. Nonstationarity—and the related concept of partial observability—is a common feature of reinforcement learning problems and when learning online.  </p>
<h2 id="Section-5-Conclusion"><a href="#Section-5-Conclusion" class="headerlink" title="Section 5: Conclusion"></a>Section 5: Conclusion</h2><p>Great work! You have:</p>
<ul>
<li>Implemented your first agent</li>
<li>Learned about the effect of epsilon, an exploration parameter, on the performance of an agent</li>
<li>Learned about the effect of step size on the performance of the agent</li>
<li>Learned about a good experiment practice of averaging across multiple runs</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Question-duplicates/2020/08/23/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Question-duplicates/2020/08/23/" class="post-title-link" itemprop="url">Question duplicates</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-08-23 02:47:25 / Modified: 02:47:47" itemprop="dateCreated datePublished" datetime="2020-08-23T02:47:25+08:00">2020-08-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Question-duplicates/2020/08/23/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Question-duplicates/2020/08/23/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Assignment-4-Question-duplicates"><a href="#Assignment-4-Question-duplicates" class="headerlink" title="Assignment 4:  Question duplicates"></a>Assignment 4:  Question duplicates</h1><p>Welcome to the fourth assignment of course 3. In this assignment you will explore Siamese networks applied to natural language processing. You will further explore the fundamentals of Trax and you will be able to implement a more complicated structure using it. By completing this assignment, you will learn how to implement models with different architectures. </p>
<h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li><a href="#0">Overview</a></li>
<li><a href="#1">Part 1: Importing the Data</a><ul>
<li><a href="#1.1">1.1 Loading in the data</a></li>
<li><a href="#1.2">1.2 Converting a question to a tensor</a></li>
<li><a href="#1.3">1.3 Understanding the iterator</a><ul>
<li><a href="#ex01">Exercise 01</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2">Part 2: Defining the Siamese model</a><ul>
<li><a href="#2.1">2.1 Understanding Siamese Network</a><ul>
<li><a href="#ex02">Exercise 02</a></li>
</ul>
</li>
<li><a href="#2.2">2.2 Hard  Negative Mining</a><ul>
<li><a href="#ex03">Exercise 03</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#3">Part 3: Training</a><ul>
<li><a href="#3.1">3.1 Training the model</a><ul>
<li><a href="#ex04">Exercise 04</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#4">Part 4: Evaluation</a><ul>
<li><a href="#4.1">4.1 Evaluating your siamese network</a></li>
<li><a href="#4.2">4.2 Classify</a><ul>
<li><a href="#ex05">Exercise 05</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#5">Part 5: Testing with your own questions</a><ul>
<li><a href="#ex06">Exercise 06</a></li>
</ul>
</li>
<li><a href="#6">On Siamese networks</a></li>
</ul>
<p><a name='0'></a></p>
<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>In this assignment, concretely you will: </p>
<ul>
<li>Learn about Siamese networks</li>
<li>Understand how the triplet loss works</li>
<li>Understand how to evaluate accuracy</li>
<li>Use cosine similarity between the model’s outputted vectors</li>
<li>Use the data generator to get batches of questions</li>
<li>Predict using your own model</li>
</ul>
<p>By now, you are familiar with trax and know how to make use of classes to define your model. We will start this homework by asking you to preprocess the data the same way you did in the previous assignments. After processing the data you will build a classifier that will allow you to identify whether to questions are the same or not.<br><img src = "meme.png" style="width:550px;height:300px;"/></p>
<p>You will process the data first and then pad in a similar way you have done in the previous assignment. Your model will take in the two question embeddings, run them through an LSTM, and then compare the outputs of the two sub networks using cosine similarity. Before taking a deep dive into the model, start by importing the data set.</p>
<p><a name='1'></a></p>
<h1 id="Part-1-Importing-the-Data"><a href="#Part-1-Importing-the-Data" class="headerlink" title="Part 1: Importing the Data"></a>Part 1: Importing the Data</h1><p><a name='1.1'></a></p>
<h3 id="1-1-Loading-in-the-data"><a href="#1-1-Loading-in-the-data" class="headerlink" title="1.1 Loading in the data"></a>1.1 Loading in the data</h3><p>You will be using the Quora question answer dataset to build a model that could identify similar questions. This is a useful task because you don’t want to have several versions of the same question posted. Several times when teaching I end up responding to similar questions on piazza, or on other community forums. This data set has been labeled for you. Run the cell below to import some of the packages you will be using. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">import</span> trax</span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> training</span><br><span class="line"><span class="keyword">from</span> trax.fastmath <span class="keyword">import</span> numpy <span class="keyword">as</span> fastnp</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rnd</span><br><span class="line"></span><br><span class="line"><span class="comment"># set random seeds</span></span><br><span class="line">trax.supervised.trainer_lib.init_random_number_generators(<span class="number">34</span>)</span><br><span class="line">rnd.seed(<span class="number">34</span>)</span><br></pre></td></tr></table></figure>

<p><strong>Notice that for this assignment Trax’s numpy is referred to as <code>fastnp</code>, while regular numpy is referred to as <code>np</code>.</strong></p>
<p>You will now load in the data set. We have done some preprocessing for you. If you have taken the deeplearning specialization, this is a slightly different training method than the one you have seen there. If you have not, then don’t worry about it, we will explain everything. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">&quot;questions.csv&quot;</span>)</span><br><span class="line">N=<span class="built_in">len</span>(data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Number of question pairs: &#x27;</span>, N)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>

<pre><code>Number of question pairs:  404351
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>qid1</th>
      <th>qid2</th>
      <th>question1</th>
      <th>question2</th>
      <th>is_duplicate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>What is the step by step guide to invest in sh...</td>
      <td>What is the step by step guide to invest in sh...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>3</td>
      <td>4</td>
      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>
      <td>What would happen if the Indian government sto...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>5</td>
      <td>6</td>
      <td>How can I increase the speed of my internet co...</td>
      <td>How can Internet speed be increased by hacking...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>7</td>
      <td>8</td>
      <td>Why am I mentally very lonely? How can I solve...</td>
      <td>Find the remainder when [math]23^{24}[/math] i...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>9</td>
      <td>10</td>
      <td>Which one dissolve in water quikly sugar, salt...</td>
      <td>Which fish would survive in salt water?</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



<p>We first split the data into a train and test set. The test set will be used later to evaluate our model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">N_train = <span class="number">300000</span></span><br><span class="line">N_test  = <span class="number">10</span>*<span class="number">1024</span></span><br><span class="line">data_train = data[:N_train]</span><br><span class="line">data_test  = data[N_train:N_train+N_test]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Train set:&quot;</span>, <span class="built_in">len</span>(data_train), <span class="string">&quot;Test set:&quot;</span>, <span class="built_in">len</span>(data_test))</span><br><span class="line"><span class="keyword">del</span>(data) <span class="comment"># remove to free memory</span></span><br></pre></td></tr></table></figure>

<pre><code>Train set: 300000 Test set: 10240
</code></pre>
<p>As explained in the lectures, we select only the question pairs that are duplicate to train the model. <br><br>We build two batches as input for the Siamese network and we assume that question $q1_i$ (question $i$ in the first batch) is a duplicate of $q2_i$ (question $i$ in the second batch), but all other questions in the second batch are not duplicates of $q1_i$.<br>The test set uses the original pairs of questions and the status describing if the questions are duplicates.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">td_index = (data_train[<span class="string">&#x27;is_duplicate&#x27;</span>] == <span class="number">1</span>).to_numpy()</span><br><span class="line">td_index = [i <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(td_index) <span class="keyword">if</span> x] </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;number of duplicate questions: &#x27;</span>, <span class="built_in">len</span>(td_index))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;indexes of first ten duplicate questions:&#x27;</span>, td_index[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<pre><code>number of duplicate questions:  111486
indexes of first ten duplicate questions: [5, 7, 11, 12, 13, 15, 16, 18, 20, 29]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(data_train[<span class="string">&#x27;question1&#x27;</span>][<span class="number">5</span>])  <span class="comment">#  Example of question duplicates (first one in data)</span></span><br><span class="line"><span class="built_in">print</span>(data_train[<span class="string">&#x27;question2&#x27;</span>][<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;is_duplicate: &#x27;</span>, data_train[<span class="string">&#x27;is_duplicate&#x27;</span>][<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<pre><code>Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?
I&#39;m a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?
is_duplicate:  1
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Q1_train_words = np.array(data_train[<span class="string">&#x27;question1&#x27;</span>][td_index])</span><br><span class="line">Q2_train_words = np.array(data_train[<span class="string">&#x27;question2&#x27;</span>][td_index])</span><br><span class="line"></span><br><span class="line">Q1_test_words = np.array(data_test[<span class="string">&#x27;question1&#x27;</span>])</span><br><span class="line">Q2_test_words = np.array(data_test[<span class="string">&#x27;question2&#x27;</span>])</span><br><span class="line">y_test  = np.array(data_test[<span class="string">&#x27;is_duplicate&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p>Above, you have seen that you only took the duplicated questions for training our model. <br>You did so on purpose, because the data generator will produce batches $([q1_1, q1_2, q1_3, …]$, $[q2_1, q2_2,q2_3, …])$  where $q1_i$ and $q2_k$ are duplicate if and only if $i = k$.</p>
<p><br>Let’s print to see what your data looks like.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;TRAINING QUESTIONS:\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Question 1: &#x27;</span>, Q1_train_words[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Question 2: &#x27;</span>, Q2_train_words[<span class="number">0</span>], <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Question 1: &#x27;</span>, Q1_train_words[<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Question 2: &#x27;</span>, Q2_train_words[<span class="number">5</span>], <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;TESTING QUESTIONS:\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Question 1: &#x27;</span>, Q1_test_words[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Question 2: &#x27;</span>, Q2_test_words[<span class="number">0</span>], <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;is_duplicate =&#x27;</span>, y_test[<span class="number">0</span>], <span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>TRAINING QUESTIONS:

Question 1:  Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?
Question 2:  I&#39;m a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me? 

Question 1:  What would a Trump presidency mean for current international master’s students on an F1 visa?
Question 2:  How will a Trump presidency affect the students presently in US or planning to study in US? 

TESTING QUESTIONS:

Question 1:  How do I prepare for interviews for cse?
Question 2:  What is the best way to prepare for cse? 

is_duplicate = 0 
</code></pre>
<p>You will now encode each word of the selected duplicate pairs with an index. <br> Given a question, you can then just encode it as a list of numbers.  </p>
<p>First you tokenize the questions using <code>nltk.word_tokenize</code>. <br><br>You need a python default dictionary which later, during inference, assigns the values $0$ to all Out Of Vocabulary (OOV) words.<br><br>Then you encode each word of the selected duplicate pairs with an index. Given a question, you can then just encode it as a list of numbers. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#create arrays</span></span><br><span class="line">Q1_train = np.empty_like(Q1_train_words)</span><br><span class="line">Q2_train = np.empty_like(Q2_train_words)</span><br><span class="line"></span><br><span class="line">Q1_test = np.empty_like(Q1_test_words)</span><br><span class="line">Q2_test = np.empty_like(Q2_test_words)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Building the vocabulary with the train set         (this might take a minute)</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">vocab = defaultdict(<span class="keyword">lambda</span>: <span class="number">0</span>)</span><br><span class="line">vocab[<span class="string">&#x27;&lt;PAD&gt;&#x27;</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(Q1_train_words)):</span><br><span class="line">    Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])</span><br><span class="line">    Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])</span><br><span class="line">    q = Q1_train[idx] + Q2_train[idx]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> q:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocab:</span><br><span class="line">            vocab[word] = <span class="built_in">len</span>(vocab) + <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The length of the vocabulary is: &#x27;</span>, <span class="built_in">len</span>(vocab))</span><br></pre></td></tr></table></figure>

<pre><code>The length of the vocabulary is:  36268
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(vocab[<span class="string">&#x27;&lt;PAD&gt;&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(vocab[<span class="string">&#x27;Astrology&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(vocab[<span class="string">&#x27;Astronomy&#x27;</span>])  <span class="comment">#not in vocabulary, returns 0</span></span><br></pre></td></tr></table></figure>

<pre><code>1
2
0
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(Q1_test_words)): </span><br><span class="line">    Q1_test[idx] = nltk.word_tokenize(Q1_test_words[idx])</span><br><span class="line">    Q2_test[idx] = nltk.word_tokenize(Q2_test_words[idx])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Train set has reduced to: &#x27;</span>, <span class="built_in">len</span>(Q1_train) ) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test set length: &#x27;</span>, <span class="built_in">len</span>(Q1_test) ) </span><br></pre></td></tr></table></figure>

<pre><code>Train set has reduced to:  111486
Test set length:  10240
</code></pre>
<p><a name='1.2'></a></p>
<h3 id="1-2-Converting-a-question-to-a-tensor"><a href="#1-2-Converting-a-question-to-a-tensor" class="headerlink" title="1.2 Converting a question to a tensor"></a>1.2 Converting a question to a tensor</h3><p>You will now convert every question to a tensor, or an array of numbers, using your vocabulary built above.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Converting questions to array of integers</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(Q1_train)):</span><br><span class="line">    Q1_train[i] = [vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> Q1_train[i]]</span><br><span class="line">    Q2_train[i] = [vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> Q2_train[i]]</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(Q1_test)):</span><br><span class="line">    Q1_test[i] = [vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> Q1_test[i]]</span><br><span class="line">    Q2_test[i] = [vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> Q2_test[i]]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;first question in the train set:\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(Q1_train_words[<span class="number">0</span>], <span class="string">&#x27;\n&#x27;</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;encoded version:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(Q1_train[<span class="number">0</span>],<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;first question in the test set:\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(Q1_test_words[<span class="number">0</span>], <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;encoded version:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(Q1_test[<span class="number">0</span>]) </span><br></pre></td></tr></table></figure>

<pre><code>first question in the train set:

Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? 

encoded version:
[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] 

first question in the test set:

How do I prepare for interviews for cse? 

encoded version:
[32, 38, 4, 107, 65, 1015, 65, 11509, 21]
</code></pre>
<p>You will now split your train set into a training/validation set so that you can use it to train and evaluate your Siamese model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Splitting the data</span></span><br><span class="line">cut_off = <span class="built_in">int</span>(<span class="built_in">len</span>(Q1_train)*<span class="number">.8</span>)</span><br><span class="line">train_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]</span><br><span class="line">val_Q1, val_Q2 = Q1_train[cut_off: ], Q2_train[cut_off:]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Number of duplicate questions: &#x27;</span>, <span class="built_in">len</span>(Q1_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The length of the training set is:  &quot;</span>, <span class="built_in">len</span>(train_Q1))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The length of the validation set is: &quot;</span>, <span class="built_in">len</span>(val_Q1))</span><br></pre></td></tr></table></figure>

<pre><code>Number of duplicate questions:  111486
The length of the training set is:   89188
The length of the validation set is:  22298
</code></pre>
<p><a name='1.3'></a></p>
<h3 id="1-3-Understanding-the-iterator"><a href="#1-3-Understanding-the-iterator" class="headerlink" title="1.3 Understanding the iterator"></a>1.3 Understanding the iterator</h3><p>Most of the time in Natural Language Processing, and AI in general we use batches when training our data sets. If you were to use stochastic gradient descent with one example at a time, it will take you forever to build a model. In this example, we show you how you can build a data generator that takes in $Q1$ and $Q2$ and returns a batch of size <code>batch_size</code>  in the following format $([q1_1, q1_2, q1_3, …]$, $[q2_1, q2_2,q2_3, …])$. The tuple consists of two arrays and each array has <code>batch_size</code> questions. Again, $q1_i$ and $q2_i$ are duplicates, but they are not duplicates with any other elements in the batch. </p>
<br>

<p>The command <code>next(data_generator)</code>returns the next batch. This iterator returns the data in a format that you could directly use in your model when computing the feed-forward of your algorithm. This iterator returns a pair of arrays of questions. </p>
<p><a name='ex01'></a></p>
<h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p><strong>Instructions:</strong><br>Implement the data generator below. Here are some things you will need. </p>
<ul>
<li>While true loop.</li>
<li>if <code>index &gt;= len_Q1</code>, set the <code>idx</code> to $0$.</li>
<li>The generator should return shuffled batches of data. To achieve this without modifying the actual question lists, a list containing the indexes of the questions is created. This list can be shuffled and used to get random batches everytime the index is reset.</li>
<li>Append elements of $Q1$ and $Q2$ to <code>input1</code> and <code>input2</code> respectively.</li>
<li>if <code>len(input1) == batch_size</code>, determine <code>max_len</code> as the longest question in <code>input1</code> and <code>input2</code>. Ceil <code>max_len</code> to a power of $2$ (for computation purposes) using the following command:  <code>max_len = 2**int(np.ceil(np.log2(max_len)))</code>.</li>
<li>Pad every question by <code>vocab[&#39;&lt;PAD&gt;&#39;]</code> until you get the length <code>max_len</code>.</li>
<li>Use yield to return <code>input1, input2</code>. </li>
<li>Don’t forget to reset <code>input1, input2</code>  to empty arrays at the end (data generator resumes from where it last left).</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: data_generator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_generator</span>(<span class="params">Q1, Q2, batch_size, pad=<span class="number">1</span>, shuffle=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Generator function that yields batches of data</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        Q1 (list): List of transformed (to tensor) questions.</span></span><br><span class="line"><span class="string">        Q2 (list): List of transformed (to tensor) questions.</span></span><br><span class="line"><span class="string">        batch_size (int): Number of elements per batch.</span></span><br><span class="line"><span class="string">        pad (int, optional): Pad character from the vocab. Defaults to 1.</span></span><br><span class="line"><span class="string">        shuffle (bool, optional): If the batches should be randomnized or not. Defaults to True.</span></span><br><span class="line"><span class="string">    Yields:</span></span><br><span class="line"><span class="string">        tuple: Of the form (input1, input2) with types (numpy.ndarray, numpy.ndarray)</span></span><br><span class="line"><span class="string">        NOTE: input1: inputs to your model [q1a, q2a, q3a, ...] i.e. (q1a,q1b) are duplicates</span></span><br><span class="line"><span class="string">              input2: targets to your model [q1b, q2b,q3b, ...] i.e. (q1a,q2i) i!=a are not duplicates</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    input1 = []</span><br><span class="line">    input2 = []</span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line">    len_q = <span class="built_in">len</span>(Q1)</span><br><span class="line">    question_indexes = [*<span class="built_in">range</span>(len_q)]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        rnd.shuffle(question_indexes)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">if</span> idx &gt;= len_q:</span><br><span class="line">            <span class="comment"># if idx is greater than or equal to len_q, set idx accordingly </span></span><br><span class="line">            <span class="comment"># (Hint: look at the instructions above)</span></span><br><span class="line">            idx = <span class="number">0</span></span><br><span class="line">            <span class="comment"># shuffle to get random batches if shuffle is set to True</span></span><br><span class="line">            <span class="keyword">if</span> shuffle:</span><br><span class="line">                rnd.shuffle(question_indexes)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get questions at the `question_indexes[idx]` position in Q1 and Q2</span></span><br><span class="line">        q1 = Q1[question_indexes[idx]]</span><br><span class="line">        q2 = Q2[question_indexes[idx]]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># increment idx by 1</span></span><br><span class="line">        idx += <span class="number">1</span></span><br><span class="line">        <span class="comment"># append q1</span></span><br><span class="line">        input1.append(q1)</span><br><span class="line">        <span class="comment"># append q2</span></span><br><span class="line">        input2.append(q2)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(input1) == batch_size:</span><br><span class="line">            <span class="comment"># determine max_len as the longest question in input1 &amp; input 2</span></span><br><span class="line">            <span class="comment"># Hint: use the `max` function. </span></span><br><span class="line">            <span class="comment"># take max of input1 &amp; input2 and then max out of the two of them.</span></span><br><span class="line">            max_len = <span class="built_in">max</span>(<span class="built_in">max</span>([<span class="built_in">len</span>(_) <span class="keyword">for</span> _ <span class="keyword">in</span> input1]),<span class="built_in">max</span>([<span class="built_in">len</span>(_) <span class="keyword">for</span> _ <span class="keyword">in</span> input2]))</span><br><span class="line">            <span class="comment"># pad to power-of-2 (Hint: look at the instructions above)</span></span><br><span class="line">            max_len = <span class="number">2</span>**<span class="built_in">int</span>(np.ceil(np.log2(max_len)))</span><br><span class="line">            b1 = []</span><br><span class="line">            b2 = []</span><br><span class="line">            <span class="keyword">for</span> q1, q2 <span class="keyword">in</span> <span class="built_in">zip</span>(input1, input2):</span><br><span class="line">                <span class="comment"># add [pad] to q1 until it reaches max_len</span></span><br><span class="line">                q1 = q1 + [pad] * (max_len - <span class="built_in">len</span>(q1))</span><br><span class="line">                <span class="comment"># add [pad] to q2 until it reaches max_len</span></span><br><span class="line">                q2 = q2 + [pad] * (max_len - <span class="built_in">len</span>(q2))</span><br><span class="line">                <span class="comment"># append q1</span></span><br><span class="line">                b1.append(q1)</span><br><span class="line">                <span class="comment"># append q2</span></span><br><span class="line">                b2.append(q2)</span><br><span class="line">            <span class="comment"># use b1 and b2</span></span><br><span class="line">            <span class="keyword">yield</span> np.array(b1), np.array(b2)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">            <span class="comment"># reset the batches</span></span><br><span class="line">            input1, input2 = [], []  <span class="comment"># reset the batches</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">res1, res2 = <span class="built_in">next</span>(data_generator(train_Q1, train_Q2, batch_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First questions  : &quot;</span>,<span class="string">&#x27;\n&#x27;</span>, res1, <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Second questions : &quot;</span>,<span class="string">&#x27;\n&#x27;</span>, res2)</span><br></pre></td></tr></table></figure>

<pre><code>First questions  :  
 [[  30   87   78  134 2132 1981   28   78  594   21    1    1    1    1
     1    1]
 [  30   55   78 3541 1460   28   56  253   21    1    1    1    1    1
     1    1]] 

Second questions :  
 [[  30  156   78  134 2132 9508   21    1    1    1    1    1    1    1
     1    1]
 [  30  156   78 3541 1460  131   56  253   21    1    1    1    1    1
     1    1]]
</code></pre>
<p><strong>Note</strong>: The following expected output is valid only if you run the above test cell <strong><em>once</em></strong> (first time). The output will change on each execution.</p>
<p>If you think your implementation is correct and it is not matching the output, make sure to restart the kernel and run all the cells from the top again. </p>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">First questions  :  </span><br><span class="line"> [[  <span class="number">30</span>   <span class="number">87</span>   <span class="number">78</span>  <span class="number">134</span> <span class="number">2132</span> <span class="number">1981</span>   <span class="number">28</span>   <span class="number">78</span>  <span class="number">594</span>   <span class="number">21</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span></span><br><span class="line">     <span class="number">1</span>    <span class="number">1</span>]</span><br><span class="line"> [  <span class="number">30</span>   <span class="number">55</span>   <span class="number">78</span> <span class="number">3541</span> <span class="number">1460</span>   <span class="number">28</span>   <span class="number">56</span>  <span class="number">253</span>   <span class="number">21</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span></span><br><span class="line">     <span class="number">1</span>    <span class="number">1</span>]] </span><br><span class="line"></span><br><span class="line">Second questions :  </span><br><span class="line"> [[  <span class="number">30</span>  <span class="number">156</span>   <span class="number">78</span>  <span class="number">134</span> <span class="number">2132</span> <span class="number">9508</span>   <span class="number">21</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span></span><br><span class="line">     <span class="number">1</span>    <span class="number">1</span>]</span><br><span class="line"> [  <span class="number">30</span>  <span class="number">156</span>   <span class="number">78</span> <span class="number">3541</span> <span class="number">1460</span>  <span class="number">131</span>   <span class="number">56</span>  <span class="number">253</span>   <span class="number">21</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span></span><br><span class="line">     <span class="number">1</span>    <span class="number">1</span>]]</span><br></pre></td></tr></table></figure>
<p>Now that you have your generator, you can just call it and it will return tensors which correspond to your questions in the Quora data set.<br>Now you can go ahead and start building your neural network. </p>
<p><a name='2'></a></p>
<h1 id="Part-2-Defining-the-Siamese-model"><a href="#Part-2-Defining-the-Siamese-model" class="headerlink" title="Part 2: Defining the Siamese model"></a>Part 2: Defining the Siamese model</h1><p><a name='2.1'></a></p>
<h3 id="2-1-Understanding-Siamese-Network"><a href="#2-1-Understanding-Siamese-Network" class="headerlink" title="2.1 Understanding Siamese Network"></a>2.1 Understanding Siamese Network</h3><p>A Siamese network is a neural network which uses the same weights while working in tandem on two different input vectors to compute comparable output vectors.The Siamese network you are about to implement looks like this:</p>
<img src = "siamese.png" style="width:600px;height:300px;"/>

<p>You get the question embedding, run it through an LSTM layer, normalize $v_1$ and $v_2$, and finally use a triplet loss (explained below) to get the corresponding cosine similarity for each pair of questions. As usual, you will start by importing the data set. The triplet loss makes use of a baseline (anchor) input that is compared to a positive (truthy) input and a negative (falsy) input. The distance from the baseline (anchor) input to the positive (truthy) input is minimized, and the distance from the baseline (anchor) input to the negative (falsy) input is maximized. In math equations, you are trying to maximize the following.</p>
<p>$$\mathcal{L}(A, P, N)=\max \left(|\mathrm{f}(A)-\mathrm{f}(P)|^{2}-|\mathrm{f}(A)-\mathrm{f}(N)|^{2}+\alpha, 0\right)$$</p>
<p>$A$ is the anchor input, for example $q1_1$, $P$ the duplicate input, for example, $q2_1$, and $N$ the negative input (the non duplicate question), for example $q2_2$.<br><br>$\alpha$ is a margin; you can think about it as a safety net, or by how much you want to push the duplicates from the non duplicates.<br><br></p>
<p><a name='ex02'></a></p>
<h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p><strong>Instructions:</strong> Implement the <code>Siamese</code> function below. You should be using all the objects explained below. </p>
<p>To implement this model, you will be using <code>trax</code>. Concretely, you will be using the following functions.</p>
<ul>
<li><code>tl.Serial</code>: Combinator that applies layers serially (by function composition) allows you set up the overall structure of the feedforward. <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial">docs</a> / <a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/combinators.py#L26">source code</a><ul>
<li>You can pass in the layers as arguments to <code>Serial</code>, separated by commas. </li>
<li>For example: <code>tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))</code> </li>
</ul>
</li>
</ul>
<ul>
<li><code>tl.Embedding</code>: Maps discrete tokens to vectors. It will have shape (vocabulary length X dimension of output vectors). The dimension of output vectors (also called d_feature) is the number of elements in the word embedding. <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding">docs</a> / <a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113">source code</a><ul>
<li><code>tl.Embedding(vocab_size, d_feature)</code>.</li>
<li><code>vocab_size</code> is the number of unique words in the given vocabulary.</li>
<li><code>d_feature</code> is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).</li>
</ul>
</li>
</ul>
<ul>
<li><code>tl.LSTM</code> The LSTM layer. It leverages another Trax layer called <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTMCell"><code>LSTMCell</code></a>. The number of units should be specified and should match the number of elements in the word embedding. <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM">docs</a> / <a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/rnn.py#L87">source code</a><ul>
<li><code>tl.LSTM(n_units)</code> Builds an LSTM layer of n_units.</li>
</ul>
</li>
<li><code>tl.Mean</code>: Computes the mean across a desired axis. Mean uses one tensor axis to form groups of values and replaces each group with the mean value of that group. <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean">docs</a> / <a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L276">source code</a><ul>
<li><code>tl.Mean(axis=1)</code> mean over columns.</li>
</ul>
</li>
</ul>
<ul>
<li><code>tl.Fn</code> Layer with no weights that applies the function f, which should be specified using a lambda syntax. <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn">docs</a> / <a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/70f5364dcaf6ec11aabbd918e5f5e4b0f5bfb995/trax/layers/base.py#L576">source doce</a><ul>
<li>$x$ -&gt; This is used for cosine similarity.</li>
<li><code>tl.Fn(&#39;Normalize&#39;, lambda x: normalize(x))</code> Returns a layer with no weights that applies the function <code>f</code></li>
</ul>
</li>
<li><code>tl.parallel</code>: It is a combinator layer (like <code>Serial</code>) that applies a list of layers in parallel to its inputs. <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Parallel">docs</a> / <a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/37aba571a89a8ad86be76a569d0ec4a46bdd8642/trax/layers/combinators.py#L152">source code</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Siamese</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Siamese</span>(<span class="params">vocab_size=<span class="built_in">len</span>(<span class="params">vocab</span>), d_model=<span class="number">128</span>, mode=<span class="string">&#x27;train&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns a Siamese model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab_size (int, optional): Length of the vocabulary. Defaults to len(vocab).</span></span><br><span class="line"><span class="string">        d_model (int, optional): Depth of the model. Defaults to 128.</span></span><br><span class="line"><span class="string">        mode (str, optional): &#x27;train&#x27;, &#x27;eval&#x27; or &#x27;predict&#x27;, predict mode is for fast inference. Defaults to &#x27;train&#x27;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.layers.combinators.Parallel: A Siamese model. </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">normalize</span>(<span class="params">x</span>):</span>  <span class="comment"># normalizes the vectors to have L2 norm 1</span></span><br><span class="line">        <span class="keyword">return</span> x / fastnp.sqrt(fastnp.<span class="built_in">sum</span>(x * x, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    q_processor = tl.Serial(  <span class="comment"># Processor will run on Q1 and Q2.</span></span><br><span class="line">        tl.Embedding(vocab_size, d_model), <span class="comment"># Embedding layer</span></span><br><span class="line">        tl.LSTM(d_model), <span class="comment"># LSTM layer</span></span><br><span class="line">        tl.Mean(axis=<span class="number">1</span>), <span class="comment"># Mean over columns</span></span><br><span class="line">        tl.Fn(<span class="string">&#x27;Normalize&#x27;</span>, <span class="keyword">lambda</span> x: normalize(x))  <span class="comment"># Apply normalize function</span></span><br><span class="line">    )  <span class="comment"># Returns one vector of shape [batch_size, d_model].</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Run on Q1 and Q2 in parallel.</span></span><br><span class="line">    model = tl.Parallel(q_processor, q_processor)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Setup the Siamese network model</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># check your model</span></span><br><span class="line">model = Siamese()</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>

<pre><code>Parallel_in2_out2[
  Serial[
    Embedding_41699_128
    LSTM_128
    Mean
    Normalize
  ]
  Serial[
    Embedding_41699_128
    LSTM_128
    Mean
    Normalize
  ]
]
</code></pre>
<p><strong>Expected output:</strong>  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Parallel_in2_out2[</span><br><span class="line">  Serial[</span><br><span class="line">    Embedding_41699_128</span><br><span class="line">    LSTM_128</span><br><span class="line">    Mean</span><br><span class="line">    Normalize</span><br><span class="line">  ]</span><br><span class="line">  Serial[</span><br><span class="line">    Embedding_41699_128</span><br><span class="line">    LSTM_128</span><br><span class="line">    Mean</span><br><span class="line">    Normalize</span><br><span class="line">  ]</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p><a name='2.2'></a></p>
<h3 id="2-2-Hard-Negative-Mining"><a href="#2-2-Hard-Negative-Mining" class="headerlink" title="2.2 Hard  Negative Mining"></a>2.2 Hard  Negative Mining</h3><p>You will now implement the <code>TripletLoss</code>.<br><br>As explained in the lecture, loss is composed of two terms. One term utilizes the mean of all the non duplicates, the second utilizes the <em>closest negative</em>. Our loss expression is then:</p>
<p>\begin{align}<br> \mathcal{Loss_1(A,P,N)} &amp;=\max \left( -cos(A,P)  + mean_{neg} +\alpha, 0\right) \<br> \mathcal{Loss_2(A,P,N)} &amp;=\max \left( -cos(A,P)  + closest_{neg} +\alpha, 0\right) \<br>\mathcal{Loss(A,P,N)} &amp;= mean(Loss_1 + Loss_2) \<br>\end{align}</p>
<p>Further, two sets of instructions are provided. The first set provides a brief description of the task. If that set proves insufficient, a more detailed set can be displayed.  </p>
<p><a name='ex03'></a></p>
<h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p><strong>Instructions (Brief):</strong> Here is a list of things you should do: <br></p>
<ul>
<li>As this will be run inside trax, use <code>fastnp.xyz</code> when using any <code>xyz</code> numpy function</li>
<li>Use <code>fastnp.dot</code> to calculate the similarity matrix $v_1v_2^T$ of dimension <code>batch_size</code> x <code>batch_size</code></li>
<li>Take the score of the duplicates on the diagonal <code>fastnp.diagonal</code></li>
<li>Use the <code>trax</code> functions <code>fastnp.eye</code> and <code>fastnp.maximum</code> for the identity matrix and the maximum.</li>
</ul>
<details>    
<summary>
    <font size="3" color="darkgreen"><b>More Detailed Instructions </b></font>
</summary>
We'll describe the algorithm using a detailed example. Below, V1, V2 are the output of the normalization blocks in our model. Here we will use a batch_size of 4 and a d_model of 3. As explained in lecture, the inputs, Q1, Q2 are arranged so that corresponding inputs are duplicates while non-corresponding entries are not. The outputs will have the same pattern.
<img src = "C3_W4_triploss1.png" style="width:1021px;height:229px;"/>
This testcase arranges the outputs, v1,v2, to highlight different scenarios. Here, the first two outputs V1[0], V2[0] match exactly - so the model is generating the same vector for Q1[0] and Q2[0] inputs. The second outputs differ, circled in orange, we set, V2[1] is set to match V2[**2**], simulating a model which is generating very poor results. V1[3] and V2[3] match exactly again while V1[4] and V2[4] are set to be exactly wrong - 180 degrees from each other, circled in blue. 

<p>The first step is to compute the cosine similarity matrix or <code>score</code> in the code. As explained in lecture, this is $$V_1 V_2^T$$ This is generated with <code>fastnp.dot</code>.<br><img src = "C3_W4_triploss2.png" style="width:959px;height:236px;"/><br>The clever arrangement of inputs creates the data needed for positive <em>and</em> negative examples without having to run all pair-wise combinations. Because Q1[n] is a duplicate of only Q2[n], other combinations are explicitly created negative examples or <em>Hard Negative</em> examples. The matrix multiplication efficiently produces the cosine similarity of all positive/negative combinations as shown above on the left side of the diagram. ‘Positive’ are the results of duplicate examples and ‘negative’ are the results of explicitly created negative examples. The results for our test case are as expected, V1[0]V2[0] match producing ‘1’ while our other ‘positive’ cases (in green) don’t match well, as was arranged. The V2[2] was set to match V1[3] producing a poor match at <code>score[2,2]</code> and an undesired ‘negative’ case of a ‘1’ shown in grey. </p>
<p>With the similarity matrix (<code>score</code>) we can begin to implement the loss equations. First, we can extract $$cos(A,P)$$ by utilizing <code>fastnp.diagonal</code>. The goal is to grab all the green entries in the diagram above. This is <code>positive</code> in the code.</p>
<p>Next, we will create the <em>closest_negative</em>. This is the nonduplicate entry in V2 that is closest (has largest cosine similarity) to an entry in V1. Each row, n, of <code>score</code> represents all comparisons of the results of Q1[n] vs Q2[x] within a batch. A specific example in our testcase is row <code>score[2,:]</code>. It has the cosine similarity of V1[2] and V2[x]. The <em>closest_negative</em>, as was arranged, is V2[2] which has a score of 1. This is the maximum value of the ‘negative’ entries (blue entries in the diagram).</p>
<p>To implement this, we need to pick the maximum entry on a row of <code>score</code>, ignoring the ‘positive’/green entries. To avoid selecting the ‘positive’/green entries, we can make them larger negative numbers. Multiply <code>fastnp.eye(batch_size)</code> with 2.0 and subtract it out of <code>scores</code>. The result is <code>negative_without_positive</code>. Now we can use <code>fastnp.max</code>, row by row (axis=1), to select the maximum which is <code>closest_negative</code>.</p>
<p>Next, we’ll create <em>mean_negative</em>. As the name suggests, this is the mean of all the ‘negative’/blue values in <code>score</code> on a row by row basis. We can use <code>fastnp.eye(batch_size)</code> and a constant, this time to create a mask with zeros on the diagonal. Element-wise multiply this with <code>score</code> to get just the ‘negative values. This is <code>negative_zero_on_duplicate</code> in the code. Compute the mean by using <code>fastnp.sum</code> on <code>negative_zero_on_duplicate</code> for <code>axis=1</code> and divide it by <code>(batch_size - 1)</code> . This is <code>mean_negative</code>.</p>
<p>Now, we can compute loss using the two equations above and <code>fastnp.maximum</code>. This will form <code>triplet_loss1</code> and <code>triplet_loss2</code>. </p>
<p><code>triple_loss</code> is the <code>fastnp.mean</code> of the sum of the two individual losses.</p>
<p>Once you have this code matching the expected results, you can clip out the section between ### START CODE HERE and ### END CODE HERE it out and insert it into TripletLoss below.</p>
<p>&lt;\details&gt;  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: TripletLossFn</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TripletLossFn</span>(<span class="params">v1, v2, margin=<span class="number">0.25</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Custom Loss function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        v1 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q1.</span></span><br><span class="line"><span class="string">        v2 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q2.</span></span><br><span class="line"><span class="string">        margin (float, optional): Desired margin. Defaults to 0.25.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        jax.interpreters.xla.DeviceArray: Triplet Loss.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># use fastnp to take the dot product of the two batches (don&#x27;t forget to transpose the second argument)</span></span><br><span class="line">    scores = fastnp.dot(v1,v2.T)  <span class="comment"># pairwise cosine sim</span></span><br><span class="line">    <span class="comment"># calculate new batch size</span></span><br><span class="line">    batch_size = <span class="built_in">len</span>(scores)</span><br><span class="line">    <span class="comment"># use fastnp to grab all postive `diagonal` entries in `scores`</span></span><br><span class="line">    positive = fastnp.diagonal(scores)  <span class="comment"># the positive ones (duplicates)</span></span><br><span class="line">    <span class="comment"># multiply `fastnp.eye(batch_size)` with 2.0 and subtract it out of `scores`</span></span><br><span class="line">    negative_without_positive = scores - fastnp.eye(batch_size) * <span class="number">2.0</span> </span><br><span class="line">    <span class="comment"># take the row by row `max` of `negative_without_positive`. </span></span><br><span class="line">    <span class="comment"># Hint: negative_without_positive.max(axis = [?])  </span></span><br><span class="line">    closest_negative = negative_without_positive.<span class="built_in">max</span>(axis = <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># subtract `fastnp.eye(batch_size)` out of 1.0 and do element-wise multiplication with `scores`</span></span><br><span class="line">    negative_zero_on_duplicate = (<span class="number">1.0</span> - fastnp.eye(batch_size)) * scores</span><br><span class="line">    <span class="comment"># use `fastnp.sum` on `negative_zero_on_duplicate` for `axis=1` and divide it by `(batch_size - 1)` </span></span><br><span class="line">    mean_negative = fastnp.<span class="built_in">sum</span>(negative_zero_on_duplicate, axis=<span class="number">1</span>) / (batch_size - <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># compute `fastnp.maximum` among 0.0 and `A`</span></span><br><span class="line">    <span class="comment"># A = subtract `positive` from `margin` and add `closest_negative` </span></span><br><span class="line">    triplet_loss1 = fastnp.maximum(margin - positive + closest_negative, <span class="number">0</span> )</span><br><span class="line">    <span class="comment"># compute `fastnp.maximum` among 0.0 and `B`</span></span><br><span class="line">    <span class="comment"># B = subtract `positive` from `margin` and add `mean_negative`</span></span><br><span class="line">    triplet_loss2 = fastnp.maximum(margin - positive + mean_negative, <span class="number">0</span> )</span><br><span class="line">    <span class="comment"># add the two losses together and take the `fastnp.mean` of it</span></span><br><span class="line">    triplet_loss = fastnp.mean(triplet_loss1 + triplet_loss2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> triplet_loss</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v1 = np.array([[<span class="number">0.26726124</span>, <span class="number">0.53452248</span>, <span class="number">0.80178373</span>],[<span class="number">0.5178918</span> , <span class="number">0.57543534</span>, <span class="number">0.63297887</span>]])</span><br><span class="line">v2 = np.array([[ <span class="number">0.26726124</span>,  <span class="number">0.53452248</span>,  <span class="number">0.80178373</span>],[-<span class="number">0.5178918</span> , -<span class="number">0.57543534</span>, -<span class="number">0.63297887</span>]])</span><br><span class="line">TripletLossFn(v2,v1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Triplet Loss:&quot;</span>, TripletLossFn(v2,v1))</span><br></pre></td></tr></table></figure>

<pre><code>Triplet Loss: 0.5
</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Triplet Loss: <span class="number">0.5</span></span><br><span class="line">```   </span><br><span class="line"></span><br><span class="line">To make a layer out of a function with no trainable variables, use `tl.Fn`.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```<span class="function">python</span></span><br><span class="line"><span class="function">from functools <span class="keyword">import</span> partial</span></span><br><span class="line"><span class="function">def <span class="title">TripletLoss</span><span class="params">(margin=<span class="number">0.25</span>)</span>:</span></span><br><span class="line"><span class="function">    triplet_loss_fn =</span> <span class="built_in">partial</span>(TripletLossFn, margin=margin)</span><br><span class="line">    <span class="keyword">return</span> tl.<span class="built_in">Fn</span>(<span class="string">&#x27;TripletLoss&#x27;</span>, triplet_loss_fn)</span><br></pre></td></tr></table></figure>

<p><a name='3'></a></p>
<h1 id="Part-3-Training"><a href="#Part-3-Training" class="headerlink" title="Part 3: Training"></a>Part 3: Training</h1><p>Now you are going to train your model. As usual, you have to define the cost function and the optimizer. You also have to feed in the built model. Before, going into the training, we will use a special data set up. We will define the inputs using the data generator we built above. The lambda function acts as a seed to remember the last batch that was given. Run the cell below to get the question pairs inputs. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_generator = data_generator(train_Q1, train_Q2, batch_size, vocab[<span class="string">&#x27;&lt;PAD&gt;&#x27;</span>])</span><br><span class="line">val_generator = data_generator(val_Q1, val_Q2, batch_size, vocab[<span class="string">&#x27;&lt;PAD&gt;&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_Q1.shape &#x27;</span>, train_Q1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;val_Q1.shape   &#x27;</span>, val_Q1.shape)</span><br></pre></td></tr></table></figure>

<pre><code>train_Q1.shape  (89188,)
val_Q1.shape    (22298,)
</code></pre>
<p><a name='3.1'></a></p>
<h3 id="3-1-Training-the-model"><a href="#3-1-Training-the-model" class="headerlink" title="3.1 Training the model"></a>3.1 Training the model</h3><p>You will now write a function that takes in your model and trains it. To train your model you have to decide how many times you want to iterate over the entire data set; each iteration is defined as an <code>epoch</code>. For each epoch, you have to go over all the data, using your training iterator.</p>
<p><a name='ex04'></a></p>
<h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p><strong>Instructions:</strong> Implement the <code>train_model</code> below to train the neural network above. Here is a list of things you should do, as already shown in lecture 7: </p>
<ul>
<li>Create <code>TrainTask</code> and <code>EvalTask</code></li>
<li>Create the training loop <code>trax.supervised.training.Loop</code></li>
<li>Pass in the following depending on the context (train_task or eval_task):<ul>
<li><code>labeled_data=generator</code></li>
<li><code>metrics=[TripletLoss()]</code>,</li>
<li><code>loss_layer=TripletLoss()</code></li>
<li><code>optimizer=trax.optimizers.Adam</code> with learning rate of 0.01</li>
<li><code>lr_schedule=lr_schedule</code>,</li>
<li><code>output_dir=output_dir</code></li>
</ul>
</li>
</ul>
<p>You will be using your triplet loss function with Adam optimizer. Please read the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html?highlight=adam#trax.optimizers.adam.Adam">trax</a> documentation to get a full understanding. </p>
<p>This function should return a <code>training.Loop</code> object. To read more about this check the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html?highlight=loop#trax.supervised.training.Loop">docs</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">lr_schedule = trax.lr.warmup_and_rsqrt_decay(<span class="number">400</span>, <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: train_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span>(<span class="params">Siamese, TripletLoss, lr_schedule, train_generator=train_generator, val_generator=val_generator, output_dir=<span class="string">&#x27;model/&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Training the Siamese Model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        Siamese (function): Function that returns the Siamese model.</span></span><br><span class="line"><span class="string">        TripletLoss (function): Function that defines the TripletLoss loss function.</span></span><br><span class="line"><span class="string">        lr_schedule (function): Trax multifactor schedule function.</span></span><br><span class="line"><span class="string">        train_generator (generator, optional): Training generator. Defaults to train_generator.</span></span><br><span class="line"><span class="string">        val_generator (generator, optional): Validation generator. Defaults to val_generator.</span></span><br><span class="line"><span class="string">        output_dir (str, optional): Path to save model to. Defaults to &#x27;model/&#x27;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.supervised.training.Loop: Training loop for the model.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    output_dir = os.path.expanduser(output_dir)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line"></span><br><span class="line">    train_task = training.TrainTask(</span><br><span class="line">        labeled_data=train_generator,       <span class="comment"># Use generator (train)</span></span><br><span class="line">        loss_layer=TripletLoss(),         <span class="comment"># Use triplet loss. Don&#x27;t forget to instantiate this object</span></span><br><span class="line">        optimizer=trax.optimizers.Adam(learning_rate = <span class="number">0.01</span>),          <span class="comment"># Don&#x27;t forget to add the learning rate parameter</span></span><br><span class="line">        lr_schedule=lr_schedule, <span class="comment"># Use Trax multifactor schedule function</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    eval_task = training.EvalTask(</span><br><span class="line">        labeled_data=val_generator,       <span class="comment"># Use generator (val)</span></span><br><span class="line">        metrics=[TripletLoss()],          <span class="comment"># Use triplet loss. Don&#x27;t forget to instantiate this object</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    training_loop = training.Loop(Siamese(),</span><br><span class="line">                                  train_task,</span><br><span class="line">                                  eval_task=eval_task,</span><br><span class="line">                                  output_dir=output_dir)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> training_loop</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_steps = <span class="number">5</span></span><br><span class="line">training_loop = train_model(Siamese, TripletLoss, lr_schedule)</span><br><span class="line">training_loop.run(train_steps)</span><br></pre></td></tr></table></figure>

<pre><code>Step      1: train TripletLoss |  0.49954823
Step      1: eval  TripletLoss |  0.49950948
</code></pre>
<p>The model was only trained for 5 steps due to the constraints of this environment. For the rest of the assignment you will be using a pretrained model but now you should understand how the training can be done using Trax.</p>
<p><a name='4'></a></p>
<h1 id="Part-4-Evaluation"><a href="#Part-4-Evaluation" class="headerlink" title="Part 4:  Evaluation"></a>Part 4:  Evaluation</h1><p><a name='4.1'></a></p>
<h3 id="4-1-Evaluating-your-siamese-network"><a href="#4-1-Evaluating-your-siamese-network" class="headerlink" title="4.1 Evaluating your siamese network"></a>4.1 Evaluating your siamese network</h3><p>In this section you will learn how to evaluate a Siamese network. You will first start by loading a pretrained model and then you will use it to predict. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading in the saved model</span></span><br><span class="line">model = Siamese()</span><br><span class="line">model.init_from_file(<span class="string">&#x27;model.pkl.gz&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><a name='4.2'></a></p>
<h3 id="4-2-Classify"><a href="#4-2-Classify" class="headerlink" title="4.2 Classify"></a>4.2 Classify</h3><p>To determine the accuracy of the model, we will utilize the test set that was configured earlier. While in training we used only positive examples, the test data, Q1_test, Q2_test and y_test, is setup as pairs of questions, some of which are duplicates some are not.<br>This routine will run all the test question pairs through the model, compute the cosine simlarity of each pair, threshold it and compare the result to  y_test - the correct response from the data set. The results are accumulated to produce an accuracy.</p>
<p><a name='ex05'></a></p>
<h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p><strong>Instructions</strong>  </p>
<ul>
<li>Loop through the incoming data in batch_size chunks</li>
<li>Use the data generator to load q1, q2 a batch at a time. <strong>Don’t forget to set shuffle=False!</strong></li>
<li>copy a batch_size chunk of y into y_test</li>
<li>compute v1, v2 using the model</li>
<li>for each element of the batch<pre><code> - compute the cos similarity of each pair of entries, v1[j],v2[j]
 - determine if d &gt; threshold
 - increment accuracy if that result matches the expected results (y_test[j])
</code></pre>
</li>
<li>compute the final accuracy and return</li>
</ul>
<p>Due to some limitations of this environment, running classify multiple times may result in the kernel failing. If that happens <em>Restart Kernal &amp; clear output</em> and then run from the top. During development, consider using a smaller set of data to reduce the number of calls to model(). </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: classify</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span>(<span class="params">test_Q1, test_Q2, y, threshold, model, vocab, data_generator=data_generator, batch_size=<span class="number">64</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Function to test the accuracy of the model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        test_Q1 (numpy.ndarray): Array of Q1 questions.</span></span><br><span class="line"><span class="string">        test_Q2 (numpy.ndarray): Array of Q2 questions.</span></span><br><span class="line"><span class="string">        y (numpy.ndarray): Array of actual target.</span></span><br><span class="line"><span class="string">        threshold (float): Desired threshold.</span></span><br><span class="line"><span class="string">        model (trax.layers.combinators.Parallel): The Siamese model.</span></span><br><span class="line"><span class="string">        vocab (collections.defaultdict): The vocabulary used.</span></span><br><span class="line"><span class="string">        data_generator (function): Data generator function. Defaults to data_generator.</span></span><br><span class="line"><span class="string">        batch_size (int, optional): Size of the batches. Defaults to 64.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        float: Accuracy of the model.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    accuracy = <span class="number">0</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(test_Q1), batch_size):</span><br><span class="line">        <span class="comment"># Call the data generator (built in Ex 01) with shuffle=False using next()</span></span><br><span class="line">        <span class="comment"># use batch size chuncks of questions as Q1 &amp; Q2 arguments of the data generator. e.g x[i:i + batch_size]</span></span><br><span class="line">        <span class="comment"># Hint: use `vocab[&#x27;&lt;PAD&gt;&#x27;]` for the `pad` argument of the data generator</span></span><br><span class="line">        q1, q2 = <span class="built_in">next</span>(data_generator(test_Q1[i: i + batch_size], test_Q2[i: i+batch_size], batch_size, pad=vocab[<span class="string">&#x27;&lt;PAD&gt;&#x27;</span>], shuffle=<span class="literal">False</span>))</span><br><span class="line">        </span><br><span class="line"><span class="comment">#         print(q1.shape)</span></span><br><span class="line"><span class="comment">#         print(q2.shape)</span></span><br><span class="line"><span class="comment">#         (512, 64)</span></span><br><span class="line"><span class="comment">#         (512, 64)</span></span><br><span class="line">        <span class="comment"># use batch size chuncks of actual output targets (same syntax as example above)</span></span><br><span class="line">        y_test = y[i: i + batch_size]</span><br><span class="line">        <span class="comment"># Call the model</span></span><br><span class="line">        v1, v2 = model((q1,q2))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">            <span class="comment"># take dot product to compute cos similarity of each pair of entries, v1[j], v2[j]</span></span><br><span class="line">            <span class="comment"># don&#x27;t forget to transpose the second argument</span></span><br><span class="line">            d = fastnp.dot(v1[j],v2[j].T)</span><br><span class="line">            <span class="comment"># is d greater than the threshold?</span></span><br><span class="line">            res = d &gt; threshold</span><br><span class="line">            <span class="comment"># increment accurancy if y_test is equal `res`</span></span><br><span class="line">            accuracy += (y_test[j] == res)</span><br><span class="line">    <span class="comment"># compute accuracy using accuracy and total length of test questions</span></span><br><span class="line">    accuracy = accuracy / <span class="built_in">len</span>(test_Q1)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this takes around 1 minute</span></span><br><span class="line">accuracy = classify(Q1_test,Q2_test, y_test, <span class="number">0.7</span>, model, vocab, batch_size = <span class="number">512</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy&quot;</span>, accuracy)</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy 0.69091797
</code></pre>
<p><strong>Expected Result</strong><br>Accuracy ~0.69</p>
<p><a name='5'></a></p>
<h1 id="Part-5-Testing-with-your-own-questions"><a href="#Part-5-Testing-with-your-own-questions" class="headerlink" title="Part 5: Testing with your own questions"></a>Part 5: Testing with your own questions</h1><p>In this section you will test the model with your own questions. You will write a function <code>predict</code> which takes two questions as input and returns $1$ or $0$ depending on whether the question pair is a duplicate or not.   </p>
<p>But first, we build a reverse vocabulary that allows to map encoded questions back to words: </p>
<p>Write a function <code>predict</code>that takes in two questions, the model, and the vocabulary and returns whether the questions are duplicates ($1$) or not duplicates ($0$) given a similarity threshold. </p>
<p><a name='ex06'></a></p>
<h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p><strong>Instructions:</strong> </p>
<ul>
<li>Tokenize your question using <code>nltk.word_tokenize</code> </li>
<li>Create Q1,Q2 by encoding your questions as a list of numbers using vocab</li>
<li>pad Q1,Q2 with next(data_generator([Q1], [Q2],1,vocab[‘<PAD>‘]))</li>
<li>use model() to create v1, v2</li>
<li>compute the cosine similarity (dot product) of v1, v2</li>
<li>compute res by comparing d to the threshold</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: predict</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">question1, question2, threshold, model, vocab, data_generator=data_generator, verbose=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Function for predicting if two questions are duplicates.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        question1 (str): First question.</span></span><br><span class="line"><span class="string">        question2 (str): Second question.</span></span><br><span class="line"><span class="string">        threshold (float): Desired threshold.</span></span><br><span class="line"><span class="string">        model (trax.layers.combinators.Parallel): The Siamese model.</span></span><br><span class="line"><span class="string">        vocab (collections.defaultdict): The vocabulary used.</span></span><br><span class="line"><span class="string">        data_generator (function): Data generator function. Defaults to data_generator.</span></span><br><span class="line"><span class="string">        verbose (bool, optional): If the results should be printed out. Defaults to False.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        bool: True if the questions are duplicates, False otherwise.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    <span class="comment"># use `nltk` word tokenize function to tokenize</span></span><br><span class="line">    q1 = nltk.word_tokenize(question1)  <span class="comment"># tokenize</span></span><br><span class="line">    q2 = nltk.word_tokenize(question2)  <span class="comment"># tokenize</span></span><br><span class="line">    Q1, Q2 = [], []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> q1:  <span class="comment"># encode q1</span></span><br><span class="line">        <span class="comment"># increment by checking the &#x27;word&#x27; index in `vocab`</span></span><br><span class="line">        Q1 += [vocab[word]]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> q2:  <span class="comment"># encode q2</span></span><br><span class="line">        <span class="comment"># increment by checking the &#x27;word&#x27; index in `vocab`</span></span><br><span class="line">        Q2 += [vocab[word]]</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Call the data generator (built in Ex 01) using next()</span></span><br><span class="line">    <span class="comment"># pass [Q1] &amp; [Q2] as Q1 &amp; Q2 arguments of the data generator. Set batch size as 1</span></span><br><span class="line">    <span class="comment"># Hint: use `vocab[&#x27;&lt;PAD&gt;&#x27;]` for the `pad` argument of the data generator</span></span><br><span class="line">    Q1, Q2 = <span class="built_in">next</span>(data_generator([Q1], [Q2], <span class="number">1</span>, vocab[<span class="string">&#x27;&lt;PAD&gt;&#x27;</span>]))</span><br><span class="line">    <span class="comment"># Call the model</span></span><br><span class="line">    v1, v2 = model((Q1,Q2))</span><br><span class="line">    <span class="comment"># take dot product to compute cos similarity of each pair of entries, v1, v2</span></span><br><span class="line">    <span class="comment"># don&#x27;t forget to transpose the second argument</span></span><br><span class="line">    d = fastnp.dot(v1, v2.T)</span><br><span class="line">    <span class="comment"># is d greater than the threshold?</span></span><br><span class="line">    res = d &gt; threshold</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(verbose):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Q1  = &quot;</span>, Q1, <span class="string">&quot;\nQ2  = &quot;</span>, Q2)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;d   = &quot;</span>, d)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;res = &quot;</span>, res)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Feel free to try with your own questions</span></span><br><span class="line">question1 = <span class="string">&quot;When will I see you?&quot;</span></span><br><span class="line">question2 = <span class="string">&quot;When can I see you again?&quot;</span></span><br><span class="line"><span class="comment"># 1 means it is duplicated, 0 otherwise</span></span><br><span class="line">predict(question1 , question2, <span class="number">0.7</span>, model, vocab, verbose = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Q1  =  [[585  76   4  46  53  21   1   1]] 
Q2  =  [[ 585   33    4   46   53 7280   21    1]]
d   =  [[0.8811324]]
res =  [[ True]]





DeviceArray([[ True]], dtype=bool)
</code></pre>
<h5 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output"></a>Expected Output</h5><p>If input is:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">question1 = <span class="string">&quot;When will I see you?&quot;</span></span><br><span class="line">question2 = <span class="string">&quot;When can I see you again?&quot;</span></span><br></pre></td></tr></table></figure>

<p>Output is (d may vary a bit):</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q1  =  [[<span class="number">585</span>  <span class="number">76</span>   <span class="number">4</span>  <span class="number">46</span>  <span class="number">53</span>  <span class="number">21</span>   <span class="number">1</span>   <span class="number">1</span>]] </span><br><span class="line">Q2  =  [[ <span class="number">585</span>   <span class="number">33</span>    <span class="number">4</span>   <span class="number">46</span>   <span class="number">53</span> <span class="number">7280</span>   <span class="number">21</span>    <span class="number">1</span>]]</span><br><span class="line">d   =  <span class="number">0.88113236</span></span><br><span class="line">res =  True</span><br><span class="line">True</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Feel free to try with your own questions</span></span><br><span class="line">question1 = <span class="string">&quot;Do they enjoy eating the dessert?&quot;</span></span><br><span class="line">question2 = <span class="string">&quot;Do they like hiking in the desert?&quot;</span></span><br><span class="line"><span class="comment"># 1 means it is duplicated, 0 otherwise</span></span><br><span class="line">predict(question1 , question2, <span class="number">0.7</span>, model, vocab, verbose=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Q1  =  [[  443  1145  3159  1169    78 29017    21     1]] 
Q2  =  [[  443  1145    60 15302    28    78  7431    21]]
d   =  [[0.477536]]
res =  [[False]]





DeviceArray([[False]], dtype=bool)
</code></pre>
<h5 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h5><p>If input is:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">question1 = <span class="string">&quot;Do they enjoy eating the dessert?&quot;</span></span><br><span class="line">question2 = <span class="string">&quot;Do they like hiking in the desert?&quot;</span></span><br></pre></td></tr></table></figure>

<p>Output  (d may vary a bit):</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q1  =  [[  <span class="number">443</span>  <span class="number">1145</span>  <span class="number">3159</span>  <span class="number">1169</span>    <span class="number">78</span> <span class="number">29017</span>    <span class="number">21</span>     <span class="number">1</span>]] </span><br><span class="line">Q2  =  [[  <span class="number">443</span>  <span class="number">1145</span>    <span class="number">60</span> <span class="number">15302</span>    <span class="number">28</span>    <span class="number">78</span>  <span class="number">7431</span>    <span class="number">21</span>]]</span><br><span class="line">d   =  <span class="number">0.477536</span></span><br><span class="line">res =  False</span><br><span class="line">False</span><br></pre></td></tr></table></figure>

<p>You can see that the Siamese network is capable of catching complicated structures. Concretely it can identify question duplicates although the questions do not have many words in common. </p>
<p><a name='6'></a></p>
<h3 id="On-Siamese-networks"><a href="#On-Siamese-networks" class="headerlink" title=" On Siamese networks "></a><span style="color:blue"> On Siamese networks </span></h3><p>Siamese networks are important and useful. Many times there are several questions that are already asked in quora, or other platforms and you can use Siamese networks to avoid question duplicates. </p>
<p>Congratulations, you have now built a powerful system that can recognize question duplicates. In the next course we will use transformers for machine translation, summarization, question answering, and chatbots. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Named-Entity-Recognition-NER/2020/08/23/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Named-Entity-Recognition-NER/2020/08/23/" class="post-title-link" itemprop="url">Named Entity Recognition (NER)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-08-23 01:01:04 / Modified: 01:01:35" itemprop="dateCreated datePublished" datetime="2020-08-23T01:01:04+08:00">2020-08-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Named-Entity-Recognition-NER/2020/08/23/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Named-Entity-Recognition-NER/2020/08/23/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Assignment-3-Named-Entity-Recognition-NER"><a href="#Assignment-3-Named-Entity-Recognition-NER" class="headerlink" title="Assignment 3 - Named Entity Recognition (NER)"></a>Assignment 3 - Named Entity Recognition (NER)</h1><p>Welcome to the third programming assignment of Course 3. In this assignment, you will learn to build more complicated models with Trax. By completing this assignment, you will be able to: </p>
<ul>
<li>Design the architecture of a neural network, train it, and test it. </li>
<li>Process features and represents them</li>
<li>Understand word padding</li>
<li>Implement LSTMs</li>
<li>Test with your own sentence</li>
</ul>
<h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li><a href="#0">Introduction</a></li>
<li><a href="#1">Part 1:  Exploring the data</a><ul>
<li><a href="#1.1">1.1  Importing the Data</a></li>
<li><a href="#1.2">1.2  Data generator</a><ul>
<li><a href="#ex01">Exercise 01</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2">Part 2:  Building the model</a><ul>
<li><a href="#ex02">Exercise 02</a></li>
</ul>
</li>
<li><a href="#3">Part 3:  Train the Model </a><ul>
<li><a href="#ex03">Exercise 03</a></li>
</ul>
</li>
<li><a href="#4">Part 4:  Compute Accuracy</a><ul>
<li><a href="#ex04">Exercise 04</a></li>
</ul>
</li>
<li><a href="#5">Part 5:  Testing with your own sentence</a></li>
</ul>
<p><a name="0"></a></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>We first start by defining named entity recognition (NER). NER is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times, etc. </p>
<p>For example:</p>
<img src = 'ner.png' width="width" height="height" style="width:600px;height:150px;"/>

<p>Is labeled as follows: </p>
<ul>
<li>French: geopolitical entity</li>
<li>Morocco: geographic entity </li>
<li>Christmas: time indicator</li>
</ul>
<p>Everything else that is labeled with an <code>O</code> is not considered to be a named entity. In this assignment, you will train a named entity recognition system that could be trained in a few seconds (on a GPU) and will get around 75% accuracy. Then, you will load in the exact version of your model, which was trained for a longer period of time. You could then evaluate the trained version of your model to get 96% accuracy! Finally, you will be able to test your named entity recognition system with your own sentence.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!pip -q install trax==1.3.1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> trax </span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">import</span> os </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> get_params, get_vocab</span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rnd</span><br><span class="line"></span><br><span class="line"><span class="comment"># set random seeds to make this notebook easier to replicate</span></span><br><span class="line">trax.supervised.trainer_lib.init_random_number_generators(<span class="number">33</span>)</span><br></pre></td></tr></table></figure>

<pre><code>INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 





DeviceArray([ 0, 33], dtype=uint32)
</code></pre>
<p><a name="1"></a></p>
<h1 id="Part-1-Exploring-the-data"><a href="#Part-1-Exploring-the-data" class="headerlink" title="Part 1:  Exploring the data"></a>Part 1:  Exploring the data</h1><p>We will be using a dataset from Kaggle, which we will preprocess for you. The original data consists of four columns, the sentence number, the word, the part of speech of the word, and the tags.  A few tags you might expect to see are: </p>
<ul>
<li>geo: geographical entity</li>
<li>org: organization</li>
<li>per: person </li>
<li>gpe: geopolitical entity</li>
<li>tim: time indicator</li>
<li>art: artifact</li>
<li>eve: event</li>
<li>nat: natural phenomenon</li>
<li>O: filler word</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># display original kaggle data</span></span><br><span class="line">data = pd.read_csv(<span class="string">&quot;ner_dataset.csv&quot;</span>, encoding = <span class="string">&quot;ISO-8859-1&quot;</span>) </span><br><span class="line">train_sents = <span class="built_in">open</span>(<span class="string">&#x27;data/small/train/sentences.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>).readline()</span><br><span class="line">train_labels = <span class="built_in">open</span>(<span class="string">&#x27;data/small/train/labels.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>).readline()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;SENTENCE:&#x27;</span>, train_sents)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;SENTENCE LABEL:&#x27;</span>, train_labels)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ORIGINAL DATA:\n&#x27;</span>, data.head(<span class="number">5</span>))</span><br><span class="line"><span class="keyword">del</span>(data, train_sents, train_labels)</span><br></pre></td></tr></table></figure>

<pre><code>SENTENCE: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .

SENTENCE LABEL: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O

ORIGINAL DATA:
     Sentence #           Word  POS Tag
0  Sentence: 1      Thousands  NNS   O
1          NaN             of   IN   O
2          NaN  demonstrators  NNS   O
3          NaN           have  VBP   O
4          NaN        marched  VBN   O
</code></pre>
<p><a name="1.1"></a></p>
<h2 id="1-1-Importing-the-Data"><a href="#1-1-Importing-the-Data" class="headerlink" title="1.1  Importing the Data"></a>1.1  Importing the Data</h2><p>In this part, we will import the preprocessed data and explore it.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vocab, tag_map = get_vocab(<span class="string">&#x27;data/large/words.txt&#x27;</span>, <span class="string">&#x27;data/large/tags.txt&#x27;</span>)</span><br><span class="line">t_sentences, t_labels, t_size = get_params(vocab, tag_map, <span class="string">&#x27;data/large/train/sentences.txt&#x27;</span>, <span class="string">&#x27;data/large/train/labels.txt&#x27;</span>)</span><br><span class="line">v_sentences, v_labels, v_size = get_params(vocab, tag_map, <span class="string">&#x27;data/large/val/sentences.txt&#x27;</span>, <span class="string">&#x27;data/large/val/labels.txt&#x27;</span>)</span><br><span class="line">test_sentences, test_labels, test_size = get_params(vocab, tag_map, <span class="string">&#x27;data/large/test/sentences.txt&#x27;</span>, <span class="string">&#x27;data/large/test/labels.txt&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><code>vocab</code> is a dictionary that translates a word string to a unique number. Given a sentence, you can represent it as an array of numbers translating with this dictionary. The dictionary contains a <code>&lt;PAD&gt;</code> token. </p>
<p>When training an LSTM using batches, all your input sentences must be the same size. To accomplish this, you set the length of your sentences to a certain number and add the generic <code>&lt;PAD&gt;</code> token to fill all the empty spaces. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vocab translates from a word to a unique number</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;vocab[&quot;the&quot;]:&#x27;</span>, vocab[<span class="string">&quot;the&quot;</span>])</span><br><span class="line"><span class="comment"># Pad token</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;padded token:&#x27;</span>, vocab[<span class="string">&#x27;&lt;PAD&gt;&#x27;</span>])</span><br></pre></td></tr></table></figure>

<pre><code>vocab[&quot;the&quot;]: 9
padded token: 35180
</code></pre>
<p>The tag_map corresponds to one of the possible tags a word can have. Run the cell below to see the possible classes you will be predicting. The prepositions in the tags mean:</p>
<ul>
<li>I: Token is inside an entity.</li>
<li>B: Token begins an entity.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tag_map)</span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;O&#39;: 0, &#39;B-geo&#39;: 1, &#39;B-gpe&#39;: 2, &#39;B-per&#39;: 3, &#39;I-geo&#39;: 4, &#39;B-org&#39;: 5, &#39;I-org&#39;: 6, &#39;B-tim&#39;: 7, &#39;B-art&#39;: 8, &#39;I-art&#39;: 9, &#39;I-per&#39;: 10, &#39;I-gpe&#39;: 11, &#39;I-tim&#39;: 12, &#39;B-nat&#39;: 13, &#39;B-eve&#39;: 14, &#39;I-eve&#39;: 15, &#39;I-nat&#39;: 16&#125;
</code></pre>
<p>So the coding scheme that tags the entities is a minimal one where B- indicates the first token in a multi-token entity, and I- indicates one in the middle of a multi-token entity. If you had the sentence </p>
<p><strong>“Sharon flew to Miami on Friday”</strong></p>
<p>the outputs would look like:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Sharon B-per</span><br><span class="line">flew   O</span><br><span class="line">to     O</span><br><span class="line">Miami  B-geo</span><br><span class="line">on     O</span><br><span class="line">Friday B-tim</span><br></pre></td></tr></table></figure>

<p>your tags would reflect three tokens beginning with B-, since there are no multi-token entities in the sequence. But if you added Sharon’s last name to the sentence: </p>
<p><strong>“Sharon Floyd flew to Miami on Friday”</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Sharon B-per</span><br><span class="line">Floyd  I-per</span><br><span class="line">flew   O</span><br><span class="line">to     O</span><br><span class="line">Miami  B-geo</span><br><span class="line">on     O</span><br><span class="line">Friday B-tim</span><br></pre></td></tr></table></figure>

<p>then your tags would change to show first “Sharon” as B-per, and “Floyd” as I-per, where I- indicates an inner token in a multi-token sequence.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Exploring information about the data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The number of outputs is tag_map&#x27;</span>, <span class="built_in">len</span>(tag_map))</span><br><span class="line"><span class="comment"># The number of vocabulary tokens (including &lt;PAD&gt;)</span></span><br><span class="line">g_vocab_size = <span class="built_in">len</span>(vocab)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Num of vocabulary words: <span class="subst">&#123;g_vocab_size&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The vocab size is&#x27;</span>, <span class="built_in">len</span>(vocab))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The training size is&#x27;</span>, t_size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;The validation size is&#x27;</span>, v_size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;An example of the first sentence is&#x27;</span>, t_sentences[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;An example of its corresponding label is&#x27;</span>, t_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<pre><code>The number of outputs is tag_map 17
Num of vocabulary words: 35181
The vocab size is 35181
The training size is 33570
The validation size is 7194
An example of the first sentence is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 1, 16, 17, 18, 19, 20, 21]
An example of its corresponding label is [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]
</code></pre>
<p>So you can see that we have already encoded each sentence into a tensor by converting it into a number. We also have 16 possible classes, as shown in the tag map.</p>
<p><a name="1.2"></a></p>
<h2 id="1-2-Data-generator"><a href="#1-2-Data-generator" class="headerlink" title="1.2  Data generator"></a>1.2  Data generator</h2><p>In python, a generator is a function that behaves like an iterator. It will return the next item. Here is a <a target="_blank" rel="noopener" href="https://wiki.python.org/moin/Generators">link</a> to review python generators. </p>
<p>In many AI applications it is very useful to have a data generator. You will now implement a data generator for our NER application.</p>
<p><a name="ex01"></a></p>
<h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p><strong>Instructions:</strong> Implement a data generator function that takes in <code>batch_size, x, y, pad, shuffle</code> where x is a large list of sentences, and y is a list of the tags associated with those sentences and pad is a pad value. Return a subset of those inputs in a tuple of two arrays <code>(X,Y)</code>. Each is an array of dimension (<code>batch_size, max_len</code>), where <code>max_len</code> is the length of the longest sentence <em>in that batch</em>. You will pad the X and Y examples with the pad argument. If <code>shuffle=True</code>, the data will be traversed in a random form.</p>
<p><strong>Details:</strong></p>
<p>This code as an outer loop  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">while True:  </span><br><span class="line">...  </span><br><span class="line">yield((X,Y))  </span><br></pre></td></tr></table></figure>

<p>Which runs continuously in the fashion of generators, pausing when yielding the next values. We will generate a batch_size output on each pass of this loop.    </p>
<p>It has two inner loops. </p>
<ol>
<li><p>The first stores in temporal lists the data samples to be included in the next batch, and finds the maximum length of the sentences contained in it. By adjusting the length to include only the size of the longest sentence in each batch, overall computation is reduced. </p>
</li>
<li><p>The second loop moves those inputs from the temporal list into NumPy arrays pre-filled with pad values.</p>
</li>
</ol>
<p>There are three slightly out of the ordinary features. </p>
<ol>
<li><p>The first is the use of the NumPy <code>full</code> function to fill the NumPy arrays with a pad value. See <a target="_blank" rel="noopener" href="https://numpy.org/doc/1.18/reference/generated/numpy.full.html">full function documentation</a>.</p>
</li>
<li><p>The second is tracking the current location in the incoming lists of sentences. Generators variables hold their values between invocations, so we create an <code>index</code> variable, initialize to zero, and increment by one for each sample included in a batch. However, we do not use the <code>index</code> to access the positions of the list of sentences directly. Instead, we use it to select one index from a list of indexes. In this way, we can change the order in which we traverse our original list, keeping untouched our original list.  </p>
</li>
<li><p>The third also relates to wrapping. Because <code>batch_size</code> and the length of the input lists are not aligned, gathering a batch_size group of inputs may involve wrapping back to the beginning of the input loop. In our approach, it is just enough to reset the <code>index</code> to 0. We can re-shuffle the list of indexes to produce different batches each time.</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: data_generator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_generator</span>(<span class="params">batch_size, x, y, pad, shuffle=<span class="literal">False</span>, verbose=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">      Input: </span></span><br><span class="line"><span class="string">        batch_size - integer describing the batch size</span></span><br><span class="line"><span class="string">        x - list containing sentences where words are represented as integers</span></span><br><span class="line"><span class="string">        y - list containing tags associated with the sentences</span></span><br><span class="line"><span class="string">        shuffle - Shuffle the data order</span></span><br><span class="line"><span class="string">        pad - an integer representing a pad character</span></span><br><span class="line"><span class="string">        verbose - Print information during runtime</span></span><br><span class="line"><span class="string">      Output:</span></span><br><span class="line"><span class="string">        a tuple containing 2 elements:</span></span><br><span class="line"><span class="string">        X - np.ndarray of dim (batch_size, max_len) of padded sentences</span></span><br><span class="line"><span class="string">        Y - np.ndarray of dim (batch_size, max_len) of tags associated with the sentences in X</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># count the number of lines in data_lines</span></span><br><span class="line">    num_lines = <span class="built_in">len</span>(x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># create an array with the indexes of data_lines that can be shuffled</span></span><br><span class="line">    lines_index = [*<span class="built_in">range</span>(num_lines)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># shuffle the indexes if shuffle is set to True</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        rnd.shuffle(lines_index)</span><br><span class="line">    </span><br><span class="line">    index = <span class="number">0</span> <span class="comment"># tracks current location in x, y</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        buffer_x = [<span class="number">0</span>] * batch_size <span class="comment"># Temporal array to store the raw x data for this batch</span></span><br><span class="line">        buffer_y = [<span class="number">0</span>] * batch_size <span class="comment"># Temporal array to store the raw y data for this batch</span></span><br><span class="line">                </span><br><span class="line">  <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Copy into the temporal buffers the sentences in x[index : index + batch_size] </span></span><br><span class="line">        <span class="comment"># along with their corresponding labels y[index : index + batch_size]</span></span><br><span class="line">        <span class="comment"># Find maximum length of sentences in x[index : index + batch_size] for this batch. </span></span><br><span class="line">        <span class="comment"># Reset the index if we reach the end of the data set, and shuffle the indexes if needed.</span></span><br><span class="line">        max_len = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">             <span class="comment"># if the index is greater than or equal to the number of lines in x</span></span><br><span class="line">            <span class="keyword">if</span> index &gt;= num_lines:</span><br><span class="line">                <span class="comment"># then reset the index to 0</span></span><br><span class="line">                index = <span class="number">0</span></span><br><span class="line">                <span class="comment"># re-shuffle the indexes if shuffle is set to True</span></span><br><span class="line">                <span class="keyword">if</span> shuffle:</span><br><span class="line">                    rnd.shuffle(lines_index)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># The current position is obtained using `lines_index[index]`</span></span><br><span class="line">            <span class="comment"># Store the x value at the current position into the buffer_x</span></span><br><span class="line">            buffer_x[i] = x[lines_index[index]]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Store the y value at the current position into the buffer_y</span></span><br><span class="line">            buffer_y[i] = y[lines_index[index]]</span><br><span class="line">            </span><br><span class="line">            lenx = <span class="built_in">len</span>(x[lines_index[index]])    <span class="comment">#length of current x[]</span></span><br><span class="line">            <span class="keyword">if</span> lenx &gt; max_len:</span><br><span class="line">                max_len = lenx                   <span class="comment">#max_len tracks longest x[]</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># increment index by one</span></span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># create X,Y, NumPy arrays of size (batch_size, max_len) &#x27;full&#x27; of pad value</span></span><br><span class="line">        X = np.full((batch_size, max_len), pad)</span><br><span class="line">        Y = np.full((batch_size, max_len), pad)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># copy values from lists to NumPy arrays. Use the buffered values</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">            <span class="comment"># get the example (sentence as a tensor)</span></span><br><span class="line">            <span class="comment"># in `buffer_x` at the `i` index</span></span><br><span class="line">            x_i = buffer_x[i]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># similarly, get the example&#x27;s labels</span></span><br><span class="line">            <span class="comment"># in `buffer_y` at the `i` index</span></span><br><span class="line">            y_i = buffer_y[i]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Walk through each word in x_i</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_i)):</span><br><span class="line">                <span class="comment"># store the word in x_i at position j into X</span></span><br><span class="line">                X[i, j] = x_i[j]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># store the label in y_i at position j into Y</span></span><br><span class="line">                Y[i, j] = y_i[j]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        <span class="keyword">if</span> verbose: <span class="built_in">print</span>(<span class="string">&quot;index=&quot;</span>, index)</span><br><span class="line">        <span class="keyword">yield</span>((X,Y))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">5</span></span><br><span class="line">mini_sentences = t_sentences[<span class="number">0</span>: <span class="number">8</span>]</span><br><span class="line">mini_labels = t_labels[<span class="number">0</span>: <span class="number">8</span>]</span><br><span class="line">dg = data_generator(batch_size, mini_sentences, mini_labels, vocab[<span class="string">&quot;&lt;PAD&gt;&quot;</span>], shuffle=<span class="literal">False</span>, verbose=<span class="literal">True</span>)</span><br><span class="line">X1, Y1 = <span class="built_in">next</span>(dg)</span><br><span class="line">X2, Y2 = <span class="built_in">next</span>(dg)</span><br><span class="line"><span class="built_in">print</span>(Y1.shape, X1.shape, Y2.shape, X2.shape)</span><br><span class="line"><span class="built_in">print</span>(X1[<span class="number">0</span>][:], <span class="string">&quot;\n&quot;</span>, Y1[<span class="number">0</span>][:])</span><br></pre></td></tr></table></figure>

<pre><code>index= 5
index= 2
(5, 30) (5, 30) (5, 30) (5, 30)
[    0     1     2     3     4     5     6     7     8     9    10    11
    12    13    14     9    15     1    16    17    18    19    20    21
 35180 35180 35180 35180 35180 35180] 
 [    0     0     0     0     0     0     1     0     0     0     0     0
     1     0     0     0     0     0     2     0     0     0     0     0
 35180 35180 35180 35180 35180 35180]
</code></pre>
<p><strong>Expected output:</strong>   </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">index= 5</span><br><span class="line">index= 2</span><br><span class="line">(5, 30) (5, 30) (5, 30) (5, 30)</span><br><span class="line">[    0     1     2     3     4     5     6     7     8     9    10    11</span><br><span class="line">    12    13    14     9    15     1    16    17    18    19    20    21</span><br><span class="line"> 35180 35180 35180 35180 35180 35180] </span><br><span class="line"> [    0     0     0     0     0     0     1     0     0     0     0     0</span><br><span class="line">     1     0     0     0     0     0     2     0     0     0     0     0</span><br><span class="line"> 35180 35180 35180 35180 35180 35180]  </span><br></pre></td></tr></table></figure>

<p><a name="2"></a></p>
<h1 id="Part-2-Building-the-model"><a href="#Part-2-Building-the-model" class="headerlink" title="Part 2:  Building the model"></a>Part 2:  Building the model</h1><p>You will now implement the model. You will be using Google’s TensorFlow. Your model will be able to distinguish the following:</p>
<table>
    <tr>
        <td>
<img src = 'ner1.png' width="width" height="height" style="width:500px;height:150px;"/>
        </td>
    </tr>
</table>

<p>The model architecture will be as follows: </p>
<img src = 'ner2.png' width="width" height="height" style="width:600px;height:250px;"/>

<p>Concretely: </p>
<ul>
<li>Use the input tensors you built in your data generator</li>
<li>Feed it into an Embedding layer, to produce more semantic entries</li>
<li>Feed it into an LSTM layer</li>
<li>Run the output through a linear layer</li>
<li>Run the result through a log softmax layer to get the predicted class for each word.</li>
</ul>
<p>Good news! We won’t make you implement the LSTM unit drawn above. However, we will ask you to build the model. </p>
<p><a name="ex02"></a></p>
<h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p><strong>Instructions:</strong> Implement the initialization step and the forward function of your Named Entity Recognition system.<br>Please utilize help function e.g. <code>help(tl.Dense)</code> for more information on a layer</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/combinators.py#L26">tl.Serial</a>: Combinator that applies layers serially (by function composition).<ul>
<li>You can pass in the layers as arguments to <code>Serial</code>, separated by commas. </li>
<li>For example: <code>tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))</code> </li>
</ul>
</li>
</ul>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113">tl.Embedding</a>: Initializes the embedding. In this case it is the dimension of the model by the size of the vocabulary. </p>
<ul>
<li><code>tl.Embedding(vocab_size, d_feature)</code>.</li>
<li><code>vocab_size</code> is the number of unique words in the given vocabulary.</li>
<li><code>d_feature</code> is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/rnn.py#L87">tl.LSTM</a>:<code>Trax</code> LSTM layer of size d_model. </p>
<ul>
<li><code>LSTM(n_units)</code> Builds an LSTM layer of n_cells.</li>
</ul>
</li>
</ul>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L28">tl.Dense</a>:  A dense layer.<ul>
<li><code>tl.Dense(n_units)</code>: The parameter <code>n_units</code> is the number of units chosen for this dense layer.  </li>
</ul>
</li>
</ul>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L242">tl.LogSoftmax</a>: Log of the output probabilities.<ul>
<li>Here, you don’t need to set any parameters for <code>LogSoftMax()</code>.</li>
</ul>
</li>
</ul>
<p><strong>Online documentation</strong></p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators">tl.Serial</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding">tl.Embedding</a></p>
</li>
<li><p> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM">tl.LSTM</a></p>
</li>
<li><p> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">tl.Dense</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax">tl.LogSoftmax</a>    </p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: NER</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">NER</span>(<span class="params">vocab_size=<span class="number">35181</span>, d_model=<span class="number">50</span>, tags=tag_map</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">      Input: </span></span><br><span class="line"><span class="string">        vocab_size - integer containing the size of the vocabulary</span></span><br><span class="line"><span class="string">        d_model - integer describing the embedding size</span></span><br><span class="line"><span class="string">      Output:</span></span><br><span class="line"><span class="string">        model - a trax serial model</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    model = tl.Serial(</span><br><span class="line">      tl.Embedding(vocab_size,d_model), <span class="comment"># Embedding layer</span></span><br><span class="line">      tl.LSTM(d_model), <span class="comment"># LSTM layer</span></span><br><span class="line">      tl.Dense(<span class="built_in">len</span>(tags)), <span class="comment"># Dense layer with len(tags) units</span></span><br><span class="line">      tl.LogSoftmax()  <span class="comment"># LogSoftmax layer</span></span><br><span class="line">      )</span><br><span class="line">      <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initializing your model</span></span><br><span class="line">model = NER()</span><br><span class="line"><span class="comment"># display your model</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>

<pre><code>Serial[
  Embedding_35181_50
  LSTM_50
  Dense_17
  LogSoftmax
]
</code></pre>
<p><strong>Expected output:</strong>  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">Serial[</span><br><span class="line">  Embedding_35181_50</span><br><span class="line">  LSTM_50</span><br><span class="line">  Dense_17</span><br><span class="line">  LogSoftmax</span><br><span class="line">]</span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;a name=&quot;3&quot;&gt;&lt;/a&gt;</span><br><span class="line"># Part 3:  Train the Model </span><br><span class="line"></span><br><span class="line">This section will train your model.</span><br><span class="line"></span><br><span class="line">Before you start, you need to create the data generators for training and validation data. It is important that you mask padding in the loss weights of your data, which can be done using the `id_to_mask` argument of `trax.supervised.inputs.add_loss_weights`.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">from trax.supervised import training</span><br><span class="line"></span><br><span class="line">rnd.seed(33)</span><br><span class="line"></span><br><span class="line">batch_size = 64</span><br><span class="line"></span><br><span class="line"># Create training data, mask pad id=35180 for training.</span><br><span class="line">train_generator = trax.supervised.inputs.add_loss_weights(</span><br><span class="line">    data_generator(batch_size, t_sentences, t_labels, vocab[&#x27;&lt;PAD&gt;&#x27;], True),</span><br><span class="line">    id_to_mask=vocab[&#x27;&lt;PAD&gt;&#x27;])</span><br><span class="line"></span><br><span class="line"># Create validation data, mask pad id=35180 for training.</span><br><span class="line">eval_generator = trax.supervised.inputs.add_loss_weights(</span><br><span class="line">    data_generator(batch_size, v_sentences, v_labels, vocab[&#x27;&lt;PAD&gt;&#x27;], True),</span><br><span class="line">    id_to_mask=vocab[&#x27;&lt;PAD&gt;&#x27;])</span><br></pre></td></tr></table></figure>

<p><a name='3.1'></a></p>
<h3 id="3-1-Training-the-model"><a href="#3-1-Training-the-model" class="headerlink" title="3.1 Training the model"></a>3.1 Training the model</h3><p>You will now write a function that takes in your model and trains it.</p>
<p>As you’ve seen in the previous assignments, you will first create the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/stable/trax.supervised.html#trax.supervised.training.TrainTask">TrainTask</a> and <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/stable/trax.supervised.html#trax.supervised.training.EvalTask">EvalTask</a> using your data generator. Then you will use the <code>training.Loop</code> to train your model.</p>
<p><a name="ex03"></a></p>
<h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p><strong>Instructions:</strong> Implement the <code>train_model</code> program below to train the neural network above. Here is a list of things you should do: </p>
<ul>
<li><p>Create the trainer object by calling <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop"><code>trax.supervised.training.Loop</code></a> and pass in the following:</p>
<ul>
<li>model = <a href="#ex02">NER</a></li>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask">training task</a> that uses the train data generator defined in the cell above<ul>
<li>loss_layer = <a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/22765bb18608d376d8cd660f9865760e4ff489cd/trax/layers/metrics.py#L71">tl.CrossEntropyLoss()</a></li>
<li>optimizer = <a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/03cb32995e83fc1455b0c8d1c81a14e894d0b7e3/trax/optimizers/adam.py#L23">trax.optimizers.Adam(0.01)</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask">evaluation task</a> that uses the validation data generator defined in the cell above<ul>
<li>metrics for <code>EvalTask</code>: <code>tl.CrossEntropyLoss()</code> and <code>tl.Accuracy()</code></li>
<li>in <code>EvalTask</code> set <code>n_eval_batches=10</code> for better evaluation accuracy</li>
</ul>
</li>
<li>output_dir = output_dir</li>
</ul>
</li>
</ul>
<p>You’ll be using a <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss">cross entropy loss</a>, with an <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam">Adam optimizer</a>. Please read the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.html">trax</a> documentation to get a full understanding. The <a target="_blank" rel="noopener" href="https://github.com/google/trax">trax GitHub</a> also contains some useful information and a link to a colab notebook.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: train_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span>(<span class="params">NER, train_generator, eval_generator, train_steps=<span class="number">1</span>, output_dir=<span class="string">&#x27;model&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        NER - the model you are building</span></span><br><span class="line"><span class="string">        train_generator - The data generator for training examples</span></span><br><span class="line"><span class="string">        eval_generator - The data generator for validation examples,</span></span><br><span class="line"><span class="string">        train_steps - number of training steps</span></span><br><span class="line"><span class="string">        output_dir - folder to save your model</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        training_loop - a trax supervised training Loop</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    train_task = training.TrainTask(</span><br><span class="line">      train_generator, <span class="comment"># A train data generator</span></span><br><span class="line">      loss_layer = tl.CrossEntropyLoss(), <span class="comment"># A cross-entropy loss function</span></span><br><span class="line">      optimizer =  trax.optimizers.Adam(<span class="number">0.01</span>),  <span class="comment"># The adam optimizer</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    eval_task = training.EvalTask(</span><br><span class="line">      labeled_data = eval_generator, <span class="comment"># A labeled data generator</span></span><br><span class="line">      metrics = [tl.CrossEntropyLoss(), tl.Accuracy()], <span class="comment"># Evaluate with cross-entropy loss and accuracy</span></span><br><span class="line">      n_eval_batches = <span class="number">10</span> <span class="comment"># Number of batches to use on each evaluation</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    training_loop = training.Loop(</span><br><span class="line">        NER, <span class="comment"># A model to train</span></span><br><span class="line">        train_task, <span class="comment"># A train task</span></span><br><span class="line">        eval_task = eval_task, <span class="comment"># The evaluation task</span></span><br><span class="line">        output_dir = output_dir) <span class="comment"># The output directory</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Train with train_steps</span></span><br><span class="line">    training_loop.run(n_steps = train_steps)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> training_loop</span><br></pre></td></tr></table></figure>

<p>On your local machine, you can run this training for 1000 train_steps and get your own model. This training takes about 5 to 10 minutes to run.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_steps = <span class="number">100</span>            <span class="comment"># In coursera we can only train 100 steps</span></span><br><span class="line">!rm -f <span class="string">&#x27;model/model.pkl.gz&#x27;</span>  <span class="comment"># Remove old model.pkl if it exists</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">training_loop = train_model(NER(), train_generator, eval_generator, train_steps)</span><br></pre></td></tr></table></figure>

<pre><code>Step      1: train CrossEntropyLoss |  3.29933977
Step      1: eval  CrossEntropyLoss |  2.27930465
Step      1: eval          Accuracy |  0.22279498
Step    100: train CrossEntropyLoss |  0.61237383
Step    100: eval  CrossEntropyLoss |  0.37608672
Step    100: eval          Accuracy |  0.90983244
</code></pre>
<p><strong>Expected output (Approximately)</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">Step      1: train CrossEntropyLoss |  2.94375849</span><br><span class="line">Step      1: eval  CrossEntropyLoss |  1.93172036</span><br><span class="line">Step      1: eval          Accuracy |  0.78727312</span><br><span class="line">Step    100: train CrossEntropyLoss |  0.57727730</span><br><span class="line">Step    100: eval  CrossEntropyLoss |  0.36356260</span><br><span class="line">Step    100: eval          Accuracy |  0.90943187</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>This value may change between executions, but it must be around 90% of accuracy on train and validations sets, after 100 training steps.</p>
<p>We have trained the model longer, and we give you such a trained model. In that way, we ensure you can continue with the rest of the assignment even if you had some troubles up to here, and also we are sure that everybody will get the same outputs for the last example. However, you are free to try your model, as well. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># loading in a pretrained model..</span></span><br><span class="line">model = NER()</span><br><span class="line">model.init(trax.shapes.ShapeDtype((<span class="number">1</span>, <span class="number">1</span>), dtype=np.int32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pretrained model</span></span><br><span class="line">model.init_from_file(<span class="string">&#x27;model.pkl.gz&#x27;</span>, weights_only=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><a name="4"></a></p>
<h1 id="Part-4-Compute-Accuracy"><a href="#Part-4-Compute-Accuracy" class="headerlink" title="Part 4:  Compute Accuracy"></a>Part 4:  Compute Accuracy</h1><p>You will now evaluate in the test set. Previously, you have seen the accuracy on the training set and the validation (noted as eval) set. You will now evaluate on your test set. To get a good evaluation, you will need to create a mask to avoid counting the padding tokens when computing the accuracy. </p>
<p><a name="ex04"></a></p>
<h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p><strong>Instructions:</strong> Write a program that takes in your model and uses it to evaluate on the test set. You should be able to get an accuracy of 95%.  </p>
<details>    
<summary>
    <font size="3" color="darkgreen"><b>More Detailed Instructions </b></font>
</summary>

<ul>
<li><p><em>Step 1</em>: model(sentences) will give you the predicted output. </p>
</li>
<li><p><em>Step 2</em>: Prediction will produce an output with an added dimension. For each sentence, for each word, there will be a vector of probabilities for each tag type. For each sentence,word, you need to pick the maximum valued tag. This will require <code>np.argmax</code> and careful use of the <code>axis</code> argument.</p>
</li>
<li><p><em>Step 3</em>: Create a mask to prevent counting pad characters. It has the same dimension as output. An example below on matrix comparison provides a hint.</p>
</li>
<li><p><em>Step 4</em>: Compute the accuracy metric by comparing your outputs against your test labels. Take the sum of that and divide by the total number of <strong>unpadded</strong> tokens. Use your mask value to mask the padded tokens. Return the accuracy. </p>
</detail></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Example of a comparision on a matrix </span></span><br><span class="line">a = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">a == <span class="number">2</span></span><br></pre></td></tr></table></figure>




<pre><code>array([False,  True, False, False])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create the evaluation inputs</span></span><br><span class="line">x, y = <span class="built_in">next</span>(data_generator(<span class="built_in">len</span>(test_sentences), test_sentences, test_labels, vocab[<span class="string">&#x27;&lt;PAD&gt;&#x27;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input shapes&quot;</span>, x.shape, y.shape)</span><br></pre></td></tr></table></figure>

<pre><code>input shapes (7194, 70) (7194, 70)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sample prediction</span></span><br><span class="line">tmp_pred = model(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(tmp_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;tmp_pred has shape: <span class="subst">&#123;tmp_pred.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;jax.interpreters.xla.DeviceArray&#39;&gt;
tmp_pred has shape: (7194, 70, 17)
</code></pre>
<p>Note that the model’s prediction has 3 axes: </p>
<ul>
<li>the number of examples</li>
<li>the number of words in each example (padded to be as long as the longest sentence in the batch)</li>
<li>the number of possible targets (the 17 named entity tags).</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: evaluate_prediction</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_prediction</span>(<span class="params">pred, labels, pad</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">        pred: prediction array with shape </span></span><br><span class="line"><span class="string">            (num examples, max sentence length in batch, num of classes)</span></span><br><span class="line"><span class="string">        labels: array of size (batch_size, seq_len)</span></span><br><span class="line"><span class="string">        pad: integer representing pad character</span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">        accuracy: float</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line"><span class="comment">## step 1 ##</span></span><br><span class="line">    outputs = np.argmax(pred, axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;outputs shape:&quot;</span>, outputs.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">## step 2 ##</span></span><br><span class="line">    mask = ~(labels == pad)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;mask shape:&quot;</span>, mask.shape, <span class="string">&quot;mask[0][20:30]:&quot;</span>, mask[<span class="number">0</span>][<span class="number">20</span>:<span class="number">30</span>])</span><br><span class="line"><span class="comment">## step 3 ##</span></span><br><span class="line">    accuracy = np.<span class="built_in">sum</span>(outputs == labels) / np.<span class="built_in">sum</span>(mask)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">accuracy = evaluate_prediction(model(x), y, vocab[<span class="string">&#x27;&lt;PAD&gt;&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuracy: &quot;</span>, accuracy)</span><br></pre></td></tr></table></figure>

<pre><code>outputs shape: (7194, 70)
mask shape: (7194, 70) mask[0][20:30]: [ True  True  True False False False False False False False]
accuracy:  0.9543761281155191
</code></pre>
<p><strong>Expected output (Approximately)</strong>   </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">outputs shape: (7194, 70)</span><br><span class="line">mask shape: (7194, 70) mask[0][20:30]: [ True  True  True False False False False False False False]</span><br><span class="line">accuracy:  0.9543761281155191</span><br></pre></td></tr></table></figure>


<p><a name="5"></a></p>
<h1 id="Part-5-Testing-with-your-own-sentence"><a href="#Part-5-Testing-with-your-own-sentence" class="headerlink" title="Part 5:  Testing with your own sentence"></a>Part 5:  Testing with your own sentence</h1><p>Below, you can test it out with your own sentence! </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is the function you will be using to test your own sentence.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">sentence, model, vocab, tag_map</span>):</span></span><br><span class="line">    s = [vocab[token] <span class="keyword">if</span> token <span class="keyword">in</span> vocab <span class="keyword">else</span> vocab[<span class="string">&#x27;UNK&#x27;</span>] <span class="keyword">for</span> token <span class="keyword">in</span> sentence.split(<span class="string">&#x27; &#x27;</span>)]</span><br><span class="line">    batch_data = np.ones((<span class="number">1</span>, <span class="built_in">len</span>(s)))</span><br><span class="line">    batch_data[<span class="number">0</span>][:] = s</span><br><span class="line">    sentence = np.array(batch_data).astype(<span class="built_in">int</span>)</span><br><span class="line">    output = model(sentence)</span><br><span class="line">    outputs = np.argmax(output, axis=<span class="number">2</span>)</span><br><span class="line">    labels = <span class="built_in">list</span>(tag_map.keys())</span><br><span class="line">    pred = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(outputs[<span class="number">0</span>])):</span><br><span class="line">        idx = outputs[<span class="number">0</span>][i] </span><br><span class="line">        pred_label = labels[idx]</span><br><span class="line">        pred.append(pred_label)</span><br><span class="line">    <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Try the output for the introduction example</span></span><br><span class="line"><span class="comment">#sentence = &quot;Many French citizens are goin to visit Morocco for summer&quot;</span></span><br><span class="line"><span class="comment">#sentence = &quot;Sharon Floyd flew to Miami last Friday&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># New york times news:</span></span><br><span class="line">sentence = <span class="string">&quot;Peter Navarro, the White House director of trade and manufacturing policy of U.S, said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall, though he said it wouldn’t necessarily come&quot;</span></span><br><span class="line">s = [vocab[token] <span class="keyword">if</span> token <span class="keyword">in</span> vocab <span class="keyword">else</span> vocab[<span class="string">&#x27;UNK&#x27;</span>] <span class="keyword">for</span> token <span class="keyword">in</span> sentence.split(<span class="string">&#x27; &#x27;</span>)]</span><br><span class="line">predictions = predict(sentence, model, vocab, tag_map)</span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(sentence.split(<span class="string">&#x27; &#x27;</span>), predictions):</span><br><span class="line">    <span class="keyword">if</span> y != <span class="string">&#x27;O&#x27;</span>:</span><br><span class="line">        <span class="built_in">print</span>(x,y)</span><br></pre></td></tr></table></figure>

<pre><code>Peter B-per
Navarro, I-per
White B-org
House I-org
Sunday B-tim
morning I-tim
White B-org
House I-org
coronavirus B-tim
fall, B-tim
</code></pre>
<p>** Expected Results **</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Peter B-per</span><br><span class="line">Navarro, I-per</span><br><span class="line">White B-org</span><br><span class="line">House I-org</span><br><span class="line">Sunday B-tim</span><br><span class="line">morning I-tim</span><br><span class="line">White B-org</span><br><span class="line">House I-org</span><br><span class="line">coronavirus B-tim</span><br><span class="line">fall, B-tim</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Sentiment-with-Deep-Neural-Networks/2020/08/22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Sentiment-with-Deep-Neural-Networks/2020/08/22/" class="post-title-link" itemprop="url">Sentiment with Deep Neural Networks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-22 10:42:14" itemprop="dateCreated datePublished" datetime="2020-08-22T10:42:14+08:00">2020-08-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-02 11:16:46" itemprop="dateModified" datetime="2020-09-02T11:16:46+08:00">2020-09-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Sentiment-with-Deep-Neural-Networks/2020/08/22/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Sentiment-with-Deep-Neural-Networks/2020/08/22/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Assignment-1-Sentiment-with-Deep-Neural-Networks"><a href="#Assignment-1-Sentiment-with-Deep-Neural-Networks" class="headerlink" title="Assignment 1:  Sentiment with Deep Neural Networks"></a>Assignment 1:  Sentiment with Deep Neural Networks</h1><p>Welcome to the first assignment of course 3. In this assignment, you will explore sentiment analysis using deep neural networks. </p>
<h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li><a href="#1">Part 1:  Import libraries and try out Trax</a></li>
<li><a href="#2">Part 2:  Importing the data</a><ul>
<li><a href="#2.1">2.1  Loading in the data</a></li>
<li><a href="#2.2">2.2  Building the vocabulary</a></li>
<li><a href="#2.3">2.3  Converting a tweet to a tensor</a><ul>
<li><a href="#ex01">Exercise 01</a></li>
</ul>
</li>
<li><a href="#2.4">2.4  Creating a batch generator</a><ul>
<li><a href="#ex02">Exercise 02</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#3">Part 3:  Defining classes</a><ul>
<li><a href="#3.1">3.1  ReLU class</a><ul>
<li><a href="#ex03">Exercise 03</a></li>
</ul>
</li>
<li><a href="#3.2">3.2  Dense class </a><ul>
<li><a href="#ex04">Exercise 04</a></li>
</ul>
</li>
<li><a href="#3.3">3.3  Model</a><ul>
<li><a href="#ex05">Exercise 05</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#4">Part 4:  Training</a><ul>
<li><a href="#4.1">4.1  Training the model</a><ul>
<li><a href="#ex06">Exercise 06</a></li>
</ul>
</li>
<li><a href="#4.2">4.2  Practice Making a prediction</a></li>
</ul>
</li>
<li><a href="#5">Part 5:  Evaluation  </a><ul>
<li><a href="#5.1">5.1  Computing the accuracy on a batch</a><ul>
<li><a href="#ex07">Exercise 07</a></li>
</ul>
</li>
<li><a href="#5.2">5.2  Testing your model on Validation Data</a><ul>
<li><a href="#ex08">Exercise 08</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#6">Part 6:  Testing with your own input</a></li>
</ul>
<p>In course 1, you implemented Logistic regression and Naive Bayes for sentiment analysis. However if you were to give your old models an example like:</p>
<center> <span style='color:blue'> <b>This movie was almost good.</b> </span> </center>

<p>Your model would have predicted a positive sentiment for that review. However, that sentence has a negative sentiment and indicates that the movie was not good. To solve those kinds of misclassifications, you will write a program that uses deep neural networks to identify sentiment in text. By completing this assignment, you will: </p>
<ul>
<li>Understand how you can build/design a model using layers</li>
<li>Train a model using a training loop</li>
<li>Use a binary cross-entropy loss function</li>
<li>Compute the accuracy of your model</li>
<li>Predict using your own input</li>
</ul>
<p>As you can tell, this model follows a similar structure to the one you previously implemented in the second course of this specialization. </p>
<ul>
<li>Indeed most of the deep nets you will be implementing will have a similar structure. The only thing that changes is the model architecture, the inputs, and the outputs. Before starting the assignment, we will introduce you to the Google library <code>trax</code> that we use for building and training models.</li>
</ul>
<p>Now we will show you how to compute the gradient of a certain function <code>f</code> by just using <code>  .grad(f)</code>. </p>
<ul>
<li>Trax source code can be found on Github: <a target="_blank" rel="noopener" href="https://github.com/google/trax">Trax</a></li>
<li>The Trax code also uses the JAX library: <a target="_blank" rel="noopener" href="https://jax.readthedocs.io/en/latest/index.html">JAX</a></li>
</ul>
<p><a name="1"></a></p>
<h1 id="Part-1-Import-libraries-and-try-out-Trax"><a href="#Part-1-Import-libraries-and-try-out-Trax" class="headerlink" title="Part 1:  Import libraries and try out Trax"></a>Part 1:  Import libraries and try out Trax</h1><ul>
<li>Let’s import libraries and look at an example of using the Trax library.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os </span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rnd</span><br><span class="line"></span><br><span class="line"><span class="comment"># import relevant libraries</span></span><br><span class="line"><span class="keyword">import</span> trax</span><br><span class="line"></span><br><span class="line"><span class="comment"># set random seeds to make this notebook easier to replicate</span></span><br><span class="line">trax.supervised.trainer_lib.init_random_number_generators(<span class="number">31</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># import trax.fastmath.numpy</span></span><br><span class="line"><span class="keyword">import</span> trax.fastmath.numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># import trax.layers</span></span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"></span><br><span class="line"><span class="comment"># import Layer from the utils.py file</span></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> Layer, load_tweets, process_tweet</span><br><span class="line"><span class="comment">#from utils import </span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 


[nltk_data] Downloading package twitter_samples to
[nltk_data]     /home/jovyan/nltk_data...
[nltk_data]   Package twitter_samples is already up-to-date!
[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create an array using trax.fastmath.numpy</span></span><br><span class="line">a = np.array(<span class="number">5.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># View the returned array</span></span><br><span class="line">display(a)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(a))</span><br></pre></td></tr></table></figure>


<pre><code>DeviceArray(5., dtype=float32)


&lt;class &#39;jax.interpreters.xla.DeviceArray&#39;&gt;
</code></pre>
<p>Notice that trax.fastmath.numpy returns a DeviceArray from the jax library.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define a function that will use the trax.fastmath.numpy array</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># f = x^2</span></span><br><span class="line">    <span class="keyword">return</span> (x**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Call the function</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;f(a) for a=<span class="subst">&#123;a&#125;</span> is <span class="subst">&#123;f(a)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>f(a) for a=5.0 is 25.0
</code></pre>
<p>The gradient (derivative) of function <code>f</code> with respect to its input <code>x</code> is the derivative of $x^2$.</p>
<ul>
<li>The derivative of $x^2$ is $2x$.  </li>
<li>When x is 5, then $2x=10$.</li>
</ul>
<p>You can calculate the gradient of a function by using <code>trax.fastmath.grad(fun=)</code> and passing in the name of the function.</p>
<ul>
<li>In this case the function you want to take the gradient of is <code>f</code>.</li>
<li>The object returned (saved in <code>grad_f</code> in this example) is a function that can calculate the gradient of f for a given trax.fastmath.numpy array.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Directly use trax.fastmath.grad to calculate the gradient (derivative) of the function</span></span><br><span class="line">grad_f = trax.fastmath.grad(fun=f)  <span class="comment"># df / dx - Gradient of function f(x) with respect to x</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># View the type of the retuned object (it&#x27;s a function)</span></span><br><span class="line"><span class="built_in">type</span>(grad_f)</span><br></pre></td></tr></table></figure>




<pre><code>function
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Call the newly created function and pass in a value for x (the DeviceArray stored in &#x27;a&#x27;)</span></span><br><span class="line">grad_calculation = grad_f(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># View the result of calling the grad_f function</span></span><br><span class="line">display(grad_calculation)</span><br></pre></td></tr></table></figure>


<pre><code>DeviceArray(10., dtype=float32)
</code></pre>
<p>The function returned by trax.fastmath.grad takes in x=5 and calculates the gradient of f, which is 2*x, which is 10. The value is also stored as a DeviceArray from the jax library.</p>
<p><a name="2"></a></p>
<h1 id="Part-2-Importing-the-data"><a href="#Part-2-Importing-the-data" class="headerlink" title="Part 2:  Importing the data"></a>Part 2:  Importing the data</h1><p><a name="2.1"></a></p>
<h2 id="2-1-Loading-in-the-data"><a href="#2-1-Loading-in-the-data" class="headerlink" title="2.1  Loading in the data"></a>2.1  Loading in the data</h2><p>Import the data set.  </p>
<ul>
<li>You may recognize this from earlier assignments in the specialization.</li>
<li>Details of process_tweet function are available in utils.py file</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## DO NOT EDIT THIS CELL</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Import functions from the utils.py file</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load positive and negative tweets</span></span><br><span class="line">all_positive_tweets, all_negative_tweets = load_tweets()</span><br><span class="line"></span><br><span class="line"><span class="comment"># View the total number of positive and negative tweets.</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The number of positive tweets: <span class="subst">&#123;<span class="built_in">len</span>(all_positive_tweets)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The number of negative tweets: <span class="subst">&#123;<span class="built_in">len</span>(all_negative_tweets)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split positive set into validation and training</span></span><br><span class="line">val_pos   = all_positive_tweets[<span class="number">4000</span>:] <span class="comment"># generating validation set for positive tweets</span></span><br><span class="line">train_pos  = all_positive_tweets[:<span class="number">4000</span>]<span class="comment"># generating training set for positive tweets</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Split negative set into validation and training</span></span><br><span class="line">val_neg   = all_negative_tweets[<span class="number">4000</span>:] <span class="comment"># generating validation set for negative tweets</span></span><br><span class="line">train_neg  = all_negative_tweets[:<span class="number">4000</span>] <span class="comment"># generating training set for nagative tweets</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine training data into one set</span></span><br><span class="line">train_x = train_pos + train_neg </span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine validation data into one set</span></span><br><span class="line">val_x  = val_pos + val_neg</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the labels for the training set (1 for positive, 0 for negative)</span></span><br><span class="line">train_y = np.append(np.ones(<span class="built_in">len</span>(train_pos)), np.zeros(<span class="built_in">len</span>(train_neg)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the labels for the validation set (1 for positive, 0 for negative)</span></span><br><span class="line">val_y  = np.append(np.ones(<span class="built_in">len</span>(val_pos)), np.zeros(<span class="built_in">len</span>(val_neg)))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;length of train_x <span class="subst">&#123;<span class="built_in">len</span>(train_x)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;length of val_x <span class="subst">&#123;<span class="built_in">len</span>(val_x)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The number of positive tweets: 5000
The number of negative tweets: 5000
length of train_x 8000
length of val_x 2000
</code></pre>
<p>Now import a function that processes tweets (we’ve provided this in the utils.py file).</p>
<ul>
<li>`process_tweets’ removes unwanted characters e.g. hashtag, hyperlinks, stock tickers from tweet.</li>
<li>It also returns a list of words (it tokenizes the original string).</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import a function that processes the tweets</span></span><br><span class="line"><span class="comment"># from utils import process_tweet</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Try out function that processes tweets</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;original tweet at training position 0&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(train_pos[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tweet at training position 0 after processing:&quot;</span>)</span><br><span class="line">process_tweet(train_pos[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<pre><code>original tweet at training position 0
#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)
Tweet at training position 0 after processing:





[&#39;followfriday&#39;, &#39;top&#39;, &#39;engag&#39;, &#39;member&#39;, &#39;commun&#39;, &#39;week&#39;, &#39;:)&#39;]
</code></pre>
<p>Notice that the function <code>process_tweet</code> keeps key words, removes the hash # symbol, and ignores usernames (words that begin with ‘@’).  It also returns a list of the words.</p>
<p><a name="2.2"></a></p>
<h2 id="2-2-Building-the-vocabulary"><a href="#2-2-Building-the-vocabulary" class="headerlink" title="2.2  Building the vocabulary"></a>2.2  Building the vocabulary</h2><p>Now build the vocabulary.</p>
<ul>
<li>Map each word in each tweet to an integer (an “index”). </li>
<li>The following code does this for you, but please read it and understand what it’s doing.</li>
<li>Note that you will build the vocabulary based on the training data. </li>
<li>To do so, you will assign an index to everyword by iterating over your training set.</li>
</ul>
<p>The vocabulary will also include some special tokens</p>
<ul>
<li><code>__PAD__</code>: padding</li>
<li><code>&lt;/e&gt;</code>: end of line</li>
<li><code>__UNK__</code>: a token representing any word that is not in the vocabulary.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build the vocabulary</span></span><br><span class="line"><span class="comment"># Unit Test Note - There is no test set here only train/val</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Include special tokens </span></span><br><span class="line"><span class="comment"># started with pad, end of line and unk tokens</span></span><br><span class="line">Vocab = &#123;<span class="string">&#x27;__PAD__&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;__&lt;/e&gt;__&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;__UNK__&#x27;</span>: <span class="number">2</span>&#125; </span><br><span class="line"></span><br><span class="line"><span class="comment"># Note that we build vocab using training data</span></span><br><span class="line"><span class="keyword">for</span> tweet <span class="keyword">in</span> train_x: </span><br><span class="line">    processed_tweet = process_tweet(tweet)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> processed_tweet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> Vocab: </span><br><span class="line">            Vocab[word] = <span class="built_in">len</span>(Vocab)</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Total words in vocab are&quot;</span>,<span class="built_in">len</span>(Vocab))</span><br><span class="line">display(Vocab)</span><br></pre></td></tr></table></figure>

<pre><code>Total words in vocab are 9088



&#123;&#39;__PAD__&#39;: 0,
 &#39;__&lt;/e&gt;__&#39;: 1,
 &#39;__UNK__&#39;: 2,
 &#39;followfriday&#39;: 3,
 &#39;top&#39;: 4,
 &#39;engag&#39;: 5,
 &#39;member&#39;: 6,
 &#39;commun&#39;: 7,
 &#39;week&#39;: 8,
 &#39;:)&#39;: 9,
 &#39;hey&#39;: 10,
 &#39;jame&#39;: 11,
 ...&#125;
</code></pre>
<p>The dictionary <code>Vocab</code> will look like this:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;__PAD__&#x27;</span>: <span class="number">0</span>,</span><br><span class="line"> <span class="string">&#x27;__&lt;/e&gt;__&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&#x27;__UNK__&#x27;</span>: <span class="number">2</span>,</span><br><span class="line"> <span class="string">&#x27;followfriday&#x27;</span>: <span class="number">3</span>,</span><br><span class="line"> <span class="string">&#x27;top&#x27;</span>: <span class="number">4</span>,</span><br><span class="line"> <span class="string">&#x27;engag&#x27;</span>: <span class="number">5</span>,</span><br><span class="line"> ...</span><br></pre></td></tr></table></figure>

<ul>
<li>Each unique word has a unique integer associated with it.</li>
<li>The total number of words in Vocab: 9088</li>
</ul>
<p><a name="2.3"></a></p>
<h2 id="2-3-Converting-a-tweet-to-a-tensor"><a href="#2-3-Converting-a-tweet-to-a-tensor" class="headerlink" title="2.3  Converting a tweet to a tensor"></a>2.3  Converting a tweet to a tensor</h2><p>Write a function that will convert each tweet to a tensor (a list of unique integer IDs representing the processed tweet).</p>
<ul>
<li>Note, the returned data type will be a <strong>regular Python <code>list()</code></strong><ul>
<li>You won’t use TensorFlow in this function</li>
<li>You also won’t use a numpy array</li>
<li>You also won’t use trax.fastmath.numpy array</li>
</ul>
</li>
<li>For words in the tweet that are not in the vocabulary, set them to the unique ID for the token <code>__UNK__</code>.</li>
</ul>
<h5 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h5><p>Input a tweet:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;@happypuppy, is Maria happy?&#x27;</span></span><br></pre></td></tr></table></figure>

<p>The tweet_to_tensor will first conver the tweet into a list of tokens (including only relevant words)</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;maria&#x27;</span>, <span class="string">&#x27;happi&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>Then it will convert each word into its unique integer</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">2</span>, <span class="number">56</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li>Notice that the word “maria” is not in the vocabulary, so it is assigned the unique integer associated with the <code>__UNK__</code> token, because it is considered “unknown.”</li>
</ul>
<p><a name="ex01"></a></p>
<h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p><strong>Instructions:</strong> Write a program <code>tweet_to_tensor</code> that takes in a tweet and converts it to an array of numbers. You can use the <code>Vocab</code> dictionary you just found to help create the tensor. </p>
<ul>
<li>Use the vocab_dict parameter and not a global variable.</li>
<li>Do not hard code the integer value for the <code>__UNK__</code> token.</li>
</ul>
<details>    
<summary>
    <font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
    <li>Map each word in tweet to corresponding token in 'Vocab'</li>
    <li>Use Python's Dictionary.get(key,value) so that the function returns a default value if the key is not found in the dictionary.</li>
</ul>
</p>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: tweet_to_tensor</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tweet_to_tensor</span>(<span class="params">tweet, vocab_dict, unk_token=<span class="string">&#x27;__UNK__&#x27;</span>, verbose=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        tweet - A string containing a tweet</span></span><br><span class="line"><span class="string">        vocab_dict - The words dictionary</span></span><br><span class="line"><span class="string">        unk_token - The special string for unknown tokens</span></span><br><span class="line"><span class="string">        verbose - Print info durign runtime</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        tensor_l - A python list with</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span>  </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    <span class="comment"># Process the tweet into a list of words</span></span><br><span class="line">    <span class="comment"># where only important words are kept (stop words removed)</span></span><br><span class="line">    word_l = process_tweet(tweet)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> verbose:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;List of words from the processed tweet:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(word_l)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Initialize the list that will contain the unique integer IDs of each word</span></span><br><span class="line">    tensor_l = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the unique integer ID of the __UNK__ token</span></span><br><span class="line">    unk_ID = vocab_dict.get(unk_token, -<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> verbose:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;The unique integer ID for the unk_token is <span class="subst">&#123;unk_ID&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># for each word in the list:</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word_l:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the unique integer ID.</span></span><br><span class="line">        <span class="comment"># If the word doesn&#x27;t exist in the vocab dictionary,</span></span><br><span class="line">        <span class="comment"># use the unique ID for __UNK__ instead.</span></span><br><span class="line">        word_ID = vocab_dict.get(word, unk_ID)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Append the unique integer ID to the tensor list.</span></span><br><span class="line">        tensor_l.append(word_ID) </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tensor_l</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Actual tweet is\n&quot;</span>, val_pos[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nTensor of tweet:\n&quot;</span>, tweet_to_tensor(val_pos[<span class="number">0</span>], vocab_dict=Vocab))</span><br></pre></td></tr></table></figure>

<pre><code>Actual tweet is
 Bro:U wan cut hair anot,ur hair long Liao bo
Me:since ord liao,take it easy lor treat as save $ leave it longer :)
Bro:LOL Sibei xialan

Tensor of tweet:
 [1065, 136, 479, 2351, 745, 8148, 1123, 745, 53, 2, 2672, 791, 2, 2, 349, 601, 2, 3489, 1017, 597, 4559, 9, 1065, 157, 2, 2]
</code></pre>
<h5 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Actual tweet is</span><br><span class="line"> Bro:U wan cut hair anot,ur hair <span class="keyword">long</span> Liao bo</span><br><span class="line">Me:since ord liao,take it easy lor treat as save $ leave it longer :)</span><br><span class="line">Bro:LOL Sibei xialan</span><br><span class="line"></span><br><span class="line">Tensor of tweet:</span><br><span class="line"> [<span class="number">1065</span>, <span class="number">136</span>, <span class="number">479</span>, <span class="number">2351</span>, <span class="number">745</span>, <span class="number">8148</span>, <span class="number">1123</span>, <span class="number">745</span>, <span class="number">53</span>, <span class="number">2</span>, <span class="number">2672</span>, <span class="number">791</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">349</span>, <span class="number">601</span>, <span class="number">2</span>, <span class="number">3489</span>, <span class="number">1017</span>, <span class="number">597</span>, <span class="number">4559</span>, <span class="number">9</span>, <span class="number">1065</span>, <span class="number">157</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test tweet_to_tensor</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_tweet_to_tensor</span>():</span></span><br><span class="line">    test_cases = [</span><br><span class="line">        </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;name&quot;</span>:<span class="string">&quot;simple_test_check&quot;</span>,</span><br><span class="line">            <span class="string">&quot;input&quot;</span>: [val_pos[<span class="number">1</span>], Vocab],</span><br><span class="line">            <span class="string">&quot;expected&quot;</span>:[<span class="number">444</span>, <span class="number">2</span>, <span class="number">304</span>, <span class="number">567</span>, <span class="number">56</span>, <span class="number">9</span>],</span><br><span class="line">            <span class="string">&quot;error&quot;</span>:<span class="string">&quot;The function gives bad output for val_pos[1]. Test failed&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;name&quot;</span>:<span class="string">&quot;datatype_check&quot;</span>,</span><br><span class="line">            <span class="string">&quot;input&quot;</span>:[val_pos[<span class="number">1</span>], Vocab],</span><br><span class="line">            <span class="string">&quot;expected&quot;</span>:<span class="built_in">type</span>([]),</span><br><span class="line">            <span class="string">&quot;error&quot;</span>:<span class="string">&quot;Datatype mismatch. Need only list not np.array&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;name&quot;</span>:<span class="string">&quot;without_unk_check&quot;</span>,</span><br><span class="line">            <span class="string">&quot;input&quot;</span>:[val_pos[<span class="number">1</span>], Vocab],</span><br><span class="line">            <span class="string">&quot;expected&quot;</span>:<span class="number">6</span>,</span><br><span class="line">            <span class="string">&quot;error&quot;</span>:<span class="string">&quot;Unk word check not done- Please check if you included mapping for unknown word&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> test_case <span class="keyword">in</span> test_cases:</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> test_case[<span class="string">&#x27;name&#x27;</span>] == <span class="string">&quot;simple_test_check&quot;</span>:</span><br><span class="line">                <span class="keyword">assert</span> test_case[<span class="string">&quot;expected&quot;</span>] == tweet_to_tensor(*test_case[<span class="string">&#x27;input&#x27;</span>])</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> test_case[<span class="string">&#x27;name&#x27;</span>] == <span class="string">&quot;datatype_check&quot;</span>:</span><br><span class="line">                <span class="keyword">assert</span> <span class="built_in">isinstance</span>(tweet_to_tensor(*test_case[<span class="string">&#x27;input&#x27;</span>]), test_case[<span class="string">&quot;expected&quot;</span>])</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> test_case[<span class="string">&#x27;name&#x27;</span>] == <span class="string">&quot;without_unk_check&quot;</span>:</span><br><span class="line">                <span class="keyword">assert</span> <span class="literal">None</span> <span class="keyword">not</span> <span class="keyword">in</span> tweet_to_tensor(*test_case[<span class="string">&#x27;input&#x27;</span>])</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="built_in">print</span>(test_case[<span class="string">&#x27;error&#x27;</span>])</span><br><span class="line">    <span class="keyword">if</span> count == <span class="number">3</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\033[92m All tests passed&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(count,<span class="string">&quot; Tests passed out of 3&quot;</span>)</span><br><span class="line">test_tweet_to_tensor()            </span><br></pre></td></tr></table></figure>

<p><a name="2.4"></a></p>
<h2 id="2-4-Creating-a-batch-generator"><a href="#2-4-Creating-a-batch-generator" class="headerlink" title="2.4  Creating a batch generator"></a>2.4  Creating a batch generator</h2><p>Most of the time in Natural Language Processing, and AI in general we use batches when training our data sets. </p>
<ul>
<li>If instead of training with batches of examples, you were to train a model with one example at a time, it would take a very long time to train the model. </li>
<li>You will now build a data generator that takes in the positive/negative tweets and returns a batch of training examples. It returns the model inputs, the targets (positive or negative labels) and the weight for each target (ex: this allows us to can treat some examples as more important to get right than others, but commonly this will all be 1.0). </li>
</ul>
<p>Once you create the generator, you could include it in a for loop</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> batch_inputs, batch_targets, batch_example_weights in data_generator:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>

<p>You can also get a single batch like this:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch_inputs, batch_targets, batch_example_weights = <span class="built_in">next</span>(data_generator)</span><br></pre></td></tr></table></figure>
<p>The generator returns the next batch each time it’s called. </p>
<ul>
<li>This generator returns the data in a format (tensors) that you could directly use in your model.</li>
<li>It returns a triple: the inputs, targets, and loss weights:</li>
<li><ul>
<li>Inputs is a tensor that contains the batch of tweets we put into the model.</li>
</ul>
</li>
<li><ul>
<li>Targets is the corresponding batch of labels that we train to generate.</li>
</ul>
</li>
<li><ul>
<li>Loss weights here are just 1s with same shape as targets. Next week, you will use it to mask input padding.</li>
</ul>
</li>
</ul>
<p><a name="ex02"></a></p>
<h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p>Implement <code>data_generator</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED: Data generator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_generator</span>(<span class="params">data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        data_pos - Set of positive examples</span></span><br><span class="line"><span class="string">        data_neg - Set of negative examples</span></span><br><span class="line"><span class="string">        batch_size - number of samples per batch. Must be even</span></span><br><span class="line"><span class="string">        loop - True or False</span></span><br><span class="line"><span class="string">        vocab_dict - The words dictionary</span></span><br><span class="line"><span class="string">        shuffle - Shuffle the data order</span></span><br><span class="line"><span class="string">    Yield:</span></span><br><span class="line"><span class="string">        inputs - Subset of positive and negative examples</span></span><br><span class="line"><span class="string">        targets - The corresponding labels for the subset</span></span><br><span class="line"><span class="string">        example_weights - An array specifying the importance of each example</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span>     </span><br><span class="line"><span class="comment">### START GIVEN CODE ###</span></span><br><span class="line">    <span class="comment"># make sure the batch size is an even number</span></span><br><span class="line">    <span class="comment"># to allow an equal number of positive and negative samples</span></span><br><span class="line">    <span class="keyword">assert</span> batch_size % <span class="number">2</span> == <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Number of positive examples in each batch is half of the batch size</span></span><br><span class="line">    <span class="comment"># same with number of negative examples in each batch</span></span><br><span class="line">    n_to_take = batch_size // <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use pos_index to walk through the data_pos array</span></span><br><span class="line">    <span class="comment"># same with neg_index and data_neg</span></span><br><span class="line">    pos_index = <span class="number">0</span></span><br><span class="line">    neg_index = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    len_data_pos = <span class="built_in">len</span>(data_pos)</span><br><span class="line">    len_data_neg = <span class="built_in">len</span>(data_neg)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get and array with the data indexes</span></span><br><span class="line">    pos_index_lines = <span class="built_in">list</span>(<span class="built_in">range</span>(len_data_pos))</span><br><span class="line">    neg_index_lines = <span class="built_in">list</span>(<span class="built_in">range</span>(len_data_neg))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># shuffle lines if shuffle is set to True</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        rnd.shuffle(pos_index_lines)</span><br><span class="line">        rnd.shuffle(neg_index_lines)</span><br><span class="line">        </span><br><span class="line">    stop = <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop indefinitely</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> stop:  </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># create a batch with positive and negative examples</span></span><br><span class="line">        batch = []</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># First part: Pack n_to_take positive examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Start from pos_index and increment i up to n_to_take</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_to_take):</span><br><span class="line">                    </span><br><span class="line">            <span class="comment"># If the positive index goes past the positive dataset lenght,</span></span><br><span class="line">            <span class="keyword">if</span> pos_index &gt;= len_data_pos: </span><br><span class="line">                </span><br><span class="line">                <span class="comment"># If loop is set to False, break once we reach the end of the dataset</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> loop:</span><br><span class="line">                    stop = <span class="literal">True</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># If user wants to keep re-using the data, reset the index</span></span><br><span class="line">                pos_index = <span class="number">0</span></span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> shuffle:</span><br><span class="line">                    <span class="comment"># Shuffle the index of the positive sample</span></span><br><span class="line">                    rnd.shuffle(pos_index_lines)</span><br><span class="line">                    </span><br><span class="line">            <span class="comment"># get the tweet as pos_index</span></span><br><span class="line">            tweet = data_pos[pos_index_lines[pos_index]]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># convert the tweet into tensors of integers representing the processed words</span></span><br><span class="line">            tensor = tweet_to_tensor(tweet, vocab_dict)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># append the tensor to the batch list</span></span><br><span class="line">            batch.append(tensor)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Increment pos_index by one</span></span><br><span class="line">            pos_index = pos_index + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### END GIVEN CODE ###</span></span><br><span class="line">            </span><br><span class="line"><span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Second part: Pack n_to_take negative examples</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Using the same batch list, start from neg_index and increment i up to n_to_take</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_to_take):</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If the negative index goes past the negative dataset length,</span></span><br><span class="line">            <span class="keyword">if</span> neg_index &gt;= len_data_neg:</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># If loop is set to False, break once we reach the end of the dataset</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> loop:</span><br><span class="line">                    stop = <span class="literal">True</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                    </span><br><span class="line">                <span class="comment"># If user wants to keep re-using the data, reset the index</span></span><br><span class="line">                neg_index = <span class="number">0</span></span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> shuffle:</span><br><span class="line">                    <span class="comment"># Shuffle the index of the negative sample</span></span><br><span class="line">                    rnd.shuffle(neg_index_lines)</span><br><span class="line">            <span class="comment"># get the tweet as neg_index</span></span><br><span class="line">            tweet = data_neg[neg_index_lines[neg_index]]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># convert the tweet into tensors of integers representing the processed words</span></span><br><span class="line">            tensor = tweet_to_tensor(tweet, vocab_dict)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># append the tensor to the batch list</span></span><br><span class="line">            batch.append(tensor)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Increment neg_index by one</span></span><br><span class="line">            neg_index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### END CODE HERE ###        </span></span><br><span class="line"></span><br><span class="line"><span class="comment">### START GIVEN CODE ###</span></span><br><span class="line">        <span class="keyword">if</span> stop:</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update the start index for positive data </span></span><br><span class="line">        <span class="comment"># so that it&#x27;s n_to_take positions after the current pos_index</span></span><br><span class="line">        pos_index += n_to_take</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the start index for negative data </span></span><br><span class="line">        <span class="comment"># so that it&#x27;s n_to_take positions after the current neg_index</span></span><br><span class="line">        neg_index += n_to_take</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the max tweet length (the length of the longest tweet) </span></span><br><span class="line">        <span class="comment"># (you will pad all shorter tweets to have this length)</span></span><br><span class="line">        max_len = <span class="built_in">max</span>([<span class="built_in">len</span>(t) <span class="keyword">for</span> t <span class="keyword">in</span> batch]) </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize the input_l, which will </span></span><br><span class="line">        <span class="comment"># store the padded versions of the tensors</span></span><br><span class="line">        tensor_pad_l = []</span><br><span class="line">        <span class="comment"># Pad shorter tweets with zeros</span></span><br><span class="line">        <span class="keyword">for</span> tensor <span class="keyword">in</span> batch:</span><br><span class="line"><span class="comment">### END GIVEN CODE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">            <span class="comment"># Get the number of positions to pad for this tensor so that it will be max_len long</span></span><br><span class="line">            n_pad = max_len - <span class="built_in">len</span>(tensor)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Generate a list of zeros, with length n_pad</span></span><br><span class="line">            pad_l = [<span class="number">0</span>] * n_pad</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># concatenate the tensor and the list of padded zeros</span></span><br><span class="line">            tensor_pad = tensor + pad_l</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># append the padded tensor to the list of padded tensors</span></span><br><span class="line">            tensor_pad_l.append(tensor_pad)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># convert the list of padded tensors to a numpy array</span></span><br><span class="line">        <span class="comment"># and store this as the model inputs</span></span><br><span class="line">        inputs = np.array(tensor_pad_l)</span><br><span class="line">  </span><br><span class="line">        <span class="comment"># Generate the list of targets for the positive examples (a list of ones)</span></span><br><span class="line">        <span class="comment"># The length is the number of positive examples in the batch</span></span><br><span class="line">        target_pos = [<span class="number">1</span>] * n_to_take</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Generate the list of targets for the negative examples (a list of zeros)</span></span><br><span class="line">        <span class="comment"># The length is the number of negative examples in the batch</span></span><br><span class="line">        target_neg = [<span class="number">0</span>] * n_to_take</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Concatenate the positve and negative targets</span></span><br><span class="line">        target_l = target_pos + target_neg</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert the target list into a numpy array</span></span><br><span class="line">        targets = np.array(target_l)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Example weights: Treat all examples equally importantly.It should return an np.array. Hint: Use np.ones_like()</span></span><br><span class="line">        example_weights = np.ones_like(targets)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### GIVEN CODE ###</span></span><br><span class="line">        <span class="comment"># note we use yield and not return</span></span><br><span class="line">        <span class="keyword">yield</span> inputs, targets, example_weights</span><br></pre></td></tr></table></figure>

<p>Now you can use your data generator to create a data generator for the training data, and another data generator for the validation data.</p>
<p>We will create a third data generator that does not loop, for testing the final accuracy of the model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set the random number generator for the shuffle procedure</span></span><br><span class="line">rnd.seed(<span class="number">30</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the training data generator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_generator</span>(<span class="params">batch_size, shuffle = <span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> data_generator(train_pos, train_neg, batch_size, <span class="literal">True</span>, Vocab, shuffle)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the validation data generator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val_generator</span>(<span class="params">batch_size, shuffle = <span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> data_generator(val_pos, val_neg, batch_size, <span class="literal">True</span>, Vocab, shuffle)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the validation data generator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_generator</span>(<span class="params">batch_size, shuffle = <span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> data_generator(val_pos, val_neg, batch_size, <span class="literal">False</span>, Vocab, shuffle)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get a batch from the train_generator and inspect.</span></span><br><span class="line">inputs, targets, example_weights = <span class="built_in">next</span>(train_generator(<span class="number">4</span>, shuffle=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># this will print a list of 4 tensors padded with zeros</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Inputs: <span class="subst">&#123;inputs&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Targets: <span class="subst">&#123;targets&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Example Weights: <span class="subst">&#123;example_weights&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Inputs: [[2005 4451 3201    9    0    0    0    0    0    0    0]
 [4954  567 2000 1454 5174 3499  141 3499  130  459    9]
 [3761  109  136  583 2930 3969    0    0    0    0    0]
 [ 250 3761    0    0    0    0    0    0    0    0    0]]
Targets: [1 1 0 0]
Example Weights: [1 1 1 1]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the train_generator</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a data generator for training data,</span></span><br><span class="line"><span class="comment"># which produces batches of size 4 (for tensors and their respective targets)</span></span><br><span class="line">tmp_data_gen = train_generator(batch_size = <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Call the data generator to get one batch and its targets</span></span><br><span class="line">tmp_inputs, tmp_targets, tmp_example_weights = <span class="built_in">next</span>(tmp_data_gen)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The inputs shape is <span class="subst">&#123;tmp_inputs.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The targets shape is <span class="subst">&#123;tmp_targets.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The example weights shape is <span class="subst">&#123;tmp_example_weights.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i,t <span class="keyword">in</span> <span class="built_in">enumerate</span>(tmp_inputs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;input tensor: <span class="subst">&#123;t&#125;</span>; target <span class="subst">&#123;tmp_targets[i]&#125;</span>; example weights <span class="subst">&#123;tmp_example_weights[i]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The inputs shape is (4, 14)
The targets shape is (4,)
The example weights shape is (4,)
input tensor: [3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target 1; example weights 1
input tensor: [10 11 12 13 14 15 16 17 18 19 20  9 21 22]; target 1; example weights 1
input tensor: [5738 2901 3761    0    0    0    0    0    0    0    0    0    0    0]; target 0; example weights 1
input tensor: [ 858  256 3652 5739  307 4458  567 1230 2767  328 1202 3761    0    0]; target 0; example weights 1
</code></pre>
<h5 id="Expected-output-1"><a href="#Expected-output-1" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">The inputs shape <span class="title">is</span> <span class="params">(<span class="number">4</span>, <span class="number">14</span>)</span></span></span><br><span class="line"><span class="function">The targets shape <span class="title">is</span> <span class="params">(<span class="number">4</span>,)</span></span></span><br><span class="line"><span class="function">The example weights shape <span class="title">is</span> <span class="params">(<span class="number">4</span>,)</span></span></span><br><span class="line"><span class="function">input tensor: [<span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>];</span> target <span class="number">1</span>; example weights <span class="number">1</span></span><br><span class="line">input tensor: [<span class="number">10</span> <span class="number">11</span> <span class="number">12</span> <span class="number">13</span> <span class="number">14</span> <span class="number">15</span> <span class="number">16</span> <span class="number">17</span> <span class="number">18</span> <span class="number">19</span> <span class="number">20</span>  <span class="number">9</span> <span class="number">21</span> <span class="number">22</span>]; target <span class="number">1</span>; example weights <span class="number">1</span></span><br><span class="line">input tensor: [<span class="number">5738</span> <span class="number">2901</span> <span class="number">3761</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>]; target <span class="number">0</span>; example weights <span class="number">1</span></span><br><span class="line">input tensor: [ <span class="number">858</span>  <span class="number">256</span> <span class="number">3652</span> <span class="number">5739</span>  <span class="number">307</span> <span class="number">4458</span>  <span class="number">567</span> <span class="number">1230</span> <span class="number">2767</span>  <span class="number">328</span> <span class="number">1202</span> <span class="number">3761</span>    <span class="number">0</span>    <span class="number">0</span>]; target <span class="number">0</span>; example weights <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>Now that you have your train/val generators, you can just call them and they will return tensors which correspond to your tweets in the first column and their corresponding labels in the second column. Now you can go ahead and start building your neural network. </p>
<p><a name="3"></a></p>
<h1 id="Part-3-Defining-classes"><a href="#Part-3-Defining-classes" class="headerlink" title="Part 3:  Defining classes"></a>Part 3:  Defining classes</h1><p>In this part, you will write your own library of layers. It will be very similar<br>to the one used in Trax and also in Keras and PyTorch. Writing your own small<br>framework will help you understand how they all work and use them effectively<br>in the future.</p>
<p>Your framework will be based on the following <code>Layer</code> class from utils.py.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">class <span class="title">Layer</span><span class="params">(object)</span>:</span></span><br><span class="line"><span class="function">    <span class="string">&quot;&quot;</span><span class="string">&quot; Base class for layers.</span></span></span><br><span class="line"><span class="string"><span class="function">    &quot;</span><span class="string">&quot;&quot;</span></span></span><br><span class="line"><span class="function">      </span></span><br><span class="line"><span class="function">    # Constructor</span></span><br><span class="line"><span class="function">    def __init__(self):</span></span><br><span class="line"><span class="function">        # set weights to None</span></span><br><span class="line"><span class="function">        self.weights =</span> None</span><br><span class="line"></span><br><span class="line">    # The forward propagation should be implemented</span><br><span class="line">    <span class="meta"># by subclasses of this Layer class</span></span><br><span class="line">    <span class="function">def <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line"><span class="function">        raise NotImplementedError</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    # This function initializes the weights</span></span><br><span class="line"><span class="function">    # based on the input signature and random key,</span></span><br><span class="line"><span class="function">    # should be implemented by subclasses of this Layer class</span></span><br><span class="line"><span class="function">    def init_weights_and_state(self, input_signature, random_key):</span></span><br><span class="line"><span class="function">        pass</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    # This initializes and returns the weights, do not override.</span></span><br><span class="line"><span class="function">    def init(self, input_signature, random_key):</span></span><br><span class="line"><span class="function">        self.init_weights_and_state(input_signature, random_key)</span></span><br><span class="line"><span class="function">        return self.weights</span></span><br><span class="line"><span class="function"> </span></span><br><span class="line"><span class="function">    # __call__ allows an object of this class</span></span><br><span class="line"><span class="function">    # to be called like it<span class="string">&#x27;s a function.</span></span></span><br><span class="line"><span class="string"><span class="function">    def __call__(self, x):</span></span></span><br><span class="line"><span class="string"><span class="function">        # When this layer object is called, </span></span></span><br><span class="line"><span class="string"><span class="function">        # it calls its forward propagation function</span></span></span><br><span class="line"><span class="string"><span class="function">        return self.forward(x)</span></span></span><br></pre></td></tr></table></figure>

<p><a name="3.1"></a></p>
<h2 id="3-1-ReLU-class"><a href="#3-1-ReLU-class" class="headerlink" title="3.1  ReLU class"></a>3.1  ReLU class</h2><p>You will now implement the ReLU activation function in a class below. The ReLU function looks as follows:<br><img src = "relu.jpg" style="width:300px;height:150px;"/></p>
<p>$$ \mathrm{ReLU}(x) = \mathrm{max}(0,x) $$</p>
<p><a name="ex03"></a></p>
<h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p><strong>Instructions:</strong> Implement the ReLU activation function below. Your function should take in a matrix or vector and it should transform all the negative numbers into 0 while keeping all the positive numbers intact. </p>
<details>    
<summary>
    <font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
    <li>Please use numpy.maximum(A,k) to find the maximum between each element in A and a scalar k</li>
</ul>
</p>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Relu</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Relu</span>(<span class="params">Layer</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Relu activation function implementation&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Input: </span></span><br><span class="line"><span class="string">            - x (a numpy array): the input</span></span><br><span class="line"><span class="string">        Output:</span></span><br><span class="line"><span class="string">            - activation (numpy array): all positive or 0 version of x</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">        </span><br><span class="line">        activation = np.maximum(x, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> activation</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test your relu function</span></span><br><span class="line">x = np.array([[-<span class="number">2.0</span>, -<span class="number">1.0</span>, <span class="number">0.0</span>], [<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>]], dtype=<span class="built_in">float</span>)</span><br><span class="line">relu_layer = Relu()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test data is:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output of Relu is:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(relu_layer(x))</span><br></pre></td></tr></table></figure>

<pre><code>Test data is:
[[-2. -1.  0.]
 [ 0.  1.  2.]]
Output of Relu is:
[[0. 0. 0.]
 [0. 1. 2.]]
</code></pre>
<h5 id="Expected-Outout"><a href="#Expected-Outout" class="headerlink" title="Expected Outout"></a>Expected Outout</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Test data is:</span><br><span class="line">[[<span class="number">-2.</span> <span class="number">-1.</span>  <span class="number">0.</span>]</span><br><span class="line"> [ <span class="number">0.</span>  <span class="number">1.</span>  <span class="number">2.</span>]]</span><br><span class="line">Output of Relu is:</span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">2.</span>]]</span><br></pre></td></tr></table></figure>

<p><a name="3.2"></a></p>
<h2 id="3-2-Dense-class"><a href="#3-2-Dense-class" class="headerlink" title="3.2  Dense class"></a>3.2  Dense class</h2><h3 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h3><p>Implement the forward function of the Dense class. </p>
<ul>
<li>The forward function multiplies the input to the layer (<code>x</code>) by the weight matrix (<code>W</code>)</li>
</ul>
<p>$$\mathrm{forward}(\mathbf{x},\mathbf{W}) = \mathbf{xW} $$</p>
<ul>
<li>You can use <code>numpy.dot</code> to perform the matrix multiplication.</li>
</ul>
<p>Note that for more efficient code execution, you will use the trax version of <code>math</code>, which includes a trax version of <code>numpy</code> and also <code>random</code>.</p>
<p>Implement the weight initializer <code>new_weights</code> function</p>
<ul>
<li>Weights are initialized with a random key.</li>
<li>The second parameter is a tuple for the desired shape of the weights (num_rows, num_cols)</li>
<li>The num of rows for weights should equal the number of columns in x, because for forward propagation, you will multiply x times weights.</li>
</ul>
<p>Please use <code>trax.fastmath.random.normal(key, shape, dtype=tf.float32)</code> to generate random values for the weight matrix. The key difference between this function<br>and the standard <code>numpy</code> randomness is the explicit use of random keys, which<br>need to be passed. While it can look tedious at the first sight to pass the random key everywhere, you will learn in Course 4 why this is very helpful when<br>implementing some advanced models.</p>
<ul>
<li><code>key</code> can be generated by calling <code>random.get_prng(seed=)</code> and passing in a number for the <code>seed</code>.</li>
<li><code>shape</code> is a tuple with the desired shape of the weight matrix.<ul>
<li>The number of rows in the weight matrix should equal the number of columns in the variable <code>x</code>.  Since <code>x</code> may have 2 dimensions if it reprsents a single training example (row, col), or three dimensions (batch_size, row, col), get the last dimension from the tuple that holds the dimensions of x.</li>
<li>The number of columns in the weight matrix is the number of units chosen for that dense layer.  Look at the <code>__init__</code> function to see which variable stores the number of units.</li>
</ul>
</li>
<li><code>dtype</code> is the data type of the values in the generated matrix; keep the default of <code>tf.float32</code>. In this case, don’t explicitly set the dtype (just let it use the default value).</li>
</ul>
<p>Set the standard deviation of the random values to 0.1</p>
<ul>
<li>The values generated have a mean of 0 and standard deviation of 1.</li>
<li>Set the default standard deviation <code>stdev</code> to be 0.1 by multiplying the standard deviation to each of the values in the weight matrix.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># use the fastmath module within trax</span></span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> fastmath</span><br><span class="line"></span><br><span class="line"><span class="comment"># use the numpy module from trax</span></span><br><span class="line">np = fastmath.numpy</span><br><span class="line"></span><br><span class="line"><span class="comment"># use the fastmath.random module from trax</span></span><br><span class="line">random = fastmath.random</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># See how the fastmath.trax.random.normal function works</span></span><br><span class="line">tmp_key = random.get_prng(seed=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The random seed generated by random.get_prng&quot;</span>)</span><br><span class="line">display(tmp_key)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;choose a matrix with 2 rows and 3 columns&quot;</span>)</span><br><span class="line">tmp_shape=(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">display(tmp_shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate a weight matrix</span></span><br><span class="line"><span class="comment"># Note that you&#x27;ll get an error if you try to set dtype to tf.float32, where tf is tensorflow</span></span><br><span class="line"><span class="comment"># Just avoid setting the dtype and allow it to use the default data type</span></span><br><span class="line">tmp_weight = trax.fastmath.random.normal(key=tmp_key, shape=tmp_shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weight matrix generated with a normal distribution with mean 0 and stdev of 1&quot;</span>)</span><br><span class="line">display(tmp_weight)</span><br></pre></td></tr></table></figure>

<pre><code>The random seed generated by random.get_prng



DeviceArray([0, 1], dtype=uint32)


choose a matrix with 2 rows and 3 columns



(2, 3)


Weight matrix generated with a normal distribution with mean 0 and stdev of 1



DeviceArray([[ 0.95730704, -0.96992904,  1.0070664 ],
             [ 0.36619025,  0.17294823,  0.29092228]], dtype=float32)
</code></pre>
<p><a name="ex04"></a></p>
<h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p>Implement the <code>Dense</code> class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: Dense</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dense</span>(<span class="params">Layer</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A dense (fully-connected) layer.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># __init__ is implemented for you</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_units, init_stdev=<span class="number">0.1</span></span>):</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set the number of units in this layer</span></span><br><span class="line">        self._n_units = n_units</span><br><span class="line">        self._init_stdev = init_stdev</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Please implement &#x27;forward()&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Matrix multiply x and the weight matrix</span></span><br><span class="line">        dense = np.dot(x, self.weights)</span><br><span class="line">        </span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line">        <span class="keyword">return</span> dense</span><br><span class="line"></span><br><span class="line">    <span class="comment"># init_weights</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights_and_state</span>(<span class="params">self, input_signature, random_key</span>):</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">        <span class="comment"># The input_signature has a .shape attribute that gives the shape as a tuple    </span></span><br><span class="line">        input_shape = (input_signature.shape[-<span class="number">1</span>], self._n_units)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Generate the weight matrix from a normal distribution, </span></span><br><span class="line">        <span class="comment"># and standard deviation of &#x27;stdev&#x27;        </span></span><br><span class="line">        w = random.normal(key = random_key, shape = input_shape) * self._init_stdev</span><br><span class="line">        </span><br><span class="line"><span class="comment">### END CODE HERE ###     </span></span><br><span class="line">        self.weights = w</span><br><span class="line">        <span class="keyword">return</span> self.weights</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Testing your Dense layer </span></span><br><span class="line">dense_layer = Dense(n_units=<span class="number">10</span>)  <span class="comment">#sets  number of units in dense layer</span></span><br><span class="line">random_key = random.get_prng(seed=<span class="number">0</span>)  <span class="comment"># sets random seed</span></span><br><span class="line">z = np.array([[<span class="number">2.0</span>, <span class="number">7.0</span>, <span class="number">25.0</span>]]) <span class="comment"># input array </span></span><br><span class="line"></span><br><span class="line">dense_layer.init(z, random_key)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights are\n &quot;</span>,dense_layer.weights) <span class="comment">#Returns randomly generated weights</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Foward function output is &quot;</span>, dense_layer(z)) <span class="comment"># Returns multiplied values of units and weights</span></span><br></pre></td></tr></table></figure>

<pre><code>Weights are
  [[-0.02837108  0.09368162 -0.10050076  0.14165013  0.10543301  0.09108126
  -0.04265672  0.0986188  -0.05575325  0.00153249]
 [-0.20785688  0.0554837   0.09142365  0.05744595  0.07227863  0.01210617
  -0.03237354  0.16234995  0.02450038 -0.13809784]
 [-0.06111237  0.01403724  0.08410042 -0.1094358  -0.10775021 -0.11396459
  -0.05933381 -0.01557652 -0.03832145 -0.11144515]]
Foward function output is  [[-3.0395496   0.9266802   2.5414743  -2.050473   -1.9769388  -2.582209
  -1.7952735   0.94427425 -0.8980402  -3.7497487 ]]
</code></pre>
<h5 id="Expected-Outout-1"><a href="#Expected-Outout-1" class="headerlink" title="Expected Outout"></a>Expected Outout</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Weights are</span><br><span class="line">  [[<span class="number">-0.02837108</span>  <span class="number">0.09368162</span> <span class="number">-0.10050076</span>  <span class="number">0.14165013</span>  <span class="number">0.10543301</span>  <span class="number">0.09108126</span></span><br><span class="line">  <span class="number">-0.04265672</span>  <span class="number">0.0986188</span>  <span class="number">-0.05575325</span>  <span class="number">0.00153249</span>]</span><br><span class="line"> [<span class="number">-0.20785688</span>  <span class="number">0.0554837</span>   <span class="number">0.09142365</span>  <span class="number">0.05744595</span>  <span class="number">0.07227863</span>  <span class="number">0.01210617</span></span><br><span class="line">  <span class="number">-0.03237354</span>  <span class="number">0.16234995</span>  <span class="number">0.02450038</span> <span class="number">-0.13809784</span>]</span><br><span class="line"> [<span class="number">-0.06111237</span>  <span class="number">0.01403724</span>  <span class="number">0.08410042</span> <span class="number">-0.1094358</span>  <span class="number">-0.10775021</span> <span class="number">-0.11396459</span></span><br><span class="line">  <span class="number">-0.05933381</span> <span class="number">-0.01557652</span> <span class="number">-0.03832145</span> <span class="number">-0.11144515</span>]]</span><br><span class="line">Foward function output is  [[<span class="number">-3.0395496</span>   <span class="number">0.9266802</span>   <span class="number">2.5414743</span>  <span class="number">-2.050473</span>   <span class="number">-1.9769388</span>  <span class="number">-2.582209</span></span><br><span class="line">  <span class="number">-1.7952735</span>   <span class="number">0.94427425</span> <span class="number">-0.8980402</span>  <span class="number">-3.7497487</span> ]]</span><br></pre></td></tr></table></figure>

<p><a name="3.3"></a></p>
<h2 id="3-3-Model"><a href="#3-3-Model" class="headerlink" title="3.3  Model"></a>3.3  Model</h2><p>Now you will implement a classifier using neural networks. Here is the model architecture you will be implementing. </p>
<img src = "nn.jpg" style="width:400px;height:250px;"/>

<p>For the model implementation, you will use the Trax layers library <code>tl</code>.<br>Note that the second character of <code>tl</code> is the lowercase of letter <code>L</code>, not the number 1. Trax layers are very similar to the ones you implemented above,<br>but in addition to trainable weights also have a non-trainable state.<br>State is used in layers like batch normalization and for inference, you will learn more about it in course 4.</p>
<p>First, look at the code of the Trax Dense layer and compare to your implementation above.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/master/trax/layers/core.py#L29">tl.Dense</a>: Trax Dense layer implementation</li>
</ul>
<p>One other important layer that you will use a lot is one that allows to execute one layer after another in sequence.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/master/trax/layers/combinators.py#L26">tl.Serial</a>: Combinator that applies layers serially.  <ul>
<li>You can pass in the layers as arguments to <code>Serial</code>, separated by commas. </li>
<li>For example: <code>tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))</code></li>
</ul>
</li>
</ul>
<p>Please use the <code>help</code> function to view documentation for each layer.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View documentation on tl.Dense</span></span><br><span class="line"><span class="built_in">help</span>(tl.Dense)</span><br></pre></td></tr></table></figure>

<pre><code>Help on class Dense in module trax.layers.core:

class Dense(trax.layers.base.Layer)
 |  Dense(n_units, kernel_initializer=&lt;function ScaledInitializer.&lt;locals&gt;.Init at 0x7fb32d622620&gt;, bias_initializer=&lt;function RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt; at 0x7fb32d6226a8&gt;, use_bias=True)
 |  
 |  A dense (a.k.a. fully-connected, affine) layer.
 |  
 |  Dense layers are the prototypical example of a trainable layer, i.e., a layer
 |  with trainable weights. Each node in a dense layer computes a weighted sum of
 |  all node values from the preceding layer and adds to that sum a node-specific
 |  bias term. The full layer computation is expressed compactly in linear
 |  algebra as an affine map `y = Wx + b`, where `W` is a matrix and `y`, `x`,
 |  and `b` are vectors. The layer is trained, or &quot;learns&quot;, by updating the
 |  values in `W` and `b`.
 |  
 |  Less commonly, a dense layer can omit the bias term and be a pure linear map:
 |  `y = Wx`.
 |  
 |  Method resolution order:
 |      Dense
 |      trax.layers.base.Layer
 |      builtins.object
 |  
 |  Methods defined here:
 |  
 |  __init__(self, n_units, kernel_initializer=&lt;function ScaledInitializer.&lt;locals&gt;.Init at 0x7fb32d622620&gt;, bias_initializer=&lt;function RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt; at 0x7fb32d6226a8&gt;, use_bias=True)
 |      Returns a dense (fully connected) layer of width `n_units`.
 |      
 |      A dense layer maps collections of `R^m` vectors to `R^n`, where `n`
 |      (`= n_units`) is fixed at layer creation time, and `m` is set at layer
 |      initialization time.
 |      
 |      Args:
 |        n_units: Number of nodes in the layer, also known as the width of the
 |            layer.
 |        kernel_initializer: Function that creates a matrix of (random) initial
 |            connection weights `W` for the layer.
 |        bias_initializer: Function that creates a vector of (random) initial
 |            bias weights `b` for the layer.
 |        use_bias: If `True`, compute an affine map `y = Wx + b`; else compute
 |            a linear map `y = Wx`.
 |  
 |  forward(self, x)
 |      Executes this layer as part of a forward pass through the model.
 |      
 |      Args:
 |        x: Tensor of same shape and dtype as the input signature used to
 |            initialize this layer.
 |      
 |      Returns:
 |        Tensor of same shape and dtype as the input, except the final dimension
 |        is the layer&#39;s `n_units` value.
 |  
 |  init_weights_and_state(self, input_signature)
 |      Returns newly initialized weights for this layer.
 |      
 |      Weights are a `(w, b)` tuple for layers created with `use_bias=True` (the
 |      default case), or a `w` tensor for layers created with `use_bias=False`.
 |      
 |      Args:
 |        input_signature: `ShapeDtype` instance characterizing the input this layer
 |            should compute on.
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from trax.layers.base.Layer:
 |  
 |  __call__(self, x, weights=None, state=None, rng=None)
 |      Makes layers callable; for use in tests or interactive settings.
 |      
 |      This convenience method helps library users play with, test, or otherwise
 |      probe the behavior of layers outside of a full training environment. It
 |      presents the layer as callable function from inputs to outputs, with the
 |      option of manually specifying weights and non-parameter state per individual
 |      call. For convenience, weights and non-parameter state are cached per layer
 |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`,
 |      and acquiring non-empty values either by initialization or from values
 |      explicitly provided via the weights and state keyword arguments.
 |      
 |      Args:
 |        x: Zero or more input tensors, packaged as described in the `Layer` class
 |            docstring.
 |        weights: Weights or `None`; if `None`, use self&#39;s cached weights value.
 |        state: State or `None`; if `None`, use self&#39;s cached state value.
 |        rng: Single-use random number generator (JAX PRNG key), or `None`;
 |            if `None`, use a default computed from an integer 0 seed.
 |      
 |      Returns:
 |        Zero or more output tensors, packaged as described in the `Layer` class
 |        docstring.
 |  
 |  __repr__(self)
 |      Return repr(self).
 |  
 |  backward(self, inputs, output, grad, weights, state, new_state, rng)
 |      Custom backward pass to propagate gradients in a custom way.
 |      
 |      Args:
 |        inputs: Input tensors; can be a (possibly nested) tuple.
 |        output: The result of running this layer on inputs.
 |        grad: Gradient signal computed based on subsequent layers; its structure
 |            and shape must match output.
 |        weights: This layer&#39;s weights.
 |        state: This layer&#39;s state prior to the current forward pass.
 |        new_state: This layer&#39;s state after the current forward pass.
 |        rng: Single-use random number generator (JAX PRNG key).
 |      
 |      Returns:
 |        The custom gradient signal for the input. Note that we need to return
 |        a gradient for each argument of forward, so it will usually be a tuple
 |        of signals: the gradient for inputs and weights.
 |  
 |  init(self, input_signature, rng=None, use_cache=False)
 |      Initializes weights/state of this layer and its sublayers recursively.
 |      
 |      Initialization creates layer weights and state, for layers that use them.
 |      It derives the necessary array shapes and data types from the layer&#39;s input
 |      signature, which is itself just shape and data type information.
 |      
 |      For layers without weights or state, this method safely does nothing.
 |      
 |      This method is designed to create weights/state only once for each layer
 |      instance, even if the same layer instance occurs in multiple places in the
 |      network. This enables weight sharing to be implemented as layer sharing.
 |      
 |      Args:
 |        input_signature: `ShapeDtype` instance (if this layer takes one input)
 |            or list/tuple of `ShapeDtype` instances.
 |        rng: Single-use random number generator (JAX PRNG key), or `None`;
 |            if `None`, use a default computed from an integer 0 seed.
 |        use_cache: If `True`, and if this layer instance has already been
 |            initialized elsewhere in the network, then return special marker
 |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.
 |            Else return this layer&#39;s newly initialized weights and state.
 |      
 |      Returns:
 |        A `(weights, state)` tuple.
 |  
 |  init_from_file(self, file_name, weights_only=False, input_signature=None)
 |      Initializes this layer and its sublayers from a pickled checkpoint.
 |      
 |      In the common case (`weights_only=False`), the file must be a gziped pickled
 |      dictionary containing items with keys `&#39;flat_weights&#39;, `&#39;flat_state&#39;` and
 |      `&#39;input_signature&#39;`, which are used to initialize this layer.
 |      If `input_signature` is specified, it&#39;s used instead of the one in the file.
 |      If `weights_only` is `True`, the dictionary does not need to have the
 |      `&#39;flat_state&#39;` item and the state it not restored either.
 |      
 |      Args:
 |        file_name: Name/path of the pickeled weights/state file.
 |        weights_only: If `True`, initialize only the layer&#39;s weights. Else
 |            initialize both weights and state.
 |        input_signature: Input signature to be used instead of the one from file.
 |  
 |  output_signature(self, input_signature)
 |      Returns output signature this layer would give for `input_signature`.
 |  
 |  pure_fn(self, x, weights, state, rng, use_cache=False)
 |      Applies this layer as a pure function with no optional args.
 |      
 |      This method exposes the layer&#39;s computation as a pure function. This is
 |      especially useful for JIT compilation. Do not override, use `forward`
 |      instead.
 |      
 |      Args:
 |        x: Zero or more input tensors, packaged as described in the `Layer` class
 |            docstring.
 |        weights: A tuple or list of trainable weights, with one element for this
 |            layer if this layer has no sublayers, or one for each sublayer if
 |            this layer has sublayers. If a layer (or sublayer) has no trainable
 |            weights, the corresponding weights element is an empty tuple.
 |        state: Layer-specific non-parameter state that can update between batches.
 |        rng: Single-use random number generator (JAX PRNG key).
 |        use_cache: if `True`, cache weights and state in the layer object; used
 |          to implement layer sharing in combinators.
 |      
 |      Returns:
 |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`)
 |        promised by this layer, and are packaged as described in the `Layer`
 |        class docstring.
 |  
 |  weights_and_state_signature(self, input_signature)
 |      Return a pair containing the signatures of weights and state.
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from trax.layers.base.Layer:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)
 |  
 |  has_backward
 |      Returns `True` if this layer provides its own custom backward pass code.
 |      
 |      A layer subclass that provides custom backward pass code (for custom
 |      gradients) must override this method to return `True`.
 |  
 |  n_in
 |      Returns how many tensors this layer expects as input.
 |  
 |  n_out
 |      Returns how many tensors this layer promises as output.
 |  
 |  name
 |      Returns the name of this layer.
 |  
 |  rng
 |      Returns a single-use random number generator without advancing it.
 |  
 |  state
 |      Returns a tuple containing this layer&#39;s state; may be empty.
 |  
 |  sublayers
 |      Returns a tuple containing this layer&#39;s sublayers; may be empty.
 |  
 |  weights
 |      Returns this layer&#39;s weights.
 |      
 |      Depending on the layer, the weights can be in the form of:
 |      
 |        - an empty tuple
 |        - a tensor (ndarray)
 |        - a nested structure of tuples and tensors
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View documentation on tl.Serial</span></span><br><span class="line"><span class="built_in">help</span>(tl.Serial)</span><br></pre></td></tr></table></figure>

<pre><code>Help on class Serial in module trax.layers.combinators:

class Serial(trax.layers.base.Layer)
 |  Serial(*sublayers, name=None, sublayers_to_print=None)
 |  
 |  Combinator that applies layers serially (by function composition).
 |  
 |  This combinator is commonly used to construct deep networks, e.g., like this::
 |  
 |      mlp = tl.Serial(
 |        tl.Dense(128),
 |        tl.Relu(),
 |        tl.Dense(10),
 |        tl.LogSoftmax()
 |      )
 |  
 |  A Serial combinator uses stack semantics to manage data for its sublayers.
 |  Each sublayer sees only the inputs it needs and returns only the outputs it
 |  has generated. The sublayers interact via the data stack. For instance, a
 |  sublayer k, following sublayer j, gets called with the data stack in the
 |  state left after layer j has applied. The Serial combinator then:
 |  
 |    - takes n_in items off the top of the stack (n_in = k.n_in) and calls
 |      layer k, passing those items as arguments; and
 |  
 |    - takes layer k&#39;s n_out return values (n_out = k.n_out) and pushes
 |      them onto the data stack.
 |  
 |  A Serial instance with no sublayers acts as a special-case (but useful)
 |  1-input 1-output no-op.
 |  
 |  Method resolution order:
 |      Serial
 |      trax.layers.base.Layer
 |      builtins.object
 |  
 |  Methods defined here:
 |  
 |  __init__(self, *sublayers, name=None, sublayers_to_print=None)
 |      Creates a partially initialized, unconnected layer instance.
 |      
 |      Args:
 |        n_in: Number of inputs expected by this layer.
 |        n_out: Number of outputs promised by this layer.
 |        name: Class-like name for this layer; for use when printing this layer.
 |        sublayers_to_print: Sublayers to display when printing out this layer;
 |          By default (when None) we display all sublayers.
 |  
 |  forward(self, xs)
 |      Computes this layer&#39;s output as part of a forward pass through the model.
 |      
 |      Authors of new layer subclasses should override this method to define the
 |      forward computation that their layer performs. Use `self.weights` to access
 |      trainable weights of this layer. If you need to use local non-trainable
 |      state or randomness, use `self.rng` for the random seed (no need to set it)
 |      and use `self.state` for non-trainable state (and set it to the new value).
 |      
 |      Args:
 |        inputs: Zero or more input tensors, packaged as described in the `Layer`
 |            class docstring.
 |      
 |      Returns:
 |        Zero or more output tensors, packaged as described in the `Layer` class
 |        docstring.
 |  
 |  init_weights_and_state(self, input_signature)
 |      Initializes weights and state for inputs with the given signature.
 |      
 |      Authors of new layer subclasses should override this method if their layer
 |      uses trainable weights or non-trainable state. To initialize trainable
 |      weights, set `self.weights` and to initialize non-trainable state,
 |      set `self.state` to the intended value.
 |      
 |      Args:
 |        input_signature: A `ShapeDtype` instance (if this layer takes one input)
 |            or a list/tuple of `ShapeDtype` instances; signatures of inputs.
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors defined here:
 |  
 |  state
 |      Returns a tuple containing this layer&#39;s state; may be empty.
 |  
 |  weights
 |      Returns this layer&#39;s weights.
 |      
 |      Depending on the layer, the weights can be in the form of:
 |      
 |        - an empty tuple
 |        - a tensor (ndarray)
 |        - a nested structure of tuples and tensors
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from trax.layers.base.Layer:
 |  
 |  __call__(self, x, weights=None, state=None, rng=None)
 |      Makes layers callable; for use in tests or interactive settings.
 |      
 |      This convenience method helps library users play with, test, or otherwise
 |      probe the behavior of layers outside of a full training environment. It
 |      presents the layer as callable function from inputs to outputs, with the
 |      option of manually specifying weights and non-parameter state per individual
 |      call. For convenience, weights and non-parameter state are cached per layer
 |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`,
 |      and acquiring non-empty values either by initialization or from values
 |      explicitly provided via the weights and state keyword arguments.
 |      
 |      Args:
 |        x: Zero or more input tensors, packaged as described in the `Layer` class
 |            docstring.
 |        weights: Weights or `None`; if `None`, use self&#39;s cached weights value.
 |        state: State or `None`; if `None`, use self&#39;s cached state value.
 |        rng: Single-use random number generator (JAX PRNG key), or `None`;
 |            if `None`, use a default computed from an integer 0 seed.
 |      
 |      Returns:
 |        Zero or more output tensors, packaged as described in the `Layer` class
 |        docstring.
 |  
 |  __repr__(self)
 |      Return repr(self).
 |  
 |  backward(self, inputs, output, grad, weights, state, new_state, rng)
 |      Custom backward pass to propagate gradients in a custom way.
 |      
 |      Args:
 |        inputs: Input tensors; can be a (possibly nested) tuple.
 |        output: The result of running this layer on inputs.
 |        grad: Gradient signal computed based on subsequent layers; its structure
 |            and shape must match output.
 |        weights: This layer&#39;s weights.
 |        state: This layer&#39;s state prior to the current forward pass.
 |        new_state: This layer&#39;s state after the current forward pass.
 |        rng: Single-use random number generator (JAX PRNG key).
 |      
 |      Returns:
 |        The custom gradient signal for the input. Note that we need to return
 |        a gradient for each argument of forward, so it will usually be a tuple
 |        of signals: the gradient for inputs and weights.
 |  
 |  init(self, input_signature, rng=None, use_cache=False)
 |      Initializes weights/state of this layer and its sublayers recursively.
 |      
 |      Initialization creates layer weights and state, for layers that use them.
 |      It derives the necessary array shapes and data types from the layer&#39;s input
 |      signature, which is itself just shape and data type information.
 |      
 |      For layers without weights or state, this method safely does nothing.
 |      
 |      This method is designed to create weights/state only once for each layer
 |      instance, even if the same layer instance occurs in multiple places in the
 |      network. This enables weight sharing to be implemented as layer sharing.
 |      
 |      Args:
 |        input_signature: `ShapeDtype` instance (if this layer takes one input)
 |            or list/tuple of `ShapeDtype` instances.
 |        rng: Single-use random number generator (JAX PRNG key), or `None`;
 |            if `None`, use a default computed from an integer 0 seed.
 |        use_cache: If `True`, and if this layer instance has already been
 |            initialized elsewhere in the network, then return special marker
 |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.
 |            Else return this layer&#39;s newly initialized weights and state.
 |      
 |      Returns:
 |        A `(weights, state)` tuple.
 |  
 |  init_from_file(self, file_name, weights_only=False, input_signature=None)
 |      Initializes this layer and its sublayers from a pickled checkpoint.
 |      
 |      In the common case (`weights_only=False`), the file must be a gziped pickled
 |      dictionary containing items with keys `&#39;flat_weights&#39;, `&#39;flat_state&#39;` and
 |      `&#39;input_signature&#39;`, which are used to initialize this layer.
 |      If `input_signature` is specified, it&#39;s used instead of the one in the file.
 |      If `weights_only` is `True`, the dictionary does not need to have the
 |      `&#39;flat_state&#39;` item and the state it not restored either.
 |      
 |      Args:
 |        file_name: Name/path of the pickeled weights/state file.
 |        weights_only: If `True`, initialize only the layer&#39;s weights. Else
 |            initialize both weights and state.
 |        input_signature: Input signature to be used instead of the one from file.
 |  
 |  output_signature(self, input_signature)
 |      Returns output signature this layer would give for `input_signature`.
 |  
 |  pure_fn(self, x, weights, state, rng, use_cache=False)
 |      Applies this layer as a pure function with no optional args.
 |      
 |      This method exposes the layer&#39;s computation as a pure function. This is
 |      especially useful for JIT compilation. Do not override, use `forward`
 |      instead.
 |      
 |      Args:
 |        x: Zero or more input tensors, packaged as described in the `Layer` class
 |            docstring.
 |        weights: A tuple or list of trainable weights, with one element for this
 |            layer if this layer has no sublayers, or one for each sublayer if
 |            this layer has sublayers. If a layer (or sublayer) has no trainable
 |            weights, the corresponding weights element is an empty tuple.
 |        state: Layer-specific non-parameter state that can update between batches.
 |        rng: Single-use random number generator (JAX PRNG key).
 |        use_cache: if `True`, cache weights and state in the layer object; used
 |          to implement layer sharing in combinators.
 |      
 |      Returns:
 |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`)
 |        promised by this layer, and are packaged as described in the `Layer`
 |        class docstring.
 |  
 |  weights_and_state_signature(self, input_signature)
 |      Return a pair containing the signatures of weights and state.
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from trax.layers.base.Layer:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)
 |  
 |  has_backward
 |      Returns `True` if this layer provides its own custom backward pass code.
 |      
 |      A layer subclass that provides custom backward pass code (for custom
 |      gradients) must override this method to return `True`.
 |  
 |  n_in
 |      Returns how many tensors this layer expects as input.
 |  
 |  n_out
 |      Returns how many tensors this layer promises as output.
 |  
 |  name
 |      Returns the name of this layer.
 |  
 |  rng
 |      Returns a single-use random number generator without advancing it.
 |  
 |  sublayers
 |      Returns a tuple containing this layer&#39;s sublayers; may be empty.
</code></pre>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113">tl.Embedding</a>: Layer constructor function for an embedding layer.  <ul>
<li><code>tl.Embedding(vocab_size, d_feature)</code>.</li>
<li><code>vocab_size</code> is the number of unique words in the given vocabulary.</li>
<li><code>d_feature</code> is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View documentation for tl.Embedding</span></span><br><span class="line"><span class="built_in">help</span>(tl.Embedding)</span><br></pre></td></tr></table></figure>

<pre><code>Help on class Embedding in module trax.layers.core:

class Embedding(trax.layers.base.Layer)
 |  Embedding(vocab_size, d_feature, kernel_initializer=&lt;function RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt; at 0x7fb32d6228c8&gt;)
 |  
 |  Trainable layer that maps discrete tokens/ids to vectors.
 |  
 |  Method resolution order:
 |      Embedding
 |      trax.layers.base.Layer
 |      builtins.object
 |  
 |  Methods defined here:
 |  
 |  __init__(self, vocab_size, d_feature, kernel_initializer=&lt;function RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt; at 0x7fb32d6228c8&gt;)
 |      Returns an embedding layer with given vocabulary size and vector size.
 |      
 |      The layer clips input values (token ids) to the range `[0, vocab_size)`.
 |      That is, negative token ids all clip to `0` before being mapped to a
 |      vector, and token ids with value `vocab_size` or greater all clip to
 |      `vocab_size - 1` before being mapped to a vector.
 |      
 |      Args:
 |        vocab_size: Size of the input vocabulary. The layer will assign a unique
 |            vector to each id in `range(vocab_size)`.
 |        d_feature: Dimensionality/depth of the output vectors.
 |        kernel_initializer: Function that creates (random) initial vectors for
 |            the embedding.
 |  
 |  forward(self, x)
 |      Returns embedding vectors corresponding to input token id&#39;s.
 |      
 |      Args:
 |        x: Tensor of token id&#39;s.
 |      
 |      Returns:
 |        Tensor of embedding vectors.
 |  
 |  init_weights_and_state(self, input_signature)
 |      Returns tensor of newly initialized embedding vectors.
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from trax.layers.base.Layer:
 |  
 |  __call__(self, x, weights=None, state=None, rng=None)
 |      Makes layers callable; for use in tests or interactive settings.
 |      
 |      This convenience method helps library users play with, test, or otherwise
 |      probe the behavior of layers outside of a full training environment. It
 |      presents the layer as callable function from inputs to outputs, with the
 |      option of manually specifying weights and non-parameter state per individual
 |      call. For convenience, weights and non-parameter state are cached per layer
 |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`,
 |      and acquiring non-empty values either by initialization or from values
 |      explicitly provided via the weights and state keyword arguments.
 |      
 |      Args:
 |        x: Zero or more input tensors, packaged as described in the `Layer` class
 |            docstring.
 |        weights: Weights or `None`; if `None`, use self&#39;s cached weights value.
 |        state: State or `None`; if `None`, use self&#39;s cached state value.
 |        rng: Single-use random number generator (JAX PRNG key), or `None`;
 |            if `None`, use a default computed from an integer 0 seed.
 |      
 |      Returns:
 |        Zero or more output tensors, packaged as described in the `Layer` class
 |        docstring.
 |  
 |  __repr__(self)
 |      Return repr(self).
 |  
 |  backward(self, inputs, output, grad, weights, state, new_state, rng)
 |      Custom backward pass to propagate gradients in a custom way.
 |      
 |      Args:
 |        inputs: Input tensors; can be a (possibly nested) tuple.
 |        output: The result of running this layer on inputs.
 |        grad: Gradient signal computed based on subsequent layers; its structure
 |            and shape must match output.
 |        weights: This layer&#39;s weights.
 |        state: This layer&#39;s state prior to the current forward pass.
 |        new_state: This layer&#39;s state after the current forward pass.
 |        rng: Single-use random number generator (JAX PRNG key).
 |      
 |      Returns:
 |        The custom gradient signal for the input. Note that we need to return
 |        a gradient for each argument of forward, so it will usually be a tuple
 |        of signals: the gradient for inputs and weights.
 |  
 |  init(self, input_signature, rng=None, use_cache=False)
 |      Initializes weights/state of this layer and its sublayers recursively.
 |      
 |      Initialization creates layer weights and state, for layers that use them.
 |      It derives the necessary array shapes and data types from the layer&#39;s input
 |      signature, which is itself just shape and data type information.
 |      
 |      For layers without weights or state, this method safely does nothing.
 |      
 |      This method is designed to create weights/state only once for each layer
 |      instance, even if the same layer instance occurs in multiple places in the
 |      network. This enables weight sharing to be implemented as layer sharing.
 |      
 |      Args:
 |        input_signature: `ShapeDtype` instance (if this layer takes one input)
 |            or list/tuple of `ShapeDtype` instances.
 |        rng: Single-use random number generator (JAX PRNG key), or `None`;
 |            if `None`, use a default computed from an integer 0 seed.
 |        use_cache: If `True`, and if this layer instance has already been
 |            initialized elsewhere in the network, then return special marker
 |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.
 |            Else return this layer&#39;s newly initialized weights and state.
 |      
 |      Returns:
 |        A `(weights, state)` tuple.
 |  
 |  init_from_file(self, file_name, weights_only=False, input_signature=None)
 |      Initializes this layer and its sublayers from a pickled checkpoint.
 |      
 |      In the common case (`weights_only=False`), the file must be a gziped pickled
 |      dictionary containing items with keys `&#39;flat_weights&#39;, `&#39;flat_state&#39;` and
 |      `&#39;input_signature&#39;`, which are used to initialize this layer.
 |      If `input_signature` is specified, it&#39;s used instead of the one in the file.
 |      If `weights_only` is `True`, the dictionary does not need to have the
 |      `&#39;flat_state&#39;` item and the state it not restored either.
 |      
 |      Args:
 |        file_name: Name/path of the pickeled weights/state file.
 |        weights_only: If `True`, initialize only the layer&#39;s weights. Else
 |            initialize both weights and state.
 |        input_signature: Input signature to be used instead of the one from file.
 |  
 |  output_signature(self, input_signature)
 |      Returns output signature this layer would give for `input_signature`.
 |  
 |  pure_fn(self, x, weights, state, rng, use_cache=False)
 |      Applies this layer as a pure function with no optional args.
 |      
 |      This method exposes the layer&#39;s computation as a pure function. This is
 |      especially useful for JIT compilation. Do not override, use `forward`
 |      instead.
 |      
 |      Args:
 |        x: Zero or more input tensors, packaged as described in the `Layer` class
 |            docstring.
 |        weights: A tuple or list of trainable weights, with one element for this
 |            layer if this layer has no sublayers, or one for each sublayer if
 |            this layer has sublayers. If a layer (or sublayer) has no trainable
 |            weights, the corresponding weights element is an empty tuple.
 |        state: Layer-specific non-parameter state that can update between batches.
 |        rng: Single-use random number generator (JAX PRNG key).
 |        use_cache: if `True`, cache weights and state in the layer object; used
 |          to implement layer sharing in combinators.
 |      
 |      Returns:
 |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`)
 |        promised by this layer, and are packaged as described in the `Layer`
 |        class docstring.
 |  
 |  weights_and_state_signature(self, input_signature)
 |      Return a pair containing the signatures of weights and state.
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from trax.layers.base.Layer:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)
 |  
 |  has_backward
 |      Returns `True` if this layer provides its own custom backward pass code.
 |      
 |      A layer subclass that provides custom backward pass code (for custom
 |      gradients) must override this method to return `True`.
 |  
 |  n_in
 |      Returns how many tensors this layer expects as input.
 |  
 |  n_out
 |      Returns how many tensors this layer promises as output.
 |  
 |  name
 |      Returns the name of this layer.
 |  
 |  rng
 |      Returns a single-use random number generator without advancing it.
 |  
 |  state
 |      Returns a tuple containing this layer&#39;s state; may be empty.
 |  
 |  sublayers
 |      Returns a tuple containing this layer&#39;s sublayers; may be empty.
 |  
 |  weights
 |      Returns this layer&#39;s weights.
 |      
 |      Depending on the layer, the weights can be in the form of:
 |      
 |        - an empty tuple
 |        - a tensor (ndarray)
 |        - a nested structure of tuples and tensors
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tmp_embed = tl.Embedding(vocab_size=<span class="number">3</span>, d_feature=<span class="number">2</span>)</span><br><span class="line">display(tmp_embed)</span><br></pre></td></tr></table></figure>


<pre><code>Embedding_3_2
</code></pre>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L276">tl.Mean</a>: Calculates means across an axis.  In this case, please choose axis = 1 to get an average embedding vector (an embedding vector that is an average of all words in the vocabulary).  </li>
<li>For example, if the embedding matrix is 300 elements and vocab size is 10,000 words, taking the mean of the embedding matrix along axis=1 will yield a vector of 300 elements.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># view the documentation for tl.mean</span></span><br><span class="line"><span class="built_in">help</span>(tl.Mean)</span><br></pre></td></tr></table></figure>

<pre><code>Help on function Mean in module trax.layers.core:

Mean(axis=-1, keepdims=False)
    Returns a layer that computes mean values using one tensor axis.
    
    `Mean` uses one tensor axis to form groups of values and replaces each group
    with the mean value of that group. The resulting values can either remain
    in their own size 1 axis (`keepdims=True`), or that axis can be removed from
    the overall tensor (default `keepdims=False`), lowering the rank of the
    tensor by one.
    
    Args:
      axis: Axis along which values are grouped for computing a mean.
      keepdims: If `True`, keep the resulting size 1 axis as a separate tensor
          axis; else, remove that axis.
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Pretend the embedding matrix uses </span></span><br><span class="line"><span class="comment"># 2 elements for embedding the meaning of a word</span></span><br><span class="line"><span class="comment"># and has a vocabulary size of 3</span></span><br><span class="line"><span class="comment"># So it has shape (2,3)</span></span><br><span class="line">tmp_embed = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,],</span><br><span class="line">                    [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">                   ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># take the mean along axis 0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The mean along axis 0 creates a vector whose length equals the vocabulary size&quot;</span>)</span><br><span class="line">display(np.mean(tmp_embed,axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The mean along axis 1 creates a vector whose length equals the number of elements in a word embedding&quot;</span>)</span><br><span class="line">display(np.mean(tmp_embed,axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<pre><code>The mean along axis 0 creates a vector whose length equals the vocabulary size



DeviceArray([2.5, 3.5, 4.5], dtype=float32)


The mean along axis 1 creates a vector whose length equals the number of elements in a word embedding



DeviceArray([2., 5.], dtype=float32)
</code></pre>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L242">tl.LogSoftmax</a>: Implements log softmax function</li>
<li>Here, you don’t need to set any parameters for <code>LogSoftMax()</code>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">help</span>(tl.LogSoftmax)</span><br></pre></td></tr></table></figure>

<pre><code>Help on function LogSoftmax in module trax.layers.core:

LogSoftmax(axis=-1)
    Returns a layer that applies log softmax along one tensor axis.
    
    `LogSoftmax` acts on a group of values and normalizes them to look like a set
    of log probability values. (Probability values must be non-negative, and as
    a set must sum to 1. A group of log probability values can be seen as the
    natural logarithm function applied to a set of probability values.)
    
    Args:
      axis: Axis along which values are grouped for computing log softmax.
</code></pre>
<p><strong>Online documentation</strong></p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">tl.Dense</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators">tl.Serial</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding">tl.Embedding</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean">tl.Mean</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax">tl.LogSoftmax</a></p>
</li>
</ul>
<p><a name="ex05"></a></p>
<h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p>Implement the classifier function. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: classifier</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifier</span>(<span class="params">vocab_size=<span class="built_in">len</span>(<span class="params">Vocab</span>), embedding_dim=<span class="number">256</span>, output_dim=<span class="number">2</span>, mode=<span class="string">&#x27;train&#x27;</span></span>):</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    <span class="comment"># create embedding layer</span></span><br><span class="line">    embed_layer = tl.Embedding(</span><br><span class="line">        vocab_size=vocab_size, <span class="comment"># Size of the vocabulary</span></span><br><span class="line">        d_feature=embedding_dim)  <span class="comment"># Embedding dimension</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a mean layer, to create an &quot;average&quot; word embedding</span></span><br><span class="line">    mean_layer = tl.Mean(axis = <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a dense layer, one unit for each output</span></span><br><span class="line">    dense_output_layer = tl.Dense(n_units = output_dim)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create the log softmax layer (no parameters needed)</span></span><br><span class="line">    log_softmax_layer = tl.LogSoftmax()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use tl.Serial to combine all layers</span></span><br><span class="line">    <span class="comment"># and create the classifier</span></span><br><span class="line">    <span class="comment"># of type trax.layers.combinators.Serial</span></span><br><span class="line">    model = tl.Serial(</span><br><span class="line">      embed_layer, <span class="comment"># embedding layer</span></span><br><span class="line">      mean_layer, <span class="comment"># mean layer</span></span><br><span class="line">      dense_output_layer, <span class="comment"># dense output layer </span></span><br><span class="line">      log_softmax_layer <span class="comment"># log softmax layer</span></span><br><span class="line">    )</span><br><span class="line"><span class="comment">### END CODE HERE ###     </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># return the model of type</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmp_model = classifier()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(tmp_model))</span><br><span class="line">display(tmp_model)</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;trax.layers.combinators.Serial&#39;&gt;



Serial[
  Embedding_9088_256
  Mean
  Dense_2
  LogSoftmax
]
</code></pre>
<h5 id="Expected-Outout-2"><a href="#Expected-Outout-2" class="headerlink" title="Expected Outout"></a>Expected Outout</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">trax</span>.<span class="title">layers</span>.<span class="title">combinators</span>.<span class="title">Serial</span>&#x27;&gt;</span></span><br><span class="line">Serial[</span><br><span class="line">  Embedding_9088_256</span><br><span class="line">  Mean</span><br><span class="line">  Dense_2</span><br><span class="line">  LogSoftmax</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p><a name="4"></a></p>
<h1 id="Part-4-Training"><a href="#Part-4-Training" class="headerlink" title="Part 4:  Training"></a>Part 4:  Training</h1><p>To train a model on a task, Trax defines an abstraction <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask"><code>trax.supervised.training.TrainTask</code></a> which packages the train data, loss and optimizer (among other things) together into an object.</p>
<p>Similarly to evaluate a model, Trax defines an abstraction <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask"><code>trax.supervised.training.EvalTask</code></a> which packages the eval data and metrics (among other things) into another object.</p>
<p>The final piece tying things together is the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop"><code>trax.supervised.training.Loop</code></a> abstraction that is a very simple and flexible way to put everything together and train the model, all the while evaluating it and saving checkpoints.<br>Using <code>Loop</code> will save you a lot of code compared to always writing the training loop by hand, like you did in courses 1 and 2. More importantly, you are less likely to have a bug in that code that would ruin your training.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># View documentation for trax.supervised.training.TrainTask</span></span><br><span class="line"><span class="built_in">help</span>(trax.supervised.training.TrainTask)</span><br></pre></td></tr></table></figure>

<pre><code>Help on class TrainTask in module trax.supervised.training:

class TrainTask(builtins.object)
 |  TrainTask(labeled_data, loss_layer, optimizer, lr_schedule=None, n_steps_per_checkpoint=100)
 |  
 |  A supervised task (labeled data + feedback mechanism) for training.
 |  
 |  Methods defined here:
 |  
 |  __init__(self, labeled_data, loss_layer, optimizer, lr_schedule=None, n_steps_per_checkpoint=100)
 |      Configures a training task.
 |      
 |      Args:
 |        labeled_data: Iterator of batches of labeled data tuples. Each tuple has
 |            1+ data (input value) tensors followed by 1 label (target value)
 |            tensor.  All tensors are NumPy ndarrays or their JAX counterparts.
 |        loss_layer: Layer that computes a scalar value (the &quot;loss&quot;) by comparing
 |            model output :math:`\hat&#123;y&#125;=f(x)` to the target :math:`y`.
 |        optimizer: Optimizer object that computes model weight updates from
 |            loss-function gradients.
 |        lr_schedule: Learning rate schedule, a function step -&gt; learning_rate.
 |        n_steps_per_checkpoint: How many steps to run between checkpoints.
 |  
 |  learning_rate(self, step)
 |      Return the learning rate for the given step.
 |  
 |  next_batch(self)
 |      Returns one batch of labeled data: a tuple of input(s) plus label.
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors defined here:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)
 |  
 |  labeled_data
 |  
 |  loss_layer
 |  
 |  n_steps_per_checkpoint
 |  
 |  optimizer
 |  
 |  sample_batch
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View documentation for trax.supervised.training.EvalTask</span></span><br><span class="line"><span class="built_in">help</span>(trax.supervised.training.EvalTask)</span><br></pre></td></tr></table></figure>

<pre><code>Help on class EvalTask in module trax.supervised.training:

class EvalTask(builtins.object)
 |  EvalTask(labeled_data, metrics, metric_names=None, n_eval_batches=1)
 |  
 |  Labeled data plus scalar functions for (periodically) measuring a model.
 |  
 |  An eval task specifies how (`labeled_data` + `metrics`) and with what
 |  precision (`n_eval_batches`) to measure a model as it is training.
 |  The variance of each scalar output is reduced by measuring over multiple
 |  (`n_eval_batches`) batches and reporting the average from those measurements.
 |  
 |  Methods defined here:
 |  
 |  __init__(self, labeled_data, metrics, metric_names=None, n_eval_batches=1)
 |      Configures an eval task: named metrics run with a given data source.
 |      
 |      Args:
 |        labeled_data: Iterator of batches of labeled data tuples. Each tuple has
 |            1+ data tensors (NumPy ndarrays) followed by 1 label (target value)
 |            tensor.
 |        metrics: List of layers; each computes a scalar value per batch by
 |            comparing model output :math:`\hat&#123;y&#125;=f(x)` to the target :math:`y`.
 |        metric_names: List of names, one for each item in `metrics`, in matching
 |             order, to be used when recording/reporting eval output. If None,
 |             generate default names using layer names from metrics.
 |        n_eval_batches: Integer N that specifies how many eval batches to run;
 |            the output is then the average of the outputs from the N batches.
 |  
 |  next_batch(self)
 |      Returns one batch of labeled data: a tuple of input(s) plus label.
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors defined here:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)
 |  
 |  labeled_data
 |  
 |  metric_names
 |  
 |  metrics
 |  
 |  n_eval_batches
 |  
 |  sample_batch
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View documentation for trax.supervised.training.Loop</span></span><br><span class="line"><span class="built_in">help</span>(trax.supervised.training.Loop)</span><br></pre></td></tr></table></figure>

<pre><code>Help on class Loop in module trax.supervised.training:

class Loop(builtins.object)
 |  Loop(model, task, eval_model=None, eval_task=None, output_dir=None, checkpoint_at=None, eval_at=None)
 |  
 |  Loop that can run for a given number of steps to train a supervised model.
 |  
 |  The typical supervised training process randomly initializes a model and
 |  updates its weights via feedback (loss-derived gradients) from a training
 |  task, by looping through batches of labeled data. A training loop can also
 |  be configured to run periodic evals and save intermediate checkpoints.
 |  
 |  For speed, the implementation takes advantage of JAX&#39;s composable function
 |  transformations (specifically, `jit` and `grad`). It creates JIT-compiled
 |  pure functions derived from variants of the core model; schematically:
 |  
 |    - training variant: jit(grad(pure_function(model+loss)))
 |    - evals variant: jit(pure_function(model+evals))
 |  
 |  In training or during evals, these variants are called with explicit
 |  arguments for all relevant input data, model weights/state, optimizer slots,
 |  and random number seeds:
 |  
 |    - batch: labeled data
 |    - model weights/state: trainable weights and input-related state (e.g., as
 |      used by batch norm)
 |    - optimizer slots: weights in the optimizer that evolve during the training
 |      process
 |    - random number seeds: JAX PRNG keys that enable high-quality, distributed,
 |      repeatable generation of pseudo-random numbers
 |  
 |  Methods defined here:
 |  
 |  __init__(self, model, task, eval_model=None, eval_task=None, output_dir=None, checkpoint_at=None, eval_at=None)
 |      Configures a training `Loop`, including a random initialization.
 |      
 |      Args:
 |        model: Trax layer, representing the core model to be trained. Loss
 |            functions and eval functions (a.k.a. metrics) are considered to be
 |            outside the core model, taking core model output and data labels as
 |            their two inputs.
 |        task: TrainTask instance, which defines the training data, loss function,
 |            and optimizer to be used in this training loop.
 |        eval_model: Optional Trax layer, representing model used for evaluation,
 |          e.g., with dropout turned off. If None, the training model (model)
 |          will be used.
 |        eval_task: EvalTask instance or None. If None, don&#39;t do any evals.
 |        output_dir: Path telling where to save outputs (evals and checkpoints).
 |            Can be None if both `eval_task` and `checkpoint_at` are None.
 |        checkpoint_at: Function (integer --&gt; boolean) telling, for step n, whether
 |            that step should have its checkpoint saved. If None, the default is
 |            periodic checkpointing at `task.n_steps_per_checkpoint`.
 |        eval_at: Function (integer --&gt; boolean) that says, for training step n,
 |            whether that step should run evals. If None, run when checkpointing.
 |  
 |  new_rng(self)
 |      Returns a new single-use random number generator (JAX PRNG key).
 |  
 |  run(self, n_steps=1)
 |      Runs this training loop for n steps.
 |      
 |      Optionally runs evals and saves checkpoints at specified points.
 |      
 |      Args:
 |        n_steps: Stop training after completing n steps.
 |  
 |  run_evals(self, weights=None, state=None)
 |      Runs and records evals for this training session.
 |      
 |      Args:
 |        weights: Current weights from model in training.
 |        state: Current state from model in training.
 |  
 |  save_checkpoint(self, weights=None, state=None, slots=None)
 |      Saves checkpoint to disk for the current training step.
 |      
 |      Args:
 |        weights: Weights from model being trained.
 |        state: State (non-weight parameters) from model being trained.
 |        slots: Updatable weights for the optimizer in this training loop.
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors defined here:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)
 |  
 |  current_step
 |      Returns current step number in this training session.
 |  
 |  eval_model
 |      Returns the model used for evaluation.
 |  
 |  model
 |      Returns the model that is training.
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View optimizers that you could choose from</span></span><br><span class="line"><span class="built_in">help</span>(trax.optimizers)</span><br></pre></td></tr></table></figure>

<pre><code>Help on package trax.optimizers in trax:

NAME
    trax.optimizers - Optimizers for use with Trax layers.

PACKAGE CONTENTS
    adafactor
    adam
    base
    momentum
    optimizers_test
    rms_prop
    sm3

FUNCTIONS
    opt_configure(*args, **kwargs)

FILE
    /opt/conda/lib/python3.7/site-packages/trax/optimizers/__init__.py
</code></pre>
<p>Notice some available optimizers include:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">adafactor</span><br><span class="line">adam</span><br><span class="line">momentum</span><br><span class="line">rms_prop</span><br><span class="line">sm3</span><br></pre></td></tr></table></figure>

<p><a name="4.1"></a></p>
<h2 id="4-1-Training-the-model"><a href="#4-1-Training-the-model" class="headerlink" title="4.1  Training the model"></a>4.1  Training the model</h2><p>Now you are going to train your model. </p>
<p>Let’s define the <code>TrainTask</code>, <code>EvalTask</code> and <code>Loop</code> in preparation to train the model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> training</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">rnd.seed(<span class="number">271</span>)</span><br><span class="line"></span><br><span class="line">train_task = training.TrainTask(</span><br><span class="line">    labeled_data=train_generator(batch_size=batch_size, shuffle=<span class="literal">True</span>),</span><br><span class="line">    loss_layer=tl.CrossEntropyLoss(),</span><br><span class="line">    optimizer=trax.optimizers.Adam(<span class="number">0.01</span>),</span><br><span class="line">    n_steps_per_checkpoint=<span class="number">10</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">eval_task = training.EvalTask(</span><br><span class="line">    labeled_data=val_generator(batch_size=batch_size, shuffle=<span class="literal">True</span>),</span><br><span class="line">    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = classifier()</span><br></pre></td></tr></table></figure>

<p>This defines a model trained using <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss"><code>tl.CrossEntropyLoss</code></a> optimized with the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam"><code>trax.optimizers.Adam</code></a> optimizer, all the while tracking the accuracy using <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy"><code>tl.Accuracy</code></a> metric. We also track <code>tl.CrossEntropyLoss</code> on the validation set.</p>
<p>Now let’s make an output directory and train the model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">output_dir = <span class="string">&#x27;~/model/&#x27;</span></span><br><span class="line">output_dir_expand = os.path.expanduser(output_dir)</span><br><span class="line"><span class="built_in">print</span>(output_dir_expand)</span><br></pre></td></tr></table></figure>

<pre><code>/home/jovyan/model/
</code></pre>
<p><a name="ex06"></a></p>
<h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p><strong>Instructions:</strong> Implement <code>train_model</code> to train the model (<code>classifier</code> that you wrote earlier) for the given number of training steps (<code>n_steps</code>) using <code>TrainTask</code>, <code>EvalTask</code> and <code>Loop</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: train_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span>(<span class="params">classifier, train_task, eval_task, n_steps, output_dir</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        classifier - the model you are building</span></span><br><span class="line"><span class="string">        train_task - Training task</span></span><br><span class="line"><span class="string">        eval_task - Evaluation task</span></span><br><span class="line"><span class="string">        n_steps - the evaluation steps</span></span><br><span class="line"><span class="string">        output_dir - folder to save your files</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        trainer -  trax trainer</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    training_loop = training.Loop(</span><br><span class="line">                                classifier, <span class="comment"># The learning model</span></span><br><span class="line">                                train_task, <span class="comment"># The training task</span></span><br><span class="line">                                eval_task = eval_task, <span class="comment"># The evaluation task</span></span><br><span class="line">                                output_dir = output_dir) <span class="comment"># The output directory</span></span><br><span class="line"></span><br><span class="line">    training_loop.run(n_steps = n_steps)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the training_loop, since it has the model.</span></span><br><span class="line">    <span class="keyword">return</span> training_loop</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">training_loop = train_model(model, train_task, eval_task, <span class="number">100</span>, output_dir_expand)</span><br></pre></td></tr></table></figure>

<pre><code>Step      1: train CrossEntropyLoss |  0.88939196
Step      1: eval  CrossEntropyLoss |  0.68833977
Step      1: eval          Accuracy |  0.50000000
Step     10: train CrossEntropyLoss |  0.61036736
Step     10: eval  CrossEntropyLoss |  0.52182281
Step     10: eval          Accuracy |  0.68750000
Step     20: train CrossEntropyLoss |  0.34137666
Step     20: eval  CrossEntropyLoss |  0.20654774
Step     20: eval          Accuracy |  1.00000000
Step     30: train CrossEntropyLoss |  0.20208922
Step     30: eval  CrossEntropyLoss |  0.21594886
Step     30: eval          Accuracy |  0.93750000
Step     40: train CrossEntropyLoss |  0.19611198
Step     40: eval  CrossEntropyLoss |  0.17582777
Step     40: eval          Accuracy |  1.00000000
Step     50: train CrossEntropyLoss |  0.11203773
Step     50: eval  CrossEntropyLoss |  0.07589275
Step     50: eval          Accuracy |  1.00000000
Step     60: train CrossEntropyLoss |  0.09375446
Step     60: eval  CrossEntropyLoss |  0.09290724
Step     60: eval          Accuracy |  1.00000000
Step     70: train CrossEntropyLoss |  0.08785903
Step     70: eval  CrossEntropyLoss |  0.09610598
Step     70: eval          Accuracy |  1.00000000
Step     80: train CrossEntropyLoss |  0.08858261
Step     80: eval  CrossEntropyLoss |  0.02319432
Step     80: eval          Accuracy |  1.00000000
Step     90: train CrossEntropyLoss |  0.05699894
Step     90: eval  CrossEntropyLoss |  0.01778970
Step     90: eval          Accuracy |  1.00000000
Step    100: train CrossEntropyLoss |  0.03663783
Step    100: eval  CrossEntropyLoss |  0.00210550
Step    100: eval          Accuracy |  1.00000000
</code></pre>
<h5 id="Expected-output-Approximately"><a href="#Expected-output-Approximately" class="headerlink" title="Expected output (Approximately)"></a>Expected output (Approximately)</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">Step      <span class="number">1</span>: train CrossEntropyLoss |  <span class="number">0.88939196</span></span><br><span class="line">Step      <span class="number">1</span>: eval  CrossEntropyLoss |  <span class="number">0.68833977</span></span><br><span class="line">Step      <span class="number">1</span>: eval          Accuracy |  <span class="number">0.50000000</span></span><br><span class="line">Step     <span class="number">10</span>: train CrossEntropyLoss |  <span class="number">0.61036736</span></span><br><span class="line">Step     <span class="number">10</span>: eval  CrossEntropyLoss |  <span class="number">0.52182281</span></span><br><span class="line">Step     <span class="number">10</span>: eval          Accuracy |  <span class="number">0.68750000</span></span><br><span class="line">Step     <span class="number">20</span>: train CrossEntropyLoss |  <span class="number">0.34137666</span></span><br><span class="line">Step     <span class="number">20</span>: eval  CrossEntropyLoss |  <span class="number">0.20654774</span></span><br><span class="line">Step     <span class="number">20</span>: eval          Accuracy |  <span class="number">1.00000000</span></span><br><span class="line">Step     <span class="number">30</span>: train CrossEntropyLoss |  <span class="number">0.20208922</span></span><br><span class="line">Step     <span class="number">30</span>: eval  CrossEntropyLoss |  <span class="number">0.21594886</span></span><br><span class="line">Step     <span class="number">30</span>: eval          Accuracy |  <span class="number">0.93750000</span></span><br><span class="line">Step     <span class="number">40</span>: train CrossEntropyLoss |  <span class="number">0.19611198</span></span><br><span class="line">Step     <span class="number">40</span>: eval  CrossEntropyLoss |  <span class="number">0.17582777</span></span><br><span class="line">Step     <span class="number">40</span>: eval          Accuracy |  <span class="number">1.00000000</span></span><br><span class="line">Step     <span class="number">50</span>: train CrossEntropyLoss |  <span class="number">0.11203773</span></span><br><span class="line">Step     <span class="number">50</span>: eval  CrossEntropyLoss |  <span class="number">0.07589275</span></span><br><span class="line">Step     <span class="number">50</span>: eval          Accuracy |  <span class="number">1.00000000</span></span><br><span class="line">Step     <span class="number">60</span>: train CrossEntropyLoss |  <span class="number">0.09375446</span></span><br><span class="line">Step     <span class="number">60</span>: eval  CrossEntropyLoss |  <span class="number">0.09290724</span></span><br><span class="line">Step     <span class="number">60</span>: eval          Accuracy |  <span class="number">1.00000000</span></span><br><span class="line">Step     <span class="number">70</span>: train CrossEntropyLoss |  <span class="number">0.08785903</span></span><br><span class="line">Step     <span class="number">70</span>: eval  CrossEntropyLoss |  <span class="number">0.09610598</span></span><br><span class="line">Step     <span class="number">70</span>: eval          Accuracy |  <span class="number">1.00000000</span></span><br><span class="line">Step     <span class="number">80</span>: train CrossEntropyLoss |  <span class="number">0.08858261</span></span><br><span class="line">Step     <span class="number">80</span>: eval  CrossEntropyLoss |  <span class="number">0.02319432</span></span><br><span class="line">Step     <span class="number">80</span>: eval          Accuracy |  <span class="number">1.00000000</span></span><br><span class="line">Step     <span class="number">90</span>: train CrossEntropyLoss |  <span class="number">0.05699894</span></span><br><span class="line">Step     <span class="number">90</span>: eval  CrossEntropyLoss |  <span class="number">0.01778970</span></span><br><span class="line">Step     <span class="number">90</span>: eval          Accuracy |  <span class="number">1.00000000</span></span><br><span class="line">Step    <span class="number">100</span>: train CrossEntropyLoss |  <span class="number">0.03663783</span></span><br><span class="line">Step    <span class="number">100</span>: eval  CrossEntropyLoss |  <span class="number">0.00210550</span></span><br><span class="line">Step    <span class="number">100</span>: eval          Accuracy |  <span class="number">1.00000000</span></span><br></pre></td></tr></table></figure>

<p><a name="4.2"></a></p>
<h2 id="4-2-Practice-Making-a-prediction"><a href="#4-2-Practice-Making-a-prediction" class="headerlink" title="4.2  Practice Making a prediction"></a>4.2  Practice Making a prediction</h2><p>Now that you have trained a model, you can access it as <code>training_loop.model</code> object. We will actually use <code>training_loop.eval_model</code> and in the next weeks you will learn why we sometimes use a different model for evaluation, e.g., one without dropout. For now, make predictions with your model.</p>
<p>Use the training data just to see how the prediction process works.  </p>
<ul>
<li>Later, you will use validation data to evaluate your model’s performance.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a generator object</span></span><br><span class="line">tmp_train_generator = train_generator(<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get one batch</span></span><br><span class="line">tmp_batch = <span class="built_in">next</span>(tmp_train_generator)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Position 0 has the model inputs (tweets as tensors)</span></span><br><span class="line"><span class="comment"># position 1 has the targets (the actual labels)</span></span><br><span class="line">tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The batch is a tuple of length <span class="subst">&#123;<span class="built_in">len</span>(tmp_batch)&#125;</span> because position 0 contains the tweets, and position 1 contains the targets.&quot;</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The shape of the tweet tensors is <span class="subst">&#123;tmp_inputs.shape&#125;</span> (num of examples, length of tweet tensors)&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The shape of the labels is <span class="subst">&#123;tmp_targets.shape&#125;</span>, which is the batch size.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The shape of the example_weights is <span class="subst">&#123;tmp_example_weights.shape&#125;</span>, which is the same as inputs/targets size.&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The batch is a tuple of length 3 because position 0 contains the tweets, and position 1 contains the targets.
The shape of the tweet tensors is (16, 15) (num of examples, length of tweet tensors)
The shape of the labels is (16,), which is the batch size.
The shape of the example_weights is (16,), which is the same as inputs/targets size.
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feed the tweet tensors into the model to get a prediction</span></span><br><span class="line">tmp_pred = training_loop.eval_model(tmp_inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The prediction shape is <span class="subst">&#123;tmp_pred.shape&#125;</span>, num of tensor_tweets as rows&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Column 0 is the probability of a negative sentiment (class 0)&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Column 1 is the probability of a positive sentiment (class 1)&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;View the prediction array&quot;</span>)</span><br><span class="line">tmp_pred</span><br></pre></td></tr></table></figure>

<pre><code>The prediction shape is (16, 2), num of tensor_tweets as rows
Column 0 is the probability of a negative sentiment (class 0)
Column 1 is the probability of a positive sentiment (class 1)

View the prediction array





DeviceArray([[-4.9417334e+00, -7.1678162e-03],
             [-6.5846415e+00, -1.3823509e-03],
             [-5.4463043e+00, -4.3215752e-03],
             [-4.3487482e+00, -1.3007164e-02],
             [-4.9131694e+00, -7.3764324e-03],
             [-4.7097692e+00, -9.0477467e-03],
             [-5.2801600e+00, -5.1045418e-03],
             [-4.1103225e+00, -1.6538620e-02],
             [-1.8327236e-03, -6.3028107e+00],
             [-4.7376156e-03, -5.3545618e+00],
             [-3.4697056e-03, -5.6654320e+00],
             [-1.1444092e-05, -1.1379558e+01],
             [-1.0051131e-02, -4.6050973e+00],
             [-1.0130405e-03, -6.8951964e+00],
             [-6.1047077e-03, -5.1017356e+00],
             [-7.4422359e-03, -4.9043016e+00]], dtype=float32)
</code></pre>
<p>To turn these probabilities into categories (negative or positive sentiment prediction), for each row:</p>
<ul>
<li>Compare the probabilities in each column.</li>
<li>If column 1 has a value greater than column 0, classify that as a positive tweet.</li>
<li>Otherwise if column 1 is less than or equal to column 0, classify that example as a negative tweet.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># turn probabilites into category predictions</span></span><br><span class="line">tmp_is_positive = tmp_pred[:,<span class="number">1</span>] &gt; tmp_pred[:,<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(tmp_is_positive):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Neg log prob <span class="subst">&#123;tmp_pred[i,<span class="number">0</span>]:<span class="number">.4</span>f&#125;</span>\tPos log prob <span class="subst">&#123;tmp_pred[i,<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>\t is positive? <span class="subst">&#123;p&#125;</span>\t actual <span class="subst">&#123;tmp_targets[i]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Neg log prob -4.9417    Pos log prob -0.0072     is positive? True   actual 1
Neg log prob -6.5846    Pos log prob -0.0014     is positive? True   actual 1
Neg log prob -5.4463    Pos log prob -0.0043     is positive? True   actual 1
Neg log prob -4.3487    Pos log prob -0.0130     is positive? True   actual 1
Neg log prob -4.9132    Pos log prob -0.0074     is positive? True   actual 1
Neg log prob -4.7098    Pos log prob -0.0090     is positive? True   actual 1
Neg log prob -5.2802    Pos log prob -0.0051     is positive? True   actual 1
Neg log prob -4.1103    Pos log prob -0.0165     is positive? True   actual 1
Neg log prob -0.0018    Pos log prob -6.3028     is positive? False  actual 0
Neg log prob -0.0047    Pos log prob -5.3546     is positive? False  actual 0
Neg log prob -0.0035    Pos log prob -5.6654     is positive? False  actual 0
Neg log prob -0.0000    Pos log prob -11.3796    is positive? False  actual 0
Neg log prob -0.0101    Pos log prob -4.6051     is positive? False  actual 0
Neg log prob -0.0010    Pos log prob -6.8952     is positive? False  actual 0
Neg log prob -0.0061    Pos log prob -5.1017     is positive? False  actual 0
Neg log prob -0.0074    Pos log prob -4.9043     is positive? False  actual 0
</code></pre>
<p>Notice that since you are making a prediction using a training batch, it’s more likely that the model’s predictions match the actual targets (labels).  </p>
<ul>
<li>Every prediction that the tweet is positive is also matching the actual target of 1 (positive sentiment).</li>
<li>Similarly, all predictions that the sentiment is not positive matches the actual target of 0 (negative sentiment)</li>
</ul>
<p>One more useful thing to know is how to compare if the prediction is matching the actual target (label).  </p>
<ul>
<li>The result of calculation <code>is_positive</code> is a boolean.</li>
<li>The target is a type trax.fastmath.numpy.int32</li>
<li>If you expect to be doing division, you may prefer to work with decimal numbers with the data type type trax.fastmath.numpy.int32</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View the array of booleans</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Array of booleans&quot;</span>)</span><br><span class="line">display(tmp_is_positive)</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert boolean to type int32</span></span><br><span class="line"><span class="comment"># True is converted to 1</span></span><br><span class="line"><span class="comment"># False is converted to 0</span></span><br><span class="line">tmp_is_positive_int = tmp_is_positive.astype(np.int32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># View the array of integers</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Array of integers&quot;</span>)</span><br><span class="line">display(tmp_is_positive_int)</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert boolean to type float32</span></span><br><span class="line">tmp_is_positive_float = tmp_is_positive.astype(np.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># View the array of floats</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Array of floats&quot;</span>)</span><br><span class="line">display(tmp_is_positive_float)</span><br></pre></td></tr></table></figure>

<pre><code>Array of booleans



DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,
             False, False, False, False, False, False, False, False],            dtype=bool)


Array of integers



DeviceArray([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)


Array of floats



DeviceArray([1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
             0.], dtype=float32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmp_pred.shape</span><br></pre></td></tr></table></figure>




<pre><code>(16, 2)
</code></pre>
<p>Note that Python usually does type conversion for you when you compare a boolean to an integer</p>
<ul>
<li>True compared to 1 is True, otherwise any other integer is False.</li>
<li>False compared to 0 is True, otherwise any ohter integer is False.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;True == 1: <span class="subst">&#123;<span class="literal">True</span> == <span class="number">1</span>&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;True == 2: <span class="subst">&#123;<span class="literal">True</span> == <span class="number">2</span>&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;False == 0: <span class="subst">&#123;<span class="literal">False</span> == <span class="number">0</span>&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;False == 2: <span class="subst">&#123;<span class="literal">False</span> == <span class="number">2</span>&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>True == 1: True
True == 2: False
False == 0: True
False == 2: False
</code></pre>
<p>However, we recommend that you keep track of the data type of your variables to avoid unexpected outcomes.  So it helps to convert the booleans into integers</p>
<ul>
<li>Compare 1 to 1 rather than comparing True to 1.</li>
</ul>
<p>Hopefully you are now familiar with what kinds of inputs and outputs the model uses when making a prediction.</p>
<ul>
<li>This will help you implement a function that estimates the accuracy of the model’s predictions.</li>
</ul>
<p><a name="5"></a></p>
<h1 id="Part-5-Evaluation"><a href="#Part-5-Evaluation" class="headerlink" title="Part 5:  Evaluation"></a>Part 5:  Evaluation</h1><p><a name="5.1"></a></p>
<h2 id="5-1-Computing-the-accuracy-on-a-batch"><a href="#5-1-Computing-the-accuracy-on-a-batch" class="headerlink" title="5.1  Computing the accuracy on a batch"></a>5.1  Computing the accuracy on a batch</h2><p>You will now write a function that evaluates your model on the validation set and returns the accuracy. </p>
<ul>
<li><code>preds</code> contains the predictions.<ul>
<li>Its dimensions are <code>(batch_size, output_dim)</code>.  <code>output_dim</code> is two in this case.  Column 0 contains the probability that the tweet belongs to class 0 (negative sentiment). Column 1 contains probability that it belongs to class 1 (positive sentiment).</li>
<li>If the probability in column 1 is greater than the probability in column 0, then interpret this as the model’s prediction that the example has label 1 (positive sentiment).  </li>
<li>Otherwise, if the probabilities are equal or the probability in column 0 is higher, the model’s prediction is 0 (negative sentiment).</li>
</ul>
</li>
<li><code>y</code> contains the actual labels.</li>
<li><code>y_weights</code> contains the weights to give to predictions.</li>
</ul>
<p><a name="ex07"></a></p>
<h3 id="Exercise-07"><a href="#Exercise-07" class="headerlink" title="Exercise 07"></a>Exercise 07</h3><p>Implement <code>compute_accuracy</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: compute_accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span>(<span class="params">preds, y, y_weights</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        preds: a tensor of shape (dim_batch, output_dim) </span></span><br><span class="line"><span class="string">        y: a tensor of shape (dim_batch, output_dim) with the true labels</span></span><br><span class="line"><span class="string">        y_weights: a n.ndarray with the a weight for each example</span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        accuracy: a float between 0-1 </span></span><br><span class="line"><span class="string">        weighted_num_correct (np.float32): Sum of the weighted correct predictions</span></span><br><span class="line"><span class="string">        sum_weights (np.float32): Sum of the weights</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    <span class="comment"># Create an array of booleans, </span></span><br><span class="line">    <span class="comment"># True if the probability of positive sentiment is greater than</span></span><br><span class="line">    <span class="comment"># the probability of negative sentiment</span></span><br><span class="line">    <span class="comment"># else False</span></span><br><span class="line">    is_pos =  preds[:,<span class="number">1</span>] &gt; preds[:,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># convert the array of booleans into an array of np.int32</span></span><br><span class="line">    is_pos_int = is_pos.astype(np.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compare the array of predictions (as int32) with the target (labels) of type int32</span></span><br><span class="line">    correct = is_pos_int == y</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Count the sum of the weights.</span></span><br><span class="line">    sum_weights = np.<span class="built_in">sum</span>(y_weights)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># convert the array of correct predictions (boolean) into an arrayof np.float32</span></span><br><span class="line">    correct_float = correct.astype(np.float32)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Multiply each prediction with its corresponding weight.</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    weighted_correct_float = correct_float * y_weights</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Sum up the weighted correct predictions (of type np.float32), to go in the</span></span><br><span class="line">    <span class="comment"># denominator.</span></span><br><span class="line">    weighted_num_correct = np.<span class="built_in">sum</span>(weighted_correct_float)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Divide the number of weighted correct predictions by the sum of the</span></span><br><span class="line">    <span class="comment"># weights.</span></span><br><span class="line">    accuracy = weighted_num_correct / sum_weights</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> accuracy, weighted_num_correct, sum_weights</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your function</span></span><br><span class="line">tmp_val_generator = val_generator(<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get one batch</span></span><br><span class="line">tmp_batch = <span class="built_in">next</span>(tmp_val_generator)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Position 0 has the model inputs (tweets as tensors)</span></span><br><span class="line"><span class="comment"># position 1 has the targets (the actual labels)</span></span><br><span class="line">tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch</span><br><span class="line"></span><br><span class="line"><span class="comment"># feed the tweet tensors into the model to get a prediction</span></span><br><span class="line">tmp_pred = training_loop.eval_model(tmp_inputs)</span><br><span class="line"></span><br><span class="line">tmp_acc, tmp_num_correct, tmp_num_predictions = compute_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model&#x27;s prediction accuracy on a single training batch is: <span class="subst">&#123;<span class="number">100</span> * tmp_acc&#125;</span>%&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Weighted number of correct predictions <span class="subst">&#123;tmp_num_correct&#125;</span>; weighted number of total observations predicted <span class="subst">&#123;tmp_num_predictions&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Model&#39;s prediction accuracy on a single training batch is: 100.0%
Weighted number of correct predictions 64.0; weighted number of total observations predicted 64
</code></pre>
<h5 id="Expected-output-Approximately-1"><a href="#Expected-output-Approximately-1" class="headerlink" title="Expected output (Approximately)"></a>Expected output (Approximately)</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Model&#x27;s prediction accuracy on a single training batch is: 100.0%</span><br><span class="line">Weighted number of correct predictions 64.0; weighted number of total observations predicted 64</span><br></pre></td></tr></table></figure>

<p><a name="5.2"></a></p>
<h2 id="5-2-Testing-your-model-on-Validation-Data"><a href="#5-2-Testing-your-model-on-Validation-Data" class="headerlink" title="5.2  Testing your model on Validation Data"></a>5.2  Testing your model on Validation Data</h2><p>Now you will write test your model’s prediction accuracy on validation data. </p>
<p>This program will take in a data generator and your model. </p>
<ul>
<li>The generator allows you to get batches of data. You can use it with a <code>for</code> loop:</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for batch in iterator: </span><br><span class="line">   # do something with that batch</span><br></pre></td></tr></table></figure>

<p><code>batch</code> has dimensions <code>(X, Y, weights)</code>. </p>
<ul>
<li>Column 0 corresponds to the tweet as a tensor (input).</li>
<li>Column 1 corresponds to its target (actual label, positive or negative sentiment).</li>
<li>Column 2 corresponds to the weights associated (example weights)</li>
<li>You can feed the tweet into model and it will return the predictions for the batch. </li>
</ul>
<p><a name="ex08"></a></p>
<h3 id="Exercise-08"><a href="#Exercise-08" class="headerlink" title="Exercise 08"></a>Exercise 08</h3><p><strong>Instructions:</strong> </p>
<ul>
<li>Compute the accuracy over all the batches in the validation iterator. </li>
<li>Make use of <code>compute_accuracy</code>, which you recently implemented, and return the overall accuracy.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: test_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_model</span>(<span class="params">generator, model</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input: </span></span><br><span class="line"><span class="string">        generator: an iterator instance that provides batches of inputs and targets</span></span><br><span class="line"><span class="string">        model: a model instance </span></span><br><span class="line"><span class="string">    Output: </span></span><br><span class="line"><span class="string">        accuracy: float corresponding to the accuracy</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    accuracy = <span class="number">0.</span></span><br><span class="line">    total_num_correct = <span class="number">0</span></span><br><span class="line">    total_num_pred = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> generator: </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve the inputs from the batch</span></span><br><span class="line">        inputs = batch[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve the targets (actual labels) from the batch</span></span><br><span class="line">        targets = batch[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve the example weight.</span></span><br><span class="line">        example_weight = batch[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Make predictions using the inputs</span></span><br><span class="line">        pred = model(inputs)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate accuracy for the batch by comparing its predictions and targets</span></span><br><span class="line">        batch_accuracy, batch_num_correct, batch_num_pred = compute_accuracy(pred, targets, example_weight)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the total number of correct predictions</span></span><br><span class="line">        <span class="comment"># by adding the number of correct predictions from this batch</span></span><br><span class="line">        total_num_correct += batch_num_correct</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the total number of predictions </span></span><br><span class="line">        <span class="comment"># by adding the number of predictions made for the batch</span></span><br><span class="line">        total_num_pred +=batch_num_pred</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate accuracy over all examples</span></span><br><span class="line">    accuracy = total_num_correct / total_num_pred</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DO NOT EDIT THIS CELL</span></span><br><span class="line"><span class="comment"># testing the accuracy of your model: this takes around 20 seconds</span></span><br><span class="line">model = training_loop.eval_model</span><br><span class="line">accuracy = test_model(test_generator(<span class="number">16</span>), model)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;The accuracy of your model on the validation set is <span class="subst">&#123;accuracy:<span class="number">.4</span>f&#125;</span>&#x27;</span>, )</span><br></pre></td></tr></table></figure>

<pre><code>The accuracy of your model on the validation set is 0.9931
</code></pre>
<h5 id="Expected-Output-Approximately"><a href="#Expected-Output-Approximately" class="headerlink" title="Expected Output (Approximately)"></a>Expected Output (Approximately)</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The accuracy of your model on the validation set is <span class="number">0.9931</span></span><br></pre></td></tr></table></figure>

<p><a name="6"></a></p>
<h1 id="Part-6-Testing-with-your-own-input"><a href="#Part-6-Testing-with-your-own-input" class="headerlink" title="Part 6:  Testing with your own input"></a>Part 6:  Testing with your own input</h1><p>Finally you will test with your own input. You will see that deepnets are more powerful than the older methods you have used before. Although you go close to 100% accuracy on the first two assignments, the task was way easier. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is used to predict on your own sentnece</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">sentence</span>):</span></span><br><span class="line">    inputs = np.array(tweet_to_tensor(sentence, vocab_dict=Vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Batch size 1, add dimension for batch, to work with the model</span></span><br><span class="line">    inputs = inputs[<span class="literal">None</span>, :]  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># predict with the model</span></span><br><span class="line">    preds_probs = model(inputs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Turn probabilities into categories</span></span><br><span class="line">    preds = <span class="built_in">int</span>(preds_probs[<span class="number">0</span>, <span class="number">1</span>] &gt; preds_probs[<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    sentiment = <span class="string">&quot;negative&quot;</span></span><br><span class="line">    <span class="keyword">if</span> preds == <span class="number">1</span>:</span><br><span class="line">        sentiment = <span class="string">&#x27;positive&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> preds, sentiment</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># try a positive sentence</span></span><br><span class="line">sentence = <span class="string">&quot;It&#x27;s such a nice day, think i&#x27;ll be taking Sid to Ramsgate fish and chips for lunch at Peter&#x27;s fish factory and then the beach maybe&quot;</span></span><br><span class="line">tmp_pred, tmp_sentiment = predict(sentence)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The sentiment of the sentence \n***\n\&quot;<span class="subst">&#123;sentence&#125;</span>\&quot;\n***\nis <span class="subst">&#123;tmp_sentiment&#125;</span>.&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="comment"># try a negative sentence</span></span><br><span class="line">sentence = <span class="string">&quot;I hated my day, it was the worst, I&#x27;m so sad.&quot;</span></span><br><span class="line">tmp_pred, tmp_sentiment = predict(sentence)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The sentiment of the sentence \n***\n\&quot;<span class="subst">&#123;sentence&#125;</span>\&quot;\n***\nis <span class="subst">&#123;tmp_sentiment&#125;</span>.&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The sentiment of the sentence 
***
&quot;It&#39;s such a nice day, think i&#39;ll be taking Sid to Ramsgate fish and chips for lunch at Peter&#39;s fish factory and then the beach maybe&quot;
***
is positive.

The sentiment of the sentence 
***
&quot;I hated my day, it was the worst, I&#39;m so sad.&quot;
***
is negative.
</code></pre>
<p>Notice that the model works well even for complex sentences.</p>
<h3 id="On-Deep-Nets"><a href="#On-Deep-Nets" class="headerlink" title="On Deep Nets"></a>On Deep Nets</h3><p>Deep nets allow you to understand and capture dependencies that you would have not been able to capture with a simple linear regression, or logistic regression. </p>
<ul>
<li>It also allows you to better use pre-trained embeddings for classification and tends to generalize better.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Create-a-Siamese-Network-with-Triplet-Loss-in-Keras/2020/08/11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Create-a-Siamese-Network-with-Triplet-Loss-in-Keras/2020/08/11/" class="post-title-link" itemprop="url">Create a Siamese Network with Triplet Loss in Keras</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-11 19:04:16" itemprop="dateCreated datePublished" datetime="2020-08-11T19:04:16+08:00">2020-08-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-12 10:56:29" itemprop="dateModified" datetime="2020-08-12T10:56:29+08:00">2020-08-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Create-a-Siamese-Network-with-Triplet-Loss-in-Keras/2020/08/11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Create-a-Siamese-Network-with-Triplet-Loss-in-Keras/2020/08/11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Create-a-Siamese-Network-with-Triplet-Loss-in-Keras"><a href="#Create-a-Siamese-Network-with-Triplet-Loss-in-Keras" class="headerlink" title="Create a Siamese Network with Triplet Loss in Keras"></a>Create a Siamese Network with Triplet Loss in Keras</h1><h1 id="Task-1-Understanding-the-Approach"><a href="#Task-1-Understanding-the-Approach" class="headerlink" title="Task 1: Understanding the Approach"></a>Task 1: Understanding the Approach</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pca_plotter <span class="keyword">import</span> PCAPlotter</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;TensorFlow version:&#x27;</span>, tf.__version__)</span><br></pre></td></tr></table></figure>

<pre><code>TensorFlow version: 2.1.0
</code></pre>
<h2 id="Understanding-the-Approach"><a href="#Understanding-the-Approach" class="headerlink" title="Understanding the Approach"></a>Understanding the Approach</h2><p>This appraoch is taken from the popular <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.03832">FaceNet</a> paper.</p>
<p>We have a CNN model called <code>EmbeddingModel</code>:</p>
<p><img src="CNN.png" alt="CNN"></p>
<p>We use three images for each training example:</p>
<ol>
<li><code>person1_image1.jpg</code> (Anchor Example, represented below in green)</li>
<li><code>person1_image2.jpg</code> (Positive Example, in blue)</li>
<li><code>person2_image1.jpg</code> (Negative Example, in red).</li>
</ol>
<p><img src="embeddings.png" alt="Embeddings"></p>
<h2 id="Siamese-Network"><a href="#Siamese-Network" class="headerlink" title="Siamese Network"></a>Siamese Network</h2><p>All the three images of an example pass through the model, and we get the three Embeddings: One for the Anchor Example, one for the Positive Example, and one for the Negative Example.</p>
<p><img src="siamese.png" alt="Siamese Network"></p>
<p>The three instances of the <code>EmbeddingModel</code> shown above are not different instances. It’s the same, shared model instance - i.e. the parameters are shared, and are updated for all the three paths simultaneously.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PCAPlotter</span>(<span class="params">tf.keras.callbacks.Callback</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, plt, embedding_model, x_test, y_test</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PCAPlotter, self).__init__()</span><br><span class="line">        self.embedding_model = embedding_model</span><br><span class="line">        self.x_test = x_test</span><br><span class="line">        self.y_test = y_test</span><br><span class="line">        self.fig = plt.figure(figsize=(<span class="number">9</span>, <span class="number">4</span>))</span><br><span class="line">        self.ax1 = plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.ax2 = plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        plt.ion()</span><br><span class="line">        </span><br><span class="line">        self.losses = []</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot</span>(<span class="params">self, epoch=<span class="literal">None</span>, plot_loss=<span class="literal">False</span></span>):</span></span><br><span class="line">        x_test_embeddings = self.embedding_model.predict(self.x_test)</span><br><span class="line">        pca_out = PCA(n_components=<span class="number">2</span>).fit_transform(x_test_embeddings)</span><br><span class="line">        self.ax1.clear()</span><br><span class="line">        self.ax1.scatter(pca_out[:, <span class="number">0</span>], pca_out[:, <span class="number">1</span>], c=self.y_test, cmap=<span class="string">&#x27;seismic&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> plot_loss:</span><br><span class="line">            self.ax2.clear()</span><br><span class="line">            self.ax2.plot(<span class="built_in">range</span>(epoch), self.losses)</span><br><span class="line">            self.ax2.set_xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">            self.ax2.set_ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">        self.fig.canvas.draw()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_train_begin</span>(<span class="params">self, logs=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.losses = []</span><br><span class="line">        self.fig.show()</span><br><span class="line">        self.fig.canvas.draw()</span><br><span class="line">        self.plot()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span>(<span class="params">self, epoch, logs=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.losses.append(logs.get(<span class="string">&#x27;loss&#x27;</span>))</span><br><span class="line">        self.plot(epoch+<span class="number">1</span>, plot_loss=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Task-2-Importing-the-Data"><a href="#Task-2-Importing-the-Data" class="headerlink" title="Task 2: Importing the Data"></a>Task 2: Importing the Data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()</span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(60000, 28, 28)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_train = np.reshape(x_train, (x_train.shape[<span class="number">0</span>], <span class="number">784</span>))/<span class="number">255.</span></span><br><span class="line">x_test = np.reshape(x_test, (x_test.shape[<span class="number">0</span>], <span class="number">784</span>))/<span class="number">255.</span></span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(60000, 784)
</code></pre>
<h1 id="Task-3-Plotting-Examples"><a href="#Task-3-Plotting-Examples" class="headerlink" title="Task 3: Plotting Examples"></a>Task 3: Plotting Examples</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_triplets</span>(<span class="params">examples</span>):</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">6</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span> + i)</span><br><span class="line">        plt.imshow(np.reshape(examples[i], (<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line">        plt.xticks([])</span><br><span class="line">        plt.yticks([])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_triplets([x_train[<span class="number">0</span>], x_train[<span class="number">1</span>], x_train[<span class="number">2</span>]])</span><br></pre></td></tr></table></figure>

<p><img src="plot1.png" alt="plot1"></p>
<h1 id="Task-4-A-Batch-of-Triplets"><a href="#Task-4-A-Batch-of-Triplets" class="headerlink" title="Task 4: A Batch of Triplets"></a>Task 4: A Batch of Triplets</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_batch</span>(<span class="params">batch_size=<span class="number">256</span></span>):</span></span><br><span class="line">    x_anchors = np.zeros((batch_size, <span class="number">784</span>))</span><br><span class="line">    x_positives = np.zeros((batch_size, <span class="number">784</span>))</span><br><span class="line">    x_negatives = np.zeros((batch_size, <span class="number">784</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, batch_size):</span><br><span class="line">        <span class="comment"># We need to find an anchor, a positive example and a negative example</span></span><br><span class="line">        random_index = random.randint(<span class="number">0</span>, x_train.shape[<span class="number">0</span>] - <span class="number">1</span>)</span><br><span class="line">        x_anchor = x_train[random_index]</span><br><span class="line">        y = y_train[random_index]</span><br><span class="line">        </span><br><span class="line">        indices_for_pos = np.squeeze(np.where(y_train == y))</span><br><span class="line">        indices_for_neg = np.squeeze(np.where(y_train != y))</span><br><span class="line">        </span><br><span class="line">        x_positive = x_train[indices_for_pos[random.randint(<span class="number">0</span>, <span class="built_in">len</span>(indices_for_pos) - <span class="number">1</span>)]]</span><br><span class="line">        x_negative = x_train[indices_for_neg[random.randint(<span class="number">0</span>, <span class="built_in">len</span>(indices_for_neg) - <span class="number">1</span>)]]</span><br><span class="line">        </span><br><span class="line">        x_anchors[i] = x_anchor</span><br><span class="line">        x_positives[i] = x_positive</span><br><span class="line">        x_negatives[i] = x_negative</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> [x_anchors, x_positives, x_negatives]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">examples = create_batch(<span class="number">1</span>)</span><br><span class="line">plot_triplets(examples)</span><br></pre></td></tr></table></figure>

<p><img src="plot2.png" alt="plot2"></p>
<h1 id="Task-5-Embedding-Model"><a href="#Task-5-Embedding-Model" class="headerlink" title="Task 5: Embedding Model"></a>Task 5: Embedding Model</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">emb_size = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">embedding_model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">784</span>,)),</span><br><span class="line">    tf.keras.layers.Dense(emb_size, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">embedding_model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 64)                50240     
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160      
=================================================================
Total params: 54,400
Trainable params: 54,400
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">example = np.expand_dims(x_train[<span class="number">0</span>], axis=<span class="number">0</span>)</span><br><span class="line">example_emb = embedding_model.predict(example)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(example_emb)</span><br></pre></td></tr></table></figure>

<pre><code>[0.42349347 0.43482512 0.5846526  0.5047948  0.4264534  0.48105526
 0.37568194 0.5898737  0.61923265 0.38126072 0.51810735 0.6918024
 0.42151055 0.31393877 0.550636   0.4718757  0.72107047 0.5304595
 0.60560906 0.54731256 0.47088197 0.57321566 0.38795182 0.3528969
 0.5260858  0.5058847  0.60069776 0.5351782  0.45879558 0.49318898
 0.52481294 0.48127335 0.41399142 0.53644794 0.596148   0.35952103
 0.4660656  0.51290053 0.34802675 0.28829136 0.49941048 0.41946915
 0.5193161  0.59598917 0.42652634 0.7554737  0.51301926 0.3393702
 0.61319596 0.3912717  0.58737236 0.5881264  0.5892425  0.62002826
 0.47996673 0.44889334 0.47385594 0.4038328  0.60131633 0.57539546
 0.47411144 0.5514124  0.6192302  0.60763264]
</code></pre>
<h1 id="Task-6-Siamese-Network"><a href="#Task-6-Siamese-Network" class="headerlink" title="Task 6: Siamese Network"></a>Task 6: Siamese Network</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input_anchor = tf.keras.layers.Input(shape=(<span class="number">784</span>,))</span><br><span class="line">input_positive = tf.keras.layers.Input(shape=(<span class="number">784</span>,))</span><br><span class="line">input_negative = tf.keras.layers.Input(shape=(<span class="number">784</span>,))</span><br><span class="line"></span><br><span class="line">embedding_anchor = embedding_model(input_anchor)</span><br><span class="line">embedding_positive = embedding_model(input_positive)</span><br><span class="line">embedding_negative = embedding_model(input_negative)</span><br><span class="line"></span><br><span class="line">output = tf.keras.layers.concatenate([embedding_anchor, embedding_positive, embedding_negative], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">net = tf.keras.models.Model([input_anchor, input_positive, input_negative], output)</span><br><span class="line">net.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;model&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 784)]        0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 784)]        0                                            
__________________________________________________________________________________________________
input_3 (InputLayer)            [(None, 784)]        0                                            
__________________________________________________________________________________________________
sequential (Sequential)         (None, 64)           54400       input_1[0][0]                    
                                                                 input_2[0][0]                    
                                                                 input_3[0][0]                    
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 192)          0           sequential[1][0]                 
                                                                 sequential[2][0]                 
                                                                 sequential[3][0]                 
==================================================================================================
Total params: 54,400
Trainable params: 54,400
Non-trainable params: 0
__________________________________________________________________________________________________
</code></pre>
<h1 id="Task-7-Triplet-Loss"><a href="#Task-7-Triplet-Loss" class="headerlink" title="Task 7: Triplet Loss"></a>Task 7: Triplet Loss</h1><p>A loss function that tries to pull the Embeddings of Anchor and Positive Examples closer, and tries to push the Embeddings of Anchor and Negative Examples away from each other.</p>
<p>Root mean square difference between Anchor and Positive examples in a batch of N images is:<br>$<br>\begin{equation}<br>d_p = \sqrt{\frac{\sum_{i=0}^{N-1}(f(a_i) - f(p_i))^2}{N}}<br>\end{equation}<br>$</p>
<p>Root mean square difference between Anchor and Negative examples in a batch of N images is:<br>$<br>\begin{equation}<br>d_n = \sqrt{\frac{\sum_{i=0}^{N-1}(f(a_i) - f(n_i))^2}{N}}<br>\end{equation}<br>$</p>
<p>For each example, we want:<br>$<br>\begin{equation}<br>d_p \leq d_n<br>\end{equation}<br>$</p>
<p>Therefore,<br>$<br>\begin{equation}<br>d_p - d_n \leq 0<br>\end{equation}<br>$</p>
<p>This condition is quite easily satisfied during the training.</p>
<p>We will make it non-trivial by adding a margin (alpha):<br>$<br>\begin{equation}<br>d_p - d_n + \alpha \leq 0<br>\end{equation}<br>$</p>
<p>Given the condition above, the Triplet Loss L is defined as:<br>$<br>\begin{equation}<br>L = max(d_p - d_n + \alpha, 0)<br>\end{equation}<br>$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">alpha = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triplet_loss</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">    anchor, positive, negative = y_pred[:,:emb_size], y_pred[:,emb_size:<span class="number">2</span>*emb_size], y_pred[:,<span class="number">2</span>*emb_size:]</span><br><span class="line">    positive_dist = tf.reduce_mean(tf.square(anchor - positive), axis=<span class="number">1</span>)</span><br><span class="line">    negative_dist = tf.reduce_mean(tf.square(anchor - negative), axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.maximum(positive_dist - negative_dist + alpha, <span class="number">0.</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Task-8-Data-Generator"><a href="#Task-8-Data-Generator" class="headerlink" title="Task 8: Data Generator"></a>Task 8: Data Generator</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_generator</span>(<span class="params">batch_size=<span class="number">256</span></span>):</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        x = create_batch(batch_size)</span><br><span class="line">        y = np.zeros((batch_size, <span class="number">3</span>*emb_size))</span><br><span class="line">        <span class="keyword">yield</span> x, y</span><br></pre></td></tr></table></figure>

<h1 id="Task-9-Model-Training"><a href="#Task-9-Model-Training" class="headerlink" title="Task 9: Model Training"></a>Task 9: Model Training</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">2048</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line">steps_per_epoch = <span class="built_in">int</span>(x_train.shape[<span class="number">0</span>]/batch_size)</span><br><span class="line"></span><br><span class="line">net.<span class="built_in">compile</span>(loss=triplet_loss, optimizer=<span class="string">&#x27;adam&#x27;</span>)</span><br><span class="line"></span><br><span class="line">_ = net.fit(</span><br><span class="line">    data_generator(batch_size),</span><br><span class="line">    steps_per_epoch=steps_per_epoch,</span><br><span class="line">    epochs=epochs, verbose=<span class="literal">False</span>,</span><br><span class="line">    callbacks=[</span><br><span class="line">        PCAPlotter(</span><br><span class="line">            plt, embedding_model,</span><br><span class="line">            x_test[:<span class="number">1000</span>], y_test[:<span class="number">1000</span>]</span><br><span class="line">        )]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><img src="plot3.png" alt="plot3"></p>
<pre><code>WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  [&#39;...&#39;]
</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Image-Super-Resolution/2020/08/11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Image-Super-Resolution/2020/08/11/" class="post-title-link" itemprop="url">Image Super Resolution</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-11 18:34:27" itemprop="dateCreated datePublished" datetime="2020-08-11T18:34:27+08:00">2020-08-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-12 08:12:40" itemprop="dateModified" datetime="2020-08-12T08:12:40+08:00">2020-08-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Image-Super-Resolution/2020/08/11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Image-Super-Resolution/2020/08/11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.rhyme.com"> <img src="https://www.rhyme.com/assets/img/logo-dark.png" alt="Header" style="width: 150px;"/> </a></p>
<h1 align=center> Image Super Resolution using Autoencoders</h1>

 

<!-- <img src="images/high_res_v_low_res.jpg" width=550px> -->

<h2 id="Task-1-Project-Overview-and-Import-Libraries"><a href="#Task-1-Project-Overview-and-Import-Libraries" class="headerlink" title="Task 1: Project Overview and Import Libraries"></a>Task 1: Project Overview and Import Libraries</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Input, Dense, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, UpSampling2D, add</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> regularizers</span><br></pre></td></tr></table></figure>

<h2 id="Task-2-Build-the-Encoder"><a href="#Task-2-Build-the-Encoder" class="headerlink" title="Task 2: Build the Encoder"></a>Task 2: Build the Encoder</h2><!-- <img src="images/autoencoder.jpg"> -->
<p>Credit: Autoencoder Schema by <a target="_blank" rel="noopener" href="https://blog.keras.io/img/ae/autoencoder_schema.jpg">Francois Chollet, 2016</a>.</p>
<!-- <h4 align=center>Encoder Architecture</h4>
<img src="images/encoder.png" width=450px align=center> -->


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">input_img = Input(shape=(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">l1 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(input_img)</span><br><span class="line">l2 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l1)</span><br><span class="line">l3 = MaxPooling2D(padding=<span class="string">&#x27;same&#x27;</span>)(l2)</span><br><span class="line">l3 = Dropout(<span class="number">0.3</span>)(l3)</span><br><span class="line">l4 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>),  padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l3)</span><br><span class="line">l5 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l4)</span><br><span class="line">l6 = MaxPooling2D(padding=<span class="string">&#x27;same&#x27;</span>)(l5)</span><br><span class="line">l7 = Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l6)</span><br><span class="line">encoder = Model(input_img, l7)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoder.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 256, 256, 3)]     0         
_________________________________________________________________
conv2d (Conv2D)              (None, 256, 256, 64)      1792      
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 256, 256, 64)      36928     
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 128, 128, 64)      0         
_________________________________________________________________
dropout (Dropout)            (None, 128, 128, 64)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 128, 128, 128)     73856     
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 128, 128, 128)     147584    
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 64, 128)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 64, 64, 256)       295168    
=================================================================
Total params: 555,328
Trainable params: 555,328
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<h2 id="Task-3-Build-the-Decoder-to-Complete-the-Network"><a href="#Task-3-Build-the-Decoder-to-Complete-the-Network" class="headerlink" title="Task 3: Build the Decoder to Complete the Network"></a>Task 3: Build the Decoder to Complete the Network</h2><!-- <img src="images/decoder.png" width=450px> -->


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Input, Dense, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, UpSampling2D, add</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> regularizers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">input_img = Input(shape=(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>))</span><br><span class="line">l1 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(input_img)</span><br><span class="line">l2 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l1)</span><br><span class="line"></span><br><span class="line">l3 = MaxPooling2D(padding=<span class="string">&#x27;same&#x27;</span>)(l2)</span><br><span class="line">l3 = Dropout(<span class="number">0.3</span>)(l3)</span><br><span class="line">l4 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>),  padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l3)</span><br><span class="line">l5 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l4)</span><br><span class="line"></span><br><span class="line">l6 = MaxPooling2D(padding=<span class="string">&#x27;same&#x27;</span>)(l5)</span><br><span class="line">l7 = Conv2D(<span class="number">256</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l6)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Decoder</span></span><br><span class="line"></span><br><span class="line">l8 = UpSampling2D()(l7)</span><br><span class="line"></span><br><span class="line">l9 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">            activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l8)</span><br><span class="line">l10 = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">             activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l9)</span><br><span class="line"></span><br><span class="line">l11 = add([l5, l10])</span><br><span class="line">l12 = UpSampling2D()(l11)</span><br><span class="line">l13 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">             activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l12)</span><br><span class="line">l14 = Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">             activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l13)</span><br><span class="line"></span><br><span class="line">l15 = add([l14, l2])</span><br><span class="line"></span><br><span class="line"><span class="comment"># chan = 3, for RGB</span></span><br><span class="line">decoded = Conv2D(<span class="number">3</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>, activity_regularizer=regularizers.l1(<span class="number">10e-10</span>))(l15)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create our network</span></span><br><span class="line">autoencoder = Model(input_img, decoded)</span><br><span class="line"><span class="comment"># You&#x27;ll understand later what this is</span></span><br><span class="line">autoencoder_hfenn = Model(input_img, decoded)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autoencoder.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;model_1&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 256, 256, 3) 0                                            
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 256, 256, 64) 1792        input_2[0][0]                    
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 256, 256, 64) 36928       conv2d_5[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 128, 128, 64) 0           conv2d_6[0][0]                   
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 128, 128, 64) 0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 128, 128, 128 73856       dropout_1[0][0]                  
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 128, 128, 128 147584      conv2d_7[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 128)  0           conv2d_8[0][0]                   
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 64, 64, 256)  295168      max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
up_sampling2d (UpSampling2D)    (None, 128, 128, 256 0           conv2d_9[0][0]                   
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 128, 128, 128 295040      up_sampling2d[0][0]              
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 128, 128, 128 147584      conv2d_10[0][0]                  
__________________________________________________________________________________________________
add (Add)                       (None, 128, 128, 128 0           conv2d_8[0][0]                   
                                                                 conv2d_11[0][0]                  
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 256, 256, 128 0           add[0][0]                        
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 256, 256, 64) 73792       up_sampling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_12[0][0]                  
__________________________________________________________________________________________________
add_1 (Add)                     (None, 256, 256, 64) 0           conv2d_13[0][0]                  
                                                                 conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 256, 256, 3)  1731        add_1[0][0]                      
==================================================================================================
Total params: 1,110,403
Trainable params: 1,110,403
Non-trainable params: 0
__________________________________________________________________________________________________
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autoencoder.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adadelta&#x27;</span>, loss=<span class="string">&#x27;mean_squared_error&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Task-4-Create-Dataset-and-Specify-Training-Routine"><a href="#Task-4-Create-Dataset-and-Specify-Training-Routine" class="headerlink" title="Task 4: Create Dataset and Specify Training Routine"></a>Task 4: Create Dataset and Specify Training Routine</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage, misc</span><br><span class="line"><span class="keyword">from</span> skimage.transform <span class="keyword">import</span> resize, rescale</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_batches</span>(<span class="params">just_load_dataset=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">    batches = <span class="number">256</span> <span class="comment"># Number of images to have at the same time in a batch</span></span><br><span class="line"></span><br><span class="line">    batch = <span class="number">0</span> <span class="comment"># Number if images in the current batch (grows over time and then resets for each batch)</span></span><br><span class="line">    batch_nb = <span class="number">0</span> <span class="comment"># Batch current index</span></span><br><span class="line"></span><br><span class="line">    max_batches = -<span class="number">1</span> <span class="comment"># If you want to train only on a limited number of images to finish the training even faster.</span></span><br><span class="line">    </span><br><span class="line">    ep = <span class="number">4</span> <span class="comment"># Number of epochs</span></span><br><span class="line"></span><br><span class="line">    images = []</span><br><span class="line">    x_train_n = []</span><br><span class="line">    x_train_down = []</span><br><span class="line">    </span><br><span class="line">    x_train_n2 = [] <span class="comment"># Resulting high res dataset</span></span><br><span class="line">    x_train_down2 = [] <span class="comment"># Resulting low res dataset</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> root, dirnames, filenames <span class="keyword">in</span> os.walk(<span class="string">&quot;/home/rhyme/Desktop/Project/data/cars_train&quot;</span>):</span><br><span class="line">        <span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">            <span class="keyword">if</span> re.search(<span class="string">&quot;\.(jpg|jpeg|JPEG|png|bmp|tiff)$&quot;</span>, filename):</span><br><span class="line">                <span class="keyword">if</span> batch_nb == max_batches: <span class="comment"># If we limit the number of batches, just return earlier</span></span><br><span class="line">                    <span class="keyword">return</span> x_train_n2, x_train_down2</span><br><span class="line">                filepath = os.path.join(root, filename)</span><br><span class="line">                image = pyplot.imread(filepath)</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(image.shape) &gt; <span class="number">2</span>:</span><br><span class="line">                        </span><br><span class="line">                    image_resized = resize(image, (<span class="number">256</span>, <span class="number">256</span>)) <span class="comment"># Resize the image so that every image is the same size</span></span><br><span class="line">                    x_train_n.append(image_resized) <span class="comment"># Add this image to the high res dataset</span></span><br><span class="line">                    x_train_down.append(rescale(rescale(image_resized, <span class="number">0.5</span>), <span class="number">2.0</span>)) <span class="comment"># Rescale it 0.5x and 2x so that it is a low res image but still has 256x256 resolution</span></span><br><span class="line">                    batch += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> batch == batches:</span><br><span class="line">                        batch_nb += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                        x_train_n2 = np.array(x_train_n)</span><br><span class="line">                        x_train_down2 = np.array(x_train_down)</span><br><span class="line">                        </span><br><span class="line">                        <span class="keyword">if</span> just_load_dataset:</span><br><span class="line">                            <span class="keyword">return</span> x_train_n2, x_train_down2</span><br><span class="line">                        </span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">&#x27;Training batch&#x27;</span>, batch_nb, <span class="string">&#x27;(&#x27;</span>, batches, <span class="string">&#x27;)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">                        autoencoder.fit(x_train_down2, x_train_n2,</span><br><span class="line">                            epochs=ep,</span><br><span class="line">                            batch_size=<span class="number">10</span>,</span><br><span class="line">                            shuffle=<span class="literal">True</span>,</span><br><span class="line">                            validation_split=<span class="number">0.15</span>)</span><br><span class="line">                    </span><br><span class="line">                        x_train_n = []</span><br><span class="line">                        x_train_down = []</span><br><span class="line">                    </span><br><span class="line">                        batch = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_train_n2, x_train_down2</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Task-5-Load-the-Dataset-and-Pre-trained-Model"><a href="#Task-5-Load-the-Dataset-and-Pre-trained-Model" class="headerlink" title="Task 5: Load the Dataset and Pre-trained Model"></a>Task 5: Load the Dataset and Pre-trained Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_train_n, x_train_down = train_batches(just_load_dataset=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<pre><code>/home/rhyme/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, &#39;constant&#39;, will be changed to &#39;reflect&#39; in skimage 0.15.
  warn(&quot;The default mode, &#39;constant&#39;, will be changed to &#39;reflect&#39; in &quot;
/home/rhyme/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.
  warn(&quot;Anti-aliasing will be enabled by default in skimage 0.15 to &quot;
/home/rhyme/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:24: UserWarning: The default multichannel argument (None) is deprecated.  Please specify either True or False explicitly.  multichannel will default to False starting with release 0.16.
  warn(&#39;The default multichannel argument (None) is deprecated.  Please &#39;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autoencoder.load_weights(<span class="string">&quot;/home/rhyme/Desktop/Project/data/sr.img_net.mse.final_model5.no_patch.weights.best.hdf5&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Task-6-Model-Predictions-and-Visualizing-the-Results"><a href="#Task-6-Model-Predictions-and-Visualizing-the-Results" class="headerlink" title="Task 6: Model Predictions and Visualizing the Results"></a>Task 6: Model Predictions and Visualizing the Results</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoder.load_weights(<span class="string">&#x27;/home/rhyme/Desktop/Project/data/encoder_weights.hdf5&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoded_imgs = encoder.predict(x_train_down)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoded_imgs.shape</span><br></pre></td></tr></table></figure>




<pre><code>(256, 64, 64, 256)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We clip the output so that it doesn&#x27;t produce weird colors</span></span><br><span class="line">sr1 = np.clip(autoencoder.predict(x_train_down), <span class="number">0.0</span>, <span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image_index = <span class="number">251</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">128</span>, <span class="number">128</span>))</span><br><span class="line">i = <span class="number">1</span></span><br><span class="line">ax = plt.subplot(<span class="number">10</span>, <span class="number">10</span>, i)</span><br><span class="line">plt.imshow(x_train_down[image_index])</span><br><span class="line">i += <span class="number">1</span></span><br><span class="line">ax = plt.subplot(<span class="number">10</span>, <span class="number">10</span>, i)</span><br><span class="line">plt.imshow(x_train_down[image_index], interpolation=<span class="string">&quot;bicubic&quot;</span>)</span><br><span class="line">i += <span class="number">1</span></span><br><span class="line">ax = plt.subplot(<span class="number">10</span>, <span class="number">10</span>, i)</span><br><span class="line">plt.imshow(encoded_imgs[image_index].reshape((<span class="number">64</span>*<span class="number">64</span>, <span class="number">256</span>)))</span><br><span class="line">i += <span class="number">1</span></span><br><span class="line">ax = plt.subplot(<span class="number">10</span>, <span class="number">10</span>, i)</span><br><span class="line">plt.imshow(sr1[image_index])</span><br><span class="line">i += <span class="number">1</span></span><br><span class="line">ax = plt.subplot(<span class="number">10</span>, <span class="number">10</span>, i)</span><br><span class="line">plt.imshow(x_train_n[image_index])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_28_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Understanding-Deepfakes-with-Keras/2020/07/30/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Understanding-Deepfakes-with-Keras/2020/07/30/" class="post-title-link" itemprop="url">Understanding Deepfakes with Keras</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-07-30 15:00:32 / Modified: 15:03:40" itemprop="dateCreated datePublished" datetime="2020-07-30T15:00:32+08:00">2020-07-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Understanding-Deepfakes-with-Keras/2020/07/30/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Understanding-Deepfakes-with-Keras/2020/07/30/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Understanding-Deepfakes-with-Keras"><a href="#Understanding-Deepfakes-with-Keras" class="headerlink" title="Understanding Deepfakes with Keras"></a>Understanding Deepfakes with Keras</h1><p><img src="DCGAN.png" alt="DCGAN"></p>
<h1 id="Task-1-Importing-Libraries-and-Helper-Functions"><a href="#Task-1-Importing-Libraries-and-Helper-Functions" class="headerlink" title="Task 1: Importing Libraries and Helper Functions"></a>Task 1: Importing Libraries and Helper Functions</h1><p>Please note: If you haven’t already, please install the required packages by executing the code cell below.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip3 install tensorflow==2.1.0 pillow matplotlib</span></span><br><span class="line"><span class="comment"># !pip3 install git+https://github.com/am1tyadav/tfutils.git</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tfutils</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, Flatten, Conv2D, BatchNormalization</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Conv2DTranspose, Reshape, LeakyReLU</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model, Sequential</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;TensorFlow version:&#x27;</span>, tf.__version__)</span><br></pre></td></tr></table></figure>

<pre><code>TensorFlow version: 2.1.0
</code></pre>
<h1 id="Task-2-Importing-and-Plotting-the-Data"><a href="#Task-2-Importing-and-Plotting-the-Data" class="headerlink" title="Task 2: Importing and Plotting the Data"></a>Task 2: Importing and Plotting the Data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(x_train, y_train), (x_test, y_test) = tfutils.datasets.mnist.load_data(one_hot=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">x_train = tfutils.datasets.mnist.load_subset([<span class="number">0</span>], x_train, y_train)</span><br><span class="line">x_test = tfutils.datasets.mnist.load_subset([<span class="number">0</span>], x_test, y_test)</span><br><span class="line"></span><br><span class="line">x = np.concatenate([x_train, x_test], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure>




<pre><code>(6903, 784)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tfutils.datasets.mnist.plot_ten_random_examples(plt, x, np.zeros((x.shape[<span class="number">0</span>], <span class="number">1</span>))).show()</span><br></pre></td></tr></table></figure>


<pre><code>&lt;IPython.core.display.Javascript object&gt;
</code></pre>
<img src="data:image/png;base64,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" width="1000">


<h1 id="Task-3-Discriminator"><a href="#Task-3-Discriminator" class="headerlink" title="Task 3: Discriminator"></a>Task 3: Discriminator</h1><p><img src="artist_critic.png" alt="Artist and Critic"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">size = <span class="number">28</span></span><br><span class="line">noise_dim = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">discriminator = Sequential([</span><br><span class="line">    Conv2D(<span class="number">64</span>, <span class="number">3</span>, strides=<span class="number">2</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)),</span><br><span class="line">    LeakyReLU(),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    Conv2D(<span class="number">128</span>, <span class="number">5</span>, strides=<span class="number">2</span>),</span><br><span class="line">    LeakyReLU(),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    Conv2D(<span class="number">256</span>, <span class="number">5</span>, strides=<span class="number">2</span>),</span><br><span class="line">    LeakyReLU(),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    Flatten(),</span><br><span class="line">    Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">opt = tf.keras.optimizers.Adam(lr=<span class="number">2e-4</span>, beta_1=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">discriminator.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=opt, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">discriminator.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 13, 13, 64)        640       
_________________________________________________________________
leaky_re_lu (LeakyReLU)      (None, 13, 13, 64)        0         
_________________________________________________________________
batch_normalization (BatchNo (None, 13, 13, 64)        256       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 5, 5, 128)         204928    
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 5, 5, 128)         0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 5, 5, 128)         512       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 1, 1, 256)         819456    
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 1, 1, 256)         0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 1, 1, 256)         1024      
_________________________________________________________________
flatten (Flatten)            (None, 256)               0         
_________________________________________________________________
dense (Dense)                (None, 1)                 257       
=================================================================
Total params: 1,027,073
Trainable params: 1,026,177
Non-trainable params: 896
_________________________________________________________________
</code></pre>
<h1 id="Task-4-Generator"><a href="#Task-4-Generator" class="headerlink" title="Task 4: Generator"></a>Task 4: Generator</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">generator = Sequential([</span><br><span class="line">    Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(noise_dim,)),</span><br><span class="line">    Reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>)),</span><br><span class="line">    </span><br><span class="line">    Conv2DTranspose(<span class="number">256</span>, <span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    Conv2DTranspose(<span class="number">128</span>, <span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line"></span><br><span class="line">    Conv2DTranspose(<span class="number">64</span>, <span class="number">5</span>, strides=<span class="number">2</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    Conv2DTranspose(<span class="number">32</span>, <span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line"></span><br><span class="line">    Conv2DTranspose(<span class="number">1</span>, <span class="number">4</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line"></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">generator.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 256)               512       
_________________________________________________________________
reshape (Reshape)            (None, 1, 1, 256)         0         
_________________________________________________________________
conv2d_transpose (Conv2DTran (None, 5, 5, 256)         1638656   
_________________________________________________________________
batch_normalization_3 (Batch (None, 5, 5, 256)         1024      
_________________________________________________________________
conv2d_transpose_1 (Conv2DTr (None, 9, 9, 128)         819328    
_________________________________________________________________
batch_normalization_4 (Batch (None, 9, 9, 128)         512       
_________________________________________________________________
conv2d_transpose_2 (Conv2DTr (None, 21, 21, 64)        204864    
_________________________________________________________________
batch_normalization_5 (Batch (None, 21, 21, 64)        256       
_________________________________________________________________
conv2d_transpose_3 (Conv2DTr (None, 25, 25, 32)        51232     
_________________________________________________________________
batch_normalization_6 (Batch (None, 25, 25, 32)        128       
_________________________________________________________________
conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         513       
=================================================================
Total params: 2,717,025
Trainable params: 2,716,065
Non-trainable params: 960
_________________________________________________________________
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">noise = np.random.randn(<span class="number">1</span>, noise_dim)</span><br><span class="line">gen_image = generator.predict(noise)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(np.reshape(gen_image, (<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">&#x27;binary&#x27;</span>);</span><br></pre></td></tr></table></figure>


<pre><code>&lt;IPython.core.display.Javascript object&gt;
</code></pre>
<img src="data:image/png;base64,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" width="640">


<h1 id="Task-5-Generative-Adversarial-Network-GAN"><a href="#Task-5-Generative-Adversarial-Network-GAN" class="headerlink" title="Task 5: Generative Adversarial Network (GAN)"></a>Task 5: Generative Adversarial Network (GAN)</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input_layer = tf.keras.layers.Input(shape=(noise_dim,))</span><br><span class="line">gen_out = generator(input_layer)</span><br><span class="line">disc_out = discriminator(gen_out)</span><br><span class="line"></span><br><span class="line">gan = Model(</span><br><span class="line">    input_layer,</span><br><span class="line">    disc_out</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">discriminator.trainable = <span class="literal">False</span></span><br><span class="line">gan.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=opt, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">gan.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
sequential_1 (Sequential)    (None, 28, 28, 1)         2717025   
_________________________________________________________________
sequential (Sequential)      (None, 1)                 1027073   
=================================================================
Total params: 3,744,098
Trainable params: 2,716,065
Non-trainable params: 1,028,033
_________________________________________________________________
</code></pre>
<h1 id="Tasks-6-and-7-Training-the-GAN"><a href="#Tasks-6-and-7-Training-the-GAN" class="headerlink" title="Tasks 6 and 7: Training the GAN"></a>Tasks 6 and 7: Training the GAN</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">25</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">steps_per_epoch = <span class="built_in">int</span>(<span class="number">2</span> * x.shape[<span class="number">0</span>]/batch_size)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Steps per epoch=&#x27;</span>, steps_per_epoch)</span><br><span class="line"></span><br><span class="line">dp = tfutils.plotting.DynamicPlot(plt, <span class="number">5</span>, <span class="number">5</span>, (<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, epochs):</span><br><span class="line">    </span><br><span class="line">    dp.start_of_epoch(e)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, steps_per_epoch):</span><br><span class="line">        true_examples = x[<span class="built_in">int</span>(batch_size/<span class="number">2</span>)*step: <span class="built_in">int</span>(batch_size/<span class="number">2</span>)*(step + <span class="number">1</span>)]</span><br><span class="line">        true_examples = np.reshape(true_examples, (true_examples.shape[<span class="number">0</span>], <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        noise = np.random.randn(<span class="built_in">int</span>(batch_size/<span class="number">2</span>), noise_dim)</span><br><span class="line">        generated_examples = generator.predict(noise)</span><br><span class="line"></span><br><span class="line">        x_batch = np.concatenate([generated_examples, true_examples], axis=<span class="number">0</span>)</span><br><span class="line">        y_batch = np.array([<span class="number">0</span>] * <span class="built_in">int</span>(batch_size/<span class="number">2</span>) + [<span class="number">1</span>] * <span class="built_in">int</span>(batch_size/<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        indices = np.random.choice(<span class="built_in">range</span>(batch_size), batch_size, replace=<span class="literal">False</span>)</span><br><span class="line">        x_batch = x_batch[indices]</span><br><span class="line">        y_batch = y_batch[indices]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># train the discriminator</span></span><br><span class="line">        discriminator.trainable = <span class="literal">True</span></span><br><span class="line">        discriminator.train_on_batch(x_batch, y_batch)</span><br><span class="line">        discriminator.trainable = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># train the generator</span></span><br><span class="line">        loss, _ = gan.train_on_batch(noise, np.ones((<span class="built_in">int</span>(batch_size/<span class="number">2</span>), <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">        _, acc = discriminator.evaluate(x_batch, y_batch, verbose=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    noise = np.random.randn(<span class="number">1</span>, noise_dim)</span><br><span class="line">    generated_example = generator.predict(noise)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    dp.end_of_epoch(np.reshape(generated_example, (<span class="number">28</span>, <span class="number">28</span>)), <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;DiscAcc:&#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(acc), <span class="string">&#x27;GANLoss:&#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(loss))</span><br></pre></td></tr></table></figure>

<pre><code>Steps per epoch= 107



&lt;IPython.core.display.Javascript object&gt;
</code></pre>
<img src="data:image/png;base64,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" width="800">



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Auto-Complete/2020/07/19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Auto-Complete/2020/07/19/" class="post-title-link" itemprop="url">Auto-Complete</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-07-19 17:45:54 / Modified: 17:46:20" itemprop="dateCreated datePublished" datetime="2020-07-19T17:45:54+08:00">2020-07-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Auto-Complete/2020/07/19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Auto-Complete/2020/07/19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Language-Models-Auto-Complete"><a href="#Language-Models-Auto-Complete" class="headerlink" title="Language Models: Auto-Complete"></a>Language Models: Auto-Complete</h1><p>In this assignment, you will build an auto-complete system.  Auto-complete system is something you may see every day</p>
<ul>
<li>When you google something, you often have suggestions to help you complete your search. </li>
<li>When you are writing an email, you get suggestions telling you possible endings to your sentence.  </li>
</ul>
<p>By the end of this assignment, you will develop a prototype of such a system.</p>
<img src = "stanford.png" style="width:700px;height:300px;"/>

<h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li><a href="#1">1 Load and Preprocess Data</a></li>
<li><a href="#1.1">1.1: Load the data</a></li>
<li><a href="#1.2">1.2 Pre-process the data</a><ul>
<li><a href="#ex-01">Exercise 01</a></li>
<li><a href="#ex-02">Exercise 02</a></li>
<li><a href="#ex-03">Exercise 03</a></li>
<li><a href="#ex-04">Exercise 04</a></li>
<li><a href="#ex-05">Exercise 05</a></li>
<li><a href="#ex-06">Exercise 06</a></li>
<li><a href="#ex-07">Exercise 07</a></li>
</ul>
</li>
<li><a href="#2">2 Develop n-gram based language models</a><ul>
<li><a href="#ex-08">Exercise 08</a></li>
<li><a href="#ex-09">Exercise 09</a>    </li>
</ul>
</li>
<li><a href="#3">3 Perplexity</a><ul>
<li><a href="#ex-10">Exercise 10</a></li>
</ul>
</li>
<li><a href="#4">4 Build an auto-complete system</a><ul>
<li><a href="#ex-11">Exercise 11</a></li>
</ul>
</li>
</ul>
<p>A key building block for an auto-complete system is a language model.<br>A language model assigns the probability to a sequence of words, in a way that more “likely” sequences receive higher scores.  For example, </p>
<blockquote>
<p>“I have a pen”<br>is expected to have a higher probability than<br>“I am a pen”<br>since the first one seems to be a more natural sentence in the real world.</p>
</blockquote>
<p>You can take advantage of this probability calculation to develop an auto-complete system.<br>Suppose the user typed </p>
<blockquote>
<p>“I eat scrambled”<br>Then you can find a word <code>x</code>  such that “I eat scrambled x” receives the highest probability.  If x = “eggs”, the sentence would be<br>“I eat scrambled eggs”</p>
</blockquote>
<p>While a variety of language models have been developed, this assignment uses <strong>N-grams</strong>, a simple but powerful method for language modeling.</p>
<ul>
<li>N-grams are also used in machine translation and speech recognition. </li>
</ul>
<p>Here are the steps of this assignment:</p>
<ol>
<li>Load and preprocess data<ul>
<li>Load and tokenize data.</li>
<li>Split the sentences into train and test sets.</li>
<li>Replace words with a low frequency by an unknown marker <code>&lt;unk&gt;</code>.</li>
</ul>
</li>
<li>Develop N-gram based language models<ul>
<li>Compute the count of n-grams from a given data set.</li>
<li>Estimate the conditional probability of a next word with k-smoothing.</li>
</ul>
</li>
<li>Evaluate the N-gram models by computing the perplexity score.</li>
<li>Use your own model to suggest an upcoming word given your sentence. </li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.data.path.append(<span class="string">&#x27;.&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><a name='1'></a></p>
<h2 id="Part-1-Load-and-Preprocess-Data"><a href="#Part-1-Load-and-Preprocess-Data" class="headerlink" title="Part 1: Load and Preprocess Data"></a>Part 1: Load and Preprocess Data</h2><p><a name='1.1'></a></p>
<h3 id="Part-1-1-Load-the-data"><a href="#Part-1-1-Load-the-data" class="headerlink" title="Part 1.1: Load the data"></a>Part 1.1: Load the data</h3><p>You will use twitter data.<br>Load the data and view the first few sentences by running the next cell.</p>
<p>Notice that data is a long string that contains many many tweets.<br>Observe that there is a line break “\n” between tweets.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;en_US.twitter.txt&quot;</span>, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = f.read()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Data type:&quot;</span>, <span class="built_in">type</span>(data))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of letters:&quot;</span>, <span class="built_in">len</span>(data))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First 300 letters of the data&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-------&quot;</span>)</span><br><span class="line">display(data[<span class="number">0</span>:<span class="number">300</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Last 300 letters of the data&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-------&quot;</span>)</span><br><span class="line">display(data[-<span class="number">300</span>:])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-------&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Data type: &lt;class &#39;str&#39;&gt;
Number of letters: 3335477
First 300 letters of the data
-------



&quot;How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\nWhen you meet someone special... you&#39;ll know. Your heart will beat more rapidly and you&#39;ll smile for no reason.\nthey&#39;ve decided its more fun if I don&#39;t.\nSo Tired D; Played Lazer Tag &amp; Ran A &quot;


-------
Last 300 letters of the data
-------



&quot;ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\nColombia is with an &#39;o&#39;...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\n#GutsiestMovesYouCanMake Giving a cat a bath.\nCoffee after 5 was a TERRIBLE idea.\n&quot;


-------
</code></pre>
<p><a name='1.2'></a></p>
<h3 id="Part-1-2-Pre-process-the-data"><a href="#Part-1-2-Pre-process-the-data" class="headerlink" title="Part 1.2 Pre-process the data"></a>Part 1.2 Pre-process the data</h3><p>Preprocess this data with the following steps:</p>
<ol>
<li>Split data into sentences using “\n” as the delimiter.</li>
<li>Split each sentence into tokens. Note that in this assignment we use “token” and “words” interchangeably.</li>
<li>Assign sentences into train or test sets.</li>
<li>Find tokens that appear at least N times in the training data.</li>
<li>Replace tokens that appear less than N times by <code>&lt;unk&gt;</code></li>
</ol>
<p>Note: we omit validation data in this exercise.</p>
<ul>
<li>In real applications, we should hold a part of data as a validation set and use it to tune our training.</li>
<li>We skip this process for simplicity.</li>
</ul>
<p><a name='ex-01'></a></p>
<h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p>Split data into sentences.</p>
<details>    
<summary>
    <font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
    <li> Use <a target="_blank" rel="noopener" href="https://docs.python.org/3/library/stdtypes.html?highlight=split#str.split" >str.split</a> </li>
</ul>
</p>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: split_to_sentences ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_to_sentences</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Split data by linebreak &quot;\n&quot;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data: str</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A list of sentences</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    sentences = data.split(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Additional clearning (This part is already implemented)</span></span><br><span class="line">    <span class="comment"># - Remove leading and trailing spaces from each sentence</span></span><br><span class="line">    <span class="comment"># - Drop sentences if they are empty strings.</span></span><br><span class="line">    sentences = [s.strip() <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">    sentences = [s <span class="keyword">for</span> s <span class="keyword">in</span> sentences <span class="keyword">if</span> <span class="built_in">len</span>(s) &gt; <span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> sentences    </span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">x = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">I have a pen.\nI have an apple. \nAh\nApple pen.\n</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">split_to_sentences(x)</span><br></pre></td></tr></table></figure>

<pre><code>I have a pen.
I have an apple. 
Ah
Apple pen.







[&#39;I have a pen.&#39;, &#39;I have an apple.&#39;, &#39;Ah&#39;, &#39;Apple pen.&#39;]
</code></pre>
<p>Expected answer: </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;I have a pen.&#x27;</span>, <span class="string">&#x27;I have an apple.&#x27;</span>, <span class="string">&#x27;Ah&#x27;</span>, <span class="string">&#x27;Apple pen.&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p><a name='ex-02'></a></p>
<h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p>The next step is to tokenize sentences (split a sentence into a list of words). </p>
<ul>
<li>Convert all tokens into lower case so that words which are capitalized (for example, at the start of a sentence) in the original text are treated the same as the lowercase versions of the words.</li>
<li>Append each tokenized list of words into a list of tokenized sentences.</li>
</ul>
<details>    
<summary>
    <font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
    <li>Use <a target="_blank" rel="noopener" href="https://docs.python.org/3/library/stdtypes.html?highlight=split#str.lower" >str.lower</a> to convert strings to lowercase. </li>
    <li>Please use <a target="_blank" rel="noopener" href="https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize" >nltk.word_tokenize</a> to split sentences into tokens.</li>
    <li>If you used str.split insteaad of nltk.word_tokenize, there are additional edge cases to handle, such as the punctuation (comma, period) that follows a word.</li>
</ul>
</p>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: tokenize_sentences ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_sentences</span>(<span class="params">sentences</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Tokenize sentences into tokens (words)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        sentences: List of strings</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of lists of tokens</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the list of lists of tokenized sentences</span></span><br><span class="line">    tokenized_sentences = []</span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each sentence</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert to lowercase letters</span></span><br><span class="line">        sentence = sentence.lower()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert into a list of words</span></span><br><span class="line">        tokenized = nltk.word_tokenize(sentence)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># append the list of words to the list of lists</span></span><br><span class="line">        tokenized_sentences.append(tokenized)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tokenized_sentences</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [<span class="string">&quot;Sky is blue.&quot;</span>, <span class="string">&quot;Leaves are green.&quot;</span>, <span class="string">&quot;Roses are red.&quot;</span>]</span><br><span class="line">tokenize_sentences(sentences)</span><br></pre></td></tr></table></figure>




<pre><code>[[&#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;],
 [&#39;leaves&#39;, &#39;are&#39;, &#39;green&#39;, &#39;.&#39;],
 [&#39;roses&#39;, &#39;are&#39;, &#39;red&#39;, &#39;.&#39;]]
</code></pre>
<h3 id="Expected-output"><a href="#Expected-output" class="headerlink" title="Expected output"></a>Expected output</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="string">&#x27;sky&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;leaves&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;green&#x27;</span>, <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;roses&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]]</span><br></pre></td></tr></table></figure>

<p><a name='ex-03'></a></p>
<h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p>Use the two functions that you have just implemented to get the tokenized data.</p>
<ul>
<li>split the data into sentences</li>
<li>tokenize those sentences</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: get_tokenized_data ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tokenized_data</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Make a list of tokenized sentences</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data: String</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of lists of tokens</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the sentences by splitting up the data</span></span><br><span class="line">    sentences = split_to_sentences(data)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the list of lists of tokens by tokenizing the sentences</span></span><br><span class="line">    tokenized_sentences = tokenize_sentences(sentences)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tokenized_sentences</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your function</span></span><br><span class="line">x = <span class="string">&quot;Sky is blue.\nLeaves are green\nRoses are red.&quot;</span></span><br><span class="line">get_tokenized_data(x)</span><br></pre></td></tr></table></figure>




<pre><code>[[&#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;],
 [&#39;leaves&#39;, &#39;are&#39;, &#39;green&#39;],
 [&#39;roses&#39;, &#39;are&#39;, &#39;red&#39;, &#39;.&#39;]]
</code></pre>
<h5 id="Expected-outcome"><a href="#Expected-outcome" class="headerlink" title="Expected outcome"></a>Expected outcome</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="string">&#x27;sky&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;leaves&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;green&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;roses&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]]</span><br></pre></td></tr></table></figure>

<h3 id="Split-into-train-and-test-sets"><a href="#Split-into-train-and-test-sets" class="headerlink" title="Split into train and test sets"></a>Split into train and test sets</h3><p>Now run the cell below to split data into training and test sets.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenized_data = get_tokenized_data(data)</span><br><span class="line">random.seed(<span class="number">87</span>)</span><br><span class="line">random.shuffle(tokenized_data)</span><br><span class="line"></span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="built_in">len</span>(tokenized_data) * <span class="number">0.8</span>)</span><br><span class="line">train_data = tokenized_data[<span class="number">0</span>:train_size]</span><br><span class="line">test_data = tokenized_data[train_size:]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; data are split into &#123;&#125; train and &#123;&#125; test set&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">    <span class="built_in">len</span>(tokenized_data), <span class="built_in">len</span>(train_data), <span class="built_in">len</span>(test_data)))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First training sample:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(train_data[<span class="number">0</span>])</span><br><span class="line">      </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First test sample&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(test_data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<pre><code>47961 data are split into 38368 train and 9593 test set
First training sample:
[&#39;i&#39;, &#39;personally&#39;, &#39;would&#39;, &#39;like&#39;, &#39;as&#39;, &#39;our&#39;, &#39;official&#39;, &#39;glove&#39;, &#39;of&#39;, &#39;the&#39;, &#39;team&#39;, &#39;local&#39;, &#39;company&#39;, &#39;and&#39;, &#39;quality&#39;, &#39;production&#39;]
First test sample
[&#39;that&#39;, &#39;picture&#39;, &#39;i&#39;, &#39;just&#39;, &#39;seen&#39;, &#39;whoa&#39;, &#39;dere&#39;, &#39;!&#39;, &#39;!&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;]
</code></pre>
<h5 id="Expected-output-1"><a href="#Expected-output-1" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">47961</span> data are split into <span class="number">38368</span> train <span class="keyword">and</span> <span class="number">9593</span> test set</span><br><span class="line">First training sample:</span><br><span class="line">[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;personally&#x27;</span>, <span class="string">&#x27;would&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;as&#x27;</span>, <span class="string">&#x27;our&#x27;</span>, <span class="string">&#x27;official&#x27;</span>, <span class="string">&#x27;glove&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;team&#x27;</span>, <span class="string">&#x27;local&#x27;</span>, <span class="string">&#x27;company&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;quality&#x27;</span>, <span class="string">&#x27;production&#x27;</span>]</span><br><span class="line">First test sample</span><br><span class="line">[<span class="string">&#x27;that&#x27;</span>, <span class="string">&#x27;picture&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;just&#x27;</span>, <span class="string">&#x27;seen&#x27;</span>, <span class="string">&#x27;whoa&#x27;</span>, <span class="string">&#x27;dere&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p><a name='ex-04'></a></p>
<h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p>You won’t use all the tokens (words) appearing in the data for training.  Instead, you will use the more frequently used words.  </p>
<ul>
<li>You will focus on the words that appear at least N times in the data.</li>
<li>First count how many times each word appears in the data.</li>
</ul>
<p>You will need a double for-loop, one for sentences and the other for tokens within a sentence.</p>
<details>    
<summary>
    <font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
    <li>If you decide to import and use defaultdict, remember to cast the dictionary back to a regular 'dict' before returning it. </li>
</ul>
</p>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: count_words ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_words</span>(<span class="params">tokenized_sentences</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Count the number of word appearence in the tokenized sentences</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tokenized_sentences: List of lists of strings</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dict that maps word (str) to the frequency (int)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">    word_counts = &#123;&#125;</span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop through each sentence</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> tokenized_sentences: <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Go through each token in the sentence</span></span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> sentence: <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># If the token is not in the dictionary yet, set the count to 1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> token <span class="keyword">in</span> word_counts: <span class="comment"># complete this line</span></span><br><span class="line">                word_counts[token] = <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If the token is already in the dictionary, increment the count by 1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                word_counts[token] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> word_counts</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">tokenized_sentences = [[<span class="string">&#x27;sky&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line">                       [<span class="string">&#x27;leaves&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;green&#x27;</span>, <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line">                       [<span class="string">&#x27;roses&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]]</span><br><span class="line">count_words(tokenized_sentences)</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;sky&#39;: 1,
 &#39;is&#39;: 1,
 &#39;blue&#39;: 1,
 &#39;.&#39;: 3,
 &#39;leaves&#39;: 1,
 &#39;are&#39;: 2,
 &#39;green&#39;: 1,
 &#39;roses&#39;: 1,
 &#39;red&#39;: 1&#125;
</code></pre>
<h5 id="Expected-output-2"><a href="#Expected-output-2" class="headerlink" title="Expected output"></a>Expected output</h5><p>Note that the order may differ.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;sky&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&#x27;is&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&#x27;blue&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&#x27;.&#x27;</span>: <span class="number">3</span>,</span><br><span class="line"> <span class="string">&#x27;leaves&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&#x27;are&#x27;</span>: <span class="number">2</span>,</span><br><span class="line"> <span class="string">&#x27;green&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&#x27;roses&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&#x27;red&#x27;</span>: <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Handling-‘Out-of-Vocabulary’-words"><a href="#Handling-‘Out-of-Vocabulary’-words" class="headerlink" title="Handling ‘Out of Vocabulary’ words"></a>Handling ‘Out of Vocabulary’ words</h3><p>If your model is performing autocomplete, but encounters a word that it never saw during training, it won’t have an input word to help it determine the next word to suggest. The model will not be able to predict the next word because there are no counts for the current word. </p>
<ul>
<li>This ‘new’ word is called an ‘unknown word’, or <b>out of vocabulary (OOV)</b> words.</li>
<li>The percentage of unknown words in the test set is called the <b> OOV </b> rate. </li>
</ul>
<p>To handle unknown words during prediction, use a special token to represent all unknown words ‘unk’. </p>
<ul>
<li>Modify the training data so that it has some ‘unknown’ words to train on.</li>
<li>Words to convert into “unknown” words are those that do not occur very frequently in the training set.</li>
<li>Create a list of the most frequent words in the training set, called the <b> closed vocabulary </b>. </li>
<li>Convert all the other words that are not part of the closed vocabulary to the token ‘unk’. </li>
</ul>
<p><a name='ex-05'></a></p>
<h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p>You will now create a function that takes in a text document and a threshold ‘count_threshold’.</p>
<ul>
<li>Any word whose count is greater than or equal to the threshold ‘count_threshold’ is kept in the closed vocabulary.</li>
<li>used that you want to keep, returns the document containing only the word closed vocabulary and the word unk. </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: get_words_with_nplus_frequency ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_words_with_nplus_frequency</span>(<span class="params">tokenized_sentences, count_threshold</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Find the words that appear N times or more</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tokenized_sentences: List of lists of sentences</span></span><br><span class="line"><span class="string">        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of words that appear N times or more</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Initialize an empty list to contain the words that</span></span><br><span class="line">    <span class="comment"># appear at least &#x27;minimum_freq&#x27; times.</span></span><br><span class="line">    closed_vocab = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the word couts of the tokenized sentences</span></span><br><span class="line">    <span class="comment"># Use the function that you defined earlier to count the words</span></span><br><span class="line">    word_counts = count_words(tokenized_sentences)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># for each word and its count</span></span><br><span class="line">    <span class="keyword">for</span> word, cnt <span class="keyword">in</span> word_counts.items(): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># check that the word&#x27;s count</span></span><br><span class="line">        <span class="comment"># is at least as great as the minimum count</span></span><br><span class="line">        <span class="keyword">if</span> cnt &gt;= count_threshold:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># append the word to the list</span></span><br><span class="line">            closed_vocab.append(word)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> closed_vocab</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">tokenized_sentences = [[<span class="string">&#x27;sky&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line">                       [<span class="string">&#x27;leaves&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;green&#x27;</span>, <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line">                       [<span class="string">&#x27;roses&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]]</span><br><span class="line">tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Closed vocabulary:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tmp_closed_vocab)</span><br></pre></td></tr></table></figure>

<pre><code>Closed vocabulary:
[&#39;.&#39;, &#39;are&#39;]
</code></pre>
<h5 id="Expected-output-3"><a href="#Expected-output-3" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Closed vocabulary:</span><br><span class="line">[<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;are&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p><a name='ex-06'></a></p>
<h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p>The words that appear ‘count_threshold’ times or more are in the ‘closed vocabulary. </p>
<ul>
<li>All other words are regarded as ‘unknown’.</li>
<li>Replace words not in the closed vocabulary with the token “&lt;unk&gt;“.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: replace_oov_words_by_unk ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replace_oov_words_by_unk</span>(<span class="params">tokenized_sentences, vocabulary, unknown_token=<span class="string">&quot;&lt;unk&gt;&quot;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Replace words not in the given vocabulary with &#x27;&lt;unk&gt;&#x27; token.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        tokenized_sentences: List of lists of strings</span></span><br><span class="line"><span class="string">        vocabulary: List of strings that we will use</span></span><br><span class="line"><span class="string">        unknown_token: A string representing unknown (out-of-vocabulary) words</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of lists of strings, with words not in the vocabulary replaced</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Place vocabulary into a set for faster search</span></span><br><span class="line">    vocabulary = <span class="built_in">set</span>(vocabulary)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a list that will hold the sentences</span></span><br><span class="line">    <span class="comment"># after less frequent words are replaced by the unknown token</span></span><br><span class="line">    replaced_tokenized_sentences = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each sentence</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> tokenized_sentences:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize the list that will contain</span></span><br><span class="line">        <span class="comment"># a single sentence with &quot;unknown_token&quot; replacements</span></span><br><span class="line">        replaced_sentence = []</span><br><span class="line">        <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># for each token in the sentence</span></span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> sentence: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Check if the token is in the closed vocabulary</span></span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">in</span> vocabulary: <span class="comment"># complete this line</span></span><br><span class="line">                <span class="comment"># If so, append the word to the replaced_sentence</span></span><br><span class="line">                replaced_sentence.append(token)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># otherwise, append the unknown token instead</span></span><br><span class="line">                replaced_sentence.append(unknown_token)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Append the list of tokens to the list of lists</span></span><br><span class="line">        replaced_tokenized_sentences.append(replaced_sentence)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> replaced_tokenized_sentences</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenized_sentences = [[<span class="string">&quot;dogs&quot;</span>, <span class="string">&quot;run&quot;</span>], [<span class="string">&quot;cats&quot;</span>, <span class="string">&quot;sleep&quot;</span>]]</span><br><span class="line">vocabulary = [<span class="string">&quot;dogs&quot;</span>, <span class="string">&quot;sleep&quot;</span>]</span><br><span class="line">tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Original sentence:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenized_sentences)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;tokenized_sentences with less frequent words converted to &#x27;&lt;unk&gt;&#x27;:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tmp_replaced_tokenized_sentences)</span><br></pre></td></tr></table></figure>

<pre><code>Original sentence:
[[&#39;dogs&#39;, &#39;run&#39;], [&#39;cats&#39;, &#39;sleep&#39;]]
tokenized_sentences with less frequent words converted to &#39;&lt;unk&gt;&#39;:
[[&#39;dogs&#39;, &#39;&lt;unk&gt;&#39;], [&#39;&lt;unk&gt;&#39;, &#39;sleep&#39;]]
</code></pre>
<h3 id="Expected-answer"><a href="#Expected-answer" class="headerlink" title="Expected answer"></a>Expected answer</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Original sentence:</span><br><span class="line">[[<span class="string">&#x27;dogs&#x27;</span>, <span class="string">&#x27;run&#x27;</span>], [<span class="string">&#x27;cats&#x27;</span>, <span class="string">&#x27;sleep&#x27;</span>]]</span><br><span class="line">tokenized_sentences with less frequent words converted to <span class="string">&#x27;&lt;unk&gt;&#x27;</span>:</span><br><span class="line">[[<span class="string">&#x27;dogs&#x27;</span>, <span class="string">&#x27;&lt;unk&gt;&#x27;</span>], [<span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;sleep&#x27;</span>]]</span><br></pre></td></tr></table></figure>

<p><a name='ex-07'></a></p>
<h3 id="Exercise-07"><a href="#Exercise-07" class="headerlink" title="Exercise 07"></a>Exercise 07</h3><p>Now we are ready to process our data by combining the functions that you just implemented.</p>
<ol>
<li>Find tokens that appear at least count_threshold times in the training data.</li>
<li>Replace tokens that appear less than count_threshold times by “&lt;unk&gt;“ both for training and test data.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED_FUNCTION: preprocess_data ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_data</span>(<span class="params">train_data, test_data, count_threshold</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Preprocess data, i.e.,</span></span><br><span class="line"><span class="string">        - Find tokens that appear at least N times in the training data.</span></span><br><span class="line"><span class="string">        - Replace tokens that appear less than N times by &quot;&lt;unk&gt;&quot; both for training and test data.        </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        train_data, test_data: List of lists of strings.</span></span><br><span class="line"><span class="string">        count_threshold: Words whose count is less than this are </span></span><br><span class="line"><span class="string">                      treated as unknown.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Tuple of</span></span><br><span class="line"><span class="string">        - training data with low frequent words replaced by &quot;&lt;unk&gt;&quot;</span></span><br><span class="line"><span class="string">        - test data with low frequent words replaced by &quot;&lt;unk&gt;&quot;</span></span><br><span class="line"><span class="string">        - vocabulary of words that appear n times or more in the training data</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get the closed vocabulary using the train data</span></span><br><span class="line">    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># For the train data, replace less common words with &quot;&lt;unk&gt;&quot;</span></span><br><span class="line">    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary, unknown_token=<span class="string">&quot;&lt;unk&gt;&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># For the test data, replace less common words with &quot;&lt;unk&gt;&quot;</span></span><br><span class="line">    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary, unknown_token=<span class="string">&quot;&lt;unk&gt;&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> train_data_replaced, test_data_replaced, vocabulary</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">tmp_train = [[<span class="string">&#x27;sky&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;.&#x27;</span>],</span><br><span class="line">     [<span class="string">&#x27;leaves&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;green&#x27;</span>]]</span><br><span class="line">tmp_test = [[<span class="string">&#x27;roses&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train, </span><br><span class="line">                                                           tmp_test, </span><br><span class="line">                                                           count_threshold = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tmp_train_repl&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tmp_train_repl)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tmp_test_repl&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tmp_test_repl)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tmp_vocab&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tmp_vocab)</span><br></pre></td></tr></table></figure>

<pre><code>tmp_train_repl
[[&#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;], [&#39;leaves&#39;, &#39;are&#39;, &#39;green&#39;]]

tmp_test_repl
[[&#39;&lt;unk&gt;&#39;, &#39;are&#39;, &#39;&lt;unk&gt;&#39;, &#39;.&#39;]]

tmp_vocab
[&#39;sky&#39;, &#39;is&#39;, &#39;blue&#39;, &#39;.&#39;, &#39;leaves&#39;, &#39;are&#39;, &#39;green&#39;]
</code></pre>
<h5 id="Expected-outcome-1"><a href="#Expected-outcome-1" class="headerlink" title="Expected outcome"></a>Expected outcome</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tmp_train_repl</span><br><span class="line">[[<span class="string">&#x27;sky&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;.&#x27;</span>], [<span class="string">&#x27;leaves&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;green&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">tmp_test_repl</span><br><span class="line">[[<span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">tmp_vocab</span><br><span class="line">[<span class="string">&#x27;sky&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;leaves&#x27;</span>, <span class="string">&#x27;are&#x27;</span>, <span class="string">&#x27;green&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h3 id="Preprocess-the-train-and-test-data"><a href="#Preprocess-the-train-and-test-data" class="headerlink" title="Preprocess the train and test data"></a>Preprocess the train and test data</h3><p>Run the cell below to complete the preprocessing both for training and test sets.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">minimum_freq = <span class="number">2</span></span><br><span class="line">train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, </span><br><span class="line">                                                                        test_data, </span><br><span class="line">                                                                        minimum_freq)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First preprocessed training sample:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(train_data_processed[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First preprocessed test sample:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(test_data_processed[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First 10 vocabulary:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(vocabulary[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Size of vocabulary:&quot;</span>, <span class="built_in">len</span>(vocabulary))</span><br></pre></td></tr></table></figure>

<pre><code>First preprocessed training sample:
[&#39;i&#39;, &#39;personally&#39;, &#39;would&#39;, &#39;like&#39;, &#39;as&#39;, &#39;our&#39;, &#39;official&#39;, &#39;glove&#39;, &#39;of&#39;, &#39;the&#39;, &#39;team&#39;, &#39;local&#39;, &#39;company&#39;, &#39;and&#39;, &#39;quality&#39;, &#39;production&#39;]

First preprocessed test sample:
[&#39;that&#39;, &#39;picture&#39;, &#39;i&#39;, &#39;just&#39;, &#39;seen&#39;, &#39;whoa&#39;, &#39;dere&#39;, &#39;!&#39;, &#39;!&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;]

First 10 vocabulary:
[&#39;i&#39;, &#39;personally&#39;, &#39;would&#39;, &#39;like&#39;, &#39;as&#39;, &#39;our&#39;, &#39;official&#39;, &#39;glove&#39;, &#39;of&#39;, &#39;the&#39;]

Size of vocabulary: 14821
</code></pre>
<h5 id="Expected-output-4"><a href="#Expected-output-4" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">First preprocessed training sample:</span><br><span class="line">[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;personally&#x27;</span>, <span class="string">&#x27;would&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;as&#x27;</span>, <span class="string">&#x27;our&#x27;</span>, <span class="string">&#x27;official&#x27;</span>, <span class="string">&#x27;glove&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;team&#x27;</span>, <span class="string">&#x27;local&#x27;</span>, <span class="string">&#x27;company&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;quality&#x27;</span>, <span class="string">&#x27;production&#x27;</span>]</span><br><span class="line"></span><br><span class="line">First preprocessed test sample:</span><br><span class="line">[<span class="string">&#x27;that&#x27;</span>, <span class="string">&#x27;picture&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;just&#x27;</span>, <span class="string">&#x27;seen&#x27;</span>, <span class="string">&#x27;whoa&#x27;</span>, <span class="string">&#x27;dere&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>]</span><br><span class="line"></span><br><span class="line">First <span class="number">10</span> vocabulary:</span><br><span class="line">[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;personally&#x27;</span>, <span class="string">&#x27;would&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;as&#x27;</span>, <span class="string">&#x27;our&#x27;</span>, <span class="string">&#x27;official&#x27;</span>, <span class="string">&#x27;glove&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;the&#x27;</span>]</span><br><span class="line"></span><br><span class="line">Size of vocabulary: <span class="number">14821</span></span><br></pre></td></tr></table></figure>

<p>You are done with the preprocessing section of the assignment.<br>Objects <code>train_data_processed</code>, <code>test_data_processed</code>, and <code>vocabulary</code> will be used in the rest of the exercises.</p>
<p><a name='2'></a></p>
<h2 id="Part-2-Develop-n-gram-based-language-models"><a href="#Part-2-Develop-n-gram-based-language-models" class="headerlink" title="Part 2: Develop n-gram based language models"></a>Part 2: Develop n-gram based language models</h2><p>In this section, you will develop the n-grams language model.</p>
<ul>
<li>Assume the probability of the next word depends only on the previous n-gram.</li>
<li>The previous n-gram is the series of the previous ‘n’ words.</li>
</ul>
<p>The conditional probability for the word at position ‘t’ in the sentence, given that the words preceding it are $w_{t-1}, w_{t-2} \cdots w_{t-n}$ is:</p>
<p>$$ P(w_t | w_{t-1}\dots w_{t-n}) \tag{1}$$</p>
<p>You can estimate this probability  by counting the occurrences of these series of words in the training data.</p>
<ul>
<li>The probability can be estimated as a ratio, where</li>
<li>The numerator is the number of times word ‘t’ appears after words t-1 through t-n appear in the training data.</li>
<li>The denominator is the number of times word t-1 through t-n appears in the training data.</li>
</ul>
<p>$$ \hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n)}{C(w_{t-1}\dots w_{t-n})} \tag{2} $$</p>
<ul>
<li>The function $C(\cdots)$ denotes the number of occurence of the given sequence. </li>
<li>$\hat{P}$ means the estimation of $P$. </li>
<li>Notice that denominator of the equation (2) is the number of occurence of the previous $n$ words, and the numerator is the same sequence followed by the word $w_t$.</li>
</ul>
<p>Later, you will modify the equation (2) by adding k-smoothing, which avoids errors when any counts are zero.</p>
<p>The equation (2) tells us that to estimate probabilities based on n-grams, you need the counts of n-grams (for denominator) and (n+1)-grams (for numerator).</p>
<p><a name='ex-08'></a></p>
<h3 id="Exercise-08"><a href="#Exercise-08" class="headerlink" title="Exercise 08"></a>Exercise 08</h3><p>Next, you will implement a function that computes the counts of n-grams for an arbitrary number $n$.</p>
<p>When computing the counts for n-grams, prepare the sentence beforehand by prepending $n-1$ starting markers “&lt;s&gt;“ to indicate the beginning of the sentence.  </p>
<ul>
<li>For example, in the bi-gram model (N=2), a sequence with two start tokens “&lt;s&gt;&lt;s&gt;“ should predict the first word of a sentence.</li>
<li>So, if the sentence is “I like food”, modify it to be “&lt;s&gt;&lt;s&gt; I like food”.</li>
<li>Also prepare the sentence for counting by appending an end token “&lt;e&gt;“ so that the model can predict when to finish a sentence.</li>
</ul>
<p>Technical note: In this implementation, you will store the counts as a dictionary.</p>
<ul>
<li>The key of each key-value pair in the dictionary is a <strong>tuple</strong> of n words (and not a list)</li>
<li>The value in the key-value pair is the number of occurrences.  </li>
<li>The reason for using a tuple as a key instead of a list is because a list in Python is a mutable object (it can be changed after it is first created).  A tuple is “immutable”, so it cannot be altered after it is first created.  This makes a tuple suitable as a data type for the key in a dictionary.</li>
</ul>
<details>    
<summary>
    <font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
    <li> To prepend or append, you can create lists and concatenate them using the + operator </li>
    <li> To create a list of a repeated value, you can follow this syntax: <code>['a'] * 3</code> to get <code>['a','a','a']</code> </li>
    <li>To set the range for index 'i', think of this example: An n-gram where n=2 (bigram), and the sentence is length N=5 (including two start tokens and one end token).  So the index positions are <code>[0,1,2,3,4]</code>.  The largest index 'i' where a bigram can start is at position i=3, because the word tokens at position 3 and 4 will form the bigram. </li>
    <li>Remember that the <code>range()</code> function excludes the value that is used for the maximum of the range.  <code> range(3) </code> produces (0,1,2) but excludes 3. </li>
</ul>
</p>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED FUNCTION: count_n_grams ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_n_grams</span>(<span class="params">data, n, start_token=<span class="string">&#x27;&lt;s&gt;&#x27;</span>, end_token = <span class="string">&#x27;&lt;e&gt;&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Count all n-grams in the data</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data: List of lists of words</span></span><br><span class="line"><span class="string">        n: number of words in a sequence</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A dictionary that maps a tuple of n-words to its frequency</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize dictionary of n-grams and their counts</span></span><br><span class="line">    n_grams = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Go through each sentence in the data</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> data: <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># prepend start token n times, and  append &lt;e&gt; one time</span></span><br><span class="line">        sentence = [start_token] * n + sentence + [end_token]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># convert list to tuple</span></span><br><span class="line">        <span class="comment"># So that the sequence of words can be used as</span></span><br><span class="line">        <span class="comment"># a key in the dictionary</span></span><br><span class="line">        sentence = <span class="built_in">tuple</span>(sentence)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use &#x27;i&#x27; to indicate the start of the n-gram</span></span><br><span class="line">        <span class="comment"># from index 0</span></span><br><span class="line">        <span class="comment"># to the last index where the end of the n-gram</span></span><br><span class="line">        <span class="comment"># is within the sentence.</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentence)-n+<span class="number">1</span>): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Get the n-gram from i to i+n</span></span><br><span class="line">            n_gram = sentence[i:i+n]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># check if the n-gram is in the dictionary</span></span><br><span class="line">            <span class="keyword">if</span> n_gram <span class="keyword">in</span> n_grams: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">                <span class="comment"># Increment the count for this n-gram</span></span><br><span class="line">                n_grams[n_gram] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># Initialize this n-gram count to 1</span></span><br><span class="line">                n_grams[n_gram] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">            <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> n_grams</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line"><span class="comment"># CODE REVIEW COMMENT: Outcome does not match expected outcome</span></span><br><span class="line">sentences = [[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>],</span><br><span class="line">             [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Uni-gram:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(count_n_grams(sentences, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bi-gram:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(count_n_grams(sentences, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<pre><code>Uni-gram:
&#123;(&#39;&lt;s&gt;&#39;,): 2, (&#39;i&#39;,): 1, (&#39;like&#39;,): 2, (&#39;a&#39;,): 2, (&#39;cat&#39;,): 2, (&#39;&lt;e&gt;&#39;,): 2, (&#39;this&#39;,): 1, (&#39;dog&#39;,): 1, (&#39;is&#39;,): 1&#125;
Bi-gram:
&#123;(&#39;&lt;s&gt;&#39;, &#39;&lt;s&gt;&#39;): 2, (&#39;&lt;s&gt;&#39;, &#39;i&#39;): 1, (&#39;i&#39;, &#39;like&#39;): 1, (&#39;like&#39;, &#39;a&#39;): 2, (&#39;a&#39;, &#39;cat&#39;): 2, (&#39;cat&#39;, &#39;&lt;e&gt;&#39;): 2, (&#39;&lt;s&gt;&#39;, &#39;this&#39;): 1, (&#39;this&#39;, &#39;dog&#39;): 1, (&#39;dog&#39;, &#39;is&#39;): 1, (&#39;is&#39;, &#39;like&#39;): 1&#125;
</code></pre>
<p>Expected outcome:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Uni-gram:</span><br><span class="line">&#123;(<span class="string">&#x27;&lt;s&gt;&#x27;</span>,): <span class="number">2</span>, (<span class="string">&#x27;i&#x27;</span>,): <span class="number">1</span>, (<span class="string">&#x27;like&#x27;</span>,): <span class="number">2</span>, (<span class="string">&#x27;a&#x27;</span>,): <span class="number">2</span>, (<span class="string">&#x27;cat&#x27;</span>,): <span class="number">2</span>, (<span class="string">&#x27;&lt;e&gt;&#x27;</span>,): <span class="number">2</span>, (<span class="string">&#x27;this&#x27;</span>,): <span class="number">1</span>, (<span class="string">&#x27;dog&#x27;</span>,): <span class="number">1</span>, (<span class="string">&#x27;is&#x27;</span>,): <span class="number">1</span>&#125;</span><br><span class="line">Bi-gram:</span><br><span class="line">&#123;(<span class="string">&#x27;&lt;s&gt;&#x27;</span>, <span class="string">&#x27;&lt;s&gt;&#x27;</span>): <span class="number">2</span>, (<span class="string">&#x27;&lt;s&gt;&#x27;</span>, <span class="string">&#x27;i&#x27;</span>): <span class="number">1</span>, (<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>): <span class="number">1</span>, (<span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>): <span class="number">2</span>, (<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>): <span class="number">2</span>, (<span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;&lt;e&gt;&#x27;</span>): <span class="number">2</span>, (<span class="string">&#x27;&lt;s&gt;&#x27;</span>, <span class="string">&#x27;this&#x27;</span>): <span class="number">1</span>, (<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>): <span class="number">1</span>, (<span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>): <span class="number">1</span>, (<span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>): <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>

<p><a name='ex-09'></a></p>
<h3 id="Exercise-09"><a href="#Exercise-09" class="headerlink" title="Exercise 09"></a>Exercise 09</h3><p>Next, estimate the probability of a word given the prior ‘n’ words using the n-gram counts.</p>
<p>$$ \hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n)}{C(w_{t-1}\dots w_{t-n})} \tag{2} $$</p>
<p>This formula doesn’t work when a count of an n-gram is zero..</p>
<ul>
<li>Suppose we encounter an n-gram that did not occur in the training data.  </li>
<li>Then, the equation (2) cannot be evaluated (it becomes zero divided by zero).</li>
</ul>
<p>A way to handle zero counts is to add k-smoothing.  </p>
<ul>
<li>K-smoothing adds a positive constant $k$ to each numerator and $k \times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary.</li>
</ul>
<p>$$ \hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n) + k}{C(w_{t-1}\dots w_{t-n}) + k|V|} \tag{3} $$</p>
<p>For n-grams that have a zero count, the equation (3) becomes $\frac{1}{|V|}$.</p>
<ul>
<li>This means that any n-gram with zero count has the same probability of $\frac{1}{|V|}$.</li>
</ul>
<p>Define a function that computes the probability estimate (3) from n-gram counts and a constant $k$.</p>
<ul>
<li>The function takes in a dictionary ‘n_gram_counts’, where the key is the n-gram and the value is the count of that n-gram.</li>
<li>The function also takes another dictionary n_plus1_gram_counts, which you’ll use to find the count for the previous n-gram plus the current word.</li>
</ul>
<details>    
<summary>
    <font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
    <li>To define a tuple containing a single value, add a comma after that value.  For example: <code>('apple',)</code> is a tuple containing a single string 'apple' </li>
    <li>To concatenate two tuples, use the '+' operator</li>
    <li><a href="" > words </a> </li>
</ul>
</p>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment">### GRADED FUNCTION: estimate_probabilityy ###</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">estimate_probability</span>(<span class="params">word, previous_n_gram, </span></span></span><br><span class="line"><span class="params"><span class="function">                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Estimate the probabilities of a next word using the n-gram counts with k-smoothing</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        word: next word</span></span><br><span class="line"><span class="string">        previous_n_gram: A sequence of words of length n</span></span><br><span class="line"><span class="string">        n_gram_counts: Dictionary of counts of n-grams</span></span><br><span class="line"><span class="string">        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        vocabulary_size: number of words in the vocabulary</span></span><br><span class="line"><span class="string">        k: positive constant, smoothing parameter</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A probability</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># convert list to tuple to use it as a dictionary key</span></span><br><span class="line">    previous_n_gram = <span class="built_in">tuple</span>(previous_n_gram)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the denominator</span></span><br><span class="line">    <span class="comment"># If the previous n-gram exists in the dictionary of n-gram counts,</span></span><br><span class="line">    <span class="comment"># Get its count.  Otherwise set the count to zero</span></span><br><span class="line">    <span class="comment"># Use the dictionary that has counts for n-grams</span></span><br><span class="line">    previous_n_gram_count = n_gram_counts.get(previous_n_gram, <span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Calculate the denominator using the count of the previous n gram</span></span><br><span class="line">    <span class="comment"># and apply k-smoothing</span></span><br><span class="line">    denominator = previous_n_gram_count + k * vocabulary_size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define n plus 1 gram as the previous n-gram plus the current word as a tuple</span></span><br><span class="line">    n_plus1_gram = n_gram_counts.get(previous_n_gram, <span class="number">0</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Set the count to the count in the dictionary,</span></span><br><span class="line">    <span class="comment"># otherwise 0 if not in the dictionary</span></span><br><span class="line">    <span class="comment"># use the dictionary that has counts for the n-gram plus current word</span></span><br><span class="line">    n_plus1_gram_count = n_plus1_gram_counts.get(previous_n_gram + (word, ) ,<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Define the numerator use the count of the n-gram plus current word,</span></span><br><span class="line">    <span class="comment"># and apply smoothing</span></span><br><span class="line">    numerator = n_plus1_gram_count + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the probability as the numerator divided by denominator</span></span><br><span class="line">    probability = numerator / denominator</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> probability</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>],</span><br><span class="line">             [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]]</span><br><span class="line">unique_words = <span class="built_in">list</span>(<span class="built_in">set</span>(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line">tmp_prob = estimate_probability(<span class="string">&quot;cat&quot;</span>, <span class="string">&quot;a&quot;</span>, unigram_counts, bigram_counts, <span class="built_in">len</span>(unique_words), k=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The estimated probability of word &#x27;cat&#x27; given the previous n-gram &#x27;a&#x27; is: <span class="subst">&#123;tmp_prob:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The estimated probability of word &#39;cat&#39; given the previous n-gram &#39;a&#39; is: 0.3333
</code></pre>
<h5 id="Expected-output-5"><a href="#Expected-output-5" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The estimated probability of word <span class="string">&#x27;cat&#x27;</span> given the previous n-gram <span class="string">&#x27;a&#x27;</span> is: <span class="number">0.3333</span></span><br></pre></td></tr></table></figure>

<h3 id="Estimate-probabilities-for-all-words"><a href="#Estimate-probabilities-for-all-words" class="headerlink" title="Estimate probabilities for all words"></a>Estimate probabilities for all words</h3><p>The function defined below loops over all words in vocabulary to calculate probabilities for all possible words.</p>
<ul>
<li>This function is provided for you.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">estimate_probabilities</span>(<span class="params">previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Estimate the probabilities of next words using the n-gram counts with k-smoothing</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        previous_n_gram: A sequence of words of length n</span></span><br><span class="line"><span class="string">        n_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        vocabulary: List of words</span></span><br><span class="line"><span class="string">        k: positive constant, smoothing parameter</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A dictionary mapping from next words to the probability.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># convert list to tuple to use it as a dictionary key</span></span><br><span class="line">    previous_n_gram = <span class="built_in">tuple</span>(previous_n_gram)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># add &lt;e&gt; &lt;unk&gt; to the vocabulary</span></span><br><span class="line">    <span class="comment"># &lt;s&gt; is not needed since it should not appear as the next word</span></span><br><span class="line">    vocabulary = vocabulary + [<span class="string">&quot;&lt;e&gt;&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>]</span><br><span class="line">    vocabulary_size = <span class="built_in">len</span>(vocabulary)</span><br><span class="line">    </span><br><span class="line">    probabilities = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> vocabulary:</span><br><span class="line">        probability = estimate_probability(word, previous_n_gram, </span><br><span class="line">                                           n_gram_counts, n_plus1_gram_counts, </span><br><span class="line">                                           vocabulary_size, k=k)</span><br><span class="line">        probabilities[word] = probability</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> probabilities</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>],</span><br><span class="line">             [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]]</span><br><span class="line">unique_words = <span class="built_in">list</span>(<span class="built_in">set</span>(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line">estimate_probabilities(<span class="string">&quot;a&quot;</span>, unigram_counts, bigram_counts, unique_words, k=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;dog&#39;: 0.09090909090909091,
 &#39;like&#39;: 0.09090909090909091,
 &#39;cat&#39;: 0.2727272727272727,
 &#39;i&#39;: 0.09090909090909091,
 &#39;is&#39;: 0.09090909090909091,
 &#39;this&#39;: 0.09090909090909091,
 &#39;a&#39;: 0.09090909090909091,
 &#39;&lt;e&gt;&#39;: 0.09090909090909091,
 &#39;&lt;unk&gt;&#39;: 0.09090909090909091&#125;
</code></pre>
<h5 id="Expected-output-6"><a href="#Expected-output-6" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;cat&#x27;</span>: <span class="number">0.2727272727272727</span>,</span><br><span class="line"> <span class="string">&#x27;i&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;this&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;a&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;is&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;like&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;dog&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;&lt;e&gt;&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;&lt;unk&gt;&#x27;</span>: <span class="number">0.09090909090909091</span>&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Additional test</span></span><br><span class="line">trigram_counts = count_n_grams(sentences, <span class="number">3</span>)</span><br><span class="line">estimate_probabilities([<span class="string">&quot;&lt;s&gt;&quot;</span>, <span class="string">&quot;&lt;s&gt;&quot;</span>], bigram_counts, trigram_counts, unique_words, k=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;dog&#39;: 0.09090909090909091,
 &#39;like&#39;: 0.09090909090909091,
 &#39;cat&#39;: 0.09090909090909091,
 &#39;i&#39;: 0.18181818181818182,
 &#39;is&#39;: 0.09090909090909091,
 &#39;this&#39;: 0.18181818181818182,
 &#39;a&#39;: 0.09090909090909091,
 &#39;&lt;e&gt;&#39;: 0.09090909090909091,
 &#39;&lt;unk&gt;&#39;: 0.09090909090909091&#125;
</code></pre>
<h5 id="Expected-output-7"><a href="#Expected-output-7" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;cat&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;i&#x27;</span>: <span class="number">0.18181818181818182</span>,</span><br><span class="line"> <span class="string">&#x27;this&#x27;</span>: <span class="number">0.18181818181818182</span>,</span><br><span class="line"> <span class="string">&#x27;a&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;is&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;like&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;dog&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;&lt;e&gt;&#x27;</span>: <span class="number">0.09090909090909091</span>,</span><br><span class="line"> <span class="string">&#x27;&lt;unk&gt;&#x27;</span>: <span class="number">0.09090909090909091</span>&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Count-and-probability-matrices"><a href="#Count-and-probability-matrices" class="headerlink" title="Count and probability matrices"></a>Count and probability matrices</h3><p>As we have seen so far, the n-gram counts computed above are sufficient for computing the probabilities of the next word.  </p>
<ul>
<li>It can be more intuitive to present them as count or probability matrices.</li>
<li>The functions defined in the next cells return count or probability matrices.</li>
<li>This function is provided for you.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_count_matrix</span>(<span class="params">n_plus1_gram_counts, vocabulary</span>):</span></span><br><span class="line">    <span class="comment"># add &lt;e&gt; &lt;unk&gt; to the vocabulary</span></span><br><span class="line">    <span class="comment"># &lt;s&gt; is omitted since it should not appear as the next word</span></span><br><span class="line">    vocabulary = vocabulary + [<span class="string">&quot;&lt;e&gt;&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># obtain unique n-grams</span></span><br><span class="line">    n_grams = []</span><br><span class="line">    <span class="keyword">for</span> n_plus1_gram <span class="keyword">in</span> n_plus1_gram_counts.keys():</span><br><span class="line">        n_gram = n_plus1_gram[<span class="number">0</span>:-<span class="number">1</span>]</span><br><span class="line">        n_grams.append(n_gram)</span><br><span class="line">    n_grams = <span class="built_in">list</span>(<span class="built_in">set</span>(n_grams))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># mapping from n-gram to row</span></span><br><span class="line">    row_index = &#123;n_gram:i <span class="keyword">for</span> i, n_gram <span class="keyword">in</span> <span class="built_in">enumerate</span>(n_grams)&#125;</span><br><span class="line">    <span class="comment"># mapping from next word to column</span></span><br><span class="line">    col_index = &#123;word:j <span class="keyword">for</span> j, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocabulary)&#125;</span><br><span class="line">    </span><br><span class="line">    nrow = <span class="built_in">len</span>(n_grams)</span><br><span class="line">    ncol = <span class="built_in">len</span>(vocabulary)</span><br><span class="line">    count_matrix = np.zeros((nrow, ncol))</span><br><span class="line">    <span class="keyword">for</span> n_plus1_gram, count <span class="keyword">in</span> n_plus1_gram_counts.items():</span><br><span class="line">        n_gram = n_plus1_gram[<span class="number">0</span>:-<span class="number">1</span>]</span><br><span class="line">        word = n_plus1_gram[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocabulary:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        i = row_index[n_gram]</span><br><span class="line">        j = col_index[word]</span><br><span class="line">        count_matrix[i, j] = count</span><br><span class="line">    </span><br><span class="line">    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)</span><br><span class="line">    <span class="keyword">return</span> count_matrix</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sentences = [[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]]</span><br><span class="line">unique_words = <span class="built_in">list</span>(<span class="built_in">set</span>(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;bigram counts&#x27;</span>)</span><br><span class="line">display(make_count_matrix(bigram_counts, unique_words))</span><br></pre></td></tr></table></figure>

<pre><code>bigram counts
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dog</th>
      <th>like</th>
      <th>cat</th>
      <th>i</th>
      <th>is</th>
      <th>this</th>
      <th>a</th>
      <th>&lt;e&gt;</th>
      <th>&lt;unk&gt;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(dog,)</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>(like,)</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>(&lt;s&gt;,)</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>(i,)</th>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>(is,)</th>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>(this,)</th>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>(cat,)</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>(a,)</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>


<h5 id="Expected-output-8"><a href="#Expected-output-8" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">bigram counts</span><br><span class="line">          cat    i   <span class="keyword">this</span>   a  is   like  dog  &lt;e&gt;   &lt;unk&gt;</span><br><span class="line">(&lt;s&gt;,)    <span class="number">0.0</span>   <span class="number">1.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(a,)      <span class="number">2.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(<span class="keyword">this</span>,)   <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">1.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(like,)   <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">2.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(dog,)    <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(cat,)    <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">2.0</span>    <span class="number">0.0</span></span><br><span class="line">(is,)     <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(i,)      <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Show trigram counts</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\ntrigram counts&#x27;</span>)</span><br><span class="line">trigram_counts = count_n_grams(sentences, <span class="number">3</span>)</span><br><span class="line">display(make_count_matrix(trigram_counts, unique_words))</span><br></pre></td></tr></table></figure>

<pre><code>trigram counts
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dog</th>
      <th>like</th>
      <th>cat</th>
      <th>i</th>
      <th>is</th>
      <th>this</th>
      <th>a</th>
      <th>&lt;e&gt;</th>
      <th>&lt;unk&gt;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(&lt;s&gt;, &lt;s&gt;)</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>(&lt;s&gt;, i)</th>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>(is, like)</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>(i, like)</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>(&lt;s&gt;, this)</th>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>(a, cat)</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>(like, a)</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>(dog, is)</th>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>(this, dog)</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>


<h5 id="Expected-output-9"><a href="#Expected-output-9" class="headerlink" title="Expected output"></a>Expected output</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">trigram counts</span><br><span class="line">              cat    i   <span class="keyword">this</span>   a  is   like  dog  &lt;e&gt;   &lt;unk&gt;</span><br><span class="line">(dog, is)     <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(<span class="keyword">this</span>, dog)   <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(a, cat)      <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">2.0</span>    <span class="number">0.0</span></span><br><span class="line">(like, a)     <span class="number">2.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(is, like)    <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(&lt;s&gt;, i)      <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(i, like)     <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(&lt;s&gt;, &lt;s&gt;)    <span class="number">0.0</span>   <span class="number">1.0</span>  <span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br><span class="line">(&lt;s&gt;, <span class="keyword">this</span>)   <span class="number">0.0</span>   <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span>   <span class="number">1.0</span>  <span class="number">0.0</span>    <span class="number">0.0</span></span><br></pre></td></tr></table></figure>

<p>The following function calculates the probabilities of each word given the previous n-gram, and stores this in matrix form.</p>
<ul>
<li>This function is provided for you.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_probability_matrix</span>(<span class="params">n_plus1_gram_counts, vocabulary, k</span>):</span></span><br><span class="line">    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)</span><br><span class="line">    count_matrix += k</span><br><span class="line">    prob_matrix = count_matrix.div(count_matrix.<span class="built_in">sum</span>(axis=<span class="number">1</span>), axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> prob_matrix</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sentences = [[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]]</span><br><span class="line">unique_words = <span class="built_in">list</span>(<span class="built_in">set</span>(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;bigram probabilities&quot;</span>)</span><br><span class="line">display(make_probability_matrix(bigram_counts, unique_words, k=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<pre><code>bigram probabilities
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dog</th>
      <th>like</th>
      <th>cat</th>
      <th>i</th>
      <th>is</th>
      <th>this</th>
      <th>a</th>
      <th>&lt;e&gt;</th>
      <th>&lt;unk&gt;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(dog,)</th>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.200000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
    </tr>
    <tr>
      <th>(like,)</th>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.272727</td>
      <td>0.090909</td>
      <td>0.090909</td>
    </tr>
    <tr>
      <th>(&lt;s&gt;,)</th>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.181818</td>
      <td>0.090909</td>
      <td>0.181818</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
    </tr>
    <tr>
      <th>(i,)</th>
      <td>0.100000</td>
      <td>0.200000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
    </tr>
    <tr>
      <th>(is,)</th>
      <td>0.100000</td>
      <td>0.200000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
    </tr>
    <tr>
      <th>(this,)</th>
      <td>0.200000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
    </tr>
    <tr>
      <th>(cat,)</th>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.272727</td>
      <td>0.090909</td>
    </tr>
    <tr>
      <th>(a,)</th>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.272727</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
    </tr>
  </tbody>
</table>
</div>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;trigram probabilities&quot;</span>)</span><br><span class="line">trigram_counts = count_n_grams(sentences, <span class="number">3</span>)</span><br><span class="line">display(make_probability_matrix(trigram_counts, unique_words, k=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<pre><code>trigram probabilities
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dog</th>
      <th>like</th>
      <th>cat</th>
      <th>i</th>
      <th>is</th>
      <th>this</th>
      <th>a</th>
      <th>&lt;e&gt;</th>
      <th>&lt;unk&gt;</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(&lt;s&gt;, &lt;s&gt;)</th>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.181818</td>
      <td>0.090909</td>
      <td>0.181818</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
    </tr>
    <tr>
      <th>(&lt;s&gt;, i)</th>
      <td>0.100000</td>
      <td>0.200000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
    </tr>
    <tr>
      <th>(is, like)</th>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.200000</td>
      <td>0.100000</td>
      <td>0.100000</td>
    </tr>
    <tr>
      <th>(i, like)</th>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.200000</td>
      <td>0.100000</td>
      <td>0.100000</td>
    </tr>
    <tr>
      <th>(&lt;s&gt;, this)</th>
      <td>0.200000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
    </tr>
    <tr>
      <th>(a, cat)</th>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.272727</td>
      <td>0.090909</td>
    </tr>
    <tr>
      <th>(like, a)</th>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.272727</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
      <td>0.090909</td>
    </tr>
    <tr>
      <th>(dog, is)</th>
      <td>0.100000</td>
      <td>0.200000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
    </tr>
    <tr>
      <th>(this, dog)</th>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.200000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
      <td>0.100000</td>
    </tr>
  </tbody>
</table>
</div>


<p>Confirm that you obtain the same results as for the <code>estimate_probabilities</code> function that you implemented.</p>
<p><a name='3'></a></p>
<h2 id="Part-3-Perplexity"><a href="#Part-3-Perplexity" class="headerlink" title="Part 3: Perplexity"></a>Part 3: Perplexity</h2><p>In this section, you will generate the perplexity score to evaluate your model on the test set. </p>
<ul>
<li>You will also use back-off when needed. </li>
<li>Perplexity is used as an evaluation metric of your language model. </li>
<li>To calculate the  the perplexity score of the test set on an n-gram model, use: </li>
</ul>
<p>$$ PP(W) =\sqrt[N]{ \prod_{t=n+1}^N \frac{1}{P(w_t | w_{t-n} \cdots w_{t-1})} } \tag{4}$$</p>
<ul>
<li>where $N$ is the length of the sentence.</li>
<li>$n$ is the number of words in the n-gram (e.g. 2 for a bigram).</li>
<li>In math, the numbering starts at one and not zero.</li>
</ul>
<p>In code, array indexing starts at zero, so the code will use ranges for $t$ according to this formula:</p>
<p>$$ PP(W) =\sqrt[N]{ \prod_{t=n}^{N-1} \frac{1}{P(w_t | w_{t-n} \cdots w_{t-1})} } \tag{4.1}$$</p>
<p>The higher the probabilities are, the lower the perplexity will be. </p>
<ul>
<li>The more the n-grams tell us about the sentence, the lower the perplexity score will be. </li>
</ul>
<p><a name='ex-10'></a></p>
<h3 id="Exercise-10"><a href="#Exercise-10" class="headerlink" title="Exercise 10"></a>Exercise 10</h3><p>Compute the perplexity score given an N-gram count matrix and a sentence. </p>
<details>    
<summary>
    <font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
    <li>Remember that <code>range(2,4)</code> produces the integers [2, 3] (and excludes 4).</li>
</ul>
</p>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: calculate_perplexity</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_perplexity</span>(<span class="params">sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculate perplexity for a list of sentences</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        sentence: List of strings</span></span><br><span class="line"><span class="string">        n_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        vocabulary_size: number of unique words in the vocabulary</span></span><br><span class="line"><span class="string">        k: Positive smoothing constant</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Perplexity score</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># length of previous words</span></span><br><span class="line">    n = <span class="built_in">len</span>(<span class="built_in">list</span>(n_gram_counts.keys())[<span class="number">0</span>]) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># prepend &lt;s&gt; and append &lt;e&gt;</span></span><br><span class="line">    sentence = [<span class="string">&quot;&lt;s&gt;&quot;</span>] * n + sentence + [<span class="string">&quot;&lt;e&gt;&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Cast the sentence from a list to a tuple</span></span><br><span class="line">    sentence = <span class="built_in">tuple</span>(sentence)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># length of sentence (after adding &lt;s&gt; and &lt;e&gt; tokens)</span></span><br><span class="line">    N = <span class="built_in">len</span>(sentence)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The variable p will hold the product</span></span><br><span class="line">    <span class="comment"># that is calculated inside the n-root</span></span><br><span class="line">    <span class="comment"># Update this in the code below</span></span><br><span class="line">    product_pi = <span class="number">1.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Index t ranges from n to N - 1, inclusive on both ends</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(n, N): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get the n-gram preceding the word at position t</span></span><br><span class="line">        n_gram = sentence[t-n:t]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get the word at position t</span></span><br><span class="line">        word = sentence[t]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Estimate the probability of the word given the n-gram</span></span><br><span class="line">        <span class="comment"># using the n-gram counts, n-plus1-gram counts,</span></span><br><span class="line">        <span class="comment"># vocabulary size, and smoothing constant</span></span><br><span class="line">        probability = estimate_probability(word, n_gram, </span><br><span class="line">                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the product of the probabilities</span></span><br><span class="line">        <span class="comment"># This &#x27;product_pi&#x27; is a cumulative product </span></span><br><span class="line">        <span class="comment"># of the (1/P) factors that are calculated in the loop</span></span><br><span class="line">        product_pi *= (<span class="number">1</span>/probability)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Take the Nth root of the product</span></span><br><span class="line">    perplexity = product_pi ** (<span class="number">1</span>/N)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> perplexity</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line"></span><br><span class="line">sentences = [[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>],</span><br><span class="line">                 [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]]</span><br><span class="line">unique_words = <span class="built_in">list</span>(<span class="built_in">set</span>(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">perplexity_train1 = calculate_perplexity(sentences[<span class="number">0</span>],</span><br><span class="line">                                         unigram_counts, bigram_counts,</span><br><span class="line">                                         <span class="built_in">len</span>(unique_words), k=<span class="number">1.0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Perplexity for first train sample: <span class="subst">&#123;perplexity_train1:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">test_sentence = [<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>]</span><br><span class="line">perplexity_test = calculate_perplexity(test_sentence,</span><br><span class="line">                                       unigram_counts, bigram_counts,</span><br><span class="line">                                       <span class="built_in">len</span>(unique_words), k=<span class="number">1.0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Perplexity for test sample: <span class="subst">&#123;perplexity_test:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Perplexity for first train sample: 2.8040
Perplexity for test sample: 3.9654
</code></pre>
<h3 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output"></a>Expected Output</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Perplexity <span class="keyword">for</span> first train sample: <span class="number">2.8040</span></span><br><span class="line">Perplexity <span class="keyword">for</span> test sample: <span class="number">3.9654</span></span><br></pre></td></tr></table></figure>

<p><b> Note: </b> If your sentence is really long, there will be underflow when multiplying many fractions.</p>
<ul>
<li>To handle longer sentences, modify your implementation to take the sum of the log of the probabilities.</li>
</ul>
<p><a name='4'></a></p>
<h2 id="Part-4-Build-an-auto-complete-system"><a href="#Part-4-Build-an-auto-complete-system" class="headerlink" title="Part 4: Build an auto-complete system"></a>Part 4: Build an auto-complete system</h2><p>In this section, you will combine the language models developed so far to implement an auto-complete system. </p>
<p><a name='ex-11'></a></p>
<h3 id="Exercise-11"><a href="#Exercise-11" class="headerlink" title="Exercise 11"></a>Exercise 11</h3><p>Compute probabilities for all possible next words and suggest the most likely one.</p>
<ul>
<li>This function also take an optional argument <code>start_with</code>, which specifies the first few letters of the next words.</li>
</ul>
<details>    
<summary>
    <font size="3" color="darkgreen"><b>Hints</b></font>
</summary>
<p>
<ul>
    <li><code>estimate_probabilities</code> returns a dictionary where the key is a word and the value is the word's probability.</li>
    <li> Use <code>str1.startswith(str2)</code> to determine if a string starts with the letters of another string.  For example, <code>'learning'.startswith('lea')</code> returns True, whereas <code>'learning'.startswith('ear')</code> returns False. There are two additional parameters in <code>str.startswith()</code>, but you can use the default values for those parameters in this case.</li>
</ul>
</p>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: suggest_a_word</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">suggest_a_word</span>(<span class="params">previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=<span class="number">1.0</span>, start_with=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Get suggestion for the next word</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        previous_tokens: The sentence you input where each token is a word. Must have length &gt; n </span></span><br><span class="line"><span class="string">        n_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span></span><br><span class="line"><span class="string">        vocabulary: List of words</span></span><br><span class="line"><span class="string">        k: positive constant, smoothing parameter</span></span><br><span class="line"><span class="string">        start_with: If not None, specifies the first few letters of the next word</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A tuple of </span></span><br><span class="line"><span class="string">          - string of the most likely next word</span></span><br><span class="line"><span class="string">          - corresponding probability</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># length of previous words</span></span><br><span class="line">    n = <span class="built_in">len</span>(<span class="built_in">list</span>(n_gram_counts.keys())[<span class="number">0</span>]) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># From the words that the user already typed</span></span><br><span class="line">    <span class="comment"># get the most recent &#x27;n&#x27; words as the previous n-gram</span></span><br><span class="line">    previous_n_gram = previous_tokens[-n:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Estimate the probabilities that each word in the vocabulary</span></span><br><span class="line">    <span class="comment"># is the next word,</span></span><br><span class="line">    <span class="comment"># given the previous n-gram, the dictionary of n-gram counts,</span></span><br><span class="line">    <span class="comment"># the dictionary of n plus 1 gram counts, and the smoothing constant</span></span><br><span class="line">    probabilities = estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize suggested word to None</span></span><br><span class="line">    <span class="comment"># This will be set to the word with highest probability</span></span><br><span class="line">    suggestion = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the highest word probability to 0</span></span><br><span class="line">    <span class="comment"># this will be set to the highest probability </span></span><br><span class="line">    <span class="comment"># of all words to be suggested</span></span><br><span class="line">    max_prob = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (Replace instances of &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># For each word and its probability in the probabilities dictionary:</span></span><br><span class="line">    <span class="keyword">for</span> word, prob <span class="keyword">in</span> probabilities.items(): <span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If the optional start_with string is set</span></span><br><span class="line">        <span class="keyword">if</span> start_with: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Check if the beginning of word does not match with the letters in &#x27;start_with&#x27;</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> word.startswith(start_with): <span class="comment"># complete this line</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># if they don&#x27;t match, skip this word (move onto the next word)</span></span><br><span class="line">                 <span class="keyword">continue</span><span class="comment"># complete this line</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Check if this word&#x27;s probability</span></span><br><span class="line">        <span class="comment"># is greater than the current maximum probability</span></span><br><span class="line">        <span class="keyword">if</span> prob &gt; max_prob: <span class="comment"># complete this line</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># If so, save this word as the best suggestion (so far)</span></span><br><span class="line">            suggestion = word</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Save the new maximum probability</span></span><br><span class="line">            max_prob = prob</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> suggestion, max_prob</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>],</span><br><span class="line">             [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]]</span><br><span class="line">unique_words = <span class="built_in">list</span>(<span class="built_in">set</span>(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">previous_tokens = [<span class="string">&quot;i&quot;</span>, <span class="string">&quot;like&quot;</span>]</span><br><span class="line">tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=<span class="number">1.0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The previous words are &#x27;i like&#x27;,\n\tand the suggested word is `<span class="subst">&#123;tmp_suggest1[<span class="number">0</span>]&#125;</span>` with a probability of <span class="subst">&#123;tmp_suggest1[<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="comment"># test your code when setting the starts_with</span></span><br><span class="line">tmp_starts_with = <span class="string">&#x27;c&#x27;</span></span><br><span class="line">tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=<span class="number">1.0</span>, start_with=tmp_starts_with)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The previous words are &#x27;i like&#x27;, the suggestion must start with `<span class="subst">&#123;tmp_starts_with&#125;</span>`\n\tand the suggested word is `<span class="subst">&#123;tmp_suggest2[<span class="number">0</span>]&#125;</span>` with a probability of <span class="subst">&#123;tmp_suggest2[<span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The previous words are &#39;i like&#39;,
    and the suggested word is `a` with a probability of 0.2727

The previous words are &#39;i like&#39;, the suggestion must start with `c`
    and the suggested word is `cat` with a probability of 0.0909
</code></pre>
<h3 id="Expected-output-10"><a href="#Expected-output-10" class="headerlink" title="Expected output"></a>Expected output</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">The previous words are <span class="string">&#x27;i like&#x27;</span>,</span><br><span class="line">    <span class="keyword">and</span> the suggested word is `a` with a probability of <span class="number">0.2727</span></span><br><span class="line"></span><br><span class="line">The previous words are <span class="string">&#x27;i like&#x27;</span>, the suggestion must start with `c`</span><br><span class="line">    <span class="keyword">and</span> the suggested word is `cat` with a probability of <span class="number">0.0909</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Get-multiple-suggestions"><a href="#Get-multiple-suggestions" class="headerlink" title="Get multiple suggestions"></a>Get multiple suggestions</h3><p>The function defined below loop over varioud n-gram models to get multiple suggestions.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_suggestions</span>(<span class="params">previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>, start_with=<span class="literal">None</span></span>):</span></span><br><span class="line">    model_counts = <span class="built_in">len</span>(n_gram_counts_list)</span><br><span class="line">    suggestions = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(model_counts-<span class="number">1</span>):</span><br><span class="line">        n_gram_counts = n_gram_counts_list[i]</span><br><span class="line">        n_plus1_gram_counts = n_gram_counts_list[i+<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        suggestion = suggest_a_word(previous_tokens, n_gram_counts,</span><br><span class="line">                                    n_plus1_gram_counts, vocabulary,</span><br><span class="line">                                    k=k, start_with=start_with)</span><br><span class="line">        suggestions.append(suggestion)</span><br><span class="line">    <span class="keyword">return</span> suggestions</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test your code</span></span><br><span class="line">sentences = [[<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>],</span><br><span class="line">             [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;like&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]]</span><br><span class="line">unique_words = <span class="built_in">list</span>(<span class="built_in">set</span>(sentences[<span class="number">0</span>] + sentences[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">unigram_counts = count_n_grams(sentences, <span class="number">1</span>)</span><br><span class="line">bigram_counts = count_n_grams(sentences, <span class="number">2</span>)</span><br><span class="line">trigram_counts = count_n_grams(sentences, <span class="number">3</span>)</span><br><span class="line">quadgram_counts = count_n_grams(sentences, <span class="number">4</span>)</span><br><span class="line">qintgram_counts = count_n_grams(sentences, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]</span><br><span class="line">previous_tokens = [<span class="string">&quot;i&quot;</span>, <span class="string">&quot;like&quot;</span>]</span><br><span class="line">tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The previous words are &#x27;i like&#x27;, the suggestions are:&quot;</span>)</span><br><span class="line">display(tmp_suggest3)</span><br></pre></td></tr></table></figure>

<pre><code>The previous words are &#39;i like&#39;, the suggestions are:



[(&#39;a&#39;, 0.2727272727272727),
 (&#39;a&#39;, 0.2),
 (&#39;dog&#39;, 0.1111111111111111),
 (&#39;dog&#39;, 0.1111111111111111)]
</code></pre>
<h3 id="Suggest-multiple-words-using-n-grams-of-varying-length"><a href="#Suggest-multiple-words-using-n-grams-of-varying-length" class="headerlink" title="Suggest multiple words using n-grams of varying length"></a>Suggest multiple words using n-grams of varying length</h3><p>Congratulations!  You have developed all building blocks for implementing your own auto-complete systems.</p>
<p>Let’s see this with n-grams of varying lengths (unigrams, bigrams, trigrams, 4-grams…6-grams).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">n_gram_counts_list = []</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Computing n-gram counts with n =&quot;</span>, n, <span class="string">&quot;...&quot;</span>)</span><br><span class="line">    n_model_counts = count_n_grams(train_data_processed, n)</span><br><span class="line">    n_gram_counts_list.append(n_model_counts)</span><br></pre></td></tr></table></figure>

<pre><code>Computing n-gram counts with n = 1 ...
Computing n-gram counts with n = 2 ...
Computing n-gram counts with n = 3 ...
Computing n-gram counts with n = 4 ...
Computing n-gram counts with n = 5 ...
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">&quot;i&quot;</span>, <span class="string">&quot;am&quot;</span>, <span class="string">&quot;to&quot;</span>]</span><br><span class="line">tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:&quot;</span>)</span><br><span class="line">display(tmp_suggest4)</span><br></pre></td></tr></table></figure>

<pre><code>The previous words are [&#39;i&#39;, &#39;am&#39;, &#39;to&#39;], the suggestions are:



[(&#39;be&#39;, 0.027665685098338604),
 (&#39;have&#39;, 0.00013487086115044844),
 (&#39;have&#39;, 0.00013490725126475548),
 (&#39;i&#39;, 6.746272684341901e-05)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">&quot;i&quot;</span>, <span class="string">&quot;want&quot;</span>, <span class="string">&quot;to&quot;</span>, <span class="string">&quot;go&quot;</span>]</span><br><span class="line">tmp_suggest5 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:&quot;</span>)</span><br><span class="line">display(tmp_suggest5)</span><br></pre></td></tr></table></figure>

<pre><code>The previous words are [&#39;i&#39;, &#39;want&#39;, &#39;to&#39;, &#39;go&#39;], the suggestions are:



[(&#39;to&#39;, 0.014051961029228078),
 (&#39;to&#39;, 0.004697942168993581),
 (&#39;to&#39;, 0.0009424436216762033),
 (&#39;to&#39;, 0.0004044489383215369)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">&quot;hey&quot;</span>, <span class="string">&quot;how&quot;</span>, <span class="string">&quot;are&quot;</span>]</span><br><span class="line">tmp_suggest6 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:&quot;</span>)</span><br><span class="line">display(tmp_suggest6)</span><br></pre></td></tr></table></figure>

<pre><code>The previous words are [&#39;hey&#39;, &#39;how&#39;, &#39;are&#39;], the suggestions are:



[(&#39;you&#39;, 0.023426812585499317),
 (&#39;you&#39;, 0.003559435862995299),
 (&#39;you&#39;, 0.00013491635186184566),
 (&#39;i&#39;, 6.746272684341901e-05)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">&quot;hey&quot;</span>, <span class="string">&quot;how&quot;</span>, <span class="string">&quot;are&quot;</span>, <span class="string">&quot;you&quot;</span>]</span><br><span class="line">tmp_suggest7 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:&quot;</span>)</span><br><span class="line">display(tmp_suggest7)</span><br></pre></td></tr></table></figure>

<pre><code>The previous words are [&#39;hey&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;], the suggestions are:



[(&quot;&#39;re&quot;, 0.023973994311255586),
 (&#39;?&#39;, 0.002888465830762161),
 (&#39;?&#39;, 0.0016134453781512605),
 (&#39;&lt;e&gt;&#39;, 0.00013491635186184566)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">previous_tokens = [<span class="string">&quot;hey&quot;</span>, <span class="string">&quot;how&quot;</span>, <span class="string">&quot;are&quot;</span>, <span class="string">&quot;you&quot;</span>]</span><br><span class="line">tmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=<span class="number">1.0</span>, start_with=<span class="string">&quot;d&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The previous words are <span class="subst">&#123;previous_tokens&#125;</span>, the suggestions are:&quot;</span>)</span><br><span class="line">display(tmp_suggest8)</span><br></pre></td></tr></table></figure>

<pre><code>The previous words are [&#39;hey&#39;, &#39;how&#39;, &#39;are&#39;, &#39;you&#39;], the suggestions are:



[(&#39;do&#39;, 0.009020723283218204),
 (&#39;doing&#39;, 0.0016411737674785006),
 (&#39;doing&#39;, 0.00047058823529411766),
 (&#39;dvd&#39;, 6.745817593092283e-05)]
</code></pre>
<h1 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations!"></a>Congratulations!</h1><p>You’ve completed this assignment by building an autocomplete model using an n-gram language model!  </p>
<p>Please continue onto the fourth and final week of this course!</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">266</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail → mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '[object Object]';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
