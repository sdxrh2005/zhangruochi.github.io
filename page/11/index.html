<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="RUOCHI.AI">
<meta property="og:url" content="https://zhangruochi.com/page/11/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Ruochi Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhangruochi.com/page/11/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/rails-tutorials-data-model/2020/01/17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/rails-tutorials-data-model/2020/01/17/" class="post-title-link" itemprop="url">rails tutorials: data model</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-01-17 10:19:10 / Modified: 23:42:17" itemprop="dateCreated datePublished" datetime="2020-01-17T10:19:10+08:00">2020-01-17</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Full-Stack/" itemprop="url" rel="index"><span itemprop="name">Full Stack</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Full-Stack/Ruby-on-Rails/" itemprop="url" rel="index"><span itemprop="name">Ruby on Rails</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/rails-tutorials-data-model/2020/01/17/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/rails-tutorials-data-model/2020/01/17/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>How to create data model and store the data</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/rails-tutorials-data-model/2020/01/17/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/rails-tutorials-static-page-and-automated-testing/2020/01/11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/rails-tutorials-static-page-and-automated-testing/2020/01/11/" class="post-title-link" itemprop="url">rails-tutorials: static page and automated testing</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-01-11 15:28:09 / Modified: 17:11:13" itemprop="dateCreated datePublished" datetime="2020-01-11T15:28:09+08:00">2020-01-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Full-Stack/" itemprop="url" rel="index"><span itemprop="name">Full Stack</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Full-Stack/Ruby-on-Rails/" itemprop="url" rel="index"><span itemprop="name">Ruby on Rails</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/rails-tutorials-static-page-and-automated-testing/2020/01/11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/rails-tutorials-static-page-and-automated-testing/2020/01/11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>how to use static page and automated testing in ROR</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/rails-tutorials-static-page-and-automated-testing/2020/01/11/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/rails-tutorials-toy-app/2020/01/11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/rails-tutorials-toy-app/2020/01/11/" class="post-title-link" itemprop="url">rails tutorials - toy_app</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-11 14:51:49" itemprop="dateCreated datePublished" datetime="2020-01-11T14:51:49+08:00">2020-01-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-17 23:19:48" itemprop="dateModified" datetime="2020-01-17T23:19:48+08:00">2020-01-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Full-Stack/" itemprop="url" rel="index"><span itemprop="name">Full Stack</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Full-Stack/Ruby-on-Rails/" itemprop="url" rel="index"><span itemprop="name">Ruby on Rails</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/rails-tutorials-toy-app/2020/01/11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/rails-tutorials-toy-app/2020/01/11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>a simple tutorial of ror</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/rails-tutorials-toy-app/2020/01/11/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Fastai-Data-Block-API/2019/12/29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Fastai-Data-Block-API/2019/12/29/" class="post-title-link" itemprop="url">Fastai Data Block API</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2019-12-29 16:08:18 / Modified: 21:03:57" itemprop="dateCreated datePublished" datetime="2019-12-29T16:08:18+08:00">2019-12-29</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Fastai-Data-Block-API/2019/12/29/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Fastai-Data-Block-API/2019/12/29/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>a journey through the fastai data block API</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/Fastai-Data-Block-API/2019/12/29/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Face-Blindness-Saver/2019/12/25/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Face-Blindness-Saver/2019/12/25/" class="post-title-link" itemprop="url">Face Blindness Saver</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-25 01:20:49" itemprop="dateCreated datePublished" datetime="2019-12-25T01:20:49+08:00">2019-12-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-24 03:01:03" itemprop="dateModified" datetime="2020-01-24T03:01:03+08:00">2020-01-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Face-Blindness-Saver/2019/12/25/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Face-Blindness-Saver/2019/12/25/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>I want to make a classifier to classify the akita and shiba inu, alaska and husky.</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/Face-Blindness-Saver/2019/12/25/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/ELMo-OpenAI-GPT-BERT/2019/12/21/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/ELMo-OpenAI-GPT-BERT/2019/12/21/" class="post-title-link" itemprop="url">ELMo,OpenAI GPT,BERT</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-21 22:31:19" itemprop="dateCreated datePublished" datetime="2019-12-21T22:31:19+08:00">2019-12-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-24 03:06:41" itemprop="dateModified" datetime="2020-01-24T03:06:41+08:00">2020-01-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/ELMo-OpenAI-GPT-BERT/2019/12/21/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/ELMo-OpenAI-GPT-BERT/2019/12/21/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="the-problems-of-rnn">The problems of RNN</h2>
<ol type="1">
<li>Sequential computation inhibit parallelization</li>
<li>No explicit modeling of long and short range</li>
<li>We want to model hierarchy (RNNs seem wasteful)</li>
</ol>
<h2 id="elmo">ELMo</h2>
<p>ELMo means <code>Embeddings from Language Models</code>. the original paper is from <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.05365">https://arxiv.org/abs/1802.05365</a></p>
<ol type="1">
<li>Breakout version of word token vectors or contextual word vectors</li>
<li>Learn word token vectors using long contexts not context windows (here, whole sentence, could be longer)</li>
<li>Learn a deep Bi-NLM and use all its layers in prediction</li>
</ol>
<h3 id="whats-elmos-secret">What’s ELMo’s secret?</h3>
<p>ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called <code>Language Modeling</code>. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels.</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="elmo2.png" width = "70%" height="70%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
char cnn embedding from ELMo
</div>
</center>
<p>A step in the pre-training process of ELMo: Given “Let’s stick to” as input, predict the next most likely word – a language modeling task. When trained on a large dataset, the model starts to pick up on language patterns. It’s unlikely it’ll accurately guess the next word in this example. More realistically, after a word such as “hang”, it will assign a higher probability to a word like “out” (to spell “hang out”) than to “camera”.</p>
<h3 id="bilstm-lm">bilstm LM</h3>
<p>ELMo actually goes a step further and trains a bi-directional LSTM – so that its language model doesn’t only have a sense of the next word, but also the previous word.</p>
<ol type="1">
<li>前向LSTM结构: <span class="math display">\[p(t_1,t_2,...,t_N) = \prod^N_{k=1}p(t_k|t_1,t_2,...,t_{k-1})\]</span></li>
<li>反向LSTM结构: <span class="math display">\[p(t_1,t_2,...,t_N) = \prod^N_{k=1}p(t_k|t_{k+1},t_{k+2},...,t_{N})\]</span></li>
<li>最大似然函数: <span class="math display">\[\sum_{k=1}^N(logp(t_k|t_1,t_2,...,t_{k-1}) + logp(t_k|t_{k+1},t_{k+2},...,t_{N}))\]</span></li>
<li>线性组合公式： <span class="math display">\[\textrm{ELMo}_k^{task} = E(R_k;\Theta^{task}) = \gamma^{task}\sum_{j=0}^L s_j^{task}h_{k,j}^{LM} \tag{1}\]</span></li>
</ol>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="elmo.png" width = "70%" height="70%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
bilstm from ELMo
</div>
</center>
<h3 id="char-cnn-embedding">Char cnn embedding</h3>
<p>The input of elmo is char embedding, see the details from <a href="https://zhangruochi.com/Subword-Models/2019/12/19/">https://zhangruochi.com/Subword-Models/2019/12/19/</a></p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="char_cnn.png" width = "70%" height="70%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
char cnn embedding from ELMo
</div>
</center>
<h2 id="how-to-use-elmo-when-after-pre-training">How to use ELMo when after pre-training</h2>
<p>We can feed our input data to the pre-trained ELMo and get the representation of <code>dynamic word vectors</code>. And then we use them to our specific tasks.</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="elmo3.png" width = "70%" height="70%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
ELMo used in a sequence tagger
</div>
</center>
<h2 id="openai-transformer-pre-training-a-transformer-decoder-for-language-modeling">OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling</h2>
<p>It turns out we don’t need an entire Transformer to adopt transfer learning and a fine-tunable language model for NLP tasks. We can do with just the decoder of the transformer. The decoder is a good choice because it’s a natural choice for language modeling (predicting the next word) since it’s built to <code>mask future tokens</code> – a valuable feature when it’s generating a translation word by word.</p>
<p>The model stacked twelve decoder layers. Since there is no encoder in this set up, these decoder layers would not have the encoder-decoder attention sublayer that vanilla transformer decoder layers have. It would still have the self-attention layer, however (masked so it doesn’t peak at future tokens).</p>
<p>With this structure, we can proceed to train the model on the same language modeling task: predict the next word using massive (unlabeled) datasets. Just, throw the text of 7,000 books at it and have it learn! Books are great for this sort of task since it allows the model to learn to associate related information even if they’re separated by a lot of text – something you don’t get for example, when you’re training with tweets, or articles.</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="openai.png" width = "70%" height="70%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">

</div>
</center>
<p><span class="math display">\[
\begin{split}
h_0 &amp; =UW_e+W_p \\
h_l &amp; = transformer\_block(h_{l-1}) \\
P(u) &amp; = softmax(h_n W_e^T)
\end{split}
\]</span></p>
<p><span class="math inline">\(W_e\)</span> is the embedding matrix, <span class="math inline">\(W_p\)</span> is the positional embedding matrix(Note that it is different with classicial transformer)</p>
<h3 id="fine-tuning-with-openai">Fine-Tuning with OpenAI</h3>
<p>Now that the OpenAI transformer is pre-trained and its layers have been tuned to reasonably handle language, we can start using it for downstream tasks. Let’s first look at sentence classification (classify an email message as “spam” or “not spam”):</p>
<p>If our input sequence is <span class="math inline">\(x_1,\cdots,x_m\)</span>, and the label is y. We can add a <code>softmax layer</code> to do classification and use the cross entrophy to calculate the loss.</p>
<p><span class="math display">\[L_2(\mathcal{C})=\sum{x,y}logP(y|x^1,...,x^m)\]</span></p>
<p>In general, we should update the parameters to minimize the <span class="math inline">\(L_2\)</span>, but we can use <code>Multi-task Learning</code> to get a more generalize model. Therefore we can get the max likelihood of <span class="math inline">\(L3\)</span></p>
<p><span class="math display">\[L_3(\mathcal{C})=L_2(\mathcal{C})+\lambda \times L_1(\mathcal{C})\]</span></p>
<p><span class="math inline">\(L_1\)</span> if the loss of previous language model.</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="openai2.png" width = "70%" height="70%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
How to use a pre-trained OpenAI transformer to do sentence clasification
</div>
</center>
<p>The OpenAI paper outlines a number of input transformations to handle the inputs for different types of tasks. The following image from the paper shows the structures of the models and input transformations to carry out different tasks.</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="openai3.png" width = "70%" height="70%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
How to use a pre-trained OpenAI transformer to do different tasks
</div>
</center>
<h2 id="bert-from-decoders-to-encoders">BERT: From Decoders to Encoders</h2>
<p>The openAI transformer gave us a fine-tunable pre-trained model based on the Transformer. But something went missing in this transition from LSTMs to Transformers. ELMo’s language model was bi-directional, but the openAI transformer only trains a forward language model. Could we build a transformer-based model whose language model looks both forward and backwards (in the technical jargon – “is conditioned on both left and right context”)?</p>
<h3 id="input">Input</h3>
<p>The input representation of BERT is shown in the figure below. For example, the two sentences "my dog ​​is cute" and "he likes playing" are entered. I'll explain why two sentences are needed later. Here, the two sentences similar to GPT are used. First, a special Token <code>[CLS]</code> is added at the beginning of the first sentence, and a <code>[SEP]</code> is added after the cute to indicate the end of the first sentence. After ##ing, A <code>[SEP]</code> will be added later. Note that the word segmentation here will divide "playing" into "play" and "##ing" two tokens. This method of dividing words into more fine-grained Word Pieces was introduced in the previous machine translation section. This is a kind of Common methods to resolve unregistered words. Then perform 3 Embeddings on each Token: 1. Embedding of words; 2. Embedding of positions; 3. Embedding of segments.</p>
<p>The word Embedding is familiar to everyone, and the position Embedding is similar to the word embedding, mapping a position (such as 2) into a low-dimensional dense vector. And Segment embedding has only two, either belong to the first sentence (segment) or belong to the second sentence. Segment Embedding of the same sentence is shared so that it can learn information belonging to different segments. For tasks such as sentiment classification, there is only one sentence, so the Segment id is always 0; for the Entailment task, the input is two sentences, so the Segment is 0 or 1.</p>
<p>The BERT model requires a fixed sequence length, such as 128. If it is not enough, then padding in the back, otherwise it will intercept the excess Token, so as to ensure that the input is a fixed-length Token sequence. The first token is always special <code>[CLS]</code>. It does not have any semantics, so it will (must) encode the semantics of the entire sentence (other words).</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="bert.png" width = "70%" height="70%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
Input of Bert/div&gt;
</center>
<h3 id="masked-language-model">Masked Language Model</h3>
<p>Finding the right task to train a Transformer stack of encoders is a complex hurdle that BERT resolves by adopting a <code>masked language model</code> concept from earlier literature (where it’s called a Cloze task).</p>
<p>Beyond masking 15% of the input, BERT also mixes things a bit in order to improve how the model later fine-tunes. Sometimes it randomly replaces a word with another word and asks the model to predict the correct word in that position.</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="bert2.png" width = "70%" height="70%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
masked language model
</div>
</center>
<h3 id="two-sentence-tasks">Two-sentence Tasks</h3>
<p>If you look back up at the input transformations the OpenAI transformer does to handle different tasks, you’ll notice that some tasks require the model to say something intelligent about two sentences (e.g. are they simply paraphrased versions of each other? Given a wikipedia entry as input, and a question regarding that entry as another input, can we answer that question?).</p>
<p>To make BERT better at handling relationships between multiple sentences, the pre-training process includes an additional task: Given two sentences (A and B), is B likely to be the sentence that follows A, or not?</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="bert3.png" width = "70%" height="70%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
The second task BERT is pre-trained on is a two-sentence classification task.
</div>
</center>
<h3 id="task-specific-models">Task specific-Models</h3>
<p>The BERT paper shows a number of ways to use BERT for different tasks.</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="bert4.png" width = "70%" height="70%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
different ways to use BERT
</div>
</center>
<ol type="1">
<li>For common classification tasks, the input is a sequence, as shown in the upper right of the figure. All tokens belong to the same Segment (Id = 0). We use the last layer of the first special token <code>[CLS]</code> to connect it. Softmax is used for classification, and classified data is used for Fine-Tuning.</li>
<li>For tasks such as similarity calculation that are input as two sequences, the process is shown in the upper left. The tokens of the two sequences correspond to different segments (Id = 0/1). We also use the last layer output of the first special token [CLS] to connect with softmax for classification, and then use the classification data for Fine-Tuning.</li>
<li>The third type is a question-and-answer type question, such as the SQuAD v1.1 dataset. The input is a question and a long paragraph containing the answer (Paragraph), and the output finds the answer to the question in this paragraph.</li>
<li>The forth type of task is sequence labeling, such as named entity recognition. The input is a sentence (Token sequence). Except for [CLS] and [SEP], there will be output tags at each moment. For example, B-PER indicates the beginning of a person's name.</li>
</ol>
<h3 id="bert-for-feature-extraction">BERT for feature extraction</h3>
<p>The fine-tuning approach isn’t the only way to use BERT. Just like ELMo, you can use the pre-trained BERT to create contextualized word embeddings. Then you can feed these embeddings to your existing model – a process the paper shows yield results not far behind fine-tuning BERT on a task such as named-entity recognition.</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="bert5.png" width = "70%" height="70%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
Feature extraction
</div>
</center>
<p>Which vector works best as a contextualized embedding? I would think it depends on the task. The paper examines six choices (Compared to the fine-tuned model which achieved a score of 96.4):</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="bert6.png" width = "70%" height="70%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
Feature extraction
</div>
</center>
<h2 id="reference">Reference</h2>
<ol type="1">
<li>http://fancyerii.github.io/2019/03/09/bert-theory/#词汇扩展</li>
<li>https://zhuanlan.zhihu.com/p/63115885</li>
<li>http://jalammar.github.io/illustrated-bert/</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Transformer/2019/12/20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Transformer/2019/12/20/" class="post-title-link" itemprop="url">Transformer</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-20 12:24:40" itemprop="dateCreated datePublished" datetime="2019-12-20T12:24:40+08:00">2019-12-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-22 02:20:04" itemprop="dateModified" datetime="2019-12-22T02:20:04+08:00">2019-12-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Transformer/2019/12/20/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Transformer/2019/12/20/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="background">Background</h2>
<p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention.</p>
<p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations. End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks.</p>
<p>To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution.</p>
<h2 id="embeddings-and-softmax">Embeddings and Softmax</h2>
<p>Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension <span class="math inline">\(d_{\text{model}}\)</span>. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.05859">(cite)</a>. In the embedding layers, we multiply those weights by <span class="math inline">\(\sqrt{d_{\text{model}}}\)</span>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, vocab</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure>
<h2 id="positional-encoding">Positional Encoding</h2>
<p>Since our model contains no recurrence and no convolution, <strong>in order for the model to make use of the order of the sequence</strong>, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension <span class="math inline">\(d_{\text{model}}\)</span> as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1705.03122.pdf">(cite)</a>.</p>
<p>In this work, we use sine and cosine functions of different frequencies:</p>
<p><span class="math display">\[PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\text{model}}})\]</span></p>
<p><span class="math display">\[PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\text{model}}})\]</span></p>
<p>where <span class="math inline">\(pos\)</span> is the position and <span class="math inline">\(i\)</span> is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from <span class="math inline">\(2\pi\)</span> to <span class="math inline">\(10000 \cdot 2\pi\)</span>. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset <span class="math inline">\(k\)</span>, <span class="math inline">\(PE_{pos+k}\)</span> can be represented as a linear function of <span class="math inline">\(PE_{pos}\)</span>.</p>
<p>In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of <span class="math inline">\(P_{drop}=0.1\)</span>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Implement the PE function.&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<h2 id="layer-norm-and-residual-connection">Layer Norm and residual connection</h2>
<p>We employ a residual connection <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.03385">(cite)</a> around each of the two sub-layers, followed by layer normalization <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">(cite)</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Construct a layernorm module (See citation for details).&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure>
<p>That is, the output of each sub-layer is <span class="math inline">\(\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))\)</span>, where <span class="math inline">\(\mathrm{Sublayer}(x)\)</span> is the function implemented by the sub-layer itself. We apply dropout <a target="_blank" rel="noopener" href="http://jmlr.org/papers/v15/srivastava14a.html">(cite)</a> to the output of each sub-layer, before it is added to the sub-layer input and normalized.</p>
<p>To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension <span class="math inline">\(d_{\text{model}}=512\)</span>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, sublayer</span>):</span></span><br><span class="line">        <span class="string">&quot;Apply residual connection to any sublayer with the same size.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure>
<p>Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.</p>
<h2 id="attention">Attention</h2>
<h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3>
<p>An attention function can be described as mapping a <code>query</code> and a set of <code>key</code>-<code>value</code> pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p>
<p>We call our particular attention <code>Scaled Dot-Product Attention</code>. The input consists of queries and keys of dimension <span class="math inline">\(d_k\)</span>, and values of dimension <span class="math inline">\(d_v\)</span>. We compute the dot products of the query with all keys, divide each by <span class="math inline">\(\sqrt{d_k}\)</span>, and apply a softmax function to obtain the weights on the values.</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="ModalNet-19.png"  width = "20%" height="20%">
</center>
<p>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix <span class="math inline">\(Q\)</span>. The keys and values are also packed together into matrices <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span>. We compute the matrix of outputs as:</p>
<p><span class="math display">\[\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V\]</span></p>
<p>The two most commonly used attention functions are additive attention <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">(cite)</a>, and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of <span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.</p>
<p>While for small values of <span class="math inline">\(d_k\)</span> the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of <span class="math inline">\(d_k\)</span> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.03906">(cite)</a>. We suspect that for large values of <span class="math inline">\(d_k\)</span>, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients (To illustrate why the dot products get large, assume that the components of <span class="math inline">\(q\)</span> and <span class="math inline">\(k\)</span> are independent random variables with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>. Then their dot product, <span class="math inline">\(q \cdot k = \sum_{i=1}^{d_k} q_ik_i\)</span>, has mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(d_k\)</span>.). To counteract this effect, we scale the dot products by <span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>.</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="attention.png" width = "50%" height="50%">
</center>
<h3 id="multi-head-attention">Multi-head attention</h3>
<p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="ModalNet-20.png" width = "50%" height="50%">
</center>
<p><span class="math display">\[    
\mathrm{MultiHead}(Q, K, V) = \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O    \\                                           
    \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)                                
\]</span></p>
<p>Where the projections are parameter matrices <span class="math inline">\(W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>, <span class="math inline">\(W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}\)</span>, <span class="math inline">\(W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}\)</span> and <span class="math inline">\(W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}\)</span>. In this work we employ <span class="math inline">\(h=8\)</span> parallel attention layers, or heads. For each of these we use <span class="math inline">\(d_k=d_v=d_{\text{model}}/h=64\)</span>. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;Take in model size and number of heads.&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;Implements Figure 2&quot;</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, -<span class="number">1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure>
<h3 id="applications-of-attention-in-our-model">Applications of Attention in our Model</h3>
<p>The Transformer uses multi-head attention in three different ways:<br />
1. In <strong>encoder-decoder attention</strong> layers, the queries come from the previous decoder layer, and the memory keys and values come from the <code>output of the encoder</code>. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.08144">(cite)</a>.</p>
<ol start="2" type="1">
<li>The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.<br />
</li>
<li>Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to <span class="math inline">\(-\infty\)</span>) all values in the input of the softmax which correspond to illegal connections.</li>
</ol>
<h2 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</h2>
<p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.</p>
<p><span class="math display">\[\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2\]</span></p>
<p>While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is <span class="math inline">\(d_{\text{model}}=512\)</span>, and the inner-layer has dimensionality <span class="math inline">\(d_{ff}=2048\)</span>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Implements FFN equation.&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure>
<h2 id="model-architecture">Model Architecture</h2>
<p>Most competitive neural sequence transduction models have an encoder-decoder structure <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">(cite)</a>. Here, the encoder maps an input sequence of symbol representations <span class="math inline">\((x_1, ..., x_n)\)</span> to a sequence of continuous representations <span class="math inline">\(\mathbf{z} = (z_1, ..., z_n)\)</span>. Given <span class="math inline">\(\mathbf{z}\)</span>, the decoder then generates an output sequence <span class="math inline">\((y_1,...,y_m)\)</span> of symbols one element at a time. At each step the model is auto-regressive <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1308.0850">(cite)</a>, consuming the previously generated symbols as additional input when generating the next.</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="ModalNet-21.png" width = "50%" height="50%">
</center>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many </span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder, decoder, src_embed, tgt_embed, generator</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="string">&quot;Take in and process masked src and target sequences.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, src, src_mask</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Define standard linear + softmax generation step.&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, vocab</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks</h2>
<h3 id="encoder">Encoder</h3>
<p>The encoder is composed of a stack of <span class="math inline">\(N=6\)</span> identical layers.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span>(<span class="params">module, N</span>):</span></span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Core encoder is a stack of N layers&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer, N</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        <span class="string">&quot;Pass the input (and mask) through each layer in turn.&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Encoder is made up of self-attn and feed forward (defined below)&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (left) for connections.&quot;</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>
<h3 id="decoder">Decoder</h3>
<p>The decoder is also composed of a stack of <span class="math inline">\(N=6\)</span> identical layers.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Generic N layer decoder with masking.&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer, N</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<p>In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Decoder is made of self-attn, src-attn, and feed forward (defined below)&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (right) for connections.&quot;</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure></p>
<p>We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position <span class="math inline">\(i\)</span> can depend only on the known outputs at positions less than <span class="math inline">\(i\)</span>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span>(<span class="params">size</span>):</span></span><br><span class="line">    <span class="string">&quot;Mask out subsequent positions.&quot;</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h2 id="full-model">Full Model</h2>
<p>Here we define a function from hyperparameters to a full model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span>(<span class="params">src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="params"><span class="function">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;Helper: Construct a model from hyperparameters.&quot;</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h2 id="reference">Reference</h2>
<ol type="1">
<li>http://nlp.seas.harvard.edu/2018/04/03/attention.html</li>
<li>https://jalammar.github.io/illustrated-transformer/</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Subword-Models/2019/12/19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Subword-Models/2019/12/19/" class="post-title-link" itemprop="url">Subword Models</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-19 13:07:57" itemprop="dateCreated datePublished" datetime="2019-12-19T13:07:57+08:00">2019-12-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-20 05:04:03" itemprop="dateModified" datetime="2019-12-20T05:04:03+08:00">2019-12-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Subword-Models/2019/12/19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Subword-Models/2019/12/19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="character-level-models">Character-Level Models</h2>
<ol type="1">
<li>Word embeddings can be composed from character embeddings
<ul>
<li>Generates embeddings for unknown words</li>
<li>Similar spellings share similar embeddings</li>
<li>Solves OOV problem</li>
</ul></li>
<li>Motivation
<ul>
<li>Derive a powerful,robust language model effective across a variety of languages.</li>
<li>Encode subword relatedness:eventful,eventfully, uneventful...</li>
<li>Address rare-word problem of prior models.</li>
<li>Obtain comparable expressivity with fewer parameters.</li>
</ul></li>
</ol>
<h2 id="two-trends">Two trends</h2>
<ol type="1">
<li>Same architecture as forword-level model but use smaller units: “word pieces”</li>
<li>Hybrid architectures: Main model has words; something else for characters</li>
</ol>
<h2 id="byte-pair-encoding">Byte Pair Encoding</h2>
<p>A word segmentation algorithm: - Start with a vocabulary of characters - Most frequent ngram pairs -&gt; a new ngram - Have a target vocabulary size and stop when you reach it - Do deterministic longest piece segmentation of words - Segmentation is only within words identified by some prior tokenizer</p>
<p>For example, all the words in our documents database and their frequency are &gt; {'l o w': 5, 'l o w e r': 2, 'n e w e s t': 6, 'w i d e s t': 3}</p>
<p>We can initialize our vocabulary library as:<br />
&gt; { 'l', 'o', 'w', 'e', 'r', 'n', 'w', 's', 't', 'i', 'd'}</p>
<p>The most frequent ngram pair is ('e','s') and its count is 9. So we add the 'es' to our vocabulary library.</p>
<p>Our documents database now is: &gt; {'l o w': 5, 'l o w e r': 2, 'n e w es t': 6, 'w i d es t': 3}.</p>
<p>Our vocabulary library now is: &gt; { 'l', 'o', 'w', 'e', 'r', 'n', 'w', 's', 't', 'i', 'd', 'es'}</p>
<p><strong>Again</strong>, the most frequent ngram pair is ('es','t') and its count is 9，So we add the 'est' to our vocabulary library.</p>
<p>Our documents database now is: &gt; {'l o w': 5, 'l o w e r': 2, 'n e w est': 6, 'w i d est': 3}</p>
<p>Our vocabulary library now is: &gt; { 'l', 'o', 'w', 'e', 'r', 'n', 'w', 's', 't', 'i', 'd', 'es','est'}</p>
<p>the rest can be done in the same manner. We can set a threshold of total count of our vocabulary library. By doing so, we can use BPE to construct a vocabulary library to represent all the words based on subword unit.</p>
<p>Google NMT(GNMT) uses a variant of this: - V1: wordpiece model (Word piece model tokenizes inside words) - V2: sentencepiece model (Sentence piece model works from raw text)</p>
<h2 id="character-level-to-build-word-level">Character-level to build word-level</h2>
<ol type="1">
<li>Convolution over characters to generate word embeddings
<center>
<img style="border-radius: 0.3125em;
 box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
 src="1.png" width = "50%" height="50%">
</center></li>
<li>Character-based LSTM to build word representation
<center>
<img style="border-radius: 0.3125em;
 box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
 src="2.png" width = "50%" height="50%">
</center></li>
</ol>
<h2 id="cs224n-assignment5">CS224n Assignment5</h2>
<h3 id="character-based-convolutional-encoder-for-nmt.">Character-based convolutional encoder for NMT.</h3>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="3.png" width = "80%" height="80%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
Figure from cs224n. Character-based convolutional encoder, which ultimately produces a word embedding of length <span class="math inline">\(e_{word}\)</span>
</div>
</center>
<ol type="1">
<li>Convert word to character indices. We have a word <span class="math inline">\(x\)</span> (e.g. Anarchy in above figure) that we wish to represent. Assume we have a predefined ‘vocabulary’ of characters (for example, all lowercase letters, uppercase letters, numbers, and some punctuation). By looking up the index of each character, we can thus represent the length-l word x as a vector of integers: <span class="math display">\[x = \left[ c_1,c_2,\cdots,c_l  \right ] \in \mathbb{Z}^{l}\]</span> where each <span class="math inline">\(c_i\)</span> is an integer index into the character vocabulary.</li>
<li>Padding and embedding lookup. Using a special <PAD> ‘character’, we pad (or truncate) every word so that it has length <span class="math inline">\(m_word\)</span> (this is some predefined hyperparameter representing maximum word length): <span class="math display">\[x_{padded} = \left [ c_1,c_2,\cdots,c_{m_{word}}  \right ] \in \mathbb{Z}^{m_{word}}\]</span> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_sents_char</span>(<span class="params">sents, char_pad_token</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Pad list of sentences according to the longest sentence in the batch and max_word_length.</span></span><br><span class="line"><span class="string">    @param sents (list[list[list[int]]]): list of sentences, result of `words2charindices()`</span></span><br><span class="line"><span class="string">        from `vocab.py`</span></span><br><span class="line"><span class="string">    @param char_pad_token (int): index of the character-padding token</span></span><br><span class="line"><span class="string">    @returns sents_padded (list[list[list[int]]]): list of sentences where sentences/words shorter</span></span><br><span class="line"><span class="string">        than the max length sentence/word are padded out with the appropriate pad token, such that</span></span><br><span class="line"><span class="string">        each sentence in the batch now has same number of words and each word has an equal</span></span><br><span class="line"><span class="string">        number of characters</span></span><br><span class="line"><span class="string">        Output shape: (batch_size, max_sentence_length, max_word_length)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Words longer than 21 characters should be truncated</span></span><br><span class="line">    max_word_length = <span class="number">21</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE for part 1f</span></span><br><span class="line">    <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">    <span class="comment">###     Perform necessary padding to the sentences in the batch similar to the pad_sents()</span></span><br><span class="line">    <span class="comment">###     method below using the padding character from the arguments. You should ensure all</span></span><br><span class="line">    <span class="comment">###     sentences have the same number of words and each word has the same number of</span></span><br><span class="line">    <span class="comment">###     characters.</span></span><br><span class="line">    <span class="comment">###     Set padding words to a `max_word_length` sized vector of padding characters.</span></span><br><span class="line">    <span class="comment">###</span></span><br><span class="line">    <span class="comment">###     You should NOT use the method `pad_sents()` below because of the way it handles</span></span><br><span class="line">    <span class="comment">###     padding and unknown words.</span></span><br><span class="line">    sents_padded = []</span><br><span class="line">    max_sent_len = <span class="built_in">max</span>([<span class="built_in">len</span>(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> sents])</span><br><span class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> sents:</span><br><span class="line">        sent = sent + [[] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_sent_len - <span class="built_in">len</span>(sent))]</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(sent) == max_sent_len</span><br><span class="line">        tmp_sent = []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sent:</span><br><span class="line">            word = word[:max_word_length]</span><br><span class="line">            diff = max_word_length - <span class="built_in">len</span>(word)</span><br><span class="line">            word = word + [char_pad_token] * diff</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">len</span>(word) == max_word_length</span><br><span class="line">            tmp_sent.append(word)</span><br><span class="line">        sents_padded.append(tmp_sent)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sents_padded</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VocabEntry</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">words2charindices</span>(<span class="params">self, sents</span>):</span></span><br><span class="line">            <span class="string">&quot;&quot;&quot; Convert list of sentences of words into list of list of list of character indices.</span></span><br><span class="line"><span class="string">            @param sents (list[list[str]]): sentence(s) in words</span></span><br><span class="line"><span class="string">            @return word_ids (list[list[list[int]]]): sentence(s) in indices</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            <span class="comment">### YOUR CODE HERE for part 1e</span></span><br><span class="line">            <span class="comment">### <span class="doctag">TODO:</span> </span></span><br><span class="line">            <span class="comment">###     This method should convert characters in the input sentences into their </span></span><br><span class="line">            <span class="comment">###     corresponding character indices using the character vocabulary char2id </span></span><br><span class="line">            <span class="comment">###     defined above.</span></span><br><span class="line">            <span class="comment">###</span></span><br><span class="line">            <span class="comment">###     You must prepend each word with the `start_of_word` character and append </span></span><br><span class="line">            <span class="comment">###     with the `end_of_word` character. </span></span><br><span class="line"></span><br><span class="line">            word_ids = []</span><br><span class="line">            <span class="keyword">for</span> sent <span class="keyword">in</span> sents:</span><br><span class="line">                sent_chars_id = []</span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> sent:</span><br><span class="line">                    sent_chars_id.append([<span class="number">1</span>] + [ self.char2id.get(char,<span class="number">3</span>) <span class="keyword">for</span> char <span class="keyword">in</span> word ] + [<span class="number">2</span>])</span><br><span class="line">                word_ids.append(sent_chars_id)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> word_ids</span><br><span class="line"></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">to_input_tensor_char</span>(<span class="params">self, sents: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]], device: torch.device</span>) -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Convert list of sentences (words) into tensor with necessary padding for </span></span><br><span class="line"><span class="string">        shorter sentences.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param sents (List[List[str]]): list of sentences (words)</span></span><br><span class="line"><span class="string">        @param device: device on which to load the tensor, i.e. CPU or GPU</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns sents_var: tensor of (max_sentence_length, batch_size, max_word_length)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 1g</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span> </span></span><br><span class="line">        <span class="comment">###     Connect `words2charindices()` and `pad_sents_char()` which you&#x27;ve defined in </span></span><br><span class="line">        <span class="comment">###     previous parts</span></span><br><span class="line">        char_sents =  self.words2charindices(sents)</span><br><span class="line">        padded_char_sents = pad_sents_char(char_sents, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        sents_var = torch.tensor(padded_char_sents, dtype=torch.int8, device= device)</span><br><span class="line">        sents_var = sents_var.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sents_var</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ......</span></span><br></pre></td></tr></table></figure></li>
<li>For each of these characters <span class="math inline">\(c_i\)</span>, we lookup a dense character embedding (which has shape <span class="math inline">\(e_{char}\)</span>). This yields a tensor <span class="math inline">\(x_{emb}\)</span>: <span class="math display">\[x_{emb} = CharEmbedding(X_{padded}) \in \mathbb{R}^{m_{word} \times e_{char}}\]</span> We'll reshape <span class="math inline">\(x_{emb}\)</span> to obtain <span class="math inline">\(x_{reshaped} in \mathbb{R}^{e_{char} \times m_{word}}\)</span> before feeding into the convolutional network.</li>
<li><strong>Convolutional network</strong>. To combine these character embeddings, we’ll use 1-dimensional convolutions. The convolutional layer has two hyperparameters: the kernel size <span class="math inline">\(k\)</span> (also called window size), which dictates the size of the window used to compute features, and the number of filters <span class="math inline">\(f\)</span>, (also called number of output features or number of output channels). The convolutional layer has a weight matrix <span class="math inline">\(W \in \mathbb{R}^{f \times e_{char} \times k}\)</span> and a bias vector <span class="math inline">\(b \in \mathbb{R}^{f}\)</span>. Overall this produces output <span class="math inline">\(x_{conv}\)</span>. <span class="math display">\[x_{conv} = Conv1D(x_{reshaped}) \in \mathbb{R}^{f \times {m_{word} - k + 1}}\]</span> For our application, we’ll set <span class="math inline">\(f\)</span> to be equal to <span class="math inline">\(e_{word}\)</span>, the size of the final word embedding for word x. Therefore, <span class="math display">\[x_{conv} \in \mathbb{R}^{e_{word} \times (m_{word} - k + 1)}\]</span> Finally, we apply the <code>ReLU</code> function to <span class="math inline">\(x_{conv}\)</span>, then use max-pooling to reduce this to a single vector <span class="math inline">\(x_{conv_out} \in \mathbb{R}^{e_{word}}\)</span>, which is the final output of the Convolutional Network: <span class="math display">\[x_{conv\_out} = MaxPool(ReLU(x_{conv})) \in \mathbb{R}^{e_{word}}\]</span> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 5</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### YOUR CODE HERE for part 1i</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">        Init CNN which is a 1-D cnn.</span></span><br><span class="line"><span class="string">        @param embed_size (int): embedding size of char (dimensionality)</span></span><br><span class="line"><span class="string">        @param k: kernel size, also called window size</span></span><br><span class="line"><span class="string">        @param f: number of filters, should be embed_size of word</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">            embed_size: <span class="built_in">int</span> = <span class="number">50</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 m_word: <span class="built_in">int</span> = <span class="number">21</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 k: <span class="built_in">int</span> = <span class="number">5</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 f: <span class="built_in">int</span> = <span class="number">50</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">        Init CNN which is a 1-D cnn.</span></span><br><span class="line"><span class="string">        @param embed_size (int): embedding size of char (dimensionality)</span></span><br><span class="line"><span class="string">        @param k: kernel size, also called window size</span></span><br><span class="line"><span class="string">        @param f: number of filters, should be embed_size of word</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.conv1d = nn.Conv1d(in_channels = embed_size, </span><br><span class="line">                    out_channels = f,</span><br><span class="line">                    kernel_size = k)</span><br><span class="line"></span><br><span class="line">        self.maxpool = nn.MaxPool1d(kernel_size = m_word - k + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X_reshaped: torch.Tensor</span>) -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        map from X_reshaped to X_conv_out</span></span><br><span class="line"><span class="string">        @param X_reshaped (Tensor): Tensor of char-level embedding with shape ( </span></span><br><span class="line"><span class="string">                                    batch_size, e_char, m_word), where e_char = embed_size of char, </span></span><br><span class="line"><span class="string">                                    m_word = max_word_length.</span></span><br><span class="line"><span class="string">        @return X_conv_out (Tensor): Tensor of word-level embedding with shape (max_sentence_length,</span></span><br><span class="line"><span class="string">                                    batch_size)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        X_conv = self.conv1d(X_reshaped)</span><br><span class="line">        <span class="comment"># print(X_conv.size())</span></span><br><span class="line">        X_conv_out = self.maxpool(F.relu(X_conv))</span><br><span class="line">        <span class="comment"># print(X_conv_out.size())</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X_conv_out.squeeze(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li>
<li><strong>Highway layer and dropout</strong>. Highway Networks6 have a skip-connection controlled by a dynamic gate. Given the input <span class="math inline">\(x_{conv\\_out} \in \mathbb{R}^{e_{word}}\)</span>, we compute:
<center>
<img style="border-radius: 0.3125em;
 box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
 src="4.png" width = "70%" height="70%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
 display: inline-block;
 color: #999;
 padding: 2px;">
Figure from cs224n. Highway Network (Srivastava et al. 2015)
</div>
</center>
<span class="math display">\[
\begin{align}
&amp; x_{proj} = RELU(W_{proj}x_{conv\_cout} + b_{proj}) \quad \in \mathbb{R}^{e_{word}} \\
&amp; x_{gate} = \sigma(W_{gate}x_{conv\_out} + b_{gate}) \quad \in \mathbb{R}^{e_{word}} \\
&amp; x_{highway} = x_{gate} \circ x_{proj} + ( 1 - x_{gate}) \circ x_{conv\_out}  \quad \in \mathbb{R}^{e_{word}}\\
&amp; x_{word_emb} = Dropout(x_{highway}) \quad \in \mathbb{R}^{e_{word}} 
\end{align}
\]</span> Where <span class="math inline">\(W_{proj},W_{gate} \in \mathbb{R}^{e_{word} \times e_{word}}\)</span>, and <span class="math inline">\(\circ\)</span> denotes element-wise multiplication. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 5</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment">### YOUR CODE HERE for part 1h</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Highway</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Highway network for ConvNN</span></span><br><span class="line"><span class="string">        - Relu</span></span><br><span class="line"><span class="string">        - Sigmoid</span></span><br><span class="line"><span class="string">        - gating mechanism from LSTM</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,embed_size</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Init Higway network</span></span><br><span class="line"><span class="string">            @param embed_size (int): Embedding size of word, in handout, </span></span><br><span class="line"><span class="string">                                     it&#x27;s e_&#123;word&#125; (dimensionality)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(Highway, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.projection = nn.Linear(embed_size,embed_size,bias = <span class="literal">True</span>)</span><br><span class="line">        self.gate = nn.Linear(embed_size,embed_size, bias = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X_conv_out</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            Take mini-batch of sentence of ConvNN</span></span><br><span class="line"><span class="string">            @param X_conv_out (Tensor): Tensor with shape (max_sentence_length, batch_size, embed_size)</span></span><br><span class="line"><span class="string">            @return X_highway (Tensor): combinded output with shape (max_sentence_length, batch_size, embed_size)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        X_proj = F.relu(self.projection(X_conv_out))</span><br><span class="line">        X_gate = torch.sigmoid(self.gate(X_conv_out))</span><br><span class="line">        X_highway =  torch.mul(X_gate, X_proj) + torch.mul((<span class="number">1</span> - X_gate),X_conv_out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X_highway</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### END YOUR CODE </span></span><br></pre></td></tr></table></figure></li>
<li>Combine above steps together to get our <strong>Character-based word embedding model</strong>. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 5</span></span><br><span class="line"><span class="string">model_embeddings.py: Embeddings for the NMT model</span></span><br><span class="line"><span class="string">Pencheng Yin &lt;pcyin@cs.cmu.edu&gt;</span></span><br><span class="line"><span class="string">Sahil Chopra &lt;schopra8@stanford.edu&gt;</span></span><br><span class="line"><span class="string">Anand Dhoot &lt;anandd@stanford.edu&gt;</span></span><br><span class="line"><span class="string">Michael Hahn &lt;mhahn2@stanford.edu&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not change these imports; your module names should be</span></span><br><span class="line"><span class="comment">#   `CNN` in the file `cnn.py`</span></span><br><span class="line"><span class="comment">#   `Highway` in the file `highway.py`</span></span><br><span class="line"><span class="comment"># Uncomment the following two imports once you&#x27;re ready to run part 1(j)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> cnn <span class="keyword">import</span> CNN</span><br><span class="line"><span class="keyword">from</span> highway <span class="keyword">import</span> Highway</span><br><span class="line"></span><br><span class="line"><span class="comment"># End &quot;do not change&quot; </span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelEmbeddings</span>(<span class="params">nn.Module</span>):</span> </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Class that converts input words to their CNN-based embeddings.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_size, vocab</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Init the Embedding layer for one language</span></span><br><span class="line"><span class="string">        @param embed_size (int): Embedding size (dimensionality) for the output </span></span><br><span class="line"><span class="string">        @param vocab (VocabEntry): VocabEntry object. See vocab.py for documentation.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(ModelEmbeddings, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment">## A4 code</span></span><br><span class="line">        <span class="comment"># pad_token_idx = vocab.src[&#x27;&lt;pad&gt;&#x27;]</span></span><br><span class="line">        <span class="comment"># self.embeddings = nn.Embedding(len(vocab.src), embed_size, padding_idx=pad_token_idx)</span></span><br><span class="line">        <span class="comment">## End A4 code</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 1j</span></span><br><span class="line">        pad_token_idx = vocab.char2id[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        char_embed_size = <span class="number">50</span></span><br><span class="line">        self.char_embedding = nn.Embedding(<span class="built_in">len</span>(vocab.char2id),</span><br><span class="line">                                           char_embed_size,</span><br><span class="line">                                           pad_token_idx)</span><br><span class="line">        self.convNN = CNN(f=self.embed_size)</span><br><span class="line">        self.highway = Highway(embed_size=self.embed_size)</span><br><span class="line">        self.dropout = nn.Dropout(p=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Looks up character-based CNN embeddings for the words in a batch of sentences.</span></span><br><span class="line"><span class="string">        @param input: Tensor of integers of shape (sentence_length, batch_size, max_word_length) where</span></span><br><span class="line"><span class="string">            each integer is an index into the character vocabulary</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param output: Tensor of shape (sentence_length, batch_size, embed_size), containing the </span></span><br><span class="line"><span class="string">            CNN-based embeddings for each word of the sentences in the batch</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">## A4 code</span></span><br><span class="line">        <span class="comment"># output = self.embeddings(input)</span></span><br><span class="line">        <span class="comment"># return output</span></span><br><span class="line">        <span class="comment">## End A4 code</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 1j</span></span><br><span class="line">        X_word_emb_list = []</span><br><span class="line">        <span class="keyword">for</span> X_padded <span class="keyword">in</span> <span class="built_in">input</span>:</span><br><span class="line">            <span class="comment"># (batch_size,max_word_length) -&gt; (batch_size,max_word_length,embed_size)</span></span><br><span class="line">            X_emb = self.char_embedding(X_padded)</span><br><span class="line">            <span class="comment"># print(X_emb.size())</span></span><br><span class="line">            X_reshaped = X_emb.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">            X_conv_out = self.convNN(X_reshaped)</span><br><span class="line">            X_highway = self.highway(X_conv_out)</span><br><span class="line">            X_word_emb = self.dropout(X_highway)</span><br><span class="line">            X_word_emb_list.append(X_word_emb)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (sentence_length, batch_size, embed_size)</span></span><br><span class="line">        X_word_emb = torch.stack(X_word_emb_list, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X_word_emb</span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="character-based-lstm-decoder-for-nmt">Character-based LSTM decoder for NMT</h3>
We will now add a LSTM-based character-level decoder to our NMT system. The main idea is that when our word-level decoder produces and <code>&lt;UNK&gt;</code> token, we run our character-level decoder (which you can think of as a character-level conditional language model) to instead generate the target word one character at a time, as shown in below figure. This will help us to produce rare and out-of-vocabulary target words.
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="5.png" width = "50%" height="50%">
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
Figure from cs224n. A character-based decoder which is triggered if the word-based decoder produces an UNK. Figure courtesy of Luong &amp; Manning.
</div>
</center>
<p>We now describe the model in three sections: 1. <strong>Forward computation of Character Decoder</strong>: Given a sequence of integers <span class="math inline">\(x_i,\cdots,x_n \in \mathbb{Z}\)</span> representing a sequence of characters, we lookup their character embeddings <span class="math inline">\(x_i,\cdots,x_n \in \mathbb{Z}^{e_{char}}\)</span> and pass these as input in to the(unidirectional)LSTM,obtaining hidden states <span class="math inline">\(h1, \cdots, h_n\)</span> and cell states <span class="math inline">\(c_1, \cdots, c_n\)</span> <span class="math display">\[h_t,c_t = CharDecoderLSTM(x_t,h_{t-1},c_{t-1}) \quad \text{where} \quad h_t,c_t \in \mathbb{R}^{h}\]</span> where h is the hidden size of the CharDecoderLSTM. The initial hidden and cell states <span class="math inline">\(h_0\)</span> and <span class="math inline">\(c_0\)</span> are both set to the <strong>combined output</strong> vector (attentioned) for the current timestep of the main word-level NMT decoder. For every timestep <span class="math inline">\(t \in { 1, \cdots, n }\)</span> we compute scores (also called logits) <span class="math inline">\(s_t \in \mathbb{R}^{V_{char}}\)</span> <span class="math display">\[s_t = W_{dec}h_t + b_{dec} \in \mathbb{R}^{V_{char}}\]</span> Where the weight matrix <span class="math inline">\(W_{dec} \in \mathbb{R}^{V_{char} \times h}\)</span> and the bias vector <span class="math inline">\(b_{dec} \in \mathbb{R}^{V_{char}}\)</span>. If we passed <span class="math inline">\(s_t\)</span> through a softmax function, we would have the probability distribution for the next character in the sequence.</p>
<ol start="2" type="1">
<li><p><strong>Training of Character Decoder</strong> When we train the NMT system, we train the character decoder on <strong>every word</strong> in the target sentence (not just the words represented by <UNK>). For example, on a particular step of the main NMT decoder, if the target word is music then the input sequence for the CharDecoderLSTM is <span class="math inline">\([x_1,...,x_n]\)</span> = [<START>,m,u,s,i,c] and the target sequence for the CharDecoderLSTM is <span class="math inline">\([x_{2}, . . . , x_{n+1}]\)</span> = [m,u,s,i,c,<END>]. We pass the input sequence <span class="math inline">\(x_1, \cdots, x_n\)</span>, along with the initial states <span class="math inline">\(h_0\)</span> and <span class="math inline">\(c_0\)</span> obtained from the combined output vector) into the CharDecoderLSTM, thus obtaining scores <span class="math inline">\(s_1,\cdots, s_n\)</span> which we will compare to the target sequence <span class="math inline">\(x_2,\cdots, x_{n+1}\)</span>. We optimize with respect to sum of the cross-entropy loss: <span class="math display">\[p_t = softmax(s_t) \in \mathbb{R}^{V_{char}}\]</span> <span class="math display">\[loss_{char\_dec} = -\sum_{t=1}^{n}log p_t(x_{t+1})\]</span></p></li>
<li><p><strong>Decoding from the Character Decoder</strong> t test time, first we produce a translation from our word- based NMT system in the usual way (e.g. a decoding algorithm like beam search). If the translation contains any <UNK> tokens, then for each of those positions, we use the word-based decoder’s combined output vector to initialize the CharDecoderLSTM initial <span class="math inline">\(h_0\)</span> and <span class="math inline">\(c_0\)</span>, then use CharDecoderLSTM to generate a sequence of characters. To generate the sequence of characters, we use the greedy decoding algorithm, which repeatedly chooses the most probable next character, until either the <END> token is produced or we reach a predetermined max length. The algorithm is given below, for a single example (not batched).</p>
<center>
<p><img style="border-radius: 0.3125em;
 box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
 src="6.png" width = "100%" height="100%"></p>
<div style="color:orange; border-bottom: 1px solid #d9d9d9;
 display: inline-block;
 color: #999;
 padding: 2px;">
Figure from cs224n. Greedy Decoding
</div>
</center></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 5</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CharDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_size, char_embedding_size=<span class="number">50</span>, target_vocab=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Init Character Decoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param hidden_size (int): Hidden size of the decoder LSTM</span></span><br><span class="line"><span class="string">        @param char_embedding_size (int): dimensionality of character embeddings</span></span><br><span class="line"><span class="string">        @param target_vocab (VocabEntry): vocabulary for the target language. See vocab.py for documentation.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 2a</span></span><br><span class="line">        <span class="comment">### TODO - Initialize as an nn.Module.</span></span><br><span class="line">        <span class="comment">###      - Initialize the following variables:</span></span><br><span class="line">        <span class="comment">###        self.charDecoder: LSTM. Please use nn.LSTM() to construct this.</span></span><br><span class="line">        <span class="comment">###        self.char_output_projection: Linear layer, called W_&#123;dec&#125; and b_&#123;dec&#125; in the PDF</span></span><br><span class="line">        <span class="comment">###        self.decoderCharEmb: Embedding matrix of character embeddings</span></span><br><span class="line">        <span class="comment">###        self.target_vocab: vocabulary for the target language</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Hint: - Use target_vocab.char2id to access the character vocabulary for the target language.</span></span><br><span class="line">        <span class="comment">###       - Set the padding_idx argument of the embedding matrix.</span></span><br><span class="line">        <span class="comment">###       - Create a new Embedding layer. Do not reuse embeddings created in Part 1 of this assignment.</span></span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>(CharDecoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.char_embedding_size = char_embedding_size</span><br><span class="line">        self.target_vocab = target_vocab</span><br><span class="line">        self.padding_idx = self.target_vocab.char2id[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]</span><br><span class="line">        self.decoderCharEmb = nn.Embedding(<span class="built_in">len</span>(self.target_vocab.char2id),</span><br><span class="line">                                           char_embedding_size,</span><br><span class="line">                                           self.padding_idx)</span><br><span class="line">        self.charDecoder = nn.LSTM(input_size=char_embedding_size,</span><br><span class="line">                                   hidden_size=hidden_size)</span><br><span class="line">        self.char_output_projection = nn.Linear(hidden_size,</span><br><span class="line">                                                <span class="built_in">len</span>(self.target_vocab.char2id))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, dec_hidden=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Forward pass of character decoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param input: tensor of integers, shape (length, batch)</span></span><br><span class="line"><span class="string">        @param dec_hidden: internal state of the LSTM before reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns scores: called s_t in the PDF, shape (length, batch, self.vocab_size)</span></span><br><span class="line"><span class="string">        @returns dec_hidden: internal state of the LSTM after reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 2b</span></span><br><span class="line">        <span class="comment">### TODO - Implement the forward pass of the character decoder.</span></span><br><span class="line">        char_embeddings = self.decoderCharEmb(<span class="built_in">input</span>)        <span class="comment"># (length, batch, char_embed_size)</span></span><br><span class="line">        hidden_states, dec_hidden = self.charDecoder(</span><br><span class="line">            char_embeddings, dec_hidden)    <span class="comment"># (length, batch, hidden_size)</span></span><br><span class="line">        scores = self.char_output_projection(hidden_states)     <span class="comment"># (len, batch, vocab)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> scores, dec_hidden</span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_forward</span>(<span class="params">self, char_sequence, dec_hidden=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Forward computation during training.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param char_sequence: tensor of integers, shape (length, batch). Note that &quot;length&quot; here and in forward() need not be the same.</span></span><br><span class="line"><span class="string">        @param dec_hidden: initial internal state of the LSTM, obtained from the output of the word-level decoder. A tuple of two tensors of shape (1, batch, hidden_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns The cross-entropy loss, computed as the *sum* of cross-entropy losses of all the words in the batch.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 2c</span></span><br><span class="line">        <span class="comment">### TODO - Implement training forward pass.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Hint: - Make sure padding characters do not contribute to the cross-entropy loss.</span></span><br><span class="line">        <span class="comment">###       - char_sequence corresponds to the sequence x_1 ... x_&#123;n+1&#125; from the handout (e.g., &lt;START&gt;,m,u,s,i,c,&lt;END&gt;).</span></span><br><span class="line">        scores, dec_hidden = self.forward(char_sequence[:-<span class="number">1</span>], dec_hidden)</span><br><span class="line">        <span class="comment"># char_embeddings = self.decoderCharEmb(char_sequence)</span></span><br><span class="line">        <span class="comment"># hidden_states, dec_hidden = self.charDecoder(char_embeddings[:-1], dec_hidden)</span></span><br><span class="line">        <span class="comment"># scores = self.char_output_projection(hidden_states)  # (len, batch, vocab)</span></span><br><span class="line">        loss = nn.CrossEntropyLoss(ignore_index=self.padding_idx,</span><br><span class="line">                                   reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">        ce_loss = loss(scores.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">                       char_sequence[<span class="number">1</span>:].transpose(<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ce_loss</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode_greedy</span>(<span class="params">self, initialStates, device, max_length=<span class="number">21</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Greedy decoding</span></span><br><span class="line"><span class="string">        @param initialStates: initial internal state of the LSTM, a tuple of two tensors of size (1, batch, hidden_size)</span></span><br><span class="line"><span class="string">        @param device: torch.device (indicates whether the model is on CPU or GPU)</span></span><br><span class="line"><span class="string">        @param max_length: maximum length of words to decode</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns decodedWords: a list (of length batch) of strings, each of which has length &lt;= max_length.</span></span><br><span class="line"><span class="string">                              The decoded strings should NOT contain the start-of-word and end-of-word characters.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 2d</span></span><br><span class="line">        <span class="comment">### TODO - Implement greedy decoding.</span></span><br><span class="line">        <span class="comment">### Hints:</span></span><br><span class="line">        <span class="comment">###      - Use target_vocab.char2id and target_vocab.id2char to convert between integers and characters</span></span><br><span class="line">        <span class="comment">###      - Use torch.tensor(..., device=device) to turn a list of character indices into a tensor.</span></span><br><span class="line">        <span class="comment">###      - We use curly brackets as start-of-word and end-of-word characters. That is, use the character &#x27;&#123;&#x27; for &lt;START&gt; and &#x27;&#125;&#x27; for &lt;END&gt;.</span></span><br><span class="line">        <span class="comment">###        Their indices are self.target_vocab.start_of_word and self.target_vocab.end_of_word, respectively.</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">        output_words = []</span><br><span class="line">        decodedWords = []</span><br><span class="line">        start_idx = self.target_vocab.start_of_word</span><br><span class="line">        end_idx = self.target_vocab.end_of_word</span><br><span class="line">        dec_hidden = initialStates</span><br><span class="line">        batch_size = dec_hidden[<span class="number">0</span>].shape[<span class="number">1</span>]</span><br><span class="line">        current_char = torch.tensor([[start_idx] * batch_size],</span><br><span class="line">                                    device=device)  <span class="comment"># idx of &#x27;&lt;start&gt;&#x27; token</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_length):</span><br><span class="line">            scores, dec_hidden = self.forward(current_char, dec_hidden)</span><br><span class="line">            current_char = scores.argmax(-<span class="number">1</span>)</span><br><span class="line">            output_words += [current_char]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        output_words = torch.cat(output_words).t().tolist()</span><br><span class="line">        <span class="keyword">for</span> foo <span class="keyword">in</span> output_words:</span><br><span class="line">            word = <span class="string">&quot;&quot;</span></span><br><span class="line">            <span class="keyword">for</span> bar <span class="keyword">in</span> foo:</span><br><span class="line">                <span class="keyword">if</span> bar == end_idx:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                word += self.target_vocab.id2char[bar]</span><br><span class="line">            decodedWords += [word]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> decodedWords</span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br></pre></td></tr></table></figure>
<h2 id="reference">Reference</h2>
<ul>
<li>Course note and slides of <a target="_blank" rel="noopener" href="http://web.stanford.edu/class/cs224n/">cs224n</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Attention/2019/12/16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Attention/2019/12/16/" class="post-title-link" itemprop="url">Attention</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-16 20:55:07" itemprop="dateCreated datePublished" datetime="2019-12-16T20:55:07+08:00">2019-12-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-31 16:10:40" itemprop="dateModified" datetime="2021-12-31T16:10:40+08:00">2021-12-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Attention/2019/12/16/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Attention/2019/12/16/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Summary of Attention</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/Attention/2019/12/16/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Gated-RNN-Units/2019/12/15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Gated-RNN-Units/2019/12/15/" class="post-title-link" itemprop="url">Gated RNN Units</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-15 01:40:39" itemprop="dateCreated datePublished" datetime="2019-12-15T01:40:39+08:00">2019-12-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-31 15:50:13" itemprop="dateModified" datetime="2021-12-31T15:50:13+08:00">2021-12-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Gated-RNN-Units/2019/12/15/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Gated-RNN-Units/2019/12/15/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Gated RNN Units</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/Gated-RNN-Units/2019/12/15/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/10/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/12/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">268</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail → mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'en' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
