<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="RUOCHI.AI">
<meta property="og:url" content="https://zhangruochi.com/page/11/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Ruochi Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhangruochi.com/page/11/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/rails-tutorials-toy-app/2020/01/11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/rails-tutorials-toy-app/2020/01/11/" class="post-title-link" itemprop="url">rails tutorials - toy_app</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-11 14:51:49" itemprop="dateCreated datePublished" datetime="2020-01-11T14:51:49+08:00">2020-01-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-17 23:19:48" itemprop="dateModified" datetime="2020-01-17T23:19:48+08:00">2020-01-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Full-Stack/" itemprop="url" rel="index"><span itemprop="name">Full Stack</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Full-Stack/Ruby-on-Rails/" itemprop="url" rel="index"><span itemprop="name">Ruby on Rails</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/rails-tutorials-toy-app/2020/01/11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/rails-tutorials-toy-app/2020/01/11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>a simple tutorial of ror</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/rails-tutorials-toy-app/2020/01/11/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Fastai-Data-Block-API/2019/12/29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Fastai-Data-Block-API/2019/12/29/" class="post-title-link" itemprop="url">Fastai Data Block API</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2019-12-29 16:08:18 / Modified: 21:03:57" itemprop="dateCreated datePublished" datetime="2019-12-29T16:08:18+08:00">2019-12-29</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Fastai-Data-Block-API/2019/12/29/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Fastai-Data-Block-API/2019/12/29/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>a journey through the fastai data block API</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/Fastai-Data-Block-API/2019/12/29/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Face-Blindness-Saver/2019/12/25/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Face-Blindness-Saver/2019/12/25/" class="post-title-link" itemprop="url">Face Blindness Saver</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-25 01:20:49" itemprop="dateCreated datePublished" datetime="2019-12-25T01:20:49+08:00">2019-12-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-24 03:01:03" itemprop="dateModified" datetime="2020-01-24T03:01:03+08:00">2020-01-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Face-Blindness-Saver/2019/12/25/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Face-Blindness-Saver/2019/12/25/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>I want to make a classifier to classify the akita and shiba inu, alaska and husky.</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/Face-Blindness-Saver/2019/12/25/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/ELMo-OpenAI-GPT-BERT/2019/12/21/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/ELMo-OpenAI-GPT-BERT/2019/12/21/" class="post-title-link" itemprop="url">ELMo,OpenAI GPT,BERT</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-21 22:31:19" itemprop="dateCreated datePublished" datetime="2019-12-21T22:31:19+08:00">2019-12-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-24 03:06:41" itemprop="dateModified" datetime="2020-01-24T03:06:41+08:00">2020-01-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/ELMo-OpenAI-GPT-BERT/2019/12/21/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/ELMo-OpenAI-GPT-BERT/2019/12/21/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="The-problems-of-RNN"><a href="#The-problems-of-RNN" class="headerlink" title="The problems of RNN"></a>The problems of RNN</h2><ol>
<li>Sequential computation inhibit parallelization</li>
<li>No explicit modeling of long and short range</li>
<li>We want to model hierarchy (RNNs seem wasteful)</li>
</ol>
<h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><p>ELMo means <code>Embeddings from Language Models</code>. the original paper is from <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.05365">https://arxiv.org/abs/1802.05365</a></p>
<ol>
<li>Breakout version of word token vectors or contextual word vectors</li>
<li>Learn word token vectors using long contexts not context windows (here, whole sentence, could be longer)</li>
<li>Learn a deep Bi-NLM and use all its layers in prediction</li>
</ol>
<h3 id="What’s-ELMo’s-secret"><a href="#What’s-ELMo’s-secret" class="headerlink" title="What’s ELMo’s secret?"></a>What’s ELMo’s secret?</h3><p>ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called <code>Language Modeling</code>. This is convenient because we have vast amounts of text data that such a model can learn from without needing labels.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="elmo2.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">char cnn embedding from ELMo</div>
</center>

<p>A step in the pre-training process of ELMo: Given “Let’s stick to” as input, predict the next most likely word – a language modeling task. When trained on a large dataset, the model starts to pick up on language patterns. It’s unlikely it’ll accurately guess the next word in this example. More realistically, after a word such as “hang”, it will assign a higher probability to a word like “out” (to spell “hang out”) than to “camera”.</p>
<h3 id="bilstm-LM"><a href="#bilstm-LM" class="headerlink" title="bilstm LM"></a>bilstm LM</h3><p>ELMo actually goes a step further and trains a bi-directional LSTM – so that its language model doesn’t only have a sense of the next word, but also the previous word.</p>
<ol>
<li>前向LSTM结构:<br>$$p(t_1,t_2,…,t_N) = \prod^N_{k=1}p(t_k|t_1,t_2,…,t_{k-1})$$</li>
<li>反向LSTM结构:<br>$$p(t_1,t_2,…,t_N) = \prod^N_{k=1}p(t_k|t_{k+1},t_{k+2},…,t_{N})$$</li>
<li>最大似然函数:<br>$$\sum_{k=1}^N(logp(t_k|t_1,t_2,…,t_{k-1}) + logp(t_k|t_{k+1},t_{k+2},…,t_{N}))$$</li>
<li>线性组合公式：<br>$$\textrm{ELMo}<em>k^{task} = E(R_k;\Theta^{task}) = \gamma^{task}\sum</em>{j=0}^L s_j^{task}h_{k,j}^{LM} \tag{1}$$</li>
</ol>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="elmo.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">bilstm from ELMo</div>
</center>




<h3 id="Char-cnn-embedding"><a href="#Char-cnn-embedding" class="headerlink" title="Char cnn embedding"></a>Char cnn embedding</h3><p>The input of elmo is char embedding, see the details from <a href="https://zhangruochi.com/Subword-Models/2019/12/19/">https://zhangruochi.com/Subword-Models/2019/12/19/</a></p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="char_cnn.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">char cnn embedding from ELMo</div>
</center>


<h2 id="How-to-use-ELMo-when-after-pre-training"><a href="#How-to-use-ELMo-when-after-pre-training" class="headerlink" title="How to use ELMo when after pre-training"></a>How to use ELMo when after pre-training</h2><p>We can feed our input data to the pre-trained ELMo and get the representation of <code>dynamic word vectors</code>. And then we use them to our specific tasks.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="elmo3.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">ELMo used in a sequence tagger</div>
</center>


<h2 id="OpenAI-Transformer-Pre-training-a-Transformer-Decoder-for-Language-Modeling"><a href="#OpenAI-Transformer-Pre-training-a-Transformer-Decoder-for-Language-Modeling" class="headerlink" title="OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling"></a>OpenAI Transformer: Pre-training a Transformer Decoder for Language Modeling</h2><p>It turns out we don’t need an entire Transformer to adopt transfer learning and a fine-tunable language model for NLP tasks. We can do with just the decoder of the transformer. The decoder is a good choice because it’s a natural choice for language modeling (predicting the next word) since it’s built to <code>mask future tokens</code> – a valuable feature when it’s generating a translation word by word.</p>
<p>The model stacked twelve decoder layers. Since there is no encoder in this set up, these decoder layers would not have the encoder-decoder attention sublayer that vanilla transformer decoder layers have. It would still have the self-attention layer, however (masked so it doesn’t peak at future tokens).</p>
<p>With this structure, we can proceed to train the model on the same language modeling task: predict the next word using massive (unlabeled) datasets. Just, throw the text of 7,000 books at it and have it learn! Books are great for this sort of task since it allows the model to learn to associate related information even if they’re separated by a lot of text – something you don’t get for example, when you’re training with tweets, or articles.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="openai.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;"></div>
</center>


<p>$$<br>\begin{split}<br>h_0 &amp; =UW_e+W_p \<br>h_l &amp; = transformer_block(h_{l-1}) \<br>P(u) &amp; = softmax(h_n W_e^T)<br>\end{split}<br>$$</p>
<p>$W_e$ is the embedding matrix, $W_p$ is the positional embedding matrix(Note that it is different with classicial transformer)</p>
<h3 id="Fine-Tuning-with-OpenAI"><a href="#Fine-Tuning-with-OpenAI" class="headerlink" title="Fine-Tuning with OpenAI"></a>Fine-Tuning with OpenAI</h3><p>Now that the OpenAI transformer is pre-trained and its layers have been tuned to reasonably handle language, we can start using it for downstream tasks. Let’s first look at sentence classification (classify an email message as “spam” or “not spam”):</p>
<p>If our input sequence is $x_1,\cdots,x_m$, and the label is y. We can add a <code>softmax layer</code> to do classification and use the cross entrophy to calculate the loss.</p>
<p>$$L_2(\mathcal{C})=\sum{x,y}logP(y|x^1,…,x^m)$$</p>
<p>In general, we should update the parameters to minimize the $L_2$, but we can use <code>Multi-task Learning</code> to get a more generalize model. Therefore we can get the max likelihood of $L3$</p>
<p>$$L_3(\mathcal{C})=L_2(\mathcal{C})+\lambda \times L_1(\mathcal{C})$$</p>
<p>$L_1$ if the loss of previous language model.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="openai2.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">How to use a pre-trained OpenAI transformer to do sentence clasification</div>
</center>


<p>The OpenAI paper outlines a number of input transformations to handle the inputs for different types of tasks. The following image from the paper shows the structures of the models and input transformations to carry out different tasks.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="openai3.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">How to use a pre-trained OpenAI transformer to do different tasks</div>
</center>


<h2 id="BERT-From-Decoders-to-Encoders"><a href="#BERT-From-Decoders-to-Encoders" class="headerlink" title="BERT: From Decoders to Encoders"></a>BERT: From Decoders to Encoders</h2><p>The openAI transformer gave us a fine-tunable pre-trained model based on the Transformer. But something went missing in this transition from LSTMs to Transformers. ELMo’s language model was bi-directional, but the openAI transformer only trains a forward language model. Could we build a transformer-based model whose language model looks both forward and backwards (in the technical jargon – “is conditioned on both left and right context”)?</p>
<h3 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h3><p>The input representation of BERT is shown in the figure below. For example, the two sentences “my dog ​​is cute” and “he likes playing” are entered. I’ll explain why two sentences are needed later. Here, the two sentences similar to GPT are used. First, a special Token <code>[CLS]</code> is added at the beginning of the first sentence, and a <code>[SEP]</code> is added after the cute to indicate the end of the first sentence. After ##ing, A <code>[SEP]</code> will be added later. Note that the word segmentation here will divide “playing” into “play” and “##ing” two tokens. This method of dividing words into more fine-grained Word Pieces was introduced in the previous machine translation section. This is a kind of Common methods to resolve unregistered words. Then perform 3 Embeddings on each Token: </p>
<ol>
<li>Embedding of words;</li>
<li>Embedding of positions; </li>
<li>Embedding of segments. </li>
</ol>
<p>The word Embedding is familiar to everyone, and the position Embedding is similar to the word embedding, mapping a position (such as 2) into a low-dimensional dense vector. And Segment embedding has only two, either belong to the first sentence (segment) or belong to the second sentence. Segment Embedding of the same sentence is shared so that it can learn information belonging to different segments. For tasks such as sentiment classification, there is only one sentence, so the Segment id is always 0; for the Entailment task, the input is two sentences, so the Segment is 0 or 1.</p>
<p>The BERT model requires a fixed sequence length, such as 128. If it is not enough, then padding in the back, otherwise it will intercept the excess Token, so as to ensure that the input is a fixed-length Token sequence. The first token is always special <code>[CLS]</code>. It does not have any semantics, so it will (must) encode the semantics of the entire sentence (other words).</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="bert.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Input of Bert/div>
</center>

<h3 id="Masked-Language-Model"><a href="#Masked-Language-Model" class="headerlink" title="Masked Language Model"></a>Masked Language Model</h3><p>Finding the right task to train a Transformer stack of encoders is a complex hurdle that BERT resolves by adopting a <code>masked language model</code> concept from earlier literature (where it’s called a Cloze task).</p>
<p>Beyond masking 15% of the input, BERT also mixes things a bit in order to improve how the model later fine-tunes. Sometimes it randomly replaces a word with another word and asks the model to predict the correct word in that position.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="bert2.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">masked language model</div>
</center>

<h3 id="Two-sentence-Tasks"><a href="#Two-sentence-Tasks" class="headerlink" title="Two-sentence Tasks"></a>Two-sentence Tasks</h3><p>If you look back up at the input transformations the OpenAI transformer does to handle different tasks, you’ll notice that some tasks require the model to say something intelligent about two sentences (e.g. are they simply paraphrased versions of each other? Given a wikipedia entry as input, and a question regarding that entry as another input, can we answer that question?).</p>
<p>To make BERT better at handling relationships between multiple sentences, the pre-training process includes an additional task: Given two sentences (A and B), is B likely to be the sentence that follows A, or not?</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="bert3.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">The second task BERT is pre-trained on is a two-sentence classification task.</div>
</center>

<h3 id="Task-specific-Models"><a href="#Task-specific-Models" class="headerlink" title="Task specific-Models"></a>Task specific-Models</h3><p>The BERT paper shows a number of ways to use BERT for different tasks.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="bert4.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">different ways to use BERT</div>
</center>

<ol>
<li>For common classification tasks, the input is a sequence, as shown in the upper right of the figure. All tokens belong to the same Segment (Id = 0). We use the last layer of the first special token <code>[CLS]</code> to connect it. Softmax is used for classification, and classified data is used for Fine-Tuning.</li>
<li>For tasks such as similarity calculation that are input as two sequences, the process is shown in the upper left. The tokens of the two sequences correspond to different segments (Id = 0/1). We also use the last layer output of the first special token [CLS] to connect with softmax for classification, and then use the classification data for Fine-Tuning.</li>
<li>The third type is a question-and-answer type question, such as the SQuAD v1.1 dataset. The input is a question and a long paragraph containing the answer (Paragraph), and the output finds the answer to the question in this paragraph.</li>
<li>The forth type of task is sequence labeling, such as named entity recognition. The input is a sentence (Token sequence). Except for [CLS] and [SEP], there will be output tags at each moment. For example, B-PER indicates the beginning of a person’s name. </li>
</ol>
<h3 id="BERT-for-feature-extraction"><a href="#BERT-for-feature-extraction" class="headerlink" title="BERT for feature extraction"></a>BERT for feature extraction</h3><p>The fine-tuning approach isn’t the only way to use BERT. Just like ELMo, you can use the pre-trained BERT to create contextualized word embeddings. Then you can feed these embeddings to your existing model – a process the paper shows yield results not far behind fine-tuning BERT on a task such as named-entity recognition.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="bert5.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Feature extraction</div>
</center>

<p>Which vector works best as a contextualized embedding? I would think it depends on the task. The paper examines six choices (Compared to the fine-tuned model which achieved a score of 96.4):</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="bert6.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Feature extraction</div>
</center>


<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a target="_blank" rel="noopener" href="http://fancyerii.github.io/2019/03/09/bert-theory/#%E8%AF%8D%E6%B1%87%E6%89%A9%E5%B1%95">http://fancyerii.github.io/2019/03/09/bert-theory/#词汇扩展</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/63115885">https://zhuanlan.zhihu.com/p/63115885</a></li>
<li><a target="_blank" rel="noopener" href="http://jalammar.github.io/illustrated-bert/">http://jalammar.github.io/illustrated-bert/</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Transformer/2019/12/20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Transformer/2019/12/20/" class="post-title-link" itemprop="url">Transformer</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-20 12:24:40" itemprop="dateCreated datePublished" datetime="2019-12-20T12:24:40+08:00">2019-12-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-22 02:20:04" itemprop="dateModified" datetime="2019-12-22T02:20:04+08:00">2019-12-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Transformer/2019/12/20/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Transformer/2019/12/20/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention.</p>
<p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations. End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and<br>language modeling tasks.</p>
<p>To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution. </p>
<h2 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h2><p>Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\text{model}}$.  We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.  In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.05859">(cite)</a>. In the embedding layers, we multiply those weights by $\sqrt{d_{\text{model}}}$.           </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, vocab</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure>

<h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>Since our model contains no recurrence and no convolution, <strong>in order for the model to make use of the order of the sequence</strong>, we must inject some information about the relative or absolute position of the tokens in the sequence.  To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks.  The positional encodings have the same dimension $d_{\text{model}}$ as the embeddings, so that the two can be summed.   There are many choices of positional encodings, learned and fixed <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1705.03122.pdf">(cite)</a>. </p>
<p>In this work, we use sine and cosine functions of different frequencies:    </p>
<p>$$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\text{model}}})$$</p>
<p>$$PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\text{model}}})$$                </p>
<p>where $pos$ is the position and $i$ is the dimension.  That is, each dimension of the positional encoding corresponds to a sinusoid.  The wavelengths form a geometric progression from $2\pi$ to $10000 \cdot 2\pi$.  We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. </p>
<p>In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.  For the base model, we use a rate of $P_{drop}=0.1$. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Implement the PE function.&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>


<h2 id="Layer-Norm-and-residual-connection"><a href="#Layer-Norm-and-residual-connection" class="headerlink" title="Layer Norm and residual connection"></a>Layer Norm and residual connection</h2><p>We employ a residual connection <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.03385">(cite)</a> around each of the two sub-layers, followed by layer normalization <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">(cite)</a>.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Construct a layernorm module (See citation for details).&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure>

<p>That is, the output of each sub-layer is $\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))$, where $\mathrm{Sublayer}(x)$ is the function implemented by the sub-layer itself.  We apply dropout <a target="_blank" rel="noopener" href="http://jmlr.org/papers/v15/srivastava14a.html">(cite)</a> to the output of each sub-layer, before it is added to the sub-layer input and normalized.  </p>
<p>To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{\text{model}}=512$.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, sublayer</span>):</span></span><br><span class="line">        <span class="string">&quot;Apply residual connection to any sublayer with the same size.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure>

<p>Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.</p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p>An attention function can be described as mapping a <code>query</code> and a set of <code>key</code>-<code>value</code> pairs to an output, where the query, keys, values, and output are all vectors.  The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p>
<p>We call our particular attention <code>Scaled Dot-Product Attention</code>.   The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.  We compute the dot products of the query with all keys, divide each by $\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="ModalNet-19.png"  width = "20%" height="20%">
</center>

<p>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.   The keys and values are also packed together into matrices $K$ and $V$.  We compute the matrix of outputs as:                      </p>
<p>$$\mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V$$</p>
<p>The two most commonly used attention functions are additive attention <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">(cite)</a>, and dot-product (multiplicative) attention.  Dot-product attention is identical to our algorithm, except for the scaling factor of $\frac{1}{\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.  While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.</p>
<p>While for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.03906">(cite)</a>. We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients  (To illustrate why the dot products get large, assume that the components of $q$ and $k$ are independent random variables with mean $0$ and variance $1$.  Then their dot product, $q \cdot k = \sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance $d_k$.). To counteract this effect, we scale the dot products by $\frac{1}{\sqrt{d_k}}$.          </p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="attention.png" width = "50%" height="50%">
</center>


<h3 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h3><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="ModalNet-20.png" width = "50%" height="50%">
</center>

<p>$$<br>\mathrm{MultiHead}(Q, K, V) = \mathrm{Concat}(\mathrm{head_1}, …, \mathrm{head_h})W^O    \<br>    \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)<br>$$</p>
<p>Where the projections are parameter matrices $W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}$ and $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$.                                                                                                                                                                                            In this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_k=d_v=d_{\text{model}}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;Take in model size and number of heads.&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;Implements Figure 2&quot;</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, -<span class="number">1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure>

<h3 id="Applications-of-Attention-in-our-Model"><a href="#Applications-of-Attention-in-our-Model" class="headerlink" title="Applications of Attention in our Model"></a>Applications of Attention in our Model</h3><p>The Transformer uses multi-head attention in three different ways:                                                        </p>
<ol>
<li><p>In <strong>encoder-decoder attention</strong> layers, the queries come from the previous decoder layer, and the memory keys and values come from the <code>output of the encoder</code>.   This allows every position in the decoder to attend over all positions in the input sequence.  This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.08144">(cite)</a>.    </p>
</li>
<li><p>The encoder contains self-attention layers.  In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.   Each position in the encoder can attend to all positions in the previous layer of the encoder.                                                   </p>
</li>
<li><p>Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.  We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.  We implement this inside of scaled dot-product attention by masking out (setting to $-\infty$) all values in the input of the softmax which correspond to illegal connections. </p>
</li>
</ol>
<h2 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h2><p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.  This consists of two linear transformations with a ReLU activation in between.</p>
<p>$$\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2$$</p>
<p>While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.  The dimensionality of input and output is $d_{\text{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Implements FFN equation.&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure>

<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>Most competitive neural sequence transduction models have an encoder-decoder structure <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">(cite)</a>. Here, the encoder maps an input sequence of symbol representations $(x_1, …, x_n)$ to a sequence of continuous representations $\mathbf{z} = (z_1, …, z_n)$. Given $\mathbf{z}$, the decoder then generates an output sequence $(y_1,…,y_m)$ of symbols one element at a time. At each step the model is auto-regressive <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1308.0850">(cite)</a>, consuming the previously generated symbols as additional input when generating the next. </p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="ModalNet-21.png" width = "50%" height="50%">
</center>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many </span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder, decoder, src_embed, tgt_embed, generator</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="string">&quot;Take in and process masked src and target sequences.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, src, src_mask</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Define standard linear + softmax generation step.&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, vocab</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Encoder-and-Decoder-Stacks"><a href="#Encoder-and-Decoder-Stacks" class="headerlink" title="Encoder and Decoder Stacks"></a>Encoder and Decoder Stacks</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>The encoder is composed of a stack of $N=6$ identical layers. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span>(<span class="params">module, N</span>):</span></span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Core encoder is a stack of N layers&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer, N</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        <span class="string">&quot;Pass the input (and mask) through each layer in turn.&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Encoder is made up of self-attn and feed forward (defined below)&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (left) for connections.&quot;</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>

<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>The decoder is also composed of a stack of $N=6$ identical layers.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Generic N layer decoder with masking.&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer, N</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>

<p>In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.  Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;Decoder is made of self-attn, src-attn, and feed forward (defined below)&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (right) for connections.&quot;</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>

<p>We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.  This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span>(<span class="params">size</span>):</span></span><br><span class="line">    <span class="string">&quot;Mask out subsequent positions.&quot;</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br></pre></td></tr></table></figure>

<h2 id="Full-Model"><a href="#Full-Model" class="headerlink" title="Full Model"></a>Full Model</h2><p>Here we define a function from hyperparameters to a full model. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span>(<span class="params">src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="params"><span class="function">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;Helper: Construct a model from hyperparameters.&quot;</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>



<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a target="_blank" rel="noopener" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></li>
<li><a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Subword-Models/2019/12/19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Subword-Models/2019/12/19/" class="post-title-link" itemprop="url">Subword Models</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-19 13:07:57" itemprop="dateCreated datePublished" datetime="2019-12-19T13:07:57+08:00">2019-12-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-20 05:04:03" itemprop="dateModified" datetime="2019-12-20T05:04:03+08:00">2019-12-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Subword-Models/2019/12/19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Subword-Models/2019/12/19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Character-Level-Models"><a href="#Character-Level-Models" class="headerlink" title="Character-Level Models"></a>Character-Level Models</h2><ol>
<li>Word embeddings can be composed from character embeddings<ul>
<li>Generates embeddings for unknown words</li>
<li>Similar spellings share similar embeddings</li>
<li>Solves OOV problem</li>
</ul>
</li>
<li>Motivation<ul>
<li>Derive a powerful,robust language model effective across a variety of languages.</li>
<li>Encode subword relatedness:eventful,eventfully, uneventful…</li>
<li>Address rare-word problem of prior models. </li>
<li>Obtain comparable expressivity with fewer parameters.</li>
</ul>
</li>
</ol>
<h2 id="Two-trends"><a href="#Two-trends" class="headerlink" title="Two trends"></a>Two trends</h2><ol>
<li>Same architecture as forword-level model but use smaller units: “word pieces”</li>
<li>Hybrid architectures: Main model has words; something else for characters</li>
</ol>
<h2 id="Byte-Pair-Encoding"><a href="#Byte-Pair-Encoding" class="headerlink" title="Byte Pair Encoding"></a>Byte Pair Encoding</h2><p>A word segmentation algorithm:</p>
<ul>
<li>Start with a vocabulary of characters</li>
<li>Most frequent ngram pairs -&gt; a new ngram</li>
<li>Have a target vocabulary size and stop when you reach it</li>
<li>Do deterministic longest piece segmentation of words</li>
<li>Segmentation is only within words identified by some prior tokenizer</li>
</ul>
<p>For example, all the words in our documents database and their frequency are</p>
<blockquote>
<p>{‘l o w’: 5, ‘l o w e r’: 2, ‘n e w e s t’: 6, ‘w i d e s t’: 3}</p>
</blockquote>
<p>We can initialize our vocabulary library as:  </p>
<blockquote>
<p>{ ‘l’, ‘o’, ‘w’, ‘e’, ‘r’, ‘n’, ‘w’, ‘s’, ‘t’, ‘i’, ‘d’}</p>
</blockquote>
<p>The most frequent ngram pair is (‘e’,’s’) and its count is 9. So we add the ‘es’ to our vocabulary library. </p>
<p>Our documents database now is:</p>
<blockquote>
<p>{‘l o w’: 5, ‘l o w e r’: 2, ‘n e w es t’: 6, ‘w i d es t’: 3}.</p>
</blockquote>
<p>Our vocabulary library now is:</p>
<blockquote>
<p>{ ‘l’, ‘o’, ‘w’, ‘e’, ‘r’, ‘n’, ‘w’, ‘s’, ‘t’, ‘i’, ‘d’, ‘es’}</p>
</blockquote>
<p><strong>Again</strong>, the most frequent ngram pair is (‘es’,’t’) and its count is 9，So we add the ‘est’ to our vocabulary library.</p>
<p>Our documents database now is: </p>
<blockquote>
<p>{‘l o w’: 5, ‘l o w e r’: 2, ‘n e w est’: 6, ‘w i d est’: 3}</p>
</blockquote>
<p>Our vocabulary library now is:</p>
<blockquote>
<p>{ ‘l’, ‘o’, ‘w’, ‘e’, ‘r’, ‘n’, ‘w’, ‘s’, ‘t’, ‘i’, ‘d’, ‘es’,’est’}</p>
</blockquote>
<p>the rest can be done in the same manner. We can set a threshold of total count of our vocabulary library. By doing so, we can use BPE to construct a vocabulary library to represent all the words based on subword unit.</p>
<p>Google NMT(GNMT) uses a variant of this:</p>
<ul>
<li>V1: wordpiece model (Word piece model tokenizes inside words)</li>
<li>V2: sentencepiece model (Sentence piece model works from raw text)</li>
</ul>
<h2 id="Character-level-to-build-word-level"><a href="#Character-level-to-build-word-level" class="headerlink" title="Character-level to build word-level"></a>Character-level to build word-level</h2><ol>
<li>Convolution over characters to generate word embeddings<center>
 <img style="border-radius: 0.3125em;
 box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
 src="1.png" width = "50%" height="50%">
</center></li>
<li>Character-based LSTM to build word representation<center>
 <img style="border-radius: 0.3125em;
 box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
 src="2.png" width = "50%" height="50%">
</center></li>
</ol>
<h2 id="CS224n-Assignment5"><a href="#CS224n-Assignment5" class="headerlink" title="CS224n Assignment5"></a>CS224n Assignment5</h2><h3 id="Character-based-convolutional-encoder-for-NMT"><a href="#Character-based-convolutional-encoder-for-NMT" class="headerlink" title="Character-based convolutional encoder for NMT."></a>Character-based convolutional encoder for NMT.</h3><center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="3.png" width = "80%" height="80%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Figure from cs224n. Character-based convolutional encoder, which ultimately produces a word embedding of length $e_{word}$</div>
</center>

<ol>
<li>Convert word to character indices. We have a word $x$ (e.g. Anarchy in above figure) that we wish to represent. Assume we have a predefined ‘vocabulary’ of characters (for example, all lowercase letters, uppercase letters, numbers, and some punctuation). By looking up the index of each character, we can thus represent the length-l word x as a vector of integers:<br>$$x = \left[ c_1,c_2,\cdots,c_l  \right ] \in \mathbb{Z}^{l}$$<br>where each $c_i$ is an integer index into the character vocabulary.</li>
<li>Padding and embedding lookup. Using a special <PAD> ‘character’, we pad (or truncate) every word so that it has length $m_word$ (this is some predefined hyperparameter representing maximum word length):<br>$$x_{padded} = \left [ c_1,c_2,\cdots,c_{m_{word}}  \right ] \in \mathbb{Z}^{m_{word}}$$<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_sents_char</span>(<span class="params">sents, char_pad_token</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Pad list of sentences according to the longest sentence in the batch and max_word_length.</span></span><br><span class="line"><span class="string">    @param sents (list[list[list[int]]]): list of sentences, result of `words2charindices()`</span></span><br><span class="line"><span class="string">        from `vocab.py`</span></span><br><span class="line"><span class="string">    @param char_pad_token (int): index of the character-padding token</span></span><br><span class="line"><span class="string">    @returns sents_padded (list[list[list[int]]]): list of sentences where sentences/words shorter</span></span><br><span class="line"><span class="string">        than the max length sentence/word are padded out with the appropriate pad token, such that</span></span><br><span class="line"><span class="string">        each sentence in the batch now has same number of words and each word has an equal</span></span><br><span class="line"><span class="string">        number of characters</span></span><br><span class="line"><span class="string">        Output shape: (batch_size, max_sentence_length, max_word_length)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Words longer than 21 characters should be truncated</span></span><br><span class="line">    max_word_length = <span class="number">21</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE for part 1f</span></span><br><span class="line">    <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">    <span class="comment">###     Perform necessary padding to the sentences in the batch similar to the pad_sents()</span></span><br><span class="line">    <span class="comment">###     method below using the padding character from the arguments. You should ensure all</span></span><br><span class="line">    <span class="comment">###     sentences have the same number of words and each word has the same number of</span></span><br><span class="line">    <span class="comment">###     characters.</span></span><br><span class="line">    <span class="comment">###     Set padding words to a `max_word_length` sized vector of padding characters.</span></span><br><span class="line">    <span class="comment">###</span></span><br><span class="line">    <span class="comment">###     You should NOT use the method `pad_sents()` below because of the way it handles</span></span><br><span class="line">    <span class="comment">###     padding and unknown words.</span></span><br><span class="line">    sents_padded = []</span><br><span class="line">    max_sent_len = <span class="built_in">max</span>([<span class="built_in">len</span>(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> sents])</span><br><span class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> sents:</span><br><span class="line">        sent = sent + [[] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_sent_len - <span class="built_in">len</span>(sent))]</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(sent) == max_sent_len</span><br><span class="line">        tmp_sent = []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sent:</span><br><span class="line">            word = word[:max_word_length]</span><br><span class="line">            diff = max_word_length - <span class="built_in">len</span>(word)</span><br><span class="line">            word = word + [char_pad_token] * diff</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">len</span>(word) == max_word_length</span><br><span class="line">            tmp_sent.append(word)</span><br><span class="line">        sents_padded.append(tmp_sent)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sents_padded</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VocabEntry</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="comment"># ......</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">words2charindices</span>(<span class="params">self, sents</span>):</span></span><br><span class="line">            <span class="string">&quot;&quot;&quot; Convert list of sentences of words into list of list of list of character indices.</span></span><br><span class="line"><span class="string">            @param sents (list[list[str]]): sentence(s) in words</span></span><br><span class="line"><span class="string">            @return word_ids (list[list[list[int]]]): sentence(s) in indices</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            <span class="comment">### YOUR CODE HERE for part 1e</span></span><br><span class="line">            <span class="comment">### <span class="doctag">TODO:</span> </span></span><br><span class="line">            <span class="comment">###     This method should convert characters in the input sentences into their </span></span><br><span class="line">            <span class="comment">###     corresponding character indices using the character vocabulary char2id </span></span><br><span class="line">            <span class="comment">###     defined above.</span></span><br><span class="line">            <span class="comment">###</span></span><br><span class="line">            <span class="comment">###     You must prepend each word with the `start_of_word` character and append </span></span><br><span class="line">            <span class="comment">###     with the `end_of_word` character. </span></span><br><span class="line"></span><br><span class="line">            word_ids = []</span><br><span class="line">            <span class="keyword">for</span> sent <span class="keyword">in</span> sents:</span><br><span class="line">                sent_chars_id = []</span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> sent:</span><br><span class="line">                    sent_chars_id.append([<span class="number">1</span>] + [ self.char2id.get(char,<span class="number">3</span>) <span class="keyword">for</span> char <span class="keyword">in</span> word ] + [<span class="number">2</span>])</span><br><span class="line">                word_ids.append(sent_chars_id)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> word_ids</span><br><span class="line"></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">to_input_tensor_char</span>(<span class="params">self, sents: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]], device: torch.device</span>) -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Convert list of sentences (words) into tensor with necessary padding for </span></span><br><span class="line"><span class="string">        shorter sentences.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param sents (List[List[str]]): list of sentences (words)</span></span><br><span class="line"><span class="string">        @param device: device on which to load the tensor, i.e. CPU or GPU</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns sents_var: tensor of (max_sentence_length, batch_size, max_word_length)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 1g</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span> </span></span><br><span class="line">        <span class="comment">###     Connect `words2charindices()` and `pad_sents_char()` which you&#x27;ve defined in </span></span><br><span class="line">        <span class="comment">###     previous parts</span></span><br><span class="line">        char_sents =  self.words2charindices(sents)</span><br><span class="line">        padded_char_sents = pad_sents_char(char_sents, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        sents_var = torch.tensor(padded_char_sents, dtype=torch.int8, device= device)</span><br><span class="line">        sents_var = sents_var.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sents_var</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ......</span></span><br></pre></td></tr></table></figure></li>
<li>For each of these characters $c_i$, we lookup a dense character embedding (which has shape $e_{char}$). This yields a tensor $x_{emb}$:<br>$$x_{emb} = CharEmbedding(X_{padded}) \in \mathbb{R}^{m_{word} \times e_{char}}$$<br>We’ll reshape $x_{emb}$ to obtain $x_{reshaped} in \mathbb{R}^{e_{char} \times m_{word}}$ before feeding into the convolutional network.</li>
<li><strong>Convolutional network</strong>. To combine these character embeddings, we’ll use 1-dimensional convolutions. The convolutional layer has two hyperparameters: the kernel size $k$ (also called window size), which dictates the size of the window used to compute features, and the number of filters $f$, (also called number of output features or number of output channels). The convolutional layer has a weight matrix $W \in \mathbb{R}^{f \times e_{char} \times k}$ and a bias vector $b \in \mathbb{R}^{f}$. Overall this produces output $x_{conv}$.<br>$$x_{conv} = Conv1D(x_{reshaped}) \in \mathbb{R}^{f \times {m_{word} - k + 1}}$$<br>For our application, we’ll set $f$ to be equal to $e_{word}$, the size of the final word embedding for word x. Therefore,<br>$$x_{conv} \in \mathbb{R}^{e_{word} \times (m_{word} - k + 1)}$$<br>Finally, we apply the <code>ReLU</code> function to $x_{conv}$, then use max-pooling to reduce this to a single vector $x_{conv_out} \in \mathbb{R}^{e_{word}}$, which is the final output of the Convolutional Network:<br>$$x_{conv_out} = MaxPool(ReLU(x_{conv})) \in \mathbb{R}^{e_{word}}$$<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 5</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### YOUR CODE HERE for part 1i</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">        Init CNN which is a 1-D cnn.</span></span><br><span class="line"><span class="string">        @param embed_size (int): embedding size of char (dimensionality)</span></span><br><span class="line"><span class="string">        @param k: kernel size, also called window size</span></span><br><span class="line"><span class="string">        @param f: number of filters, should be embed_size of word</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">            embed_size: <span class="built_in">int</span> = <span class="number">50</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 m_word: <span class="built_in">int</span> = <span class="number">21</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 k: <span class="built_in">int</span> = <span class="number">5</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 f: <span class="built_in">int</span> = <span class="number">50</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">        Init CNN which is a 1-D cnn.</span></span><br><span class="line"><span class="string">        @param embed_size (int): embedding size of char (dimensionality)</span></span><br><span class="line"><span class="string">        @param k: kernel size, also called window size</span></span><br><span class="line"><span class="string">        @param f: number of filters, should be embed_size of word</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.conv1d = nn.Conv1d(in_channels = embed_size, </span><br><span class="line">                    out_channels = f,</span><br><span class="line">                    kernel_size = k)</span><br><span class="line"></span><br><span class="line">        self.maxpool = nn.MaxPool1d(kernel_size = m_word - k + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X_reshaped: torch.Tensor</span>) -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        map from X_reshaped to X_conv_out</span></span><br><span class="line"><span class="string">        @param X_reshaped (Tensor): Tensor of char-level embedding with shape ( </span></span><br><span class="line"><span class="string">                                    batch_size, e_char, m_word), where e_char = embed_size of char, </span></span><br><span class="line"><span class="string">                                    m_word = max_word_length.</span></span><br><span class="line"><span class="string">        @return X_conv_out (Tensor): Tensor of word-level embedding with shape (max_sentence_length,</span></span><br><span class="line"><span class="string">                                    batch_size)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        X_conv = self.conv1d(X_reshaped)</span><br><span class="line">        <span class="comment"># print(X_conv.size())</span></span><br><span class="line">        X_conv_out = self.maxpool(F.relu(X_conv))</span><br><span class="line">        <span class="comment"># print(X_conv_out.size())</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X_conv_out.squeeze(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li>
<li><strong>Highway layer and dropout</strong>. Highway Networks6 have a skip-connection controlled by a dynamic gate. Given the input $x_{conv\<em>out} \in \mathbb{R}^{e</em>{word}}$, we compute:<center>
 <img style="border-radius: 0.3125em;
 box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
 src="4.png" width = "70%" height="70%">
 <div style="color:orange; border-bottom: 1px solid #d9d9d9;
 display: inline-block;
 color: #999;
 padding: 2px;">Figure from cs224n. Highway Network (Srivastava et al. 2015)</div>
</center>
$$
\begin{align}
& x_{proj} = RELU(W_{proj}x_{conv\_cout} + b_{proj}) \quad \in \mathbb{R}^{e_{word}} \\
& x_{gate} = \sigma(W_{gate}x_{conv\_out} + b_{gate}) \quad \in \mathbb{R}^{e_{word}} \\
& x_{highway} = x_{gate} \circ x_{proj} + ( 1 - x_{gate}) \circ x_{conv\_out}  \quad \in \mathbb{R}^{e_{word}}\\
& x_{word_emb} = Dropout(x_{highway}) \quad \in \mathbb{R}^{e_{word}} 
\end{align}
$$
Where $W_{proj},W_{gate} \in \mathbb{R}^{e_{word} \times e_{word}}$, and $\circ$ denotes element-wise multiplication.
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 5</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment">### YOUR CODE HERE for part 1h</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Highway</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Highway network for ConvNN</span></span><br><span class="line"><span class="string">        - Relu</span></span><br><span class="line"><span class="string">        - Sigmoid</span></span><br><span class="line"><span class="string">        - gating mechanism from LSTM</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,embed_size</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Init Higway network</span></span><br><span class="line"><span class="string">            @param embed_size (int): Embedding size of word, in handout, </span></span><br><span class="line"><span class="string">                                     it&#x27;s e_&#123;word&#125; (dimensionality)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(Highway, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.projection = nn.Linear(embed_size,embed_size,bias = <span class="literal">True</span>)</span><br><span class="line">        self.gate = nn.Linear(embed_size,embed_size, bias = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X_conv_out</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            Take mini-batch of sentence of ConvNN</span></span><br><span class="line"><span class="string">            @param X_conv_out (Tensor): Tensor with shape (max_sentence_length, batch_size, embed_size)</span></span><br><span class="line"><span class="string">            @return X_highway (Tensor): combinded output with shape (max_sentence_length, batch_size, embed_size)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        X_proj = F.relu(self.projection(X_conv_out))</span><br><span class="line">        X_gate = torch.sigmoid(self.gate(X_conv_out))</span><br><span class="line">        X_highway =  torch.mul(X_gate, X_proj) + torch.mul((<span class="number">1</span> - X_gate),X_conv_out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X_highway</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### END YOUR CODE </span></span><br></pre></td></tr></table></figure></li>
<li>Combine above steps together to get our <strong>Character-based word embedding model</strong>.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 5</span></span><br><span class="line"><span class="string">model_embeddings.py: Embeddings for the NMT model</span></span><br><span class="line"><span class="string">Pencheng Yin &lt;pcyin@cs.cmu.edu&gt;</span></span><br><span class="line"><span class="string">Sahil Chopra &lt;schopra8@stanford.edu&gt;</span></span><br><span class="line"><span class="string">Anand Dhoot &lt;anandd@stanford.edu&gt;</span></span><br><span class="line"><span class="string">Michael Hahn &lt;mhahn2@stanford.edu&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not change these imports; your module names should be</span></span><br><span class="line"><span class="comment">#   `CNN` in the file `cnn.py`</span></span><br><span class="line"><span class="comment">#   `Highway` in the file `highway.py`</span></span><br><span class="line"><span class="comment"># Uncomment the following two imports once you&#x27;re ready to run part 1(j)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> cnn <span class="keyword">import</span> CNN</span><br><span class="line"><span class="keyword">from</span> highway <span class="keyword">import</span> Highway</span><br><span class="line"></span><br><span class="line"><span class="comment"># End &quot;do not change&quot; </span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelEmbeddings</span>(<span class="params">nn.Module</span>):</span> </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Class that converts input words to their CNN-based embeddings.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_size, vocab</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Init the Embedding layer for one language</span></span><br><span class="line"><span class="string">        @param embed_size (int): Embedding size (dimensionality) for the output </span></span><br><span class="line"><span class="string">        @param vocab (VocabEntry): VocabEntry object. See vocab.py for documentation.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(ModelEmbeddings, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment">## A4 code</span></span><br><span class="line">        <span class="comment"># pad_token_idx = vocab.src[&#x27;&lt;pad&gt;&#x27;]</span></span><br><span class="line">        <span class="comment"># self.embeddings = nn.Embedding(len(vocab.src), embed_size, padding_idx=pad_token_idx)</span></span><br><span class="line">        <span class="comment">## End A4 code</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 1j</span></span><br><span class="line">        pad_token_idx = vocab.char2id[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        char_embed_size = <span class="number">50</span></span><br><span class="line">        self.char_embedding = nn.Embedding(<span class="built_in">len</span>(vocab.char2id),</span><br><span class="line">                                           char_embed_size,</span><br><span class="line">                                           pad_token_idx)</span><br><span class="line">        self.convNN = CNN(f=self.embed_size)</span><br><span class="line">        self.highway = Highway(embed_size=self.embed_size)</span><br><span class="line">        self.dropout = nn.Dropout(p=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Looks up character-based CNN embeddings for the words in a batch of sentences.</span></span><br><span class="line"><span class="string">        @param input: Tensor of integers of shape (sentence_length, batch_size, max_word_length) where</span></span><br><span class="line"><span class="string">            each integer is an index into the character vocabulary</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param output: Tensor of shape (sentence_length, batch_size, embed_size), containing the </span></span><br><span class="line"><span class="string">            CNN-based embeddings for each word of the sentences in the batch</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">## A4 code</span></span><br><span class="line">        <span class="comment"># output = self.embeddings(input)</span></span><br><span class="line">        <span class="comment"># return output</span></span><br><span class="line">        <span class="comment">## End A4 code</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 1j</span></span><br><span class="line">        X_word_emb_list = []</span><br><span class="line">        <span class="keyword">for</span> X_padded <span class="keyword">in</span> <span class="built_in">input</span>:</span><br><span class="line">            <span class="comment"># (batch_size,max_word_length) -&gt; (batch_size,max_word_length,embed_size)</span></span><br><span class="line">            X_emb = self.char_embedding(X_padded)</span><br><span class="line">            <span class="comment"># print(X_emb.size())</span></span><br><span class="line">            X_reshaped = X_emb.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">            X_conv_out = self.convNN(X_reshaped)</span><br><span class="line">            X_highway = self.highway(X_conv_out)</span><br><span class="line">            X_word_emb = self.dropout(X_highway)</span><br><span class="line">            X_word_emb_list.append(X_word_emb)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (sentence_length, batch_size, embed_size)</span></span><br><span class="line">        X_word_emb = torch.stack(X_word_emb_list, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X_word_emb</span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="Character-based-LSTM-decoder-for-NMT"><a href="#Character-based-LSTM-decoder-for-NMT" class="headerlink" title="Character-based LSTM decoder for NMT"></a>Character-based LSTM decoder for NMT</h3><p>We will now add a LSTM-based character-level decoder to our NMT system. The main idea is that when our word-level decoder produces and <code>&lt;UNK&gt;</code> token, we run our character-level decoder (which you can think of as a character-level conditional language model) to instead generate the target word one character at a time, as shown in below figure. This will help us to produce rare and out-of-vocabulary target words.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="5.png" width = "50%" height="50%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Figure from cs224n. A character-based decoder which is triggered if the word-based decoder produces an UNK. Figure courtesy of Luong & Manning.</div>
</center>

<p>We now describe the model in three sections:</p>
<ol>
<li><p><strong>Forward computation of Character Decoder</strong>: Given a sequence of integers $x_i,\cdots,x_n \in \mathbb{Z}$ representing a sequence of characters, we lookup their character embeddings $x_i,\cdots,x_n \in \mathbb{Z}^{e_{char}}$ and pass these as input in to the(unidirectional)LSTM,obtaining hidden states $h1, \cdots, h_n$ and cell states $c_1, \cdots, c_n$<br>$$h_t,c_t = CharDecoderLSTM(x_t,h_{t-1},c_{t-1}) \quad \text{where} \quad h_t,c_t \in \mathbb{R}^{h}$$<br>where h is the hidden size of the CharDecoderLSTM. The initial hidden and cell states $h_0$ and $c_0$ are both set to the <strong>combined output</strong> vector (attentioned) for the current timestep of the main word-level NMT decoder.<br>For every timestep $t \in { 1, \cdots, n }$ we compute scores (also called logits) $s_t \in \mathbb{R}^{V_{char}}$<br>$$s_t = W_{dec}h_t + b_{dec} \in \mathbb{R}^{V_{char}}$$<br>Where the weight matrix $W_{dec} \in \mathbb{R}^{V_{char} \times h}$ and the bias vector $b_{dec} \in \mathbb{R}^{V_{char}}$. If we passed $s_t$ through a softmax function, we would have the probability distribution for the next character in the sequence.</p>
</li>
<li><p><strong>Training of Character Decoder</strong> When we train the NMT system, we train the character decoder on <strong>every word</strong> in the target sentence (not just the words represented by <UNK>). For example, on a particular step of the main NMT decoder, if the target word is music then the input sequence for the CharDecoderLSTM is $[x_1,…,x_n]$ = [<START>,m,u,s,i,c] and the target sequence for the CharDecoderLSTM is $[x_{2}, . . . , x_{n+1}]$ = [m,u,s,i,c,<END>].<br>We pass the input sequence $x_1, \cdots, x_n$, along with the initial states $h_0$ and $c_0$ obtained from the combined output vector) into the CharDecoderLSTM, thus obtaining scores $s_1,\cdots, s_n$ which we will compare to the target sequence $x_2,\cdots, x_{n+1}$. We optimize with respect to sum of the cross-entropy loss:<br>$$p_t = softmax(s_t) \in \mathbb{R}^{V_{char}}$$<br>$$loss_{char_dec} = -\sum_{t=1}^{n}log p_t(x_{t+1})$$</p>
</li>
<li><p><strong>Decoding from the Character Decoder</strong> t test time, first we produce a translation from our word- based NMT system in the usual way (e.g. a decoding algorithm like beam search). If the translation contains any <UNK> tokens, then for each of those positions, we use the word-based decoder’s combined output vector to initialize the CharDecoderLSTM initial $h_0$ and $c_0$, then use CharDecoderLSTM to generate a sequence of characters. To generate the sequence of characters, we use the greedy decoding algorithm, which repeatedly chooses the most probable next character, until either the <END> token is produced or we reach a predetermined max length. The algorithm is given below, for a single example (not batched).</p>
<center>
 <img style="border-radius: 0.3125em;
 box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
 src="6.png" width = "100%" height="100%">
 <div style="color:orange; border-bottom: 1px solid #d9d9d9;
 display: inline-block;
 color: #999;
 padding: 2px;">Figure from cs224n. Greedy Decoding</div>
</center></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 5</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CharDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_size, char_embedding_size=<span class="number">50</span>, target_vocab=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Init Character Decoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param hidden_size (int): Hidden size of the decoder LSTM</span></span><br><span class="line"><span class="string">        @param char_embedding_size (int): dimensionality of character embeddings</span></span><br><span class="line"><span class="string">        @param target_vocab (VocabEntry): vocabulary for the target language. See vocab.py for documentation.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 2a</span></span><br><span class="line">        <span class="comment">### TODO - Initialize as an nn.Module.</span></span><br><span class="line">        <span class="comment">###      - Initialize the following variables:</span></span><br><span class="line">        <span class="comment">###        self.charDecoder: LSTM. Please use nn.LSTM() to construct this.</span></span><br><span class="line">        <span class="comment">###        self.char_output_projection: Linear layer, called W_&#123;dec&#125; and b_&#123;dec&#125; in the PDF</span></span><br><span class="line">        <span class="comment">###        self.decoderCharEmb: Embedding matrix of character embeddings</span></span><br><span class="line">        <span class="comment">###        self.target_vocab: vocabulary for the target language</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Hint: - Use target_vocab.char2id to access the character vocabulary for the target language.</span></span><br><span class="line">        <span class="comment">###       - Set the padding_idx argument of the embedding matrix.</span></span><br><span class="line">        <span class="comment">###       - Create a new Embedding layer. Do not reuse embeddings created in Part 1 of this assignment.</span></span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>(CharDecoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.char_embedding_size = char_embedding_size</span><br><span class="line">        self.target_vocab = target_vocab</span><br><span class="line">        self.padding_idx = self.target_vocab.char2id[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]</span><br><span class="line">        self.decoderCharEmb = nn.Embedding(<span class="built_in">len</span>(self.target_vocab.char2id),</span><br><span class="line">                                           char_embedding_size,</span><br><span class="line">                                           self.padding_idx)</span><br><span class="line">        self.charDecoder = nn.LSTM(input_size=char_embedding_size,</span><br><span class="line">                                   hidden_size=hidden_size)</span><br><span class="line">        self.char_output_projection = nn.Linear(hidden_size,</span><br><span class="line">                                                <span class="built_in">len</span>(self.target_vocab.char2id))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, dec_hidden=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Forward pass of character decoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param input: tensor of integers, shape (length, batch)</span></span><br><span class="line"><span class="string">        @param dec_hidden: internal state of the LSTM before reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns scores: called s_t in the PDF, shape (length, batch, self.vocab_size)</span></span><br><span class="line"><span class="string">        @returns dec_hidden: internal state of the LSTM after reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 2b</span></span><br><span class="line">        <span class="comment">### TODO - Implement the forward pass of the character decoder.</span></span><br><span class="line">        char_embeddings = self.decoderCharEmb(<span class="built_in">input</span>)        <span class="comment"># (length, batch, char_embed_size)</span></span><br><span class="line">        hidden_states, dec_hidden = self.charDecoder(</span><br><span class="line">            char_embeddings, dec_hidden)    <span class="comment"># (length, batch, hidden_size)</span></span><br><span class="line">        scores = self.char_output_projection(hidden_states)     <span class="comment"># (len, batch, vocab)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> scores, dec_hidden</span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_forward</span>(<span class="params">self, char_sequence, dec_hidden=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Forward computation during training.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param char_sequence: tensor of integers, shape (length, batch). Note that &quot;length&quot; here and in forward() need not be the same.</span></span><br><span class="line"><span class="string">        @param dec_hidden: initial internal state of the LSTM, obtained from the output of the word-level decoder. A tuple of two tensors of shape (1, batch, hidden_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns The cross-entropy loss, computed as the *sum* of cross-entropy losses of all the words in the batch.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 2c</span></span><br><span class="line">        <span class="comment">### TODO - Implement training forward pass.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Hint: - Make sure padding characters do not contribute to the cross-entropy loss.</span></span><br><span class="line">        <span class="comment">###       - char_sequence corresponds to the sequence x_1 ... x_&#123;n+1&#125; from the handout (e.g., &lt;START&gt;,m,u,s,i,c,&lt;END&gt;).</span></span><br><span class="line">        scores, dec_hidden = self.forward(char_sequence[:-<span class="number">1</span>], dec_hidden)</span><br><span class="line">        <span class="comment"># char_embeddings = self.decoderCharEmb(char_sequence)</span></span><br><span class="line">        <span class="comment"># hidden_states, dec_hidden = self.charDecoder(char_embeddings[:-1], dec_hidden)</span></span><br><span class="line">        <span class="comment"># scores = self.char_output_projection(hidden_states)  # (len, batch, vocab)</span></span><br><span class="line">        loss = nn.CrossEntropyLoss(ignore_index=self.padding_idx,</span><br><span class="line">                                   reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">        ce_loss = loss(scores.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">                       char_sequence[<span class="number">1</span>:].transpose(<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ce_loss</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode_greedy</span>(<span class="params">self, initialStates, device, max_length=<span class="number">21</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Greedy decoding</span></span><br><span class="line"><span class="string">        @param initialStates: initial internal state of the LSTM, a tuple of two tensors of size (1, batch, hidden_size)</span></span><br><span class="line"><span class="string">        @param device: torch.device (indicates whether the model is on CPU or GPU)</span></span><br><span class="line"><span class="string">        @param max_length: maximum length of words to decode</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns decodedWords: a list (of length batch) of strings, each of which has length &lt;= max_length.</span></span><br><span class="line"><span class="string">                              The decoded strings should NOT contain the start-of-word and end-of-word characters.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE for part 2d</span></span><br><span class="line">        <span class="comment">### TODO - Implement greedy decoding.</span></span><br><span class="line">        <span class="comment">### Hints:</span></span><br><span class="line">        <span class="comment">###      - Use target_vocab.char2id and target_vocab.id2char to convert between integers and characters</span></span><br><span class="line">        <span class="comment">###      - Use torch.tensor(..., device=device) to turn a list of character indices into a tensor.</span></span><br><span class="line">        <span class="comment">###      - We use curly brackets as start-of-word and end-of-word characters. That is, use the character &#x27;&#123;&#x27; for &lt;START&gt; and &#x27;&#125;&#x27; for &lt;END&gt;.</span></span><br><span class="line">        <span class="comment">###        Their indices are self.target_vocab.start_of_word and self.target_vocab.end_of_word, respectively.</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">        output_words = []</span><br><span class="line">        decodedWords = []</span><br><span class="line">        start_idx = self.target_vocab.start_of_word</span><br><span class="line">        end_idx = self.target_vocab.end_of_word</span><br><span class="line">        dec_hidden = initialStates</span><br><span class="line">        batch_size = dec_hidden[<span class="number">0</span>].shape[<span class="number">1</span>]</span><br><span class="line">        current_char = torch.tensor([[start_idx] * batch_size],</span><br><span class="line">                                    device=device)  <span class="comment"># idx of &#x27;&lt;start&gt;&#x27; token</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_length):</span><br><span class="line">            scores, dec_hidden = self.forward(current_char, dec_hidden)</span><br><span class="line">            current_char = scores.argmax(-<span class="number">1</span>)</span><br><span class="line">            output_words += [current_char]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        output_words = torch.cat(output_words).t().tolist()</span><br><span class="line">        <span class="keyword">for</span> foo <span class="keyword">in</span> output_words:</span><br><span class="line">            word = <span class="string">&quot;&quot;</span></span><br><span class="line">            <span class="keyword">for</span> bar <span class="keyword">in</span> foo:</span><br><span class="line">                <span class="keyword">if</span> bar == end_idx:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                word += self.target_vocab.id2char[bar]</span><br><span class="line">            decodedWords += [word]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> decodedWords</span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li>Course note and slides of <a target="_blank" rel="noopener" href="http://web.stanford.edu/class/cs224n/">cs224n</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Attention/2019/12/16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Attention/2019/12/16/" class="post-title-link" itemprop="url">Attention</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-16 20:55:07" itemprop="dateCreated datePublished" datetime="2019-12-16T20:55:07+08:00">2019-12-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-20 02:10:20" itemprop="dateModified" datetime="2019-12-20T02:10:20+08:00">2019-12-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Attention/2019/12/16/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Attention/2019/12/16/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="General-definition-of-attention"><a href="#General-definition-of-attention" class="headerlink" title="General definition of attention"></a>General definition of attention</h2><p>Given a set of vector <strong>values</strong>, and a vector <strong>query</strong>, attention is a technique to compute a <strong>weighted sum</strong> of the values, dependent on the query.</p>
<ul>
<li>We sometimes say that the query attends to the values.</li>
<li>For example, in the seq2seq + attention model, each decoder hidden state (query) attends to all the encoder hidden states<br>75 (values).<ul>
<li>The weighted sum is a <strong>selective</strong> summary of the information contained in the values, where the query determines which values to focus on.</li>
<li>Attention is a way to obtain a <strong>fixed-size representation</strong> of an arbitrary set of representations (the values), dependent on some other representation (the query).</li>
</ul>
</li>
</ul>
<h2 id="How-to-do-attention"><a href="#How-to-do-attention" class="headerlink" title="How to do attention"></a>How to do attention</h2><ol>
<li>We have some <strong>values</strong> $h1$,$\cdots$,$h_N$ $\in \mathbb{R}^{d_1}$ and a <strong>query</strong> $s \in \mathbb{R}^{d_2}$</li>
<li>Computing the attention scores (multiple ways to do this)<br>$$e \in \mathbb{R}^{N}$$</li>
<li>Taking softmax to get attention distribution $\alpha$<br>$$\alpha = softmax(e) \in \mathbb{R}^{N}$$</li>
<li>Using attention distribution to take weighted sum of values:<br>$$a = \sum_{i=1}^{N}\alpha_i h_i \in \mathbb{R}^{d_1}$$<br>thus obtaining the attention output a (sometimes called the <strong>context vector</strong>)</li>
</ol>
<h2 id="Bidirectional-RNNs"><a href="#Bidirectional-RNNs" class="headerlink" title="Bidirectional RNNs"></a>Bidirectional RNNs</h2><center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="1.png" width = "50%" height="50%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">picture from lecture notes of cs224n</div>
    <br>
    <br>
</center>

<p>Bidirectional RNNs fix this problem by traversing a sequence in both directions and concatenating the resulting outputs (both cell outputs and final hidden states). For every RNN cell, we simply add another cell but feed inputs to it in the opposite direction; the output $O_t$ corresponding to the $t\prime$ word is the concatenated vector $\left [ o_t^{(f)}, o_t^{(b)}  \right ]$ where $o_t^{(f)}$ is the output of the forward-direction RNN on word t and $o_t^{(b)}$ is the corresponding output from the reverse-direction RNN. Similarly, the final hidden state is $h = \left [   h^{(f)}, h^{(b)}  \right ]$.</p>
<h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><p>Sequence-to-sequence, or “Seq2Seq”, is a relatively new paradigm,with its first published usage in 2014 for English-French translation. At a high level, a sequence-to-sequence model is an end-to-end model made up of two recurrent neural networks:<br>Sutskever et al. 2014, “Sequence to Sequence Learning with Neural Networks”</p>
<ul>
<li>an encoder, which takes the model’s input sequence as input and encodes it into a fixed-size “context vector”</li>
<li>a decoder, which uses the context vector from above as a “seed” from which to generate an output sequence.<br>For this reason, Seq2Seq models are often referred to as “encoder- decoder models.” We’ll look at the details of these two networks separately.</li>
</ul>
<h3 id="Seq2Seq-architecture-encoder"><a href="#Seq2Seq-architecture-encoder" class="headerlink" title="Seq2Seq architecture - encoder"></a>Seq2Seq architecture - encoder</h3><blockquote>
<p>Encoder RNN produces an encoding of the source sentence.</p>
</blockquote>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="2.png" width = "50%" height="50%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">picture from lecture notes of cs224n</div>
    <br>
    <br>
</center>

<p>The encoder network’s job is to read the input sequence to our Seq2Seq model and generate a fixed-dimensional context vector <strong>C</strong> for the sequence. To do so, the encoder will use a recurrent neural network cell – usually an LSTM – to read the input tokens one at a time. The final hidden state of the cell will then become C. However, because it’s so difficult to compress an arbitrary-length sequence into a single fixed-size vector (especially for difficult tasks like transla- tion), the encoder will usually consist of stacked LSTMs: a series of LSTM “layers” where each layer’s outputs are the input sequence to the next layer. The final layer’s LSTM hidden state will be used as <strong>C</strong>.</p>
<p>Seq2Seq encoders will often do something strange: they will pro- cess the input sequence in reverse. This is actually done on purpose. The idea is that, by doing this, the last thing that the encoder sees will (roughly) corresponds to the first thing that the model outputs; this makes it easier for the decoder to “get started” on the output, which makes then gives the decoder an easier time generating a proper output sentence. In the context of translation, we’re allowing the network to translate the first few words of the input as soon as it sees them; once it has the first few words translated correctly, it’s much easier to go on to construct a correct sentence than it is to do so from scratch.</p>
<h3 id="Seq2Seq-architecture-decoder"><a href="#Seq2Seq-architecture-decoder" class="headerlink" title="Seq2Seq architecture - decoder"></a>Seq2Seq architecture - decoder</h3><blockquote>
<p>Decoder RNN is a Language Model that generates target sentence, conditioned on encoding.</p>
</blockquote>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="3.png" width = "50%" height="50%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">picture from lecture notes of cs224n</div>
    <br>
    <br>
</center>

<p>The decoder is also an LSTM network, but its usage is a little more complex than the encoder network. Essentially, we’d like to use it as a <strong>language model</strong> that’s “aware” of the words that it’s generated so far and of the input. To that end, we’ll keep the “stacked” LSTM architecture from the encoder, but we’ll initialize the hidden state of our first layer with the context vector from above; the decoder will literally use the context of the input to generate an output.</p>
<p>Once the decoder is set up with its context, we’ll pass in a special token to signify the start of output generation; in literature, this is usually an <EOS> token appended to the end of the input (there’s also one at the end of the output). Then, we’ll run all three layers of LSTM, one after the other, following up with a softmax on the final layer’s output to generate the first output word. Then, we pass that word into the first layer, and repeat the generation. This is how we get the LSTMs to act like a language model. See Fig. 2 for an example of a decoder network.</p>
<p>Once we have the output sequence, we use the same learning strat- egy as usual. We define a loss, the cross entropy on the prediction sequence, and we minimize it with a gradient descent algorithm and back-propagation. Both the encoder and decoder are trained at the same time, so that they both learn the same context vector represen- tation.</p>
<h2 id="Training-a-Neural-Machine-Translation-system"><a href="#Training-a-Neural-Machine-Translation-system" class="headerlink" title="Training a Neural Machine Translation system"></a>Training a Neural Machine Translation system</h2><center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="4.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">picture from lecture notes of cs224n</div>
    <br>
    <br>
</center>

<h3 id="Greedy-Search"><a href="#Greedy-Search" class="headerlink" title="Greedy Search"></a>Greedy Search</h3><p>At each time step, we pick the most probable token. In other words<br>$$x_t = argmax_{\tilde{x_t} \mathbb{P}(\tilde(x_t)| x_1, \cdots, x_t)}$$</p>
<p>This technique is efficient and natural, however it explores a small part of the search space and if we make a mistake at one time step, the rest of the sentence could be heavily impacted.</p>
<h3 id="Beam-search-decoding"><a href="#Beam-search-decoding" class="headerlink" title="Beam search decoding"></a>Beam search decoding</h3><center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="5.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">picture from lecture notes of cs224n</div>
    <br>
    <br>
</center>

<p>the idea is to maintain K candidates at each time step.</p>
<p>$$ H_t = \left{ (x_1^{1}, \cdots, x_t^1), \cdots, (x_1^k, \cdots, x_t^k) \right}$$</p>
<p>and compute $H_{t+1}$ by expanding $H_t$ and keeping the best K candi- dates. In other words, we pick the best K sequence in the following set</p>
<p>$$\tilde{H_{t+1}} = \cup_{k=1}^{k}H_{t+1}^{\tilde{k}}$$</p>
<p>where<br>$$ \tilde{H_t} = \left{ (x_1^{k}, \cdots, x_t^{k}, v_1), \cdots, (x_1^{k}, \cdots, x_t^{k}, V_{|v|}) \right}$$</p>
<p>As we increase K, we gain precision and we are asymptotically exact. However, the improvement is not monotonic and we can set a K that combines reasonable performance and computational efficiency. </p>
<h2 id="CS224n-Assignment4"><a href="#CS224n-Assignment4" class="headerlink" title="CS224n Assignment4"></a>CS224n Assignment4</h2><p>In Machine Translation, our goal is to convert a sentence from the source language (e.g. Spanish) to the target language (e.g. English). In this assignment, we will implement a sequence-to-sequence (Seq2Seq) network with attention, to build a Neural Machine Translation (NMT) system. In this section, we describe the training procedure for the proposed NMT system, which uses a Bidirectional LSTM Encoder and a Unidirectional LSTM Decoder.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="6.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">picture from lecture notes of cs224n</div>
    <br>
    <br>
</center>




<h3 id="Initialize"><a href="#Initialize" class="headerlink" title="Initialize"></a>Initialize</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_size, hidden_size, vocab, dropout_rate=<span class="number">0.2</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Init NMT Model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param embed_size (int): Embedding size (dimensionality)</span></span><br><span class="line"><span class="string">        @param hidden_size (int): Hidden Size (dimensionality)</span></span><br><span class="line"><span class="string">        @param vocab (Vocab): Vocabulary object containing src and tgt languages</span></span><br><span class="line"><span class="string">                              See vocab.py for documentation.</span></span><br><span class="line"><span class="string">        @param dropout_rate (float): Dropout probability, for attention</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(NMT, self).__init__()</span><br><span class="line">        self.model_embeddings = ModelEmbeddings(embed_size, vocab)</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.dropout_rate = dropout_rate</span><br><span class="line">        self.vocab = vocab</span><br><span class="line"></span><br><span class="line">        <span class="comment"># default values</span></span><br><span class="line">        self.encoder = <span class="literal">None</span> </span><br><span class="line">        self.decoder = <span class="literal">None</span></span><br><span class="line">        self.h_projection = <span class="literal">None</span></span><br><span class="line">        self.c_projection = <span class="literal">None</span></span><br><span class="line">        self.att_projection = <span class="literal">None</span></span><br><span class="line">        self.combined_output_projection = <span class="literal">None</span></span><br><span class="line">        self.target_vocab_projection = <span class="literal">None</span></span><br><span class="line">        self.dropout = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~8 Lines)</span></span><br><span class="line">        <span class="comment">### TODO - Initialize the following variables:</span></span><br><span class="line">        <span class="comment">###     self.encoder (Bidirectional LSTM with bias)</span></span><br><span class="line">        <span class="comment">###     self.decoder (LSTM Cell with bias)</span></span><br><span class="line">        <span class="comment">###     self.h_projection (Linear Layer with no bias), called W_&#123;h&#125; in the PDF.</span></span><br><span class="line">        <span class="comment">###     self.c_projection (Linear Layer with no bias), called W_&#123;c&#125; in the PDF.</span></span><br><span class="line">        <span class="comment">###     self.att_projection (Linear Layer with no bias), called W_&#123;attProj&#125; in the PDF.</span></span><br><span class="line">        <span class="comment">###     self.combined_output_projection (Linear Layer with no bias), called W_&#123;u&#125; in the PDF.</span></span><br><span class="line">        <span class="comment">###     self.target_vocab_projection (Linear Layer with no bias), called W_&#123;vocab&#125; in the PDF.</span></span><br><span class="line">        <span class="comment">###     self.dropout (Dropout Layer)</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Use the following docs to properly initialize these variables:</span></span><br><span class="line">        <span class="comment">###     LSTM:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM</span></span><br><span class="line">        <span class="comment">###     LSTM Cell:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell</span></span><br><span class="line">        <span class="comment">###     Linear Layer:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear</span></span><br><span class="line">        <span class="comment">###     Dropout Layer:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout</span></span><br><span class="line"></span><br><span class="line">        self.encoder = nn.LSTM(embed_size, self.hidden_size, dropout=self.dropout_rate,bias = <span class="literal">True</span>, bidirectional = <span class="literal">True</span>)</span><br><span class="line">        self.decoder = nn.LSTMCell(embed_size + self.hidden_size, self.hidden_size, bias = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.h_projection = nn.Linear(<span class="number">2</span> * self.hidden_size, self.hidden_size, bias = <span class="literal">False</span>)</span><br><span class="line">        self.c_projection = nn.Linear(<span class="number">2</span> * self.hidden_size, self.hidden_size, bias = <span class="literal">False</span>)</span><br><span class="line">        self.att_projection = nn.Linear(<span class="number">2</span> * self.hidden_size, self.hidden_size, bias = <span class="literal">False</span>)</span><br><span class="line">        self.combined_output_projection = nn.Linear(<span class="number">3</span> * self.hidden_size, self.hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.target_vocab_projection = nn.Linear(self.hidden_size, self.model_embeddings.target.weight.shape[<span class="number">0</span>])</span><br><span class="line">        self.dropout = nn.Dropout(p = self.dropout_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br></pre></td></tr></table></figure>


<h3 id="Encode"><a href="#Encode" class="headerlink" title="Encode"></a>Encode</h3><p>Given a sentence in the source language, we look up the word embeddings from an embeddings matrix, yielding $x_1,\cdots,x_m | x_i \in \mathbb{R}^{e x 1}$,  where m is the length of the source sentence and e is the embedding size. We feed these embeddings to the bidirectional Encoder, yielding hidden states and cell states for both the forwards (-&gt;) and backwards (&lt;-) LSTMs. The forwards and backwards versions are concatenated<br>to give hidden states $h_i^{enc}$ and cell states $c_i^{enc}$</p>
<p>$$<br>\begin{align}<br>&amp; h_i^{enc} = \left [  \overleftarrow{h_i^{enc}}; \overrightarrow{h_i^{enc}} \right ] \qquad \text{where} \qquad h_i^{enc} \in \mathbb{R}^{2h x 1},  \overleftarrow{h_i^{enc}}, \overrightarrow{h_i^{enc}} \in \mathbb{R}^{h x 1}  \qquad 1 \leq i \leq m  \<br>&amp; c_i^{enc} = \left [  \overleftarrow{c_i^{enc}}; \overrightarrow{c_i^{enc}} \right ] \qquad \text{where}  \qquad c_i^{enc} \in \mathbb{R}^{2h x 1},  \overleftarrow{c_i^{enc}}, \overrightarrow{c_i^{enc}} \in \mathbb{R}^{h x 1}  \qquad 1 \leq i \leq m \<br>\end{align}<br>$$</p>
<p>We then initialize the Decoder’s first hidden state $h_0^{dec}$ and cell state $c_0^{dec}$ with a linear projection of the Encoder’s final hidden state and final cell state</p>
<p>$$<br>\begin{align}<br>&amp; h_0^{dec} = W_h \left [  \overleftarrow{h_1^{enc}}; \overrightarrow{h_m^{enc}} \right ] \qquad \text{where} \qquad h_0^{dec} \in \mathbb{R}^{h x 1},  W_h \in \mathbb{R}^{h x 2h} \<br>&amp; c_0^{dec} = W_h \left [  \overleftarrow{c_1^{enc}}; \overrightarrow{c_m^{enc}} \right ] \qquad \text{where} \qquad c_0^{dec} \in \mathbb{R}^{h x 1},  W_c \in \mathbb{R}^{h x 2h} \<br>\end{align}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, source_padded: torch.Tensor, source_lengths: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, <span class="type">Tuple</span>[torch.Tensor, torch.Tensor]]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Apply the encoder to source sentences to obtain encoder hidden states.</span></span><br><span class="line"><span class="string">            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where</span></span><br><span class="line"><span class="string">                                        b = batch_size, src_len = maximum source sentence length. Note that </span></span><br><span class="line"><span class="string">                                       these have already been sorted in order of longest to shortest sentence.</span></span><br><span class="line"><span class="string">        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch</span></span><br><span class="line"><span class="string">        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where</span></span><br><span class="line"><span class="string">                                        b = batch size, src_len = maximum source sentence length, h = hidden size.</span></span><br><span class="line"><span class="string">        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder&#x27;s initial</span></span><br><span class="line"><span class="string">                                                hidden state and cell.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        enc_hiddens, dec_init_state = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~ 8 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1. Construct Tensor `X` of source sentences with shape (src_len, b, e) using the source model embeddings.</span></span><br><span class="line">        <span class="comment">###         src_len = maximum source sentence length, b = batch size, e = embedding size. Note</span></span><br><span class="line">        <span class="comment">###         that there is no initial hidden state or cell for the decoder.</span></span><br><span class="line">        <span class="comment">###     2. Compute `enc_hiddens`, `last_hidden`, `last_cell` by applying the encoder to `X`.</span></span><br><span class="line">        <span class="comment">###         - Before you can apply the encoder, you need to apply the `pack_padded_sequence` function to X.</span></span><br><span class="line">        <span class="comment">###         - After you apply the encoder, you need to apply the `pad_packed_sequence` function to enc_hiddens.</span></span><br><span class="line">        <span class="comment">###         - Note that the shape of the tensor returned by the encoder is (src_len b, h*2) and we want to</span></span><br><span class="line">        <span class="comment">###           return a tensor of shape (b, src_len, h*2) as `enc_hiddens`.</span></span><br><span class="line">        <span class="comment">###     3. Compute `dec_init_state` = (init_decoder_hidden, init_decoder_cell):</span></span><br><span class="line">        <span class="comment">###         - `init_decoder_hidden`:</span></span><br><span class="line">        <span class="comment">###             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.</span></span><br><span class="line">        <span class="comment">###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).</span></span><br><span class="line">        <span class="comment">###             Apply the h_projection layer to this in order to compute init_decoder_hidden.</span></span><br><span class="line">        <span class="comment">###             This is h_0^&#123;dec&#125; in the PDF. Here b = batch size, h = hidden size</span></span><br><span class="line">        <span class="comment">###         - `init_decoder_cell`:</span></span><br><span class="line">        <span class="comment">###             `last_cell` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.</span></span><br><span class="line">        <span class="comment">###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).</span></span><br><span class="line">        <span class="comment">###             Apply the c_projection layer to this in order to compute init_decoder_cell.</span></span><br><span class="line">        <span class="comment">###             This is c_0^&#123;dec&#125; in the PDF. Here b = batch size, h = hidden size</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### See the following docs, as you may need to use some of the following functions in your implementation:</span></span><br><span class="line">        <span class="comment">###     Pack the padded sequence X before passing to the encoder:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence</span></span><br><span class="line">        <span class="comment">###     Pad the packed sequence, enc_hiddens, returned by the encoder:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence</span></span><br><span class="line">        <span class="comment">###     Tensor Concatenation:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.cat</span></span><br><span class="line">        <span class="comment">###     Tensor Permute:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute</span></span><br><span class="line"></span><br><span class="line">        X = self.model_embeddings.source(source_padded)</span><br><span class="line">        output, (h_enc, c_enc) = self.encoder(</span><br><span class="line">            pack_padded_sequence(X, source_lengths))</span><br><span class="line">        enc_hiddens,sequence_length = pad_packed_sequence(output, batch_first = <span class="literal">True</span>) <span class="comment"># output of shape (batch, seq_len, num_directions * hidden_size)</span></span><br><span class="line">        h_0_dec = self.h_projection(torch.cat((h_enc[<span class="number">0</span>,:],h_enc[<span class="number">1</span>,:]), <span class="number">1</span>))</span><br><span class="line">        c_0_dec = self.c_projection(torch.cat((c_enc[<span class="number">0</span>,:],c_enc[<span class="number">1</span>,:]), <span class="number">1</span>))</span><br><span class="line">        dec_init_state = (h_0_dec,c_0_dec)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> enc_hiddens, dec_init_state</span><br></pre></td></tr></table></figure>

<h3 id="Decode"><a href="#Decode" class="headerlink" title="Decode"></a>Decode</h3><p>With the Decoder initialized, we must now feed it a matching sentence in the target language. On the $t^{th}$ step, we look up the embedding for the $t^{th}$ word, $y_t \in \mathbb{R}^{e x 1}$, we then concatenate $y_t$ with the combined-output vector $O_{t-1} \in \mathbb{R}^{h x 1}$ from the previous step to produce $\bar{y_t} \in \mathbb{R}^{(e+h) x 1}$. Note that for the first target word $O_0$ is zero-vector. We then fedd $\bar{y_t}$ as input to the Decoder LSTM.</p>
<p>$$h_t^{dec}, c_t^{dec} = Decoder(\bar{y_t},h_{t-1}^{dec}, c_{t-1}^{dec} ) \quad \text{where} \quad h_t^{dec} \in \mathbb{R}^{h x 1}$$</p>
<p><strong>We then use $h_t^{dec}$ to compute multiplicative attention ovev $h_t^{enc}, \cdots, h_m^{enc}$</strong></p>
<p>$$\begin{align}<br>&amp; e_{t_i} = (h_t^{dec})^{T}W_{attProj}h_i^{enc} \quad \text{where} \quad e_t \in \mathbb{R}^{m x 1}, W_{attProj} \in \mathbb{R}^{h x 2h} \<br>&amp; \alpha_{t} = Softmax(e_t) \quad \text{where} \quad \alpha_t \in \mathbb{R}^{m x 1} \<br>&amp; a_t = \sum_i^{m} \alpha_{t,i}h_i^{enc}  \quad \text{where} \quad a_t \in \mathbb{R}^{2h x 1}\<br>\end{align}<br>$$</p>
<p>We now <strong>concatenate</strong> the attention output $a_t$ with the decoder hidden state $h_t^{dec}$ and pass this through a linear layer, Tanh, and Dropout to attain the <strong>combined-output vector</strong> $o_t$</p>
<p>$$\begin{align}<br>&amp; u_t = \left[ a_t; h_t^{dec} \right ]  \quad \text{where} \quad u_t \in \mathbb{R}^{3h x 1} \<br>&amp; v_t = W_u u_t \quad \text{where} \quad v_t \in \mathbb{R}^{h x 1}, W_u \in \mathbb{R}^{h x 1} \<br>&amp; O_t = Dropout(Tanh(v_t)) \quad \text{where} \quad o_t \in \mathbb{R}^{h x 1} \<br>\end{align}$$</p>
<p>Then, we produce a probability distribution $P_t$ over target words at the $t^{th}$ timestep:<br>$$P_t = Softmax(W_{vocab}O_t)  \quad \text{where} \quad P_t \in \mathbb{R}^{v_t x h} $$</p>
<p>Here, $V_t$ is the size of  the target vocabulary. Finally, to train the network we then compute the softmax cross entropy loss between $P_t$ and $g_t$, where $g_t$ is the 1-hot vector of the target word at timestep t:</p>
<p>$$J(\theta) = CE(P_t, g_t)$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">                dec_init_state: <span class="type">Tuple</span>[torch.Tensor, torch.Tensor], target_padded: torch.Tensor</span>) -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute combined output vectors for a batch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where</span></span><br><span class="line"><span class="string">                                     b = batch size, src_len = maximum source sentence length, h = hidden size.</span></span><br><span class="line"><span class="string">        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where</span></span><br><span class="line"><span class="string">                                     b = batch size, src_len = maximum source sentence length.</span></span><br><span class="line"><span class="string">        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder</span></span><br><span class="line"><span class="string">        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where</span></span><br><span class="line"><span class="string">                                       tgt_len = maximum target sentence length, b = batch size. </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where</span></span><br><span class="line"><span class="string">                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Chop of the &lt;END&gt; token for max length sentences.</span></span><br><span class="line">        target_padded = target_padded[:-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize the decoder state (hidden and cell)</span></span><br><span class="line">        dec_state = dec_init_state</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize previous combined output vector o_&#123;t-1&#125; as zero</span></span><br><span class="line">        batch_size = enc_hiddens.size(<span class="number">0</span>)</span><br><span class="line">        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize a list we will use to collect the combined output o_t on each step</span></span><br><span class="line">        combined_outputs = []</span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~9 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1. Apply the attention projection layer to `enc_hiddens` to obtain `enc_hiddens_proj`,</span></span><br><span class="line">        <span class="comment">###         which should be shape (b, src_len, h),</span></span><br><span class="line">        <span class="comment">###         where b = batch size, src_len = maximum source length, h = hidden size.</span></span><br><span class="line">        <span class="comment">###         This is applying W_&#123;attProj&#125; to h^enc, as described in the PDF.</span></span><br><span class="line">        <span class="comment">###     2. Construct tensor `Y` of target sentences with shape (tgt_len, b, e) using the target model embeddings.</span></span><br><span class="line">        <span class="comment">###         where tgt_len = maximum target sentence length, b = batch size, e = embedding size.</span></span><br><span class="line">        <span class="comment">###     3. Use the torch.split function to iterate over the time dimension of Y.</span></span><br><span class="line">        <span class="comment">###         Within the loop, this will give you Y_t of shape (1, b, e) where b = batch size, e = embedding size.</span></span><br><span class="line">        <span class="comment">###             - Squeeze Y_t into a tensor of dimension (b, e). </span></span><br><span class="line">        <span class="comment">###             - Construct Ybar_t by concatenating Y_t with o_prev.</span></span><br><span class="line">        <span class="comment">###             - Use the step function to compute the the Decoder&#x27;s next (cell, state) values</span></span><br><span class="line">        <span class="comment">###               as well as the new combined output o_t.</span></span><br><span class="line">        <span class="comment">###             - Append o_t to combined_outputs</span></span><br><span class="line">        <span class="comment">###             - Update o_prev to the new o_t.</span></span><br><span class="line">        <span class="comment">###     4. Use torch.stack to convert combined_outputs from a list length tgt_len of</span></span><br><span class="line">        <span class="comment">###         tensors shape (b, h), to a single tensor shape (tgt_len, b, h)</span></span><br><span class="line">        <span class="comment">###         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Note:</span></span><br><span class="line">        <span class="comment">###    - When using the squeeze() function make sure to specify the dimension you want to squeeze</span></span><br><span class="line">        <span class="comment">###      over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.</span></span><br><span class="line">        <span class="comment">###   </span></span><br><span class="line">        <span class="comment">### Use the following docs to implement this functionality:</span></span><br><span class="line">        <span class="comment">###     Zeros Tensor:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.zeros</span></span><br><span class="line">        <span class="comment">###     Tensor Splitting (iteration):</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.split</span></span><br><span class="line">        <span class="comment">###     Tensor Dimension Squeezing:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.squeeze</span></span><br><span class="line">        <span class="comment">###     Tensor Concatenation:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.cat</span></span><br><span class="line">        <span class="comment">###     Tensor Stacking:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.stack</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#   (b, src_len, h*2) * [2h , h]  = (b, src_len, h)</span></span><br><span class="line">        enc_hiddens_proj = self.att_projection(enc_hiddens)</span><br><span class="line">        <span class="comment">#   (tgt_len, b, e)</span></span><br><span class="line">        Y = self.model_embeddings.target(target_padded)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> Y_t <span class="keyword">in</span> torch.split(Y, split_size_or_sections = <span class="number">1</span>, dim = <span class="number">0</span>):</span><br><span class="line">            squeezed_Y_t = torch.squeeze(Y_t) <span class="comment"># (b, e) + (b,h) = (b,e+h)</span></span><br><span class="line">            Ybar_t = torch.cat((o_prev,squeezed_Y_t), dim = <span class="number">1</span>)</span><br><span class="line">            dec_state, o_t, _ = self.step(Ybar_t,dec_state,enc_hiddens,enc_hiddens_proj,enc_masks)</span><br><span class="line">            combined_outputs.append(o_t)</span><br><span class="line">            o_prev = o_t</span><br><span class="line"></span><br><span class="line">        <span class="comment">#  (b, h) -&gt; (tgt_len, b, h)</span></span><br><span class="line">        combined_outputs = torch.stack(combined_outputs,dim = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> combined_outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, Ybar_t: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">            dec_state: <span class="type">Tuple</span>[torch.Tensor, torch.Tensor],</span></span></span><br><span class="line"><span class="params"><span class="function">            enc_hiddens: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">            enc_hiddens_proj: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">            enc_masks: torch.Tensor</span>) -&gt; <span class="type">Tuple</span>[<span class="type">Tuple</span>, torch.Tensor, torch.Tensor]:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Compute one forward step of the LSTM decoder, including the attention computation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,</span></span><br><span class="line"><span class="string">                                where b = batch size, e = embedding size, h = hidden size.</span></span><br><span class="line"><span class="string">        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.</span></span><br><span class="line"><span class="string">                First tensor is decoder&#x27;s prev hidden state, second tensor is decoder&#x27;s prev cell.</span></span><br><span class="line"><span class="string">        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,</span></span><br><span class="line"><span class="string">                                    src_len = maximum source length, h = hidden size.</span></span><br><span class="line"><span class="string">        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),</span></span><br><span class="line"><span class="string">                                    where b = batch size, src_len = maximum source length, h = hidden size.</span></span><br><span class="line"><span class="string">        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),</span></span><br><span class="line"><span class="string">                                    where b = batch size, src_len is maximum source length. </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.</span></span><br><span class="line"><span class="string">                First tensor is decoder&#x27;s new hidden state, second tensor is decoder&#x27;s new cell.</span></span><br><span class="line"><span class="string">        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.</span></span><br><span class="line"><span class="string">        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.</span></span><br><span class="line"><span class="string">                                Note: You will not use this outside of this function.</span></span><br><span class="line"><span class="string">                                      We are simply returning this value so that we can sanity check</span></span><br><span class="line"><span class="string">                                      your implementation.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        combined_output = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~3 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1. Apply the decoder to `Ybar_t` and `dec_state`to obtain the new dec_state.</span></span><br><span class="line">        <span class="comment">###     2. Split dec_state into its two parts (dec_hidden, dec_cell)</span></span><br><span class="line">        <span class="comment">###     3. Compute the attention scores e_t, a Tensor shape (b, src_len). </span></span><br><span class="line">        <span class="comment">###        Note: b = batch_size, src_len = maximum source length, h = hidden size.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">###       Hints:</span></span><br><span class="line">        <span class="comment">###         - dec_hidden is shape (b, h) and corresponds to h^dec_t in the PDF (batched)</span></span><br><span class="line">        <span class="comment">###         - enc_hiddens_proj is shape (b, src_len, h) and corresponds to W_&#123;attProj&#125; h^enc (batched).</span></span><br><span class="line">        <span class="comment">###         - Use batched matrix multiplication (torch.bmm) to compute e_t.</span></span><br><span class="line">        <span class="comment">###         - To get the tensors into the right shapes for bmm, you will need to do some squeezing and unsqueezing.</span></span><br><span class="line">        <span class="comment">###         - When using the squeeze() function make sure to specify the dimension you want to squeeze</span></span><br><span class="line">        <span class="comment">###             over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Use the following docs to implement this functionality:</span></span><br><span class="line">        <span class="comment">###     Batch Multiplication:</span></span><br><span class="line">        <span class="comment">###        https://pytorch.org/docs/stable/torch.html#torch.bmm</span></span><br><span class="line">        <span class="comment">###     Tensor Unsqueeze:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.unsqueeze</span></span><br><span class="line">        <span class="comment">###     Tensor Squeeze:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.squeeze</span></span><br><span class="line"></span><br><span class="line">        dec_state = self.decoder(Ybar_t, dec_state)</span><br><span class="line">        h_t_dec, c_t_dec = dec_state</span><br><span class="line">        <span class="comment">#  enc_hiddens_proj(b, src_len, h) * h_t_dec (b,h,1) = (b,src_len)</span></span><br><span class="line">        e_t = torch.squeeze(torch.bmm(enc_hiddens_proj, torch.unsqueeze(h_t_dec,<span class="number">2</span>)),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set e_t to -inf where enc_masks has 1</span></span><br><span class="line">        <span class="keyword">if</span> enc_masks <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            e_t.data.masked_fill_(enc_masks.byte(), -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~6 Lines)</span></span><br><span class="line">        <span class="comment">### <span class="doctag">TODO:</span></span></span><br><span class="line">        <span class="comment">###     1. Apply softmax to e_t to yield alpha_t</span></span><br><span class="line">        <span class="comment">###     2. Use batched matrix multiplication between alpha_t and enc_hiddens to obtain the</span></span><br><span class="line">        <span class="comment">###         attention output vector, a_t.</span></span><br><span class="line">        <span class="comment">#$$     Hints:</span></span><br><span class="line">        <span class="comment">###           - alpha_t is shape (b, src_len)</span></span><br><span class="line">        <span class="comment">###           - enc_hiddens is shape (b, src_len, 2h)</span></span><br><span class="line">        <span class="comment">###           - a_t should be shape (b, 2h)</span></span><br><span class="line">        <span class="comment">###           - You will need to do some squeezing and unsqueezing.</span></span><br><span class="line">        <span class="comment">###     Note: b = batch size, src_len = maximum source length, h = hidden size.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">###     3. Concatenate dec_hidden with a_t to compute tensor U_t</span></span><br><span class="line">        <span class="comment">###     4. Apply the combined output projection layer to U_t to compute tensor V_t</span></span><br><span class="line">        <span class="comment">###     5. Compute tensor O_t by first applying the Tanh function and then the dropout layer.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Use the following docs to implement this functionality:</span></span><br><span class="line">        <span class="comment">###     Softmax:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.functional.softmax</span></span><br><span class="line">        <span class="comment">###     Batch Multiplication:</span></span><br><span class="line">        <span class="comment">###        https://pytorch.org/docs/stable/torch.html#torch.bmm</span></span><br><span class="line">        <span class="comment">###     Tensor View:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view</span></span><br><span class="line">        <span class="comment">###     Tensor Concatenation:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.cat</span></span><br><span class="line">        <span class="comment">###     Tanh:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/torch.html#torch.tanh</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># (b,src_len)</span></span><br><span class="line">        alpha_t = nn.functional.softmax(e_t, dim = <span class="number">1</span>) </span><br><span class="line">        <span class="comment"># alpha_t(b,src_len) - (b,1,src_len) * enc_hiddens(b, src_len, h * 2) = (b, 1, h * 2) -&gt; (b,2h)</span></span><br><span class="line">        a_t = torch.squeeze(torch.bmm(torch.unsqueeze(alpha_t,<span class="number">1</span>),enc_hiddens),<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#(b,2h) + (b,h)</span></span><br><span class="line">        U_t = torch.cat((a_t,h_t_dec), dim = <span class="number">1</span>)</span><br><span class="line">        V_t = self.combined_output_projection(U_t)</span><br><span class="line">        O_t = self.dropout(nn.functional.tanh(V_t))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br><span class="line">        combined_output = O_t</span><br><span class="line">        <span class="keyword">return</span> dec_state, combined_output, e_t</span><br></pre></td></tr></table></figure>

<h3 id="Helpers"><a href="#Helpers" class="headerlink" title="Helpers"></a>Helpers</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, source: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]], target: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]]</span>) -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Take a mini-batch of source and target sentences, compute the log-likelihood of</span></span><br><span class="line"><span class="string">        target sentences under the language models learned by the NMT system.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param source (List[List[str]]): list of source sentence tokens</span></span><br><span class="line"><span class="string">        @param target (List[List[str]]): list of target sentence tokens, wrapped by `&lt;s&gt;` and `&lt;/s&gt;`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the</span></span><br><span class="line"><span class="string">                                    log-likelihood of generating the gold-standard target sentence for</span></span><br><span class="line"><span class="string">                                    each example in the input batch. Here b = batch size.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Compute sentence lengths</span></span><br><span class="line">        source_lengths = [<span class="built_in">len</span>(s) <span class="keyword">for</span> s <span class="keyword">in</span> source]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert list of lists into tensors</span></span><br><span class="line">        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   <span class="comment"># Tensor: (src_len, b)</span></span><br><span class="line">        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   <span class="comment"># Tensor: (tgt_len, b)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">###     Run the network forward:</span></span><br><span class="line">        <span class="comment">###     1. Apply the encoder to `source_padded` by calling `self.encode()`</span></span><br><span class="line">        <span class="comment">###     2. Generate sentence masks for `source_padded` by calling `self.generate_sent_masks()`</span></span><br><span class="line">        <span class="comment">###     3. Apply the decoder to compute combined-output by calling `self.decode()`</span></span><br><span class="line">        <span class="comment">###     4. Compute log probability distribution over the target vocabulary using the</span></span><br><span class="line">        <span class="comment">###        combined_outputs returned by the `self.decode()` function.</span></span><br><span class="line"></span><br><span class="line">        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)</span><br><span class="line">        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)</span><br><span class="line">        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)</span><br><span class="line">        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Zero out, probabilities for which we have nothing in the target text</span></span><br><span class="line">        target_masks = (target_padded != self.vocab.tgt[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]).<span class="built_in">float</span>()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute log probability of generating true target words</span></span><br><span class="line">        target_gold_words_log_prob = torch.gather(P, index=target_padded[<span class="number">1</span>:].unsqueeze(-<span class="number">1</span>), dim=-<span class="number">1</span>).squeeze(-<span class="number">1</span>) * target_masks[<span class="number">1</span>:]</span><br><span class="line">        scores = target_gold_words_log_prob.<span class="built_in">sum</span>(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">CS224N 2018-19: Homework 4</span></span><br><span class="line"><span class="string">model_embeddings.py: Embeddings for the NMT model</span></span><br><span class="line"><span class="string">Pencheng Yin &lt;pcyin@cs.cmu.edu&gt;</span></span><br><span class="line"><span class="string">Sahil Chopra &lt;schopra8@stanford.edu&gt;</span></span><br><span class="line"><span class="string">Anand Dhoot &lt;anandd@stanford.edu&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelEmbeddings</span>(<span class="params">nn.Module</span>):</span> </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Class that converts input words to their embeddings.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_size, vocab</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Init the Embedding layers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        @param embed_size (int): Embedding size (dimensionality)</span></span><br><span class="line"><span class="string">        @param vocab (Vocab): Vocabulary object containing src and tgt languages</span></span><br><span class="line"><span class="string">                              See vocab.py for documentation.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(ModelEmbeddings, self).__init__()</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># default values</span></span><br><span class="line">        self.source = <span class="literal">None</span></span><br><span class="line">        self.target = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        src_pad_token_idx = vocab.src[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]</span><br><span class="line">        tgt_pad_token_idx = vocab.tgt[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">### YOUR CODE HERE (~2 Lines)</span></span><br><span class="line">        <span class="comment">### TODO - Initialize the following variables:</span></span><br><span class="line">        <span class="comment">###     self.source (Embedding Layer for source language)</span></span><br><span class="line">        <span class="comment">###     self.target (Embedding Layer for target langauge)</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Note:</span></span><br><span class="line">        <span class="comment">###     1. `vocab` object contains two vocabularies:</span></span><br><span class="line">        <span class="comment">###            `vocab.src` for source</span></span><br><span class="line">        <span class="comment">###            `vocab.tgt` for target</span></span><br><span class="line">        <span class="comment">###     2. You can get the length of a specific vocabulary by running:</span></span><br><span class="line">        <span class="comment">###             `len(vocab.&lt;specific_vocabulary&gt;)`</span></span><br><span class="line">        <span class="comment">###     3. Remember to include the padding token for the specific vocabulary</span></span><br><span class="line">        <span class="comment">###        when creating your Embedding.</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">### Use the following docs to properly initialize these variables:</span></span><br><span class="line">        <span class="comment">###     Embedding Layer:</span></span><br><span class="line">        <span class="comment">###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding</span></span><br><span class="line">        self.source = nn.Embedding(<span class="built_in">len</span>(vocab.src),self.embed_size, padding_idx = src_pad_token_idx)</span><br><span class="line">        self.target = nn.Embedding(<span class="built_in">len</span>(vocab.tgt), self.embed_size, padding_idx = tgt_pad_token_idx) </span><br><span class="line">        <span class="comment">### END YOUR CODE</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_sents</span>(<span class="params">sents, pad_token</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Pad list of sentences according to the longest sentence in the batch.</span></span><br><span class="line"><span class="string">    @param sents (list[list[str]]): list of sentences, where each sentence</span></span><br><span class="line"><span class="string">                                    is represented as a list of words</span></span><br><span class="line"><span class="string">    @param pad_token (str): padding token</span></span><br><span class="line"><span class="string">    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter</span></span><br><span class="line"><span class="string">        than the max length sentence are padded out with the pad_token, such that</span></span><br><span class="line"><span class="string">        each sentences in the batch now has equal length.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    sents_padded = []</span><br><span class="line"></span><br><span class="line">    <span class="comment">### YOUR CODE HERE (~6 Lines)</span></span><br><span class="line">    max_sentence_len = <span class="built_in">max</span>([<span class="built_in">len</span>(s) <span class="keyword">for</span> s <span class="keyword">in</span> sents])</span><br><span class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> sents:</span><br><span class="line">        sents_padded.append(sent + [pad_token] * (max_sentence_len - <span class="built_in">len</span>(sent)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sents_padded</span><br></pre></td></tr></table></figure>






<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ol>
<li>course slides and notes from cs224n (<a target="_blank" rel="noopener" href="http://web.stanford.edu/class/cs224n/">http://web.stanford.edu/class/cs224n/</a>)</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Gated-RNN-Units/2019/12/15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Gated-RNN-Units/2019/12/15/" class="post-title-link" itemprop="url">Gated RNN Units</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-15 01:40:39" itemprop="dateCreated datePublished" datetime="2019-12-15T01:40:39+08:00">2019-12-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-20 02:09:48" itemprop="dateModified" datetime="2019-12-20T02:09:48+08:00">2019-12-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Gated-RNN-Units/2019/12/15/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Gated-RNN-Units/2019/12/15/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><p>RNNs have been found to perform better with the use of more complex units for activation. Here, we discuss the use of a gated activation function thereby modifying the RNN architecture. What motivates this? Well, although RNNs can theoretically capture long-term dependencies, they are very hard to actually train to do this. Gated recurrent units are designed in a manner to have more persistent memory thereby making it easier for RNNs to capture long-term dependencies. Let us see mathematically how a GRU uses $h_{t−1}$ and $x_t$ to generate the next hidden state ht. We will then dive into the intuition of this architecture.</p>
<p>$$\begin{aligned}<br>&amp; z_t = \sigma(W^{(z)}x_t + U^{(z)}h_{t-1}) &amp; \text{(Update gate)} \<br>&amp; r_t = \sigma(W^{(r)}x_t + U^{(r)}h_{t-1})  &amp; \text{(Reset gate)}\<br>&amp; \tilde{h_t} = tanh{r_t \circ Uh_{t-1} + Wx_t} &amp; \text{(New memory)} \<br>&amp; h_t = (1 - z_t) \circ \tilde{h_t} + z_t \circ h_{t-1} &amp; \text{(Hidden state)}<br>\end{aligned}$$</p>
<p>The above equations can be thought of a GRU’s four fundamental operational stages and they have intuitive interpretations that make this model much more intellectually.</p>
<ol>
<li><strong>Reset gate</strong>: controls what parts of previous hidden state are used to compute new content</li>
<li><strong>Update gate</strong>: controls what parts of hidden state are updated vs preserved</li>
<li><strong>New hidden state content</strong>: reset gate selects useful parts of prev hidden state. Use this and current input to compute new hidden content.</li>
<li><strong>Hidden state</strong>: update gate simultaneously controls what is kept from previous hidden state, and what is updated to new hidden state content</li>
</ol>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>Long-Short-Term-Memories are another type of complex activation unit that differ a little from GRUs. The motivation for using these is similar to those for GRUs however the architecture of such units does differ. Let us first take a look at the mathematical formulation of LSTM units before diving into the intuition behind this design:</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="LSTM3.png" width = "80%" height="80%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">slides of cs224n</div>
</center>

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="LSTM1.png" width = "80%" height="80%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/</div>
</center>

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="LSTM2.png" width = "80%" height="80%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/</div>
</center>

<ol>
<li>The LSTM architecture makes it easier for the RNN to preserve information over many timesteps<ul>
<li>e.g. if the forget gate is set to remember everything on every timestep, then the info in the cell is preserved indefinitely</li>
<li>By contrast, it’s harder for vanilla RNN to learn a recurrent weight matrix Wh that preserves info in hidden state</li>
</ul>
</li>
<li>LSTM doesn’t guarantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies.</li>
</ol>
<h2 id="LSTM-vs-GRU"><a href="#LSTM-vs-GRU" class="headerlink" title="LSTM vs GRU"></a>LSTM vs GRU</h2><ul>
<li>Researchers have proposed many gated RNN variants, but LSTM and GRU are the most widely-used</li>
<li>The biggest difference is that GRU is quicker to compute and has fewer parameters</li>
<li>There is no conclusive evidence that one consistently performs better than the other</li>
<li>LSTM is a good default choice (especially if your data has particularly long dependencies, or you have lots of training data)</li>
<li><strong>Rule of thumb</strong>: start with LSTM, but switch to GRU if you want something more efficient</li>
</ul>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ol>
<li>course slides and notes from cs224n (<a target="_blank" rel="noopener" href="http://web.stanford.edu/class/cs224n/">http://web.stanford.edu/class/cs224n/</a>)</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Human-Factor/2019/12/12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Human-Factor/2019/12/12/" class="post-title-link" itemprop="url">Human Factor</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2019-12-12 04:00:04 / Modified: 17:02:16" itemprop="dateCreated datePublished" datetime="2019-12-12T04:00:04+08:00">2019-12-12</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Product-Design/" itemprop="url" rel="index"><span itemprop="name">Product Design</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Human-Factor/2019/12/12/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Human-Factor/2019/12/12/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Course-Description"><a href="#Course-Description" class="headerlink" title="Course Description"></a>Course Description</h2><p> This course provides an introduction to human factors research and applications with emphasis on mature areas such as sensation and perception and manual control. <strong>Each class will introduce some concrete human factors problem and explore theory and application relevant to solving it</strong>. The term long conceptual design assignment is intended to help maintain focus on applications to design.</p>
<h2 id="Week1"><a href="#Week1" class="headerlink" title="Week1"></a>Week1</h2><p>Learning Objectives:</p>
<ol>
<li><strong>Theoretical understanding</strong>: Develop an understanding of the historical context, disciplines, and schools of thought that led to the development of the current field of human factors engineering/psychology; Develop an acquaintance with basic elements of <strong>scientific method</strong> and <strong>decision making</strong> including tests of hypotheses, dependent and independent variables, and inferential and descriptive use of statistics. </li>
<li><strong>Apply theory and skills</strong>: Use scientific methods including selection of measures and experimental tasks to evaluate the utility of a variety of pointing devices</li>
<li><strong>Proficiency in information related skills</strong>: Learn how to conduct a critical incident-based evaluation of an interactive system and to perform a standard task analysis for such a system</li>
</ol>
<h3 id="Research-Methods"><a href="#Research-Methods" class="headerlink" title="Research Methods"></a>Research Methods</h3><ol>
<li>IV &amp; DV</li>
<li>Cause vs. Chance</li>
<li>Descriptive vs. Inferential Statistics</li>
<li>Null hypothesis(H0) &amp; Experimentalhypothesis(H1)</li>
<li>Hypothesis Testing</li>
<li>Statistical Significance &amp; Practical Significance</li>
<li>Graph &amp; Interpetation</li>
</ol>
<h3 id="Goals-of-System-Evaluation"><a href="#Goals-of-System-Evaluation" class="headerlink" title="Goals of System Evaluation"></a>Goals of System Evaluation</h3><ul>
<li>Functionality<ul>
<li>Can it do what it is supposed to do?</li>
</ul>
</li>
<li>Usability<ul>
<li>Does it make the task easier?</li>
</ul>
</li>
<li>Diagnosticity<ul>
<li>Does it pinpoint what is wrong?</li>
</ul>
</li>
</ul>
<ol>
<li>User Population<ul>
<li>Who are they?</li>
<li>What are their goals?</li>
<li>What do they already know how to do?</li>
<li>Ask, don’t assume!</li>
</ul>
</li>
<li>Research Objectives<ul>
<li>Generalizability</li>
<li>Precision</li>
<li>Realism</li>
</ul>
</li>
<li>Usability Testing</li>
<li>Design an Experiment</li>
</ol>
<h3 id="Two-Models-for-Human-Factors"><a href="#Two-Models-for-Human-Factors" class="headerlink" title="Two Models for Human Factors"></a>Two Models for Human Factors</h3><ul>
<li>System component (computer)</li>
<li>Embedded organism (cybernetic)</li>
</ul>
<h3 id="Task-Analyses"><a href="#Task-Analyses" class="headerlink" title="Task Analyses"></a>Task Analyses</h3><p>为了研究如何使系统更好地和人的能力相匹配所进行的一种描述人机交互的方法</p>
<ul>
<li><strong>Sequential</strong><br>  任务的顺序和不同任务在时间序列上的关系<ul>
<li>Procedural</li>
<li>Therp</li>
</ul>
</li>
<li><strong>Hierarchical</strong><br>  描述一个大的任务如何由子任务组成以及这些任务又是如何联系起来体现其功能的<ul>
<li>HTA</li>
<li>GOMS</li>
<li>Cognitive</li>
</ul>
</li>
</ul>
<h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li>Types of Evaluation<ul>
<li>Laboratory Studies</li>
<li>Field Studies</li>
<li>Participatory Design/Rapid Prototyping</li>
<li>Brainstorming</li>
<li>StoryBoarding/Wizard of Oz</li>
<li>Workshops/Role Playing</li>
<li>Walkthrough/Talkthrough</li>
</ul>
</li>
<li>Observation Evaluation</li>
</ul>
<h4 id="Function-Allocation"><a href="#Function-Allocation" class="headerlink" title="Function Allocation"></a>Function Allocation</h4><p>每个功能由人来实现还是系统来实现</p>
<ul>
<li>Mandatory</li>
<li>Relative value</li>
<li>Cost based</li>
<li>Cognitive/affective</li>
</ul>
<h2 id="Week2-Sensation"><a href="#Week2-Sensation" class="headerlink" title="Week2 Sensation"></a>Week2 Sensation</h2><p>Learning Objectives:</p>
<ul>
<li>Psychophysical methods &amp; the problem of separating bias from measurement </li>
<li>Signal Detection: Perform specific calculations      <ul>
<li><strong>Conceptual model</strong> (associating hits, FA’s, misses, &amp; CR with areas under the curves) </li>
<li><strong>Computational understanding</strong> (interpreting problems, using table, &amp; finding d’) </li>
<li><strong>Vigilance</strong> - what the vigilance decrement is and how signal detection has been used to better understand it </li>
</ul>
</li>
<li>DESIGN: understanding of the cognitive information capabilities of humans General knowledge- wide range of sensitivity, approximately logarithmic, etc. </li>
<li>Place encoding of frequency and its consequences for: <ul>
<li>Masking </li>
<li>Threshold shift &amp; frequency related loss, etc. </li>
</ul>
</li>
<li>Measurement issues<ul>
<li>Loudness &amp; equal loudness contours </li>
<li>SPL meters </li>
<li>Articulation index, SIL, and other speech &amp; noise issues </li>
<li>Noise &amp; annoyance </li>
</ul>
</li>
</ul>
<h3 id="Classical-Errors"><a href="#Classical-Errors" class="headerlink" title="Classical Errors"></a>Classical Errors</h3><p>Error of Habituation</p>
<ul>
<li>Keep on saying the same thing<br>Error of Anticipation </li>
<li>Shift to new response</li>
</ul>
<h3 id="Signal-Detection-Model"><a href="#Signal-Detection-Model" class="headerlink" title="Signal Detection Model"></a>Signal Detection Model</h3><p><img src="resources/E893E7BC11981E867B4A94DA85979D8E.png" alt="Screen Shot 2019-10-24 at 00.00.37.png"><br><img src="resources/C2A377503B642EB567613413A81A15E0.png" alt="Screen Shot 2019-10-24 at 00.00.54.png"></p>
<h3 id="Vigilance"><a href="#Vigilance" class="headerlink" title="Vigilance"></a>Vigilance</h3><p>警报的设计必须建立在对人类的听觉加工充分了解的基础上</p>
<ul>
<li>Near threshold signals</li>
<li>Low rate of occurrence</li>
<li>Extended Watch</li>
<li>Inspection(definitetrial)</li>
<li>Free response time arbitrarily brokeninto intervals</li>
<li>Successive(noavailablestandard)</li>
<li>Simultaneous(signal to standard comparison)</li>
</ul>
<h3 id="Fatigue"><a href="#Fatigue" class="headerlink" title="Fatigue"></a>Fatigue</h3><ol>
<li>Sustained attention leads to fatigue</li>
<li>Load on working memory to keep target in mind depletes resources</li>
</ol>
<h3 id="Expectancy"><a href="#Expectancy" class="headerlink" title="Expectancy"></a>Expectancy</h3><h3 id="Remedies"><a href="#Remedies" class="headerlink" title="Remedies"></a>Remedies</h3><h3 id="Sound"><a href="#Sound" class="headerlink" title="Sound"></a>Sound</h3><p>Patterns of rare faction/compression of air</p>
<ol>
<li>Physics<ul>
<li>Intensity = amplitude</li>
<li>Frequency = cycles(hz)</li>
</ul>
</li>
<li>Perception<ul>
<li>Loudness</li>
<li>Pitch</li>
</ul>
</li>
</ol>
<ul>
<li>强度决定响度</li>
<li>频率决定音调</li>
<li>位置决定听觉定位</li>
<li>品质由频率及其掩蔽来决定</li>
</ul>
<h3 id="DECIBEL"><a href="#DECIBEL" class="headerlink" title="DECIBEL"></a>DECIBEL</h3><ul>
<li>dB(a): dB weighted by threshold equal loudness curve</li>
<li>dB(c)/dB (spl): dB with no weighting</li>
<li>dB(d) weighted by equal annoyance curves</li>
</ul>
<h3 id="Encoding-Pitch-by-Place"><a href="#Encoding-Pitch-by-Place" class="headerlink" title="Encoding Pitch by Place"></a>Encoding Pitch by Place</h3><h3 id="Auditory-Masking"><a href="#Auditory-Masking" class="headerlink" title="Auditory Masking"></a>Auditory Masking</h3><p>the presence of tone that inhibits the perception of another tone that occurs before, at the same time, or after it.</p>
<h2 id="Week3-Vision-and-Color"><a href="#Week3-Vision-and-Color" class="headerlink" title="Week3 Vision and Color"></a>Week3 Vision and Color</h2><p>Learning Objectives:</p>
<ul>
<li>Even more than place perception of pitch vision is all about relative differences (contrasts) and adaptation</li>
<li>The relation between what we experience and what is physically out there isn’t direct</li>
<li>Good Human Factors engineering requires designing so our users don’t notice</li>
</ul>
<p>Overview of vision</p>
<ul>
<li>Peripheral/central processing</li>
<li>Dark Adaptation &amp; Illusions &amp; their relation to the structure of the eye &amp; vision</li>
<li>Visual angle &amp; spatial frequency</li>
<li>VDTs, visual fatigue, &amp; ergonomic effects</li>
<li>Color measurement Munsell color wheel vs. CIE</li>
</ul>
<h3 id="Rods-and-Cones"><a href="#Rods-and-Cones" class="headerlink" title="Rods and Cones"></a>Rods and Cones</h3><ol>
<li>位置 location</li>
<li>视敏度 acuity（解析细节的能力）</li>
<li>敏感性 sensitivity (即使光很少，Rods也能工作)</li>
<li>color sensitivity （rods 是色盲）</li>
<li>adaption （rods 受光刺激的影响大）</li>
<li>diffrrential wavelength sensitivity (cones对所有光敏感，rods对红光不敏感）</li>
</ol>
<h3 id="对比敏感度"><a href="#对比敏感度" class="headerlink" title="对比敏感度"></a>对比敏感度</h3><p>c = (L-D) / (L+D)</p>
<h3 id="color-sensation"><a href="#color-sensation" class="headerlink" title="color sensation"></a>color sensation</h3><p>对单色进行设计，然后将颜色作为冗余编码信息提供</p>
<h3 id="dark-sensation"><a href="#dark-sensation" class="headerlink" title="dark sensation"></a>dark sensation</h3><p>当照明条件比较差时，所有空间评率的对比度都会降低</p>
<h3 id="检测"><a href="#检测" class="headerlink" title="检测"></a>检测</h3><ul>
<li>在很多系统中，影响作业绩效的重要因素是密切相关的两个过程：视觉搜索和物体或时间的检测</li>
<li>在视觉搜索领域，一旦某个项目被确定可能是目标，就必须对它是否是真的目标进行确认。</li>
<li>d’ 反映了一个操作者从噪音中分辨出信号的能力，它等于好结果的数目除以所有结果的总和</li>
</ul>
<h3 id="Peripheral-Processing"><a href="#Peripheral-Processing" class="headerlink" title="Peripheral Processing"></a>Peripheral Processing</h3><ul>
<li>Photo receptors are interconnected and can reciprocally inhibit one another</li>
<li>Can be tuned for featurere cognition</li>
</ul>
<h3 id="Adaptation"><a href="#Adaptation" class="headerlink" title="Adaptation"></a>Adaptation</h3><p>Adaptation is a major characteristic of sensation</p>
<h2 id="Week4-Reaction-time"><a href="#Week4-Reaction-time" class="headerlink" title="Week4 Reaction time"></a>Week4 Reaction time</h2><p>Learning Objectives:</p>
<ul>
<li>Subtractive&amp;additive factors analyses of choice reaction time</li>
<li>Limits&amp;values of psychological experimentation</li>
<li>Human bottleneck in choice responses</li>
<li>Power law of learning</li>
<li>Automaticvs.controlledresponding</li>
<li>Information theoretic interpretations of reaction time</li>
</ul>
<h3 id="Hick’s-Law"><a href="#Hick’s-Law" class="headerlink" title="Hick’s Law"></a>Hick’s Law</h3><p>Hick’s Law holds that choice reaction time is proportional to log2 of the number of alternatives.（反应时间是log2N的函数）<br>RT = a + blog2N</p>
<h3 id="Conventional-Controls-amp-Displays"><a href="#Conventional-Controls-amp-Displays" class="headerlink" title="Conventional Controls &amp; Displays"></a>Conventional Controls &amp; Displays</h3><ul>
<li>conventional practice in design of controls</li>
<li>displays Acquire familiarity with human factors design principles and heuristics </li>
</ul>
<h3 id="Control"><a href="#Control" class="headerlink" title="Control"></a>Control</h3><p>Controls are used by the human operator to communicate with the machine/device in the system. It’simportantthatcontrolsservetheirfunction. Based on:</p>
<ul>
<li>Ease of operation (considering population, biomechanics, etc.).</li>
<li>Nature of the task (force, precision, etc.). </li>
<li>Arrangement.</li>
</ul>
<h3 id="Basic-dimensions"><a href="#Basic-dimensions" class="headerlink" title="Basic dimensions:"></a>Basic dimensions:</h3><ul>
<li>Discrete (e.g., light switch) vs. continuous (e.g., dimmer)</li>
<li>Linear vs. rotary</li>
<li>Unidimensional vs. multidimensional</li>
<li>Isometric vs. isotonic</li>
<li>Plus mass, shape, range of motion, resistance to movement.</li>
</ul>
<h3 id="Control-Features"><a href="#Control-Features" class="headerlink" title="Control Features"></a>Control Features</h3><ol>
<li>Control resistance<ul>
<li>Elastic resistance: Spring loaded.<ul>
<li>Resistance increases as control gets farther from neutral.</li>
<li>Gives proprioceptive feedback about <strong>control position</strong>.</li>
<li>Returns to neutral when released (deadman switch).</li>
</ul>
</li>
<li>Frictional resistance<ul>
<li>Static friction for resting state, decreases when pushed. </li>
<li>Sliding friction not influenced by velocity or position.</li>
</ul>
</li>
<li>Viscous resistance<ul>
<li>Increases as a function of velocity.</li>
<li>Gives proprioceptive feedback about speed.</li>
<li>Promotes smooth movement.</li>
</ul>
</li>
<li>Inertial resistance<ul>
<li>Hard to start and stop.</li>
<li>Users tend to overshoot (revolving doors).</li>
</ul>
</li>
<li>Performance and resistance<ul>
<li>For frictional and inertial, the JND is 10%-20% of resting state.</li>
<li>Lighter controls preferred to heavy.</li>
<li>Viscous preferred to frictional.</li>
<li>For continuous, inertial hurts performance; </li>
<li>Elastic is the best.</li>
<li>For all of these, hard-and-fast rules are not available. It’s a function of the system.</li>
</ul>
</li>
</ul>
</li>
<li>Control-display ratio: <ul>
<li>Ratio of magnitude of control adjustment to magnitude of change in display.</li>
</ul>
</li>
<li>Gain: <ul>
<li>Responsiveness of control.</li>
</ul>
</li>
</ol>
<h3 id="Control-Panels"><a href="#Control-Panels" class="headerlink" title="Control Panels"></a>Control Panels</h3><ol>
<li>Location coding<ul>
<li>Need to be able to reliably distinguish locations.</li>
<li>Vertical localization is easier than horizontal.</li>
<li>Overuse of location coding is still a factor in some aircraft designs.</li>
</ul>
</li>
<li>Labels<ul>
<li>Not recommended as the sole code</li>
</ul>
</li>
<li>General rules for labels<ul>
<li>Locate labels systematically with respect to controls (all above, etc.).</li>
<li>Make labels brief.</li>
<li>Avoid abstract symbols; use standards.</li>
<li>Attend to fonts.</li>
<li>Position labels so they can be seen while the control is in use.</li>
</ul>
</li>
<li>Coding of controls<ul>
<li>Color coding</li>
<li>Shape coding</li>
<li>Size coding</li>
<li>Texture coding.<br>– Coding by type of operation.<br>– Redundant coding: Multiple dimensions</li>
</ul>
</li>
<li>Control arrangements<ul>
<li>Grouping is important.</li>
<li> Population stereotypes for control arrangements can be device specific</li>
<li> Attend to the reach envelope</li>
</ul>
</li>
<li>Preventing accidental operation</li>
<li>Specific Controls<ul>
<li>Hand operated controls</li>
<li>Foot operated controls</li>
<li>Specialized controls</li>
</ul>
</li>
</ol>
<h3 id="Visual-Displays"><a href="#Visual-Displays" class="headerlink" title="Visual Displays"></a>Visual Displays</h3><p>Display - anything that conveys information</p>
<ol>
<li>Requirements:<ul>
<li>Compatibility to senses</li>
<li>Language compatibility</li>
<li>Right info at the right time</li>
</ul>
</li>
<li>Types of information to display: <ul>
<li>Instructional</li>
<li>Command - direct orders </li>
<li>Advisory</li>
<li>Historical/predictive</li>
<li>Answers</li>
</ul>
</li>
<li>Functions of Dynamic Visual<br>Displays<ul>
<li>Continuous System Control</li>
<li>System Status Monitoring</li>
<li>Briefing</li>
<li>Search and Identification</li>
<li>Decision Making</li>
</ul>
</li>
<li>Visual Display Technology<ul>
<li>Mechanical </li>
<li>Electronic</li>
<li>Optical Projection</li>
</ul>
</li>
<li>General Display Principles<ul>
<li>Color</li>
<li>Shape</li>
<li>Coding</li>
<li>Approximation<ul>
<li>Get attention with one display<br>– Present detailed info with another</li>
</ul>
</li>
<li>Integration</li>
</ul>
</li>
</ol>
<h3 id="Principles-of-Display-Design"><a href="#Principles-of-Display-Design" class="headerlink" title="Principles of Display Design"></a>Principles of Display Design</h3><ul>
<li>Perceptual Principles<ul>
<li>Avoid absolute judgments</li>
<li>Top-down processing （信号的显示方式尽量与人的经验相符合）</li>
<li>Redundancy gain</li>
<li>Discriminability</li>
<li><img src="resources/088856BA3C1FA5A601838DAD16F27045.png" alt="Screen Shot 2019-12-11 at 14.23.03.png"></li>
</ul>
</li>
<li>Mental Model Principles （显示方式与操作员的心理模型一致，有助于提高正确操作）<ul>
<li>Pictorial realism （形如其表）</li>
<li>Principle of the moving part （运动一致）</li>
</ul>
</li>
<li>Principles Based on Attention<ul>
<li>Minimizing information access costs （将访问信息的消耗降到最低）</li>
<li>Proximity compatibility principle （接近相容原则）</li>
<li>Principle of multiple resources （要同时对多种信息进行加工时，可以将信息的呈现方式区分开）</li>
</ul>
</li>
<li>Memory Principles<ul>
<li>Principle of predictive aiding （预测辅助原则）</li>
<li>Principle of knowledge in the world （利用知识降低记忆负荷）</li>
<li>Principle of consistency （一致性原则）</li>
</ul>
</li>
</ul>
<ul>
<li>Two-Valued Info</li>
<li>Quantitative Information</li>
<li>Qualitative readings</li>
<li>Check Reading</li>
<li>Situation awareness</li>
</ul>
<h3 id="Three-Heuristics"><a href="#Three-Heuristics" class="headerlink" title="Three Heuristics"></a>Three Heuristics</h3><ol>
<li>Minimize Information</li>
<li>Promote Good C-D mappings</li>
<li>Provide Feedback</li>
</ol>
<h2 id="Spatial-amp-Integrative-Displays"><a href="#Spatial-amp-Integrative-Displays" class="headerlink" title="Spatial &amp; Integrative Displays"></a>Spatial &amp; Integrative Displays</h2><p>Learning Objectives:</p>
<ul>
<li><p>6 DOF &amp; moving &amp; orienting in 3 space</p>
</li>
<li><p>Problems with viewpoint &amp; situation awareness</p>
</li>
<li><p>Applications VR, games, &amp; robotics – Attitude &amp; pose</p>
</li>
<li><p>Navigation &amp; search</p>
</li>
<li><p>Cues to depth &amp; distance</p>
</li>
<li><p>Use of emergent features and perceptual salience to integrate displays</p>
</li>
<li><p>TMI and need to provide context to events</p>
</li>
</ul>
<h3 id="6-Degrees-of-Freedom-6DOF"><a href="#6-Degrees-of-Freedom-6DOF" class="headerlink" title="6 Degrees of Freedom (6DOF)"></a>6 Degrees of Freedom (6DOF)</h3><p>– Position (X,Y, Z)<br>– Orientation (Yaw, Pitch, Roll)</p>
<h3 id="3d-display-on-2d-panels"><a href="#3d-display-on-2d-panels" class="headerlink" title="3d display on 2d panels"></a>3d display on 2d panels</h3><ul>
<li>Depth is often poorly represented &amp; less discernable than other dimensions</li>
</ul>
<h3 id="Configural-Displays"><a href="#Configural-Displays" class="headerlink" title="Configural Displays"></a>Configural Displays</h3><ul>
<li>Low level data: usually individual sensor data</li>
<li>High level relation: a more global and general display of what the data means</li>
<li>Emergent property or emergent feature: a pattern or shape that is created from the low level data, is recognisable and has meaning</li>
</ul>
<h3 id="Separable"><a href="#Separable" class="headerlink" title="Separable"></a>Separable</h3><p>Show each variable as a single output</p>
<h3 id="Separable-vs-Configural-vs-Integral"><a href="#Separable-vs-Configural-vs-Integral" class="headerlink" title="Separable vs Configural vs Integral"></a>Separable vs Configural vs Integral</h3><ul>
<li>Separable generally makes it easier to extract low level information</li>
<li>Integral Show high level information but not low level information</li>
<li>Configural Arrange low level data into a meaningful form,whole is greater than the sum of the parts</li>
<li>Configural makes it harder to extract low level information</li>
</ul>
<h2 id="Tracking-manual-control"><a href="#Tracking-manual-control" class="headerlink" title="Tracking (manual control)"></a>Tracking (manual control)</h2><p>Learning Objectives:</p>
<ul>
<li>Fitts’ Law Ability to apply theory and skills: Design Use Fitts’ Law to evaluate/predict pointing performance </li>
<li>Theoretical understanding: Order of control </li>
</ul>
<h3 id="Open-versus-Closed-Loop-Systems"><a href="#Open-versus-Closed-Loop-Systems" class="headerlink" title="Open versus Closed Loop Systems"></a>Open versus Closed Loop Systems</h3><h3 id="Tracking-Terms"><a href="#Tracking-Terms" class="headerlink" title="Tracking Terms"></a>Tracking Terms</h3><ul>
<li>Control movement</li>
<li>Controlled element</li>
<li>Target</li>
<li>Forcing function- disturbances to target</li>
</ul>
<h3 id="Pursuit-and-Compensatory"><a href="#Pursuit-and-Compensatory" class="headerlink" title="Pursuit and Compensatory"></a>Pursuit and Compensatory</h3><p>补偿追踪与尾随追踪</p>
<ul>
<li>Pursuit   <ul>
<li>Target moved</li>
<li>Usually more accurate</li>
</ul>
</li>
<li>Compensatory<ul>
<li>Target fixed</li>
<li>Target &amp; control movements confounded</li>
</ul>
</li>
</ul>
<h3 id="Fitts-Law"><a href="#Fitts-Law" class="headerlink" title="Fitts Law"></a>Fitts Law</h3><p>MT(movement time) = a + blog2(2A/W)</p>
<h3 id="Tracking-vs-pointing"><a href="#Tracking-vs-pointing" class="headerlink" title="Tracking vs. pointing"></a>Tracking vs. pointing</h3><p>Pointing as expressed by Fitts law is a very special case of tracking.<br>In pointing:</p>
<ul>
<li>Stationary target</li>
<li>No lag</li>
<li>Gain is only control system parameter</li>
</ul>
<h3 id="Gain-amp-C-D-ratio"><a href="#Gain-amp-C-D-ratio" class="headerlink" title="Gain &amp; C/D ratio"></a>Gain &amp; C/D ratio</h3><ul>
<li>Gain describes the change in the controlled element (display) corresponding to a movement of the control: gain = y/x..</li>
<li>C/D ratio describes the movement of a control needed for a given change in the display: C/D = x/y</li>
</ul>
<h3 id="Order-of-Control"><a href="#Order-of-Control" class="headerlink" title="Order of Control"></a>Order of Control</h3><ul>
<li>0 order: Position<ul>
<li>A 0 order system has <strong>no</strong> integrations between input and output</li>
</ul>
</li>
<li>1 order: Velocity<ul>
<li>A 1 order system has <strong>one</strong> integrations between input and output</li>
</ul>
</li>
<li>2 order Acceleration<ul>
<li>A 2 order system has <strong>two</strong> integrations between input and output</li>
</ul>
</li>
</ul>
<h2 id="Week-8-HIP-amp-Workload"><a href="#Week-8-HIP-amp-Workload" class="headerlink" title="Week 8 HIP &amp; Workload"></a>Week 8 HIP &amp; Workload</h2><p>Learning Objectives:</p>
<ol>
<li>What is mental workload?<br> – Subjective, performance, &amp; physiological measures</li>
<li>HIP &amp; human factors<br> – Working memory, absolute judgment, &amp; other aspects of the bottleneck</li>
<li>Mental representation &amp; difficulty</li>
</ol>
<h3 id="Basic-approaches-to-measuring-mental-workload"><a href="#Basic-approaches-to-measuring-mental-workload" class="headerlink" title="Basic approaches to measuring mental workload"></a>Basic approaches to measuring mental workload</h3><ul>
<li>Analytic<br>  – Task difficulty<ul>
<li>Number of simultaneous tasks</li>
</ul>
</li>
<li>Task performance <ul>
<li>Primary task</li>
<li>Secondary task</li>
</ul>
</li>
<li>Physiological (arousal/effort)<ul>
<li>heart rate</li>
<li>evoked response amplitude</li>
<li>……</li>
</ul>
</li>
<li>Subjective assessment<ul>
<li>Cooper-Harris</li>
<li>SWAT</li>
<li>NASA</li>
</ul>
</li>
</ul>
<p><img src="resources/B1FE7729E735139A9174D4D525B76743.png" alt="Screen Shot 2019-12-11 at 17.05.25.png"></p>
<h3 id="Selective-Attention"><a href="#Selective-Attention" class="headerlink" title="Selective Attention"></a>Selective Attention</h3><p>通道的选择性注意主要受下面因素的影响<br><img src="resources/505A2612DA444E6ABADCD8DA347E4794.png" alt="Screen Shot 2019-12-11 at 17.04.47.png"></p>
<h3 id="Three-aspects-of-perception"><a href="#Three-aspects-of-perception" class="headerlink" title="Three aspects of perception"></a>Three aspects of perception</h3><ol>
<li>Sensory based<ul>
<li><strong>Bottom-up</strong> feature analysis<ul>
<li>clear stimuli/minimize sensory similarities</li>
</ul>
</li>
</ul>
</li>
<li>Memory based<ul>
<li><strong>Unitization</strong><br>  – perceive grouped features as a whole (Gestalt)</li>
<li><strong>Top-down</strong> [correct guesses &amp; fill-ins]</li>
</ul>
</li>
</ol>
<h3 id="working-memory"><a href="#working-memory" class="headerlink" title="working memory"></a>working memory</h3><ul>
<li>working memory</li>
<li>long term memory</li>
</ul>
<p><img src="resources/8424FD27B3344B794E85F01390B2C560.png" alt="Screen Shot 2019-12-12 at 03.50.04.png"></p>
<h3 id="90’s-model-nods-to-Baddeley-amp-Schneider"><a href="#90’s-model-nods-to-Baddeley-amp-Schneider" class="headerlink" title="90’s model (nods to Baddeley &amp; Schneider)"></a>90’s model (nods to Baddeley &amp; Schneider)</h3><p><img src="resources/2D6E557C69F004CC508948E5AF2DA91B.png" alt="Screen Shot 2019-12-11 at 17.13.34.png"></p>
<ol>
<li>Central executive<ul>
<li>Coordinate multiple tasks (OS)</li>
<li>Hold &amp; manipulate info from LTM (RAM)</li>
<li>Control retrieval strategies from LTM (data<br>access)</li>
<li>Attend selectively to stimuli (time share)</li>
<li>协调两个存储子系统</li>
</ul>
</li>
<li>Visual sketchpad<ul>
<li>以模拟的，空间的形式保持正在使用的信息</li>
</ul>
</li>
<li>Phonological store<ul>
<li>存储以声音的形式存在的信息</li>
</ul>
</li>
</ol>
<h2 id="Human-Error-and-Reliability"><a href="#Human-Error-and-Reliability" class="headerlink" title="Human Error and Reliability"></a>Human Error and Reliability</h2><ol>
<li>Understanding Mechanisms underlying human error </li>
<li>What types of errors can be predicted? </li>
<li>Proficiency in information-related skills: Analysis Perform THERP analyses to predict errors for a design/task </li>
</ol>
<h3 id="Error"><a href="#Error" class="headerlink" title="Error"></a>Error</h3><ul>
<li>Any act with adverse consequences</li>
<li>An act resulting from an inappropriate intention</li>
<li>Keeping the pressurizer level under control</li>
</ul>
<h3 id="Action-Schema"><a href="#Action-Schema" class="headerlink" title="Action Schema"></a>Action Schema</h3><p><img src="resources/5CB1A89659EE40E74A848B269D451306.png" alt="Screen Shot 2019-12-11 at 19.22.21.png"></p>
<ol>
<li>Intention<ul>
<li>mode errors</li>
<li>description errors</li>
</ul>
</li>
<li>Activation<ul>
<li>capture errors</li>
<li>data-driven</li>
<li>associative activation</li>
<li>loss of activation</li>
<li>sequence error</li>
</ul>
</li>
<li>Faulty Triggering<ul>
<li>Out of sequence and “mangled” execution..</li>
</ul>
</li>
</ol>
<h3 id="Reliability-Engineering"><a href="#Reliability-Engineering" class="headerlink" title="Reliability Engineering"></a>Reliability Engineering</h3><ul>
<li>The <strong>Key</strong> to reliability is Redundancy</li>
</ul>
<h3 id="Components-in-Series"><a href="#Components-in-Series" class="headerlink" title="Components in Series"></a>Components in Series</h3><p><img src="resources/E81CF8AF176C7942E34394DC8A2FF674.png" alt="Screen Shot 2019-12-12 at 03.52.33.png"></p>
<h3 id="Components-in-Parallel"><a href="#Components-in-Parallel" class="headerlink" title="Components in Parallel"></a>Components in Parallel</h3><p><img src="resources/9C5CE086D1C126A6965F4602030EB667.png" alt="Screen Shot 2019-12-12 at 03.52.39.png"></p>
<h3 id="Tradeoffs-Redundant-or-not"><a href="#Tradeoffs-Redundant-or-not" class="headerlink" title="Tradeoffs (Redundant or not)"></a>Tradeoffs (Redundant or not)</h3><h3 id="How-redundancy-works"><a href="#How-redundancy-works" class="headerlink" title="How redundancy works"></a>How redundancy works</h3><p>For the mathematics to work out the probabilities of failure for redundant components or subsystems must be <strong>completely independent</strong></p>
<h3 id="What-Reliability-Engineers-do"><a href="#What-Reliability-Engineers-do" class="headerlink" title="What Reliability Engineers do"></a>What Reliability Engineers do</h3><ul>
<li>The primary task of a reliability engineer is to defend redundancy against <strong>unexpected violations of independence</strong>.</li>
<li>In a well designed system only the <strong>human operator</strong> bridges these islands of independence</li>
</ul>
<h3 id="THERP"><a href="#THERP" class="headerlink" title="THERP"></a>THERP</h3><ul>
<li>Quality control method for estimating errors</li>
<li>Model<ul>
<li><strong>Errors</strong>: such as reading or omitting an instructional step, or choosing the wrong switch, are presumed to occur at constant rates</li>
<li>If tasks can be broken down into subtasks for which errors can be predicted, then the probability of the successful completion of the overall task can be predicted</li>
<li>The probability of successfully completing the task (if its something like warhead assembly) is then simply the <strong>joint probability</strong> that everything is done correctly</li>
</ul>
</li>
</ul>
<h3 id="Points-on-THERP-analyses"><a href="#Points-on-THERP-analyses" class="headerlink" title="Points on THERP analyses"></a>Points on THERP analyses</h3><ul>
<li>Tree is not sacrosanct but a convenient way to organize independent tasks</li>
<li>Probabilities for errors and recoveries should be entered into trees at level of aggregation at which independence holds</li>
<li>Method is ultimately simply a way to make our commonsense about the likelihood of failing more explicit</li>
</ul>
<p><img src="resources/81B1E00736CFE5835EC5B0CB6ED3E51C.png" alt="Screen Shot 2019-12-11 at 20.05.14.png"></p>
<h2 id="Human-Computer-interaction"><a href="#Human-Computer-interaction" class="headerlink" title="Human-Computer interaction"></a>Human-Computer interaction</h2><ol>
<li>Understand basic assumptions and mechanics of constructing GOMS keystroke level model</li>
<li>Contrast the HIP vs. Ecological vision of problems in HCI</li>
<li>Standard visualizations and the problem(s) they solve- finding context for local views</li>
</ol>
<h3 id="GOMS-Models"><a href="#GOMS-Models" class="headerlink" title="GOMS Models"></a>GOMS Models</h3><p>用于设计的用户绩效模型</p>
<ul>
<li>Goals</li>
<li>Operators </li>
<li>Methods</li>
<li>Selection rules</li>
</ul>
<p>用户可以通过方法和选择形成他们要达到的目标和子目标。方法是一系列知觉的、认知的或行为操作的步骤。</p>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ul>
<li>列出目标和子目标</li>
<li>明确达到问题的方法</li>
<li>写出选择关系</li>
<li>揭示问题</li>
</ul>
<h3 id="Norman’s-7-Stages-amp-design"><a href="#Norman’s-7-Stages-amp-design" class="headerlink" title="Norman’s 7 Stages &amp; design"></a>Norman’s 7 Stages &amp; design</h3><p>用户导向界面设计的七阶段理论<br><img src="resources/64FB6BD735C5ACC5560D714AD40CEBF1.png" alt="Screen Shot 2019-12-11 at 20.22.00.png"></p>
<ol>
<li>实施的鸿沟：用户的目的和软件所支持的行为之间的错误匹配（通过好的人因学方案解决，input tracking position）</li>
<li>评价的鸿沟：用户期望与系统状态的不匹配 (好的说明性显示)</li>
</ol>
<h3 id="Mplications-of-working-memory-amp-absolute-judgment-limitations"><a href="#Mplications-of-working-memory-amp-absolute-judgment-limitations" class="headerlink" title="Mplications of working memory &amp; absolute judgment limitations"></a>Mplications of working memory &amp; absolute judgment limitations</h3><ul>
<li>Recognition is MUCH easier than Recall :Make the objects of working memory available to perception..</li>
</ul>
<h3 id="The-Power-Law-of-Practice"><a href="#The-Power-Law-of-Practice" class="headerlink" title="The Power Law of Practice"></a>The Power Law of Practice</h3><p>Improvement in performance is logarithmic in the N of trials</p>
<h3 id="Mental-Models"><a href="#Mental-Models" class="headerlink" title="Mental Models"></a>Mental Models</h3><p>跨越实施和评价的鸿沟依赖于心理模型，好的心理模型可以帮助房主错误和改进绩效</p>
<ul>
<li>Allows people to make predictions about how things will work</li>
<li>Mental models are often wrong</li>
</ul>
<h3 id="Conceptual-Models"><a href="#Conceptual-Models" class="headerlink" title="Conceptual Models"></a>Conceptual Models</h3><p>使用户看不见的部分变为可见 比如”房间”</p>
<h3 id="State-Transition-Models-of-Devices"><a href="#State-Transition-Models-of-Devices" class="headerlink" title="State Transition Models of Devices"></a>State Transition Models of Devices</h3><p><img src="resources/B8C3FE244974F3F50AE2A26BFAE36589.png" alt="Screen Shot 2019-12-11 at 20.45.26.png"></p>
<h2 id="Speech"><a href="#Speech" class="headerlink" title="Speech"></a>Speech</h2><ul>
<li>Theoretical Understanding: Problems in building applications using speech recognition Design</li>
<li>Strategies for getting good performance despite poor recognition </li>
<li>Theoretical Understanding: Computer supported cooperative work </li>
<li>Theoretical Understanding: Design Strategies for using sensors to augment human inputs and improve interaction </li>
</ul>
<h3 id="Basic-Speech-Parameters"><a href="#Basic-Speech-Parameters" class="headerlink" title="Basic Speech Parameters"></a>Basic Speech Parameters</h3><ul>
<li>Speaker Dependent/Independent</li>
<li>Size/Type of Vocabulary</li>
<li>Isolated word vs. Continuous Speech</li>
<li>Grammar/constraint</li>
<li>Environment/noise tolerance</li>
<li>Noise canceling unidirectional microphones<br>– Quiet environments</li>
</ul>
<h3 id="Rec-System-maximizes-chance-of-getting-things-right-by"><a href="#Rec-System-maximizes-chance-of-getting-things-right-by" class="headerlink" title="Rec System maximizes chance of getting things right by"></a>Rec System maximizes chance of getting things right by</h3><ul>
<li>Restricting vocabulary</li>
<li>Specifying order &amp; transitions for recognition</li>
<li>Associating actions/meaning with partial recognition of phrase</li>
<li>Use of context particularly within dialog to adjust constraints</li>
</ul>
<h3 id="Lombard-Effect"><a href="#Lombard-Effect" class="headerlink" title="Lombard Effect"></a>Lombard Effect</h3><p>During noise, speakers have an automatic normalization response that causes systematic speech modifications, including increased volume, reduced speaking rate, and changes in articulation and pitch.</p>
<h3 id="Issues-in-Ubicomp"><a href="#Issues-in-Ubicomp" class="headerlink" title="Issues in Ubicomp"></a>Issues in Ubicomp</h3><p>Issues in Ubicomp</p>
<ul>
<li>Context</li>
<li>Uneven conditioning</li>
<li>Inferring user intent</li>
<li>System interoperation</li>
</ul>
<h2 id="Decision-Making-amp-Diagnosis"><a href="#Decision-Making-amp-Diagnosis" class="headerlink" title="Decision Making &amp; Diagnosis"></a>Decision Making &amp; Diagnosis</h2><p>Learning Objectives</p>
<ul>
<li>Theoretical Understanding: Normative vs. Behavioral theories of decision making</li>
<li>Models of decision making in diagnosis Name &amp; illustrate standard fallacies in decision making </li>
</ul>
<h3 id="Rational-Decision-Making"><a href="#Rational-Decision-Making" class="headerlink" title="Rational Decision Making"></a>Rational Decision Making</h3><ul>
<li>A rational decision maker is one who chooses the alternative which maximizes his expected utility.</li>
<li>A rational decision maker is presumed to maximize her <strong>Subjective Utility</strong> which is likely to be some function of objective</li>
</ul>
<h3 id="Prospect-theory"><a href="#Prospect-theory" class="headerlink" title="Prospect theory"></a>Prospect theory</h3><p>Loss hurts more than Gain helps （抛硬币，正面赢20，反面输 10，大多数人选择不玩）</p>
<h3 id="Base-Rate-Fallacy"><a href="#Base-Rate-Fallacy" class="headerlink" title="Base Rate Fallacy"></a>Base Rate Fallacy</h3><p>Undervalue base rates!! </p>
<h3 id="Behavioral-Decision-Making"><a href="#Behavioral-Decision-Making" class="headerlink" title="Behavioral Decision Making"></a>Behavioral Decision Making</h3><ol>
<li>Treate extreme values as more moderate  （感知不到极端数值，就比如考试明明只剩三天，但还觉得时间很充裕不好好看hf）</li>
<li>Imperfections in memory （记忆缺陷， 不能收集到所有过去的信息帮助做决策，比如期中复习hf就十分紧张期末还是这样） </li>
<li>Inability to do complex math in our heads （做不了复杂算数） </li>
</ol>
<h3 id="Gambler’s-Fallacy"><a href="#Gambler’s-Fallacy" class="headerlink" title="Gambler’s Fallacy"></a>Gambler’s Fallacy</h3><p>Error: treating independent events as though they were dependent</p>
<h3 id="Availability-heuristic可得性偏差"><a href="#Availability-heuristic可得性偏差" class="headerlink" title="Availability heuristic可得性偏差"></a>Availability heuristic可得性偏差</h3><p>人们做决策总会基于 avaliability &amp; imaginability </p>
<h3 id="Imaginability"><a href="#Imaginability" class="headerlink" title="Imaginability"></a>Imaginability</h3><h3 id="Confirmation-bias"><a href="#Confirmation-bias" class="headerlink" title="Confirmation bias"></a>Confirmation bias</h3><p>确定偏差:简而言之就是听不进新的观点，无论怎样论证都是认为自己原本认为的是对的， 本质还是overconfidence<br>例如：给一些本身对于某件事有观点的人接受正反两面信息，人们通常都只注意到支持自己观点的理论，暗中反驳不符合自己观点的理论 </p>
<h3 id="Representativeness-bias-代表性偏差"><a href="#Representativeness-bias-代表性偏差" class="headerlink" title="Representativeness bias 代表性偏差"></a>Representativeness bias 代表性偏差</h3><p>人类在对事件做出判断的时候，过度关注于这个事件的某个特征，而忽略了这个事件发生的大环境概率和样本大小。<br>例如，你看到一家公司连续3年利润都翻番，然后立即对它的股票做出判断——买！错在代表性偏差。连续3年利润翻番，是一个好公司的代表性特征。但这并不意味着这家公司真的就是一家好公司，这家公司还有好多信息都被你忽略掉了。比如说，业绩可能是有意调整出来的；再比如说，这家公司未来的盈利机会消失，业绩不能持续。 </p>
<h3 id="Anchoring锚定效应"><a href="#Anchoring锚定效应" class="headerlink" title="Anchoring锚定效应"></a>Anchoring锚定效应</h3><p>人们在对某人某事做出判断时，易受第一印象影响从而先入为主 </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Dependency-Parsing-and-Assignment3-of-CS224n/2019/12/10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Dependency-Parsing-and-Assignment3-of-CS224n/2019/12/10/" class="post-title-link" itemprop="url">Dependency Parsing and Assignment3 of CS224n</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-10 04:13:41" itemprop="dateCreated datePublished" datetime="2019-12-10T04:13:41+08:00">2019-12-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-20 02:09:55" itemprop="dateModified" datetime="2019-12-20T02:09:55+08:00">2019-12-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Dependency-Parsing-and-Assignment3-of-CS224n/2019/12/10/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Dependency-Parsing-and-Assignment3-of-CS224n/2019/12/10/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>the course note of deependency parsing and the details of assignment 3</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/Dependency-Parsing-and-Assignment3-of-CS224n/2019/12/10/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/10/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/12/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">266</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail → mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '[object Object]';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
