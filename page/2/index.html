<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="RUOCHI.AI">
<meta property="og:url" content="https://zhangruochi.com/page/2/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Ruochi Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhangruochi.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/TD-with-State-Aggregation/2020/10/13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/TD-with-State-Aggregation/2020/10/13/" class="post-title-link" itemprop="url">TD with State Aggregation </a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-10-13 16:06:20 / Modified: 16:07:45" itemprop="dateCreated datePublished" datetime="2020-10-13T16:06:20+08:00">2020-10-13</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/TD-with-State-Aggregation/2020/10/13/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/TD-with-State-Aggregation/2020/10/13/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="assignment-1---td-with-state-aggregation">Assignment 1 - TD with State Aggregation</h1>
<p>Welcome to your Course 3 Programming Assignment 1. In this assignment, you will implement <strong>semi-gradient TD(0) with State Aggregation</strong> in an environment with a large state space. This assignment will focus on the <strong>policy evaluation task</strong> (prediction problem) where the goal is to accurately estimate state values under a given (fixed) policy.</p>
<p><strong>In this assignment, you will:</strong> 1. Implement semi-gradient TD(0) with function approximation (state aggregation). 2. Understand how to use supervised learning approaches to approximate value functions. 3. Compare the impact of different resolutions of state aggregation, and see first hand how function approximation can speed up learning through generalization.</p>
<p><strong>Note: You can create new cells for debugging purposes but please do not duplicate any Read-only cells. This may break the grader.</strong></p>
<h2 id="state-randomwalk-environment">500-State RandomWalk Environment</h2>
<p>In this assignment, we will implement and use a smaller 500 state version of the problem we covered in lecture (see "State Aggregation with Monte Carlo‚Äù, and Example 9.1 in the <a target="_blank" rel="noopener" href="http://www.incompleteideas.net/book/RLbook2018.pdf">textbook</a>). The diagram below illustrates the problem.</p>
<p><img src="randomwalk_diagram.png" /></p>
<p>There are 500 states numbered from 1 to 500, left to right, and all episodes begin with the agent located at the center, in state 250. For simplicity, we will consider state 0 and state 501 as the left and right terminal states respectively.</p>
<p>The episode terminates when the agent reaches the terminal state (state 0) on the left, or the terminal state (state 501) on the right. Termination on the left (state 0) gives the agent a reward of -1, and termination on the right (state 501) gives the agent a reward of +1.</p>
<p>The agent can take one of two actions: go left or go right. If the agent chooses the left action, then it transitions uniform randomly into one of the 100 neighboring states to its left. If the agent chooses the right action, then it transitions randomly into one of the 100 neighboring states to its right.</p>
<p>States near the edge may have fewer than 100 neighboring states on that side. In this case, all transitions that would have taken the agent past the edge result in termination. If the agent takes the left action from state 50, then it has a 0.5 chance of terminating on the left. If it takes the right action from state 499, then it has a 0.99 chance of terminating on the right.</p>
<h3 id="your-goal">Your Goal</h3>
<p>For this assignment, we will consider the problem of <strong>policy evaluation</strong>: estimating state-value function for a fixed policy.You will evaluate a uniform random policy in the 500-State Random Walk environment. This policy takes the right action with 0.5 probability and the left with 0.5 probability, regardless of which state it is in.</p>
<p>This environment has a relatively large number of states. Generalization can significantly speed learning as we will show in this assignment. Often in realistic environments, states are high-dimensional and continuous. For these problems, function approximation is not just useful, it is also necessary.</p>
<h2 id="packages">Packages</h2>
<p>You will use the following packages in this assignment.</p>
<ul>
<li><a href="www.numpy.org">numpy</a> : Fundamental package for scientific computing with Python.</li>
<li><a target="_blank" rel="noopener" href="http://matplotlib.org">matplotlib</a> : Library for plotting graphs in Python.</li>
<li><a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/v10/tanner09a.html">RL-Glue</a> : Library for reinforcement learning experiments.</li>
<li><a target="_blank" rel="noopener" href="https://alexhagen.github.io/jdc/">jdc</a> : Jupyter magic that allows defining classes over multiple jupyter notebook cells.</li>
<li><a target="_blank" rel="noopener" href="https://tqdm.github.io/">tqdm</a> : A package to display progress bar when running experiments</li>
<li>plot_script : custom script to plot results</li>
</ul>
<p><strong>Please do not import other libraries</strong> - this will break the autograder.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jdc</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> environment <span class="keyword">import</span> BaseEnvironment</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">import</span> plot_script</span><br></pre></td></tr></table></figure>
<h2 id="section-1-create-the-500-state-randomwalk-environment">Section 1: Create the 500-State RandomWalk Environment</h2>
<p>In this section we have provided you with the implementation of the 500-State RandomWalk Environment. It is useful to know how the environment is implemented. We will also use this environment in the next programming assignment.</p>
<p>Once the agent chooses which direction to move, the environment determines how far the agent is moved in that direction. Assume the agent passes either 0 (indicating left) or 1 (indicating right) to the environment.</p>
<p>Methods needed to implement the environment are: <code>env_init</code>, <code>env_start</code>, and <code>env_step</code>.</p>
<ul>
<li><code>env_init</code>: This method sets up the environment at the very beginning of the experiment. Relevant parameters are passed through <code>env_info</code> dictionary.</li>
<li><code>env_start</code>: This is the first method called when the experiment starts, returning the start state.</li>
<li><code>env_step</code>: This method takes in action and returns reward, next_state, and is_terminal.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomWalkEnvironment</span>(<span class="params">BaseEnvironment</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_init</span>(<span class="params">self, env_info=&#123;&#125;</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Setup for the environment called when the experiment first starts.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Set parameters needed to setup the 500-state random walk environment.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Assume env_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states: 500 [int],</span></span><br><span class="line"><span class="string">            start_state: 250 [int],</span></span><br><span class="line"><span class="string">            left_terminal_state: 0 [int],</span></span><br><span class="line"><span class="string">            right_terminal_state: 501 [int],</span></span><br><span class="line"><span class="string">            seed: int</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># set random seed for each run</span></span><br><span class="line">        self.rand_generator = np.random.RandomState(env_info.get(<span class="string">&quot;seed&quot;</span>)) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># set each class attribute</span></span><br><span class="line">        self.num_states = env_info[<span class="string">&quot;num_states&quot;</span>] </span><br><span class="line">        self.start_state = env_info[<span class="string">&quot;start_state&quot;</span>] </span><br><span class="line">        self.left_terminal_state = env_info[<span class="string">&quot;left_terminal_state&quot;</span>] </span><br><span class="line">        self.right_terminal_state = env_info[<span class="string">&quot;right_terminal_state&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_start</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        The first method called when the experiment starts, called before the</span></span><br><span class="line"><span class="string">        agent starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The first state from the environment.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># set self.reward_state_term tuple</span></span><br><span class="line">        reward = <span class="number">0.0</span></span><br><span class="line">        state = self.start_state</span><br><span class="line">        is_terminal = <span class="literal">False</span></span><br><span class="line">                </span><br><span class="line">        self.reward_state_term = (reward, state, is_terminal)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># return first state from the environment</span></span><br><span class="line">        <span class="keyword">return</span> self.reward_state_term[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_step</span>(<span class="params">self, action</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;A step taken by the environment.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            action: The action taken by the agent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            (float, state, Boolean): a tuple of the reward, state,</span></span><br><span class="line"><span class="string">                and boolean indicating if it&#x27;s terminal.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        last_state = self.reward_state_term[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># set reward, current_state, and is_terminal</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># action: specifies direction of movement - 0 (indicating left) or 1 (indicating right)  [int]</span></span><br><span class="line">        <span class="comment"># current state: next state after taking action from the last state [int]</span></span><br><span class="line">        <span class="comment"># reward: -1 if terminated left, 1 if terminated right, 0 otherwise [float]</span></span><br><span class="line">        <span class="comment"># is_terminal: indicates whether the episode terminated [boolean]</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Given action (direction of movement), determine how much to move in that direction from last_state</span></span><br><span class="line">        <span class="comment"># All transitions beyond the terminal state are absorbed into the terminal state.</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> action == <span class="number">0</span>: <span class="comment"># left</span></span><br><span class="line">            current_state = <span class="built_in">max</span>(self.left_terminal_state, last_state + self.rand_generator.choice(<span class="built_in">range</span>(-<span class="number">100</span>,<span class="number">0</span>)))</span><br><span class="line">        <span class="keyword">elif</span> action == <span class="number">1</span>: <span class="comment"># right</span></span><br><span class="line">            current_state = <span class="built_in">min</span>(self.right_terminal_state, last_state + self.rand_generator.choice(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">101</span>)))</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Wrong action value&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># terminate left</span></span><br><span class="line">        <span class="keyword">if</span> current_state == self.left_terminal_state: </span><br><span class="line">            reward = -<span class="number">1.0</span></span><br><span class="line">            is_terminal = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># terminate right</span></span><br><span class="line">        <span class="keyword">elif</span> current_state == self.right_terminal_state:</span><br><span class="line">            reward = <span class="number">1.0</span></span><br><span class="line">            is_terminal = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            reward = <span class="number">0.0</span></span><br><span class="line">            is_terminal = <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        self.reward_state_term = (reward, current_state, is_terminal)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.reward_state_term</span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<h2 id="section-2-create-semi-gradient-td0-agent-with-state-aggregation">Section 2: Create Semi-gradient TD(0) Agent with State Aggregation</h2>
<p>Now let's create the Agent that interacts with the Environment.</p>
<p>You will create an Agent that learns with semi-gradient TD(0) with state aggregation. For state aggregation, if the resolution (num_groups) is 10, then 500 states are partitioned into 10 groups of 50 states each (i.e., states 1-50 are one group, states 51-100 are another, and so on.)</p>
<p>Hence, 50 states would share the same feature and value estimate, and there would be 10 distinct features. The feature vector for each state is a one-hot feature vector of length 10, with a single one indicating the group for that state. (one-hot vector of length 10)</p>
<h2 id="section-2-1-implement-useful-functions">Section 2-1: Implement Useful Functions</h2>
<p>Before we implement the agent, we need to define a couple of useful helper functions.</p>
<p><strong>Please note all random method calls should be called through random number generator. Also do not use random method calls unless specified. In the agent, only <code>agent_policy</code> requires random method calls.</strong></p>
<h2 id="section-2-1a-selecting-actions">Section 2-1a: Selecting actions</h2>
<p>In this part we have implemented <code>agent_policy()</code> for you.</p>
<p>This method is used in <code>agent_start()</code> and <code>agent_step()</code> to select appropriate action. Normally, the agent acts differently given state, but in this environment the agent chooses randomly to move either left or right with equal probability.</p>
<p>Agent returns 0 for left, and 1 for right.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_policy</span>(<span class="params">rand_generator, state</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Given random number generator and state, returns an action according to the agent&#x27;s policy.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        rand_generator: Random number generator</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        chosen action [int]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># set chosen_action as 0 or 1 with equal probability</span></span><br><span class="line">    <span class="comment"># state is unnecessary for this agent policy</span></span><br><span class="line">    chosen_action = rand_generator.choice([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> chosen_action</span><br></pre></td></tr></table></figure>
<h2 id="section-2-1b-processing-state-features-with-state-aggregation">Section 2-1b: Processing State Features with State Aggregation</h2>
<p>In this part you will implement <code>get_state_feature()</code></p>
<p>This method takes in a state and returns the aggregated feature (one-hot-vector) of that state. The feature vector size is determined by <code>num_groups</code>. Use <code>state</code> and <code>num_states_in_group</code> to determine which element in the feature vector is active.</p>
<p><code>get_state_feature()</code> is necessary whenever the agent receives a state and needs to convert it to a feature for learning. The features will thus be used in <code>agent_step()</code> and <code>agent_end()</code> when the agent updates its state values.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_state_feature</span>(<span class="params">num_states_in_group, num_groups, state</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Given state, return the feature of that state</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_states_in_group [int]</span></span><br><span class="line"><span class="string">        num_groups [int] </span></span><br><span class="line"><span class="string">        state [int] : 1~500</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        one_hot_vector [numpy array]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### Generate state feature (2~4 lines)</span></span><br><span class="line">    <span class="comment"># Create one_hot_vector with size of the num_groups, according to state</span></span><br><span class="line">    <span class="comment"># For simplicity, assume num_states is always perfectly divisible by num_groups</span></span><br><span class="line">    <span class="comment"># Note that states start from index 1, not 0!</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Example:</span></span><br><span class="line">    <span class="comment"># If num_states = 100, num_states_in_group = 20, num_groups = 5,</span></span><br><span class="line">    <span class="comment"># one_hot_vector would be of size 5.</span></span><br><span class="line">    <span class="comment"># For states 1~20, one_hot_vector would be: [1, 0, 0, 0, 0]</span></span><br><span class="line">    <span class="comment"># </span></span><br><span class="line">    <span class="comment"># one_hot_vector = ?</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    one_hot_vector = np.zeros((num_groups,))</span><br><span class="line">    index,_ = <span class="built_in">divmod</span>(state,num_states_in_group)</span><br><span class="line">    <span class="keyword">if</span> _ == <span class="number">0</span>:</span><br><span class="line">        index -= <span class="number">1</span></span><br><span class="line">    one_hot_vector[index] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> one_hot_vector</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Run the following code to verify your <code>get_state_feature()</code> function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Given that num_states = 10 and num_groups = 5, test get_state_feature()</span></span><br><span class="line"><span class="comment"># There are states 1~10, and the state feature vector would be of size 5.</span></span><br><span class="line"><span class="comment"># Only one element would be active for any state feature vector.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># get_state_feature() should support various values of num_states, num_groups, not just this example</span></span><br><span class="line"><span class="comment"># For simplicity, assume num_states will always be perfectly divisible by num_groups</span></span><br><span class="line">num_states = <span class="number">10</span></span><br><span class="line">num_groups = <span class="number">5</span></span><br><span class="line">num_states_in_group = <span class="built_in">int</span>(num_states / num_groups)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test 1st group, state = 1</span></span><br><span class="line">state = <span class="number">1</span></span><br><span class="line">features = get_state_feature(num_states_in_group, num_groups, state)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;1st group: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(features))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(features == [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test 2nd group, state = 3</span></span><br><span class="line">state = <span class="number">3</span></span><br><span class="line">features = get_state_feature(num_states_in_group, num_groups, state)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;2nd group: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(features))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(features == [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test 3rd group, state = 6</span></span><br><span class="line">state = <span class="number">6</span></span><br><span class="line">features = get_state_feature(num_states_in_group, num_groups, state)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;3rd group: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(features))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(features == [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test 4th group, state = 7</span></span><br><span class="line">state = <span class="number">7</span></span><br><span class="line">features = get_state_feature(num_states_in_group, num_groups, state)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;4th group: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(features))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(features == [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test 5th group, state = 10</span></span><br><span class="line">state = <span class="number">10</span></span><br><span class="line">features = get_state_feature(num_states_in_group, num_groups, state)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;5th group: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(features))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(features == [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>1st group: [1. 0. 0. 0. 0.]
2nd group: [0. 1. 0. 0. 0.]
3rd group: [0. 0. 1. 0. 0.]
4th group: [0. 0. 0. 1. 0.]
5th group: [0. 0. 0. 0. 1.]</code></pre>
<h2 id="section-2-2-implement-agent-methods">Section 2-2: Implement Agent Methods</h2>
<p>Now that we have implemented all the helper functions, let's create an agent. In this part, you will implement <code>agent_init()</code>, <code>agent_start()</code>, <code>agent_step()</code> and <code>agent_end()</code>. You will have to use <code>agent_policy()</code> that we implemented above. We will implement <code>agent_message()</code> later, when returning the learned state-values.</p>
<p>To save computation time, we precompute features for all states beforehand in <code>agent_init()</code>. The pre-computed features are saved in <code>self.all_state_features</code> numpy array. Hence, you do not need to call <code>get_state_feature()</code> every time in <code>agent_step()</code> and <code>agent_end()</code>.</p>
<p>The shape of <code>self.all_state_features</code> numpy array is <code>(num_states, feature_size)</code>, with features of states from State 1-500. Note that index 0 stores features for State 1 (Features for State 0 does not exist). Use <code>self.all_state_features</code> to access each feature vector for a state.</p>
<p>When saving state values in the agent, recall how the state values are represented with linear function approximation.</p>
<p><strong>State Value Representation</strong>: <span class="math inline">\(\hat{v}(s,\mathbf{w}) = \mathbf{w}\cdot\mathbf{x^T}\)</span> where <span class="math inline">\(\mathbf{w}\)</span> is a weight vector and <span class="math inline">\(\mathbf{x}\)</span> is the feature vector of the state.</p>
<p>When performing TD(0) updates with Linear Function Approximation, recall how we perform semi-gradient TD(0) updates using supervised learning.</p>
<p><strong>semi-gradient TD(0) Weight Update Rule</strong>: <span class="math inline">\(\mathbf{w_{t+1}} = \mathbf{w_{t}} + \alpha [R_{t+1} + \gamma \hat{v}(S_{t+1},\mathbf{w}) - \hat{v}(S_t,\mathbf{w})] \nabla \hat{v}(S_t,\mathbf{w})\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create TDAgent</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TDAgent</span>(<span class="params">BaseAgent</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.num_states = <span class="literal">None</span></span><br><span class="line">        self.num_groups = <span class="literal">None</span></span><br><span class="line">        self.step_size = <span class="literal">None</span></span><br><span class="line">        self.discount_factor = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span>(<span class="params">self, agent_info=&#123;&#125;</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Set parameters needed to setup the semi-gradient TD(0) state aggregation agent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Assume agent_info dict contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states: 500 [int],</span></span><br><span class="line"><span class="string">            num_groups: int, </span></span><br><span class="line"><span class="string">            step_size: float, </span></span><br><span class="line"><span class="string">            discount_factor: float,</span></span><br><span class="line"><span class="string">            seed: int</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># set random seed for each run</span></span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">&quot;seed&quot;</span>)) </span><br><span class="line"></span><br><span class="line">        <span class="comment"># set class attributes</span></span><br><span class="line">        self.num_states = agent_info.get(<span class="string">&quot;num_states&quot;</span>)</span><br><span class="line">        self.num_groups = agent_info.get(<span class="string">&quot;num_groups&quot;</span>)</span><br><span class="line">        self.step_size = agent_info.get(<span class="string">&quot;step_size&quot;</span>)</span><br><span class="line">        self.discount_factor = agent_info.get(<span class="string">&quot;discount_factor&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pre-compute all observable features</span></span><br><span class="line">        num_states_in_group = <span class="built_in">int</span>(self.num_states / self.num_groups)</span><br><span class="line">        self.all_state_features = np.array([get_state_feature(num_states_in_group, self.num_groups, state) <span class="keyword">for</span> state <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, self.num_states + <span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># initialize all weights to zero using numpy array with correct size</span></span><br><span class="line">        <span class="comment"># self.weights = ?</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.weights = np.zeros((self.num_groups,))</span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        self.last_state = <span class="literal">None</span></span><br><span class="line">        self.last_action = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;The first method called when the experiment starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">                environment&#x27;s evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            self.last_action [int] : The first action the agent takes.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment">### select action given state (using agent_policy), and save current state and action</span></span><br><span class="line">        <span class="comment"># Use self.rand_generator for agent_policy</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="comment"># self.last_state = ?</span></span><br><span class="line">        <span class="comment"># self.last_action = ?</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.last_state = state</span><br><span class="line">        self.last_action = agent_policy(self.rand_generator, self.last_state)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, state</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward [float]: the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            state [int]: the state from the environment&#x27;s step, where the agent ended up after the last step</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            self.last_action [int] : The action the agent is taking.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get relevant feature</span></span><br><span class="line">        current_state_feature = self.all_state_features[state-<span class="number">1</span>] </span><br><span class="line">        last_state_feature = self.all_state_features[self.last_state-<span class="number">1</span>] </span><br><span class="line">        </span><br><span class="line">        <span class="comment">### update weights and select action</span></span><br><span class="line">        <span class="comment"># (Hint: np.dot method is useful!)</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Update weights:</span></span><br><span class="line">        <span class="comment">#     use self.weights, current_state_feature, and last_state_feature</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Select action:</span></span><br><span class="line">        <span class="comment">#     use self.rand_generator for agent_policy</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Current state and selected action should be saved to self.last_state and self.last_action at the end</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># self.weights = ?</span></span><br><span class="line">        <span class="comment"># self.last_state = ?</span></span><br><span class="line">        <span class="comment"># self.last_action = ?</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.weights += self.step_size * (reward + self.discount_factor * (np.dot(self.weights,current_state_feature)) - np.dot(self.weights,last_state_feature)) * last_state_feature </span><br><span class="line">        self.last_state = state</span><br><span class="line">        self.last_action = agent_policy(self.rand_generator, self.last_state)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="keyword">return</span> self.last_action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get relevant feature</span></span><br><span class="line">        last_state_feature = self.all_state_features[self.last_state-<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### update weights</span></span><br><span class="line">        <span class="comment"># Update weights using self.weights and last_state_feature</span></span><br><span class="line">        <span class="comment"># (Hint: np.dot method is useful!)</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="comment"># Note that here you don&#x27;t need to choose action since the agent has reached a terminal state</span></span><br><span class="line">        <span class="comment"># Therefore you should not update self.last_state and self.last_action</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="comment"># self.weights = ?</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.weights += self.step_size * (reward - np.dot(self.weights,last_state_feature)) * last_state_feature </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_message</span>(<span class="params">self, message</span>):</span></span><br><span class="line">        <span class="comment"># We will implement this method later</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Run the following code to verify <code>agent_init()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">&quot;num_states&quot;</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">&quot;num_groups&quot;</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">&quot;discount_factor&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&quot;seed&quot;</span>: <span class="number">1</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">agent = TDAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(agent.weights == <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> agent.weights.shape == (<span class="number">10</span>,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check attributes</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;num_states: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(agent.num_states))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;num_groups: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(agent.num_groups))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;step_size: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(agent.step_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;discount_factor: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(agent.discount_factor))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;weights shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(agent.weights.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;weights init. value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(agent.weights))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>num_states: 500
num_groups: 10
step_size: 0.1
discount_factor: 1.0
weights shape: (10,)
weights init. value: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</code></pre>
<p>Run the following code to verify <code>agent_start()</code>. Although there is randomness due to <code>rand_generator.choice()</code> in <code>agent_policy()</code>, we control the seed so your output should match the expected output.</p>
<p>Make sure <code>rand_generator.choice()</code> is called only once per <code>agent_policy()</code> call.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">&quot;num_states&quot;</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">&quot;num_groups&quot;</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">&quot;discount_factor&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&quot;seed&quot;</span>: <span class="number">1</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Suppose state = 250</span></span><br><span class="line">state = <span class="number">250</span></span><br><span class="line"></span><br><span class="line">agent = TDAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line">action = agent.agent_start(state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> agent.last_state == <span class="number">250</span></span><br><span class="line"><span class="keyword">assert</span> agent.last_action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Agent state: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(agent.last_state))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Agent selected action: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(agent.last_action))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>Agent state: 250
Agent selected action: 1</code></pre>
<p>Run the following code to verify <code>agent_step()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">&quot;num_states&quot;</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">&quot;num_groups&quot;</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">&quot;discount_factor&quot;</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">&quot;seed&quot;</span>: <span class="number">1</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">agent = TDAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing the weights to arbitrary values to verify the correctness of weight update</span></span><br><span class="line">agent.weights = np.array([-<span class="number">1.5</span>, <span class="number">0.5</span>, <span class="number">1.</span>, -<span class="number">0.5</span>, <span class="number">1.5</span>, -<span class="number">0.5</span>, <span class="number">1.5</span>, <span class="number">0.0</span>, -<span class="number">0.5</span>, -<span class="number">1.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the agent started at State 50</span></span><br><span class="line">start_state = <span class="number">50</span></span><br><span class="line">action = agent.agent_start(start_state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the reward was 10.0 and the next state observed was State 120</span></span><br><span class="line">reward = <span class="number">10.0</span></span><br><span class="line">next_state = <span class="number">120</span></span><br><span class="line">action = agent.agent_step(reward, next_state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Updated weights: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(agent.weights))</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.weights, [-<span class="number">0.26</span>, <span class="number">0.5</span>, <span class="number">1.</span>, -<span class="number">0.5</span>, <span class="number">1.5</span>, -<span class="number">0.5</span>, <span class="number">1.5</span>, <span class="number">0.</span>, -<span class="number">0.5</span>, -<span class="number">1.</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> agent.last_state == <span class="number">120</span></span><br><span class="line"><span class="keyword">assert</span> agent.last_action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;last state: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(agent.last_state))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;last action: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(agent.last_action))</span><br><span class="line"></span><br><span class="line"><span class="comment"># let&#x27;s do another</span></span><br><span class="line">reward = -<span class="number">22</span></span><br><span class="line">next_state = <span class="number">222</span></span><br><span class="line">action = agent.agent_step(reward, next_state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.weights, [-<span class="number">0.26</span>, <span class="number">0.5</span>, -<span class="number">1.165</span>, -<span class="number">0.5</span>, <span class="number">1.5</span>, -<span class="number">0.5</span>, <span class="number">1.5</span>, <span class="number">0</span>, -<span class="number">0.5</span>, -<span class="number">1</span>])</span><br><span class="line"><span class="keyword">assert</span> agent.last_state == <span class="number">222</span></span><br><span class="line"><span class="keyword">assert</span> agent.last_action == <span class="number">0</span></span><br></pre></td></tr></table></figure>
<pre><code>Updated weights: [-0.26  0.5   1.   -0.5   1.5  -0.5   1.5   0.   -0.5  -1.  ]
last state: 120
last action: 1</code></pre>
<p>Run the following code to verify <code>agent_end()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">&quot;num_states&quot;</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">&quot;num_groups&quot;</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">&quot;discount_factor&quot;</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="string">&quot;seed&quot;</span>: <span class="number">1</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">agent = TDAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing the weights to arbitrary values to verify the correctness of weight update</span></span><br><span class="line">agent.weights = np.array([-<span class="number">1.5</span>, <span class="number">0.5</span>, <span class="number">1.</span>, -<span class="number">0.5</span>, <span class="number">1.5</span>, -<span class="number">0.5</span>, <span class="number">1.5</span>, <span class="number">0.0</span>, -<span class="number">0.5</span>, -<span class="number">1.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the agent started at State 50</span></span><br><span class="line">start_state = <span class="number">50</span></span><br><span class="line">action = agent.agent_start(start_state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume the reward was 10.0 and reached the terminal state</span></span><br><span class="line">agent.agent_end(<span class="number">10.0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Updated weights: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(agent.weights))</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.weights, [-<span class="number">0.35</span>, <span class="number">0.5</span>, <span class="number">1.</span>, -<span class="number">0.5</span>, <span class="number">1.5</span>, -<span class="number">0.5</span>, <span class="number">1.5</span>, <span class="number">0.</span>, -<span class="number">0.5</span>, -<span class="number">1.</span>])</span><br></pre></td></tr></table></figure>
<pre><code>Updated weights: [-0.35  0.5   1.   -0.5   1.5  -0.5   1.5   0.   -0.5  -1.  ]</code></pre>
<p><strong>Expected output</strong>: (Note only the 1st element was changed, and the result is different from <code>agent_step()</code> )</p>
<pre><code>Initial weights: [-1.5  0.5  1.  -0.5  1.5 -0.5  1.5  0.  -0.5 -1. ]
Updated weights: [-0.35  0.5   1.   -0.5   1.5  -0.5   1.5   0.   -0.5  -1.  ]</code></pre>
<h2 id="section-2-3-returning-learned-state-values">Section 2-3: Returning Learned State Values</h2>
<p>You are almost done! Now let's implement a code block in <code>agent_message()</code> that returns the learned state values.</p>
<p>The method <code>agent_message()</code> will return the learned state_value array when <code>message == 'get state value'</code>.</p>
<p><strong>Hint</strong>: Think about how state values are represented with linear function approximation. <code>state_value</code> array will be a 1D array with length equal to the number of states.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_message</span>(<span class="params">self, message</span>):</span></span><br><span class="line">    <span class="keyword">if</span> message == <span class="string">&#x27;get state value&#x27;</span>:</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### return state_value</span></span><br><span class="line">        <span class="comment"># Use self.all_state_features and self.weights to return the vector of all state values</span></span><br><span class="line">        <span class="comment"># Hint: Use np.dot()</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># state_value = ?</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        state_value = np.dot(self.all_state_features, self.weights)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> state_value</span><br></pre></td></tr></table></figure>
<p>Run the following code to verify <code>get_state_val()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;</span><br><span class="line">    <span class="string">&quot;num_states&quot;</span>: <span class="number">20</span>,</span><br><span class="line">    <span class="string">&quot;num_groups&quot;</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">&quot;discount_factor&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">agent = TDAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line">test_state_val = agent.agent_message(<span class="string">&#x27;get state value&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> test_state_val.shape == (<span class="number">20</span>,)</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(test_state_val == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;State value shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_state_val.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Initial State value for all states: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_state_val))</span><br></pre></td></tr></table></figure>
<pre><code>State value shape: (20,)
Initial State value for all states: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</code></pre>
<p><strong>Expected Output</strong>:</p>
<pre><code>State value shape: (20,)
Initial State value for all states: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</code></pre>
<h2 id="section-3-run-experiment">Section 3: Run Experiment</h2>
<p>Now that we've implemented all the components of environment and agent, let's run an experiment! We will plot two things: (1) the learned state value function and compare it against the true state values, and (2) a learning curve depicting the error in the learned value estimates over episodes. For the learning curve, what should we plot to see if the agent is learning well?</p>
<h2 id="section-3-1-prediction-objective-root-mean-squared-value-error">Section 3-1: Prediction Objective (Root Mean Squared Value Error)</h2>
<p>Recall that the Prediction Objective in function approximation is Mean Squared Value Error <span class="math inline">\(\overline{VE}(\mathbf{w}) \doteq \sum\limits_{s \in \mathcal{S}}\mu(s)[v_\pi(s)-\hat{v}(s,\mathbf{w})]^2\)</span></p>
<p>We will use the square root of this measure, the root <span class="math inline">\(\overline{VE}\)</span> to give a rough measure of how much the learned values differ from the true values.</p>
<p><code>calc RMSVE()</code> computes the Root Mean Squared Value Error given learned state value <span class="math inline">\(\hat{v}(s, \mathbf{w})\)</span>. We provide you with true state value <span class="math inline">\(v_\pi(s)\)</span> and state distribution <span class="math inline">\(\mu(s)\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Here we provide you with the true state value and state distribution</span></span><br><span class="line">true_state_val = np.load(<span class="string">&#x27;data/true_V.npy&#x27;</span>)    </span><br><span class="line">state_distribution = np.load(<span class="string">&#x27;data/state_distribution.npy&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_RMSVE</span>(<span class="params">learned_state_val</span>):</span></span><br><span class="line">    <span class="keyword">assert</span>(<span class="built_in">len</span>(true_state_val) == <span class="built_in">len</span>(learned_state_val) == <span class="built_in">len</span>(state_distribution))</span><br><span class="line">    MSVE = np.<span class="built_in">sum</span>(np.multiply(state_distribution, np.square(true_state_val - learned_state_val)))</span><br><span class="line">    RMSVE = np.sqrt(MSVE)</span><br><span class="line">    <span class="keyword">return</span> RMSVE</span><br></pre></td></tr></table></figure>
<h2 id="section-3-2a-run-experiment-with-10-state-aggregation">Section 3-2a: Run Experiment with 10-State Aggregation</h2>
<p>We have provided you the experiment/plot code in the cell below.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define function to run experiment</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span>(<span class="params">environment, agent, environment_parameters, agent_parameters, experiment_parameters</span>):</span></span><br><span class="line"></span><br><span class="line">    rl_glue = RLGlue(environment, agent)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Sweep Agent parameters</span></span><br><span class="line">    <span class="keyword">for</span> num_agg_states <span class="keyword">in</span> agent_parameters[<span class="string">&quot;num_groups&quot;</span>]:</span><br><span class="line">        <span class="keyword">for</span> step_size <span class="keyword">in</span> agent_parameters[<span class="string">&quot;step_size&quot;</span>]:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># save rmsve at the end of each evaluation episode</span></span><br><span class="line">            <span class="comment"># size: num_episode / episode_eval_frequency + 1 (includes evaluation at the beginning of training)</span></span><br><span class="line">            agent_rmsve = np.zeros(<span class="built_in">int</span>(experiment_parameters[<span class="string">&quot;num_episodes&quot;</span>]/experiment_parameters[<span class="string">&quot;episode_eval_frequency&quot;</span>]) + <span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># save learned state value at the end of each run</span></span><br><span class="line">            agent_state_val = np.zeros(environment_parameters[<span class="string">&quot;num_states&quot;</span>])</span><br><span class="line"></span><br><span class="line">            env_info = &#123;<span class="string">&quot;num_states&quot;</span>: environment_parameters[<span class="string">&quot;num_states&quot;</span>],</span><br><span class="line">                        <span class="string">&quot;start_state&quot;</span>: environment_parameters[<span class="string">&quot;start_state&quot;</span>],</span><br><span class="line">                        <span class="string">&quot;left_terminal_state&quot;</span>: environment_parameters[<span class="string">&quot;left_terminal_state&quot;</span>],</span><br><span class="line">                        <span class="string">&quot;right_terminal_state&quot;</span>: environment_parameters[<span class="string">&quot;right_terminal_state&quot;</span>]&#125;</span><br><span class="line"></span><br><span class="line">            agent_info = &#123;<span class="string">&quot;num_states&quot;</span>: environment_parameters[<span class="string">&quot;num_states&quot;</span>],</span><br><span class="line">                          <span class="string">&quot;num_groups&quot;</span>: num_agg_states,</span><br><span class="line">                          <span class="string">&quot;step_size&quot;</span>: step_size,</span><br><span class="line">                          <span class="string">&quot;discount_factor&quot;</span>: environment_parameters[<span class="string">&quot;discount_factor&quot;</span>]&#125;</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Setting - num. agg. states: &#123;&#125;, step_size: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(num_agg_states, step_size))</span><br><span class="line">            os.system(<span class="string">&#x27;sleep 0.2&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># one agent setting</span></span><br><span class="line">            <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">1</span>, experiment_parameters[<span class="string">&quot;num_runs&quot;</span>]+<span class="number">1</span>)):</span><br><span class="line">                env_info[<span class="string">&quot;seed&quot;</span>] = run</span><br><span class="line">                agent_info[<span class="string">&quot;seed&quot;</span>] = run</span><br><span class="line">                rl_glue.rl_init(agent_info, env_info)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Compute initial RMSVE before training</span></span><br><span class="line">                current_V = rl_glue.rl_agent_message(<span class="string">&quot;get state value&quot;</span>)</span><br><span class="line">                agent_rmsve[<span class="number">0</span>] += calc_RMSVE(current_V)</span><br><span class="line">                    </span><br><span class="line">                <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, experiment_parameters[<span class="string">&quot;num_episodes&quot;</span>]+<span class="number">1</span>):</span><br><span class="line">                    <span class="comment"># run episode</span></span><br><span class="line">                    rl_glue.rl_episode(<span class="number">0</span>) <span class="comment"># no step limit</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">if</span> episode % experiment_parameters[<span class="string">&quot;episode_eval_frequency&quot;</span>] == <span class="number">0</span>:</span><br><span class="line">                        current_V = rl_glue.rl_agent_message(<span class="string">&quot;get state value&quot;</span>)</span><br><span class="line">                        agent_rmsve[<span class="built_in">int</span>(episode/experiment_parameters[<span class="string">&quot;episode_eval_frequency&quot;</span>])] += calc_RMSVE(current_V)</span><br><span class="line">                        </span><br><span class="line">                <span class="comment"># store only one run of state value</span></span><br><span class="line">                <span class="keyword">if</span> run == <span class="number">50</span>:</span><br><span class="line">                    agent_state_val = rl_glue.rl_agent_message(<span class="string">&quot;get state value&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># rmsve averaged over runs</span></span><br><span class="line">            agent_rmsve /= experiment_parameters[<span class="string">&quot;num_runs&quot;</span>]</span><br><span class="line">            </span><br><span class="line">            save_name = <span class="string">&quot;&#123;&#125;_agg_states_&#123;&#125;_step_size_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&#x27;TD_agent&#x27;</span>, num_agg_states, step_size).replace(<span class="string">&#x27;.&#x27;</span>,<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;results&#x27;</span>):</span><br><span class="line">                os.makedirs(<span class="string">&#x27;results&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># save avg. state value</span></span><br><span class="line">            np.save(<span class="string">&quot;results/V_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(save_name), agent_state_val)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># save avg. rmsve</span></span><br><span class="line">            np.save(<span class="string">&quot;results/RMSVE_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(save_name), agent_rmsve)</span><br></pre></td></tr></table></figure>
<p>We will first test our implementation using state aggregation with resolution of 10, with three different step sizes: {0.01, 0.05, 0.1}.</p>
<p>Note that running the experiment cell below will take <strong><em>approximately 5 min</em></strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Run Experiment</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;num_runs&quot;</span> : <span class="number">50</span>,</span><br><span class="line">    <span class="string">&quot;num_episodes&quot;</span> : <span class="number">2000</span>,</span><br><span class="line">    <span class="string">&quot;episode_eval_frequency&quot;</span> : <span class="number">10</span> <span class="comment"># evaluate every 10 episodes</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;num_states&quot;</span> : <span class="number">500</span>, </span><br><span class="line">    <span class="string">&quot;start_state&quot;</span> : <span class="number">250</span>,</span><br><span class="line">    <span class="string">&quot;left_terminal_state&quot;</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="string">&quot;right_terminal_state&quot;</span> : <span class="number">501</span>, </span><br><span class="line">    <span class="string">&quot;discount_factor&quot;</span> : <span class="number">1.0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line"><span class="comment"># Each element is an array because we will be later sweeping over multiple values</span></span><br><span class="line">agent_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;num_groups&quot;</span>: [<span class="number">10</span>],</span><br><span class="line">    <span class="string">&quot;step_size&quot;</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = RandomWalkEnvironment</span><br><span class="line">current_agent = TDAgent</span><br><span class="line"></span><br><span class="line">run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br><span class="line">plot_script.plot_result(agent_parameters, <span class="string">&#x27;results&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Setting - num. agg. states: 10, step_size: 0.01


100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:32&lt;00:00,  1.85s/it]


Setting - num. agg. states: 10, step_size: 0.05


100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:33&lt;00:00,  1.87s/it]


Setting - num. agg. states: 10, step_size: 0.1


100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:31&lt;00:00,  1.83s/it]</code></pre>
<figure>
<img src="output_36_6.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<p>Is the learned state value plot with step-size=0.01 similar to Figure 9.2 (p.208) in Sutton and Barto?</p>
<p>(Note that our environment has less states: 500 states and we have done 2000 episodes, and averaged the performance over 50 runs)</p>
<p>Look at the plot of the learning curve. Does RMSVE decrease over time?</p>
<p>Would it be possible to reduce RMSVE to 0?</p>
<p>You should see the RMSVE decrease over time, but the error seems to plateau. It is impossible to reduce RMSVE to 0, because of function approximation (and we do not decay the step-size parameter to zero). With function approximation, the agent has limited resources and has to trade-off the accuracy of one state for another state.</p>
<p>Run the following code to verify your experimental result.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line">agent_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;num_groups&quot;</span>: [<span class="number">10</span>],</span><br><span class="line">    <span class="string">&quot;step_size&quot;</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">all_correct = <span class="literal">True</span></span><br><span class="line"><span class="keyword">for</span> num_agg_states <span class="keyword">in</span> agent_parameters[<span class="string">&quot;num_groups&quot;</span>]:</span><br><span class="line">    <span class="keyword">for</span> step_size <span class="keyword">in</span> agent_parameters[<span class="string">&quot;step_size&quot;</span>]:</span><br><span class="line">        filename = <span class="string">&#x27;RMSVE_TD_agent_agg_states_&#123;&#125;_step_size_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(num_agg_states, step_size).replace(<span class="string">&#x27;.&#x27;</span>,<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        agent_RMSVE = np.load(<span class="string">&#x27;results/&#123;&#125;.npy&#x27;</span>.<span class="built_in">format</span>(filename))</span><br><span class="line">        correct_RMSVE = np.load(<span class="string">&#x27;correct_npy/&#123;&#125;.npy&#x27;</span>.<span class="built_in">format</span>(filename))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> np.allclose(agent_RMSVE, correct_RMSVE):</span><br><span class="line">            all_correct=<span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> all_correct:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Your experiment results are correct!&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Your experiment results does not match with ours. Please check if you have implemented all methods correctly.&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Your experiment results are correct!</code></pre>
<h2 id="section-3-2b-run-experiment-with-different-state-aggregation-resolution-and-step-size">Section 3-2b: Run Experiment with Different State Aggregation Resolution and Step-Size</h2>
<p>In this section, we will run some more experiments to see how different parameter settings affect the results!</p>
<p>In particular, we will test several values of <code>num_groups</code> and <code>step_size</code>. Parameter sweeps although necessary, can take lots of time. So now that you have verified your experiment result, here we show you the results of the parameter sweeps that you would see when running the sweeps yourself.</p>
<p>We tested several different values of <code>num_groups</code>: {10, 100, 500}, and <code>step-size</code>: {0.01, 0.05, 0.1}. As before, we performed 2000 episodes per run, and averaged the results over 50 runs for each setting.</p>
<p>Run the cell below to display the sweep results.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Make sure to verify your experiment result with the test cell above.</span></span><br><span class="line"><span class="comment"># Otherwise the sweep results will not be displayed.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;num_runs&quot;</span> : <span class="number">50</span>,</span><br><span class="line">    <span class="string">&quot;num_episodes&quot;</span> : <span class="number">2000</span>,</span><br><span class="line">    <span class="string">&quot;episode_eval_frequency&quot;</span> : <span class="number">10</span> <span class="comment"># evaluate every 10 episodes</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;num_states&quot;</span> : <span class="number">500</span>,</span><br><span class="line">    <span class="string">&quot;start_state&quot;</span> : <span class="number">250</span>,</span><br><span class="line">    <span class="string">&quot;left_terminal_state&quot;</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="string">&quot;right_terminal_state&quot;</span> : <span class="number">501</span>,</span><br><span class="line">    <span class="string">&quot;discount_factor&quot;</span> : <span class="number">1.0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line"><span class="comment"># Each element is an array because we will be sweeping over multiple values</span></span><br><span class="line">agent_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;num_groups&quot;</span>: [<span class="number">10</span>, <span class="number">100</span>, <span class="number">500</span>],</span><br><span class="line">    <span class="string">&quot;step_size&quot;</span>: [<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> all_correct:</span><br><span class="line">    plot_script.plot_result(agent_parameters, <span class="string">&#x27;correct_npy&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">&quot;Make sure your experiment result is correct! Otherwise the sweep results will not be displayed.&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="output_41_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure>
<img src="output_41_1.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<figure>
<img src="output_41_2.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<h2 id="wrapping-up">Wrapping up</h2>
<p>Let‚Äôs think about the results of our parameter study.</p>
<h3 id="state-aggregation">State Aggregation</h3>
<ul>
<li><p>Which state aggregation resolution do you think is the best after running 2000 episodes? Which state aggregation resolution do you think would be the best if we could train for only 200 episodes? What if we could train for a million episodes?</p></li>
<li><p>Should we use tabular representation (state aggregation of resolution 500) whenever possible? Why might we want to use function approximation?</p></li>
</ul>
<p>From the plots, using 100 state aggregation with step-size 0.05 reaches the best performance: the lowest RMSVE after 2000 episodes. If the agent can only be trained for 200 episodes, then 10 state aggregation with step-size 0.05 reaches the lowest error. Increasing the resolution of state aggregation makes the function approximation closer to a tabular representation, which would be able to learn exactly correct state values for all states. But learning will be slower.</p>
<h3 id="step-size">Step-Size</h3>
<ul>
<li>How did different step-sizes affect learning?</li>
</ul>
<p>The best step-size is different for different state aggregation resolutions. A larger step-size allows the agent to learn faster, but might not perform as well asymptotically. A smaller step-size causes it to learn more slowly, but may perform well asymptotically.</p>
<h3 id="congratulations-you-have-successfully-implemented-course-3-programming-assignment-1."><strong>Congratulations!</strong> You have successfully implemented Course 3 Programming Assignment 1.</h3>
<p>You have implemented <strong>semi-gradient TD(0) with State Aggregation</strong> in a 500-state Random Walk. We used an environment with a large but discrete state space, where it was possible to compute the true state values. This allowed us to compare the values learned by your agent to the true state values. The same state aggregation function approximation can also be applied to continuous state space environments, where comparison to the true values is not usually possible.</p>
<p>You also successfully applied supervised learning approaches to approximate value functions with semi-gradient TD(0).</p>
<p>Finally, we plotted the learned state values and compared with true state values. We also compared learning curves of different state aggregation resolutions and learning rates.</p>
<p>From the results, you can see why it is often desirable to use function approximation, even when tabular learning is possible. Asymptotically, an agent with tabular representation would be able to learn the true state value function, but it would learn much more slowly compared to an agent with function approximation. On the other hand, we also want to ensure we do not reduce discrimination too far (a coarse state aggregation resolution), because it will hurt the asymptotic performance.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Dyna-Q-and-Dyna-Q/2020/09/30/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Dyna-Q-and-Dyna-Q/2020/09/30/" class="post-title-link" itemprop="url">Dyna-Q and Dyna-Q+</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-09-30 16:50:06 / Modified: 16:50:40" itemprop="dateCreated datePublished" datetime="2020-09-30T16:50:06+08:00">2020-09-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Dyna-Q-and-Dyna-Q/2020/09/30/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Dyna-Q-and-Dyna-Q/2020/09/30/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="assignment-dyna-q-and-dyna-q">Assignment: Dyna-Q and Dyna-Q+</h1>
<p>Welcome to this programming assignment! In this notebook, you will: 1. implement the Dyna-Q and Dyna-Q+ algorithms. 2. compare their performance on an environment which changes to become 'better' than it was before, that is, the task becomes easier.</p>
<p>We will give you the environment and infrastructure to run the experiment and visualize the performance. The assignment will be graded automatically by comparing the behavior of your agent to our implementations of the algorithms. The random seed will be set explicitly to avoid different behaviors due to randomness.</p>
<p>Please go through the cells in order.</p>
<h2 id="the-shortcut-maze-environment">The Shortcut Maze Environment</h2>
<p>In this maze environment, the goal is to reach the goal state (G) as fast as possible from the starting state (S). There are four actions √¢‚Ç¨‚Äú up, down, right, left √¢‚Ç¨‚Äú which take the agent deterministically from a state to the corresponding neighboring states, except when movement is blocked by a wall (denoted by grey) or the edge of the maze, in which case the agent remains where it is. The reward is +1 on reaching the goal state, 0 otherwise. On reaching the goal state G, the agent returns to the start state S to being a new episode. This is a discounted, episodic task with <span class="math inline">\(\gamma = 0.95\)</span>.</p>
<p><img src="shortcut_env.png" alt="environment" width="400"/></p>
<p>Later in the assignment, we will use a variant of this maze in which a 'shortcut' opens up after a certain number of timesteps. We will test if the the Dyna-Q and Dyna-Q+ agents are able to find the newly-opened shorter route to the goal state.</p>
<h2 id="packages">Packages</h2>
<p>We import the following libraries that are required for this assignment. Primarily, we shall be using the following libraries: 1. numpy: the fundamental package for scientific computing with Python. 2. matplotlib: the library for plotting graphs in Python. 3. RL-Glue: the library for reinforcement learning experiments.</p>
<p><strong>Please do not import other libraries</strong> as this will break the autograder.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> jdc</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">from</span> maze_env <span class="keyword">import</span> ShortcutMazeEnvironment</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams.update(&#123;<span class="string">&#x27;font.size&#x27;</span>: <span class="number">15</span>&#125;)</span><br><span class="line">plt.rcParams.update(&#123;<span class="string">&#x27;figure.figsize&#x27;</span>: [<span class="number">8</span>,<span class="number">5</span>]&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="section-1-dyna-q">Section 1: Dyna-Q</h2>
<p>Let's start with a quick recap of the tabular Dyna-Q algorithm.</p>
<div style="width:80%">
<img src="DynaQ.png" alt="DynaQ_pseudocode">
</div>
<p>Dyna-Q involves four basic steps: 1. Action selection: given an observation, select an action to be performed (here, using the <span class="math inline">\(\epsilon\)</span>-greedy method). 2. Direct RL: using the observed next state and reward, update the action values (here, using one-step tabular Q-learning). 3. Model learning: using the observed next state and reward, update the model (here, updating a table as the environment is assumed to be deterministic). 4. Planning: update the action values by generating <span class="math inline">\(n\)</span> simulated experiences using certain starting states and actions (here, using the random-sample one-step tabular Q-planning method). This is also known as the 'Indirect RL' step. The process of choosing the state and action to simulate an experience with is known as 'search control'.</p>
<p>Steps 1 and 2 are parts of the <a target="_blank" rel="noopener" href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=153">tabular Q-learning algorithm</a> and are denoted by line numbers (a)√¢‚Ç¨‚Äú(d) in the pseudocode above. Step 3 is performed in line (e), and Step 4 in the block of lines (f).</p>
<p>We highly recommend revising the Dyna videos in the course and the material in the RL textbook (in particular, <a target="_blank" rel="noopener" href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=183">Section 8.2</a>).</p>
<p>Alright, let's begin coding.</p>
<p>As you already know by now, you will develop an agent which interacts with the given environment via RL-Glue. More specifically, you will implement the usual methods <code>agent_start</code>, <code>agent_step</code>, and <code>agent_end</code> in your <code>DynaQAgent</code> class, along with a couple of helper methods specific to Dyna-Q, namely <code>update_model</code> and <code>planning_step</code>. We will provide detailed comments in each method describing what your code should do.</p>
<p>Let's break this down in pieces and do it one-by-one.</p>
<p>First of all, check out the <code>agent_init</code> method below. As in earlier assignments, some of the attributes are initialized with the data passed inside <code>agent_info</code>. In particular, pay attention to the attributes which are new to <code>DynaQAgent</code>, since you shall be using them later.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynaQAgent</span>(<span class="params">BaseAgent</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span>(<span class="params">self, agent_info</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                num_states (int): The number of states,</span></span><br><span class="line"><span class="string">                num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">                epsilon (float): The parameter for epsilon-greedy exploration,</span></span><br><span class="line"><span class="string">                step_size (float): The step-size,</span></span><br><span class="line"><span class="string">                discount (float): The discount factor,</span></span><br><span class="line"><span class="string">                planning_steps (int): The number of planning steps per environmental interaction</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                random_seed (int): the seed for the RNG used in epsilon-greedy</span></span><br><span class="line"><span class="string">                planning_random_seed (int): the seed for the RNG used in the planner</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># First, we get the relevant information from agent_info </span></span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> we use np.random.RandomState(seed) to set the two different RNGs</span></span><br><span class="line">        <span class="comment"># for the planner and the rest of the code</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.num_states = agent_info[<span class="string">&quot;num_states&quot;</span>]</span><br><span class="line">            self.num_actions = agent_info[<span class="string">&quot;num_actions&quot;</span>]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;You need to pass both &#x27;num_states&#x27; and &#x27;num_actions&#x27; \</span></span><br><span class="line"><span class="string">                   in agent_info to initialize the action-value table&quot;</span>)</span><br><span class="line">        self.gamma = agent_info.get(<span class="string">&quot;discount&quot;</span>, <span class="number">0.95</span>)</span><br><span class="line">        self.step_size = agent_info.get(<span class="string">&quot;step_size&quot;</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.epsilon = agent_info.get(<span class="string">&quot;epsilon&quot;</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.planning_steps = agent_info.get(<span class="string">&quot;planning_steps&quot;</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">&#x27;random_seed&#x27;</span>, <span class="number">42</span>))</span><br><span class="line">        self.planning_rand_generator = np.random.RandomState(agent_info.get(<span class="string">&#x27;planning_random_seed&#x27;</span>, <span class="number">42</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Next, we initialize the attributes required by the agent, e.g., q_values, model, etc.</span></span><br><span class="line">        <span class="comment"># A simple way to implement the model is to have a dictionary of dictionaries, </span></span><br><span class="line">        <span class="comment">#        mapping each state to a dictionary which maps actions to (reward, next state) tuples.</span></span><br><span class="line">        self.q_values = np.zeros((self.num_states, self.num_actions))</span><br><span class="line">        self.actions = <span class="built_in">list</span>(<span class="built_in">range</span>(self.num_actions))</span><br><span class="line">        self.past_action = -<span class="number">1</span></span><br><span class="line">        self.past_state = -<span class="number">1</span></span><br><span class="line">        self.model = &#123;&#125; <span class="comment"># model is a dictionary of dictionaries, which maps states to actions to </span></span><br><span class="line">                        <span class="comment"># (reward, next_state) tuples</span></span><br></pre></td></tr></table></figure>
<p>Now let's create the <code>update_model</code> method, which performs the 'Model Update' step in the pseudocode. It takes a <code>(s, a, s', r)</code> tuple and stores the next state and reward corresponding to a state-action pair.</p>
<p>Remember, because the environment is deterministic, an easy way to implement the model is to have a dictionary of encountered states, each mapping to a dictionary of actions taken in those states, which in turn maps to a tuple of next state and reward. In this way, the model can be easily accessed by <code>model[s][a]</code>, which would return the <code>(s', r)</code> tuple.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_model</span>(<span class="params">self, past_state, past_action, state, reward</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;updates the model </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        past_state       (int): s</span></span><br><span class="line"><span class="string">        past_action      (int): a</span></span><br><span class="line"><span class="string">        state            (int): s&#x27;</span></span><br><span class="line"><span class="string">        reward           (int): r</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Nothing</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Update the model with the (s,a,s&#x27;,r) tuple (1~4 lines)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> past_state <span class="keyword">in</span> self.model:</span><br><span class="line">        self.model[past_state] = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> past_action <span class="keyword">in</span> self.model[past_state]:</span><br><span class="line">        self.model[past_state][past_action] = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    self.model[past_state][past_action] = (state,reward)</span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure>
<h3 id="test-update_model">Test <code>update_model()</code></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">actions = []</span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">&quot;num_states&quot;</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">&quot;discount&quot;</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">&quot;random_seed&quot;</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">&quot;planning_random_seed&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (past_state, past_action, state, reward)</span></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="comment"># action 2 in state 0 leads back to state 0 with a reward of 1</span></span><br><span class="line">    <span class="comment"># or taking action 3 leads to state 1 with reward of 2</span></span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">2</span>: (<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="comment"># taking action 0 in state 2 leads to state 1 with a reward of 1</span></span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Next, you will implement the planning step, the crux of the Dyna-Q algorithm. You shall be calling this <code>planning_step</code> method at every timestep of every trajectory.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">planning_step</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;performs planning, i.e. indirect RL.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Nothing</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The indirect RL step:</span></span><br><span class="line">    <span class="comment"># - Choose a state and action from the set of experiences that are stored in the model. (~2 lines)</span></span><br><span class="line">    <span class="comment"># - Query the model with this state-action pair for the predicted next state and reward.(~1 line)</span></span><br><span class="line">    <span class="comment"># - Update the action values with this simulated experience.                            (2~4 lines)</span></span><br><span class="line">    <span class="comment"># - Repeat for the required number of planning steps.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that the update equation is different for terminal and non-terminal transitions. </span></span><br><span class="line">    <span class="comment"># To differentiate between a terminal and a non-terminal next state, assume that the model stores</span></span><br><span class="line">    <span class="comment"># the terminal state as a dummy state like -1</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Important: remember you have a random number generator &#x27;planning_rand_generator&#x27; as </span></span><br><span class="line">    <span class="comment">#     a part of the class which you need to use as self.planning_rand_generator.choice()</span></span><br><span class="line">    <span class="comment">#     For the sake of reproducibility and grading, *do not* use anything else like </span></span><br><span class="line">    <span class="comment">#     np.random.choice() for performing search control.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.planning_steps):</span><br><span class="line">    </span><br><span class="line">        s = self.planning_rand_generator.choice(<span class="built_in">list</span>(self.model.keys()))</span><br><span class="line">        a = self.planning_rand_generator.choice(<span class="built_in">list</span>(self.model[s].keys()))</span><br><span class="line"></span><br><span class="line">        (next_s,r) = self.model[s][a]</span><br><span class="line">        <span class="keyword">if</span> next_s == -<span class="number">1</span>:</span><br><span class="line">            expect = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            expect = self.gamma * np.<span class="built_in">max</span>(self.q_values[next_s])</span><br><span class="line">        self.q_values[s,a] += self.step_size * (r + expect - self.q_values[s,a])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure>
<h3 id="test-planning_step">Test <code>planning_step()</code></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">actions = []</span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">&quot;num_states&quot;</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">&quot;discount&quot;</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">&quot;planning_steps&quot;</span>: <span class="number">4</span>,</span><br><span class="line">              <span class="string">&quot;random_seed&quot;</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">&quot;planning_random_seed&quot;</span>: <span class="number">5</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">1</span>,-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">2</span>: (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">3</span>: (<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">1</span>: (-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line">agent.planning_step()</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0.2</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(np.isclose(agent.q_values, expected_values))</span><br></pre></td></tr></table></figure>
<p>Now before you move on to implement the rest of the agent methods, here are the helper functions that you've used in the previous assessments for choosing an action using an <span class="math inline">\(\epsilon\)</span>-greedy policy.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span>(<span class="params">self, q_values</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;argmax with random tie-breaking</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        q_values (Numpy array): the array of action values</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        action (int): an action with the highest value</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    top = <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>)</span><br><span class="line">    ties = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(q_values)):</span><br><span class="line">        <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">            top = q_values[i]</span><br><span class="line">            ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">            ties.append(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action_egreedy</span>(<span class="params">self, state</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;returns an action using an epsilon-greedy policy w.r.t. the current action-value function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Important: assume you have a random number generator &#x27;rand_generator&#x27; as a part of the class</span></span><br><span class="line"><span class="string">                which you can use as self.rand_generator.choice() or self.rand_generator.rand()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (List): coordinates of the agent (two elements)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The action taken w.r.t. the aforementioned epsilon-greedy policy</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">        action = self.rand_generator.choice(self.actions)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        values = self.q_values[state]</span><br><span class="line">        action = self.argmax(values)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure>
<p>Next, you will implement the rest of the agent-related methods, namely <code>agent_start</code>, <code>agent_step</code>, and <code>agent_end</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_start</span>(<span class="params">self, state</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;The first method called when the experiment starts, </span></span><br><span class="line"><span class="string">    called after the environment starts.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment&#x27;s env_start function.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (int) the first action the agent takes.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># given the state, select the action using self.choose_action_egreedy()), </span></span><br><span class="line">    <span class="comment"># and save current state and action (~2 lines)</span></span><br><span class="line">    <span class="comment">### self.past_state = ?</span></span><br><span class="line">    <span class="comment">### self.past_action = ?</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.past_state = state</span><br><span class="line">    self.past_action = self.choose_action_egreedy(state)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.past_action</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, state</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;A step taken by the agent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment&#x27;s step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">            last step</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (int) The action the agent takes given this state.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># - Direct-RL step (~1-3 lines)</span></span><br><span class="line">    <span class="comment"># - Model Update step (~1 line)</span></span><br><span class="line">    <span class="comment"># - `planning_step` (~1 line)</span></span><br><span class="line">    <span class="comment"># - Action Selection step (~1 line)</span></span><br><span class="line">    <span class="comment"># Save the current state and action before returning the action to be performed. (~2 lines)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.q_values[self.past_state,self.past_action] += self.step_size * (reward + self.gamma * np.<span class="built_in">max</span>(self.q_values[state]) - self.q_values[self.past_state,self.past_action])</span><br><span class="line">    self.update_model(self.past_state, self.past_action, state, reward)</span><br><span class="line">    self.planning_step()</span><br><span class="line">    </span><br><span class="line">    action = self.choose_action_egreedy(state)</span><br><span class="line">    self.past_state = state</span><br><span class="line">    self.past_action = action</span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.past_action</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_end</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Called when the agent terminates.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">            terminal state.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># - Direct RL update with this final transition (1~2 lines)</span></span><br><span class="line">    <span class="comment"># - Model Update step with this final transition (~1 line)</span></span><br><span class="line">    <span class="comment"># - One final `planning_step` (~1 line)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note: the final transition needs to be handled carefully. Since there is no next state, </span></span><br><span class="line">    <span class="comment">#       you will have to pass a dummy state (like -1), which you will be using in the planning_step() to </span></span><br><span class="line">    <span class="comment">#       differentiate between updates with usual terminal and non-terminal transitions.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.q_values[self.past_state,self.past_action] += self.step_size * (reward  - self.q_values[self.past_state,self.past_action])</span><br><span class="line">    self.update_model(self.past_state, self.past_action, -<span class="number">1</span>, reward)</span><br><span class="line">    self.planning_step()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure>
<h3 id="test-agent_start-agent_step-and-agent_end">Test <code>agent_start()</code>, <code>agent_step()</code>, and <code>agent_end()</code></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">&quot;num_states&quot;</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">&quot;discount&quot;</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">&quot;random_seed&quot;</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">&quot;planning_steps&quot;</span>: <span class="number">2</span>,</span><br><span class="line">              <span class="string">&quot;planning_random_seed&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ----------------</span></span><br><span class="line"><span class="comment"># test agent start</span></span><br><span class="line"><span class="comment"># ----------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> agent.model == &#123;&#125;</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(agent.q_values == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># test agent step</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.3439</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># test agent end</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">1</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.41051</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.01</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br></pre></td></tr></table></figure>
<h3 id="experiment-dyna-q-agent-in-the-maze-environment">Experiment: Dyna-Q agent in the maze environment</h3>
<p>Alright. Now we have all the components of the <code>DynaQAgent</code> ready. Let's try it out on the maze environment!</p>
<p>The next cell runs an experiment on this maze environment to test your implementation. The initial action values are <span class="math inline">\(0\)</span>, the step-size parameter is <span class="math inline">\(0.125\)</span>. and the exploration parameter is <span class="math inline">\(\epsilon=0.1\)</span>. After the experiment, the sum of rewards in each episode should match the correct result.</p>
<p>We will try planning steps of <span class="math inline">\(0,5,50\)</span> and compare their performance in terms of the average number of steps taken to reach the goal state in the aforementioned maze environment. For scientific rigor, we will run each experiment <span class="math inline">\(30\)</span> times. In each experiment, we set the initial random-number-generator (RNG) seeds for a fair comparison across algorithms.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span>(<span class="params">env, agent, env_parameters, agent_parameters, exp_parameters</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Experiment settings</span></span><br><span class="line">    num_runs = exp_parameters[<span class="string">&#x27;num_runs&#x27;</span>]</span><br><span class="line">    num_episodes = exp_parameters[<span class="string">&#x27;num_episodes&#x27;</span>]</span><br><span class="line">    planning_steps_all = agent_parameters[<span class="string">&#x27;planning_steps&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    env_info = env_parameters                     </span><br><span class="line">    agent_info = &#123;<span class="string">&quot;num_states&quot;</span> : agent_parameters[<span class="string">&quot;num_states&quot;</span>],  <span class="comment"># We pass the agent the information it needs. </span></span><br><span class="line">                  <span class="string">&quot;num_actions&quot;</span> : agent_parameters[<span class="string">&quot;num_actions&quot;</span>],</span><br><span class="line">                  <span class="string">&quot;epsilon&quot;</span>: agent_parameters[<span class="string">&quot;epsilon&quot;</span>], </span><br><span class="line">                  <span class="string">&quot;discount&quot;</span>: env_parameters[<span class="string">&quot;discount&quot;</span>],</span><br><span class="line">                  <span class="string">&quot;step_size&quot;</span> : agent_parameters[<span class="string">&quot;step_size&quot;</span>]&#125;</span><br><span class="line"></span><br><span class="line">    all_averages = np.zeros((<span class="built_in">len</span>(planning_steps_all), num_runs, num_episodes)) <span class="comment"># for collecting metrics </span></span><br><span class="line">    log_data = &#123;<span class="string">&#x27;planning_steps_all&#x27;</span> : planning_steps_all&#125;                     <span class="comment"># that shall be plotted later</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, planning_steps <span class="keyword">in</span> <span class="built_in">enumerate</span>(planning_steps_all):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Planning steps : &#x27;</span>, planning_steps)</span><br><span class="line">        os.system(<span class="string">&#x27;sleep 0.5&#x27;</span>)                    <span class="comment"># to prevent tqdm printing out-of-order before the above print()</span></span><br><span class="line">        agent_info[<span class="string">&quot;planning_steps&quot;</span>] = planning_steps  </span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_runs)):</span><br><span class="line"></span><br><span class="line">            agent_info[<span class="string">&#x27;random_seed&#x27;</span>] = i</span><br><span class="line">            agent_info[<span class="string">&#x27;planning_random_seed&#x27;</span>] = i</span><br><span class="line"></span><br><span class="line">            rl_glue = RLGlue(env, agent)          <span class="comment"># Creates a new RLGlue experiment with the env and agent we chose above</span></span><br><span class="line">            rl_glue.rl_init(agent_info, env_info) <span class="comment"># We pass RLGlue what it needs to initialize the agent and environment</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_episodes):</span><br><span class="line"></span><br><span class="line">                rl_glue.rl_start()                <span class="comment"># We start an episode. Here we aren&#x27;t using rl_glue.rl_episode()</span></span><br><span class="line">                                                  <span class="comment"># like the other assessments because we&#x27;ll be requiring some </span></span><br><span class="line">                is_terminal = <span class="literal">False</span>               <span class="comment"># data from within the episodes in some of the experiments here </span></span><br><span class="line">                num_steps = <span class="number">0</span></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal:</span><br><span class="line">                    reward, _, action, is_terminal = rl_glue.rl_step()  <span class="comment"># The environment and agent take a step </span></span><br><span class="line">                    num_steps += <span class="number">1</span>                                      <span class="comment"># and return the reward and action taken.</span></span><br><span class="line"></span><br><span class="line">                all_averages[idx][i][j] = num_steps</span><br><span class="line"></span><br><span class="line">    log_data[<span class="string">&#x27;all_averages&#x27;</span>] = all_averages</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> log_data</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_steps_per_episode</span>(<span class="params">data</span>):</span></span><br><span class="line">    all_averages = data[<span class="string">&#x27;all_averages&#x27;</span>]</span><br><span class="line">    planning_steps_all = data[<span class="string">&#x27;planning_steps_all&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, planning_steps <span class="keyword">in</span> <span class="built_in">enumerate</span>(planning_steps_all):</span><br><span class="line">        plt.plot(np.mean(all_averages[i], axis=<span class="number">0</span>), label=<span class="string">&#x27;Planning steps = &#x27;</span>+<span class="built_in">str</span>(planning_steps))</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Episodes&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Steps\nper\nepisode&#x27;</span>, rotation=<span class="number">0</span>, labelpad=<span class="number">40</span>)</span><br><span class="line">    plt.axhline(y=<span class="number">16</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, color=<span class="string">&#x27;grey&#x27;</span>, alpha=<span class="number">0.4</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;num_runs&quot;</span> : <span class="number">30</span>,                     <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">    <span class="string">&quot;num_episodes&quot;</span> : <span class="number">40</span>,                 <span class="comment"># The number of episodes per experiment</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123; </span><br><span class="line">    <span class="string">&quot;discount&quot;</span>: <span class="number">0.95</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;  </span><br><span class="line">    <span class="string">&quot;num_states&quot;</span> : <span class="number">54</span>,</span><br><span class="line">    <span class="string">&quot;num_actions&quot;</span> : <span class="number">4</span>, </span><br><span class="line">    <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">    <span class="string">&quot;step_size&quot;</span> : <span class="number">0.125</span>,</span><br><span class="line">    <span class="string">&quot;planning_steps&quot;</span> : [<span class="number">0</span>, <span class="number">5</span>, <span class="number">50</span>]       <span class="comment"># The list of planning_steps we want to try</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = ShortcutMazeEnvironment   <span class="comment"># The environment</span></span><br><span class="line">current_agent = DynaQAgent              <span class="comment"># The agent</span></span><br><span class="line"></span><br><span class="line">dataq = run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br><span class="line">plot_steps_per_episode(dataq)   </span><br></pre></td></tr></table></figure>
<pre><code>Planning steps :  0


100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:07&lt;00:00,  3.97it/s]


Planning steps :  5


100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:09&lt;00:00,  3.24it/s]


Planning steps :  50


100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:53&lt;00:00,  1.79s/it]</code></pre>
<figure>
<img src="output_27_6.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<p>What do you notice?</p>
<p>As the number of planning steps increases, the number of episodes taken to reach the goal decreases rapidly. Remember that the RNG seed was set the same for all the three values of planning steps, resulting in the same number of steps taken to reach the goal in the first episode. Thereafter, the performance improves. The slowest improvement is when there are <span class="math inline">\(n=0\)</span> planning steps, i.e., for the non-planning Q-learning agent, even though the step size parameter was optimized for it. Note that the grey dotted line shows the minimum number of steps required to reach the goal state under the optimal greedy policy.</p>
<hr />
<h3 id="experiments-dyna-q-agent-in-the-changing-maze-environment">Experiment(s): Dyna-Q agent in the <em>changing</em> maze environment</h3>
<p>Great! Now let us see how Dyna-Q performs on the version of the maze in which a shorter path opens up after 3000 steps. The rest of the transition and reward dynamics remain the same.</p>
<p><img src="shortcut_env_after.png" alt="environment" width="800"/></p>
<p>Before you proceed, take a moment to think about what you expect to see. Will Dyna-Q find the new, shorter path to the goal? If so, why? If not, why not?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment_with_state_visitations</span>(<span class="params">env, agent, env_parameters, agent_parameters, exp_parameters, result_file_name</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Experiment settings</span></span><br><span class="line">    num_runs = exp_parameters[<span class="string">&#x27;num_runs&#x27;</span>]</span><br><span class="line">    num_max_steps = exp_parameters[<span class="string">&#x27;num_max_steps&#x27;</span>]</span><br><span class="line">    planning_steps_all = agent_parameters[<span class="string">&#x27;planning_steps&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    env_info = &#123;<span class="string">&quot;change_at_n&quot;</span> : env_parameters[<span class="string">&quot;change_at_n&quot;</span>]&#125;                     </span><br><span class="line">    agent_info = &#123;<span class="string">&quot;num_states&quot;</span> : agent_parameters[<span class="string">&quot;num_states&quot;</span>],  </span><br><span class="line">                  <span class="string">&quot;num_actions&quot;</span> : agent_parameters[<span class="string">&quot;num_actions&quot;</span>],</span><br><span class="line">                  <span class="string">&quot;epsilon&quot;</span>: agent_parameters[<span class="string">&quot;epsilon&quot;</span>], </span><br><span class="line">                  <span class="string">&quot;discount&quot;</span>: env_parameters[<span class="string">&quot;discount&quot;</span>],</span><br><span class="line">                  <span class="string">&quot;step_size&quot;</span> : agent_parameters[<span class="string">&quot;step_size&quot;</span>]&#125;</span><br><span class="line"></span><br><span class="line">    state_visits_before_change = np.zeros((<span class="built_in">len</span>(planning_steps_all), num_runs, <span class="number">54</span>))  <span class="comment"># For saving the number of</span></span><br><span class="line">    state_visits_after_change = np.zeros((<span class="built_in">len</span>(planning_steps_all), num_runs, <span class="number">54</span>))   <span class="comment">#     state-visitations </span></span><br><span class="line">    cum_reward_all = np.zeros((<span class="built_in">len</span>(planning_steps_all), num_runs, num_max_steps))   <span class="comment"># For saving the cumulative reward</span></span><br><span class="line">    log_data = &#123;<span class="string">&#x27;planning_steps_all&#x27;</span> : planning_steps_all&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, planning_steps <span class="keyword">in</span> <span class="built_in">enumerate</span>(planning_steps_all):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Planning steps : &#x27;</span>, planning_steps)</span><br><span class="line">        os.system(<span class="string">&#x27;sleep 1&#x27;</span>)          <span class="comment"># to prevent tqdm printing out-of-order before the above print()</span></span><br><span class="line">        agent_info[<span class="string">&quot;planning_steps&quot;</span>] = planning_steps  <span class="comment"># We pass the agent the information it needs. </span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_runs)):</span><br><span class="line"></span><br><span class="line">            agent_info[<span class="string">&#x27;random_seed&#x27;</span>] = run</span><br><span class="line">            agent_info[<span class="string">&#x27;planning_random_seed&#x27;</span>] = run</span><br><span class="line"></span><br><span class="line">            rl_glue = RLGlue(env, agent)  <span class="comment"># Creates a new RLGlue experiment with the env and agent we chose above</span></span><br><span class="line">            rl_glue.rl_init(agent_info, env_info) <span class="comment"># We pass RLGlue what it needs to initialize the agent and environment</span></span><br><span class="line"></span><br><span class="line">            num_steps = <span class="number">0</span></span><br><span class="line">            cum_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> num_steps &lt; num_max_steps-<span class="number">1</span> :</span><br><span class="line"></span><br><span class="line">                state, _ = rl_glue.rl_start()  <span class="comment"># We start the experiment. We&#x27;ll be collecting the </span></span><br><span class="line">                is_terminal = <span class="literal">False</span>            <span class="comment"># state-visitation counts to visiualize the learned policy</span></span><br><span class="line">                <span class="keyword">if</span> num_steps &lt; env_parameters[<span class="string">&quot;change_at_n&quot;</span>]: </span><br><span class="line">                    state_visits_before_change[idx][run][state] += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    state_visits_after_change[idx][run][state] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal <span class="keyword">and</span> num_steps &lt; num_max_steps-<span class="number">1</span> :</span><br><span class="line">                    reward, state, action, is_terminal = rl_glue.rl_step()  </span><br><span class="line">                    num_steps += <span class="number">1</span></span><br><span class="line">                    cum_reward += reward</span><br><span class="line">                    cum_reward_all[idx][run][num_steps] = cum_reward</span><br><span class="line">                    <span class="keyword">if</span> num_steps &lt; env_parameters[<span class="string">&quot;change_at_n&quot;</span>]:</span><br><span class="line">                        state_visits_before_change[idx][run][state] += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        state_visits_after_change[idx][run][state] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    log_data[<span class="string">&#x27;state_visits_before&#x27;</span>] = state_visits_before_change</span><br><span class="line">    log_data[<span class="string">&#x27;state_visits_after&#x27;</span>] = state_visits_after_change</span><br><span class="line">    log_data[<span class="string">&#x27;cum_reward_all&#x27;</span>] = cum_reward_all</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> log_data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_cumulative_reward</span>(<span class="params">data_all, item_key, y_key, y_axis_label, legend_prefix, title</span>):</span></span><br><span class="line">    data_y_all = data_all[y_key]</span><br><span class="line">    items = data_all[item_key]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(items):</span><br><span class="line">        plt.plot(np.mean(data_y_all[i], axis=<span class="number">0</span>), label=legend_prefix+<span class="built_in">str</span>(item))</span><br><span class="line"></span><br><span class="line">    plt.axvline(x=<span class="number">3000</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, color=<span class="string">&#x27;grey&#x27;</span>, alpha=<span class="number">0.4</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Timesteps&#x27;</span>)</span><br><span class="line">    plt.ylabel(y_axis_label, rotation=<span class="number">0</span>, labelpad=<span class="number">60</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>Did you notice that the environment changes after a fixed number of <em>steps</em> and not episodes?</p>
<p>This is because the environment is separate from the agent, and the environment changes irrespective of the length of each episode (i.e., the number of environmental interactions per episode) that the agent perceives. And hence we are now plotting the data per step or interaction of the agent and the environment, in order to comfortably see the differences in the behaviours of the agents before and after the environment changes.</p>
<p>Okay, now we will first plot the cumulative reward obtained by the agent per interaction with the environment, averaged over 10 runs of the experiment on this changing world.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;num_runs&quot;</span> : <span class="number">10</span>,                     <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">    <span class="string">&quot;num_max_steps&quot;</span> : <span class="number">6000</span>,              <span class="comment"># The number of steps per experiment</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123; </span><br><span class="line">    <span class="string">&quot;discount&quot;</span>: <span class="number">0.95</span>,</span><br><span class="line">    <span class="string">&quot;change_at_n&quot;</span>: <span class="number">3000</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;  </span><br><span class="line">    <span class="string">&quot;num_states&quot;</span> : <span class="number">54</span>,</span><br><span class="line">    <span class="string">&quot;num_actions&quot;</span> : <span class="number">4</span>, </span><br><span class="line">    <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">    <span class="string">&quot;step_size&quot;</span> : <span class="number">0.125</span>,</span><br><span class="line">    <span class="string">&quot;planning_steps&quot;</span> : [<span class="number">5</span>, <span class="number">10</span>, <span class="number">50</span>]      <span class="comment"># The list of planning_steps we want to try</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = ShortcutMazeEnvironment   <span class="comment"># The environment</span></span><br><span class="line">current_agent = DynaQAgent              <span class="comment"># The agent</span></span><br><span class="line"></span><br><span class="line">dataq = run_experiment_with_state_visitations(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters, <span class="string">&quot;Dyna-Q_shortcut_steps&quot;</span>)    </span><br><span class="line">plot_cumulative_reward(dataq, <span class="string">&#x27;planning_steps_all&#x27;</span>, <span class="string">&#x27;cum_reward_all&#x27;</span>, <span class="string">&#x27;Cumulative\nreward&#x27;</span>, <span class="string">&#x27;Planning steps = &#x27;</span>, <span class="string">&#x27;Dyna-Q : Varying planning_steps&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Planning steps :  5


100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:10&lt;00:00,  1.08s/it]


Planning steps :  10


100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:16&lt;00:00,  1.70s/it]


Planning steps :  50


100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:19&lt;00:00,  7.99s/it]</code></pre>
<figure>
<img src="output_34_6.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<p>We observe that the slope of the curves is almost constant. If the agent had discovered the shortcut and begun using it, we would expect to see an increase in the slope of the curves towards the later stages of training. This is because the agent can get to the goal state faster and get the positive reward. Note that the timestep at which the shortcut opens up is marked by the grey dotted line.</p>
<p>Note that this trend is constant across the increasing number of planning steps.</p>
<p>Now let's check the heatmap of the state visitations of the agent with <code>planning_steps=10</code> during training, before and after the shortcut opens up after 3000 timesteps.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_state_visitations</span>(<span class="params">data, plot_titles, idx</span>):</span></span><br><span class="line">    data_keys = [<span class="string">&quot;state_visits_before&quot;</span>, <span class="string">&quot;state_visits_after&quot;</span>]</span><br><span class="line">    positions = [<span class="number">211</span>,<span class="number">212</span>]</span><br><span class="line">    titles = plot_titles</span><br><span class="line">    wall_ends = [<span class="literal">None</span>,-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line"></span><br><span class="line">        state_visits = data[data_keys[i]][idx]</span><br><span class="line">        average_state_visits = np.mean(state_visits, axis=<span class="number">0</span>)</span><br><span class="line">        grid_state_visits = np.rot90(average_state_visits.reshape((<span class="number">6</span>,<span class="number">9</span>)).T)</span><br><span class="line">        grid_state_visits[<span class="number">2</span>,<span class="number">1</span>:wall_ends[i]] = np.nan <span class="comment"># walls</span></span><br><span class="line">        <span class="comment">#print(average_state_visits.reshape((6,9)))</span></span><br><span class="line">        plt.subplot(positions[i])</span><br><span class="line">        plt.pcolormesh(grid_state_visits, edgecolors=<span class="string">&#x27;gray&#x27;</span>, linewidth=<span class="number">1</span>, cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line">        plt.text(<span class="number">3</span>+<span class="number">0.5</span>, <span class="number">0</span>+<span class="number">0.5</span>, <span class="string">&#x27;S&#x27;</span>, horizontalalignment=<span class="string">&#x27;center&#x27;</span>, verticalalignment=<span class="string">&#x27;center&#x27;</span>)</span><br><span class="line">        plt.text(<span class="number">8</span>+<span class="number">0.5</span>, <span class="number">5</span>+<span class="number">0.5</span>, <span class="string">&#x27;G&#x27;</span>, horizontalalignment=<span class="string">&#x27;center&#x27;</span>, verticalalignment=<span class="string">&#x27;center&#x27;</span>)</span><br><span class="line">        plt.title(titles[i])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        cm = plt.get_cmap()</span><br><span class="line">        cm.set_bad(<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplots_adjust(bottom=<span class="number">0.0</span>, right=<span class="number">0.7</span>, top=<span class="number">1.0</span>)</span><br><span class="line">    cax = plt.axes([<span class="number">1.</span>, <span class="number">0.0</span>, <span class="number">0.075</span>, <span class="number">1.</span>])</span><br><span class="line">    cbar = plt.colorbar(cax=cax)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line">plot_state_visitations(dataq, [<span class="string">&#x27;Dyna-Q : State visitations before the env changes&#x27;</span>, <span class="string">&#x27;Dyna-Q : State visitations after the env changes&#x27;</span>], <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="output_37_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<p>What do you observe?</p>
<p>The state visitation map looks almost the same before and after the shortcut opens. This means that the Dyna-Q agent hasn't quite discovered and started exploiting the new shortcut.</p>
<p>Now let's try increasing the exploration parameter <span class="math inline">\(\epsilon\)</span> to see if it helps the Dyna-Q agent discover the shortcut.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment_only_cumulative_reward</span>(<span class="params">env, agent, env_parameters, agent_parameters, exp_parameters</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Experiment settings</span></span><br><span class="line">    num_runs = exp_parameters[<span class="string">&#x27;num_runs&#x27;</span>]</span><br><span class="line">    num_max_steps = exp_parameters[<span class="string">&#x27;num_max_steps&#x27;</span>]</span><br><span class="line">    epsilons = agent_parameters[<span class="string">&#x27;epsilons&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    env_info = &#123;<span class="string">&quot;change_at_n&quot;</span> : env_parameters[<span class="string">&quot;change_at_n&quot;</span>]&#125;                     </span><br><span class="line">    agent_info = &#123;<span class="string">&quot;num_states&quot;</span> : agent_parameters[<span class="string">&quot;num_states&quot;</span>],  </span><br><span class="line">                  <span class="string">&quot;num_actions&quot;</span> : agent_parameters[<span class="string">&quot;num_actions&quot;</span>],</span><br><span class="line">                  <span class="string">&quot;planning_steps&quot;</span>: agent_parameters[<span class="string">&quot;planning_steps&quot;</span>], </span><br><span class="line">                  <span class="string">&quot;discount&quot;</span>: env_parameters[<span class="string">&quot;discount&quot;</span>],</span><br><span class="line">                  <span class="string">&quot;step_size&quot;</span> : agent_parameters[<span class="string">&quot;step_size&quot;</span>]&#125;</span><br><span class="line"></span><br><span class="line">    log_data = &#123;<span class="string">&#x27;epsilons&#x27;</span> : epsilons&#125; </span><br><span class="line">    cum_reward_all = np.zeros((<span class="built_in">len</span>(epsilons), num_runs, num_max_steps))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> eps_idx, epsilon <span class="keyword">in</span> <span class="built_in">enumerate</span>(epsilons):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Agent : Dyna-Q, epsilon : %f&#x27;</span> % epsilon)</span><br><span class="line">        os.system(<span class="string">&#x27;sleep 1&#x27;</span>)          <span class="comment"># to prevent tqdm printing out-of-order before the above print()</span></span><br><span class="line">        agent_info[<span class="string">&quot;epsilon&quot;</span>] = epsilon</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_runs)):</span><br><span class="line"></span><br><span class="line">            agent_info[<span class="string">&#x27;random_seed&#x27;</span>] = run</span><br><span class="line">            agent_info[<span class="string">&#x27;planning_random_seed&#x27;</span>] = run</span><br><span class="line"></span><br><span class="line">            rl_glue = RLGlue(env, agent)  <span class="comment"># Creates a new RLGlue experiment with the env and agent we chose above</span></span><br><span class="line">            rl_glue.rl_init(agent_info, env_info) <span class="comment"># We pass RLGlue what it needs to initialize the agent and environment</span></span><br><span class="line"></span><br><span class="line">            num_steps = <span class="number">0</span></span><br><span class="line">            cum_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> num_steps &lt; num_max_steps-<span class="number">1</span> :</span><br><span class="line"></span><br><span class="line">                rl_glue.rl_start()  <span class="comment"># We start the experiment</span></span><br><span class="line">                is_terminal = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal <span class="keyword">and</span> num_steps &lt; num_max_steps-<span class="number">1</span> :</span><br><span class="line">                    reward, _, action, is_terminal = rl_glue.rl_step()  <span class="comment"># The environment and agent take a step and return</span></span><br><span class="line">                    <span class="comment"># the reward, and action taken.</span></span><br><span class="line">                    num_steps += <span class="number">1</span></span><br><span class="line">                    cum_reward += reward</span><br><span class="line">                    cum_reward_all[eps_idx][run][num_steps] = cum_reward</span><br><span class="line"></span><br><span class="line">    log_data[<span class="string">&#x27;cum_reward_all&#x27;</span>] = cum_reward_all</span><br><span class="line">    <span class="keyword">return</span> log_data</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;num_runs&quot;</span> : <span class="number">30</span>,                     <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">    <span class="string">&quot;num_max_steps&quot;</span> : <span class="number">6000</span>,              <span class="comment"># The number of steps per experiment</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123; </span><br><span class="line">    <span class="string">&quot;discount&quot;</span>: <span class="number">0.95</span>,</span><br><span class="line">    <span class="string">&quot;change_at_n&quot;</span>: <span class="number">3000</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;  </span><br><span class="line">    <span class="string">&quot;num_states&quot;</span> : <span class="number">54</span>,</span><br><span class="line">    <span class="string">&quot;num_actions&quot;</span> : <span class="number">4</span>, </span><br><span class="line">    <span class="string">&quot;step_size&quot;</span> : <span class="number">0.125</span>,</span><br><span class="line">    <span class="string">&quot;planning_steps&quot;</span> : <span class="number">10</span>,</span><br><span class="line">    <span class="string">&quot;epsilons&quot;</span>: [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.8</span>]    <span class="comment"># The list of epsilons we want to try</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = ShortcutMazeEnvironment   <span class="comment"># The environment</span></span><br><span class="line">current_agent = DynaQAgent              <span class="comment"># The agent</span></span><br><span class="line"></span><br><span class="line">data = run_experiment_only_cumulative_reward(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)</span><br><span class="line">plot_cumulative_reward(data, <span class="string">&#x27;epsilons&#x27;</span>, <span class="string">&#x27;cum_reward_all&#x27;</span>, <span class="string">&#x27;Cumulative\nreward&#x27;</span>, <span class="string">r&#x27;$\epsilon$ = &#x27;</span>, <span class="string">r&#x27;Dyna-Q : Varying $\epsilon$&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Agent : Dyna-Q, epsilon : 0.100000


100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:52&lt;00:00,  1.75s/it]


Agent : Dyna-Q, epsilon : 0.200000


100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:49&lt;00:00,  1.65s/it]


Agent : Dyna-Q, epsilon : 0.400000


100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:50&lt;00:00,  1.69s/it]


Agent : Dyna-Q, epsilon : 0.800000


100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:52&lt;00:00,  1.74s/it]</code></pre>
<figure>
<img src="output_40_8.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<p>What do you observe?</p>
<p>Increasing the exploration via the <span class="math inline">\(\epsilon\)</span>-greedy strategy does not seem to be helping. In fact, the agent's cumulative reward decreases because it is spending more and more time trying out the exploratory actions.</p>
<p>Can we do better...?</p>
<h2 id="section-2-dyna-q">Section 2: Dyna-Q+</h2>
<p>The motivation behind Dyna-Q+ is to give a bonus reward for actions that haven't been tried for a long time, since there is a greater chance that the dynamics for that actions might have changed.</p>
<p>In particular, if the modeled reward for a transition is <span class="math inline">\(r\)</span>, and the transition has not been tried in <span class="math inline">\(\tau(s,a)\)</span> time steps, then planning updates are done as if that transition produced a reward of <span class="math inline">\(r + \kappa \sqrt{ \tau(s,a)}\)</span>, for some small <span class="math inline">\(\kappa\)</span>.</p>
<p>Let's implement that!</p>
<p>Based on your <code>DynaQAgent</code>, create a new class <code>DynaQPlusAgent</code> to implement the aforementioned exploration heuristic. Additionally : 1. actions that had never been tried before from a state should now be allowed to be considered in the planning step, 2. and the initial model for such actions is that they lead back to the same state with a reward of zero.</p>
<p>At this point, you might want to refer to the video lectures and <a target="_blank" rel="noopener" href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=188">Section 8.3</a> of the RL textbook for a refresher on Dyna-Q+.</p>
<p>As usual, let's break this down in pieces and do it one-by-one.</p>
<p>First of all, check out the <code>agent_init</code> method below. In particular, pay attention to the attributes which are new to <code>DynaQPlusAgent</code>√¢‚Ç¨‚Äú state-visitation counts <span class="math inline">\(\tau\)</span> and the scaling parameter <span class="math inline">\(\kappa\)</span> √¢‚Ç¨‚Äú because you shall be using them later.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynaQPlusAgent</span>(<span class="params">BaseAgent</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span>(<span class="params">self, agent_info</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">            &#123;</span></span><br><span class="line"><span class="string">                num_states (int): The number of states,</span></span><br><span class="line"><span class="string">                num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">                epsilon (float): The parameter for epsilon-greedy exploration,</span></span><br><span class="line"><span class="string">                step_size (float): The step-size,</span></span><br><span class="line"><span class="string">                discount (float): The discount factor,</span></span><br><span class="line"><span class="string">                planning_steps (int): The number of planning steps per environmental interaction</span></span><br><span class="line"><span class="string">                kappa (float): The scaling factor for the reward bonus</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                random_seed (int): the seed for the RNG used in epsilon-greedy</span></span><br><span class="line"><span class="string">                planning_random_seed (int): the seed for the RNG used in the planner</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># First, we get the relevant information from agent_info </span></span><br><span class="line">        <span class="comment"># Note: we use np.random.RandomState(seed) to set the two different RNGs</span></span><br><span class="line">        <span class="comment"># for the planner and the rest of the code</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.num_states = agent_info[<span class="string">&quot;num_states&quot;</span>]</span><br><span class="line">            self.num_actions = agent_info[<span class="string">&quot;num_actions&quot;</span>]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;You need to pass both &#x27;num_states&#x27; and &#x27;num_actions&#x27; \</span></span><br><span class="line"><span class="string">                   in agent_info to initialize the action-value table&quot;</span>)</span><br><span class="line">        self.gamma = agent_info.get(<span class="string">&quot;discount&quot;</span>, <span class="number">0.95</span>)</span><br><span class="line">        self.step_size = agent_info.get(<span class="string">&quot;step_size&quot;</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.epsilon = agent_info.get(<span class="string">&quot;epsilon&quot;</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.planning_steps = agent_info.get(<span class="string">&quot;planning_steps&quot;</span>, <span class="number">10</span>)</span><br><span class="line">        self.kappa = agent_info.get(<span class="string">&quot;kappa&quot;</span>, <span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">&#x27;random_seed&#x27;</span>, <span class="number">42</span>))</span><br><span class="line">        self.planning_rand_generator = np.random.RandomState(agent_info.get(<span class="string">&#x27;planning_random_seed&#x27;</span>, <span class="number">42</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Next, we initialize the attributes required by the agent, e.g., q_values, model, tau, etc.</span></span><br><span class="line">        <span class="comment"># The visitation-counts can be stored as a table as well, like the action values </span></span><br><span class="line">        self.q_values = np.zeros((self.num_states, self.num_actions))</span><br><span class="line">        self.tau = np.zeros((self.num_states, self.num_actions))</span><br><span class="line">        self.actions = <span class="built_in">list</span>(<span class="built_in">range</span>(self.num_actions))</span><br><span class="line">        self.past_action = -<span class="number">1</span></span><br><span class="line">        self.past_state = -<span class="number">1</span></span><br><span class="line">        self.model = &#123;&#125;</span><br></pre></td></tr></table></figure>
<p>Now first up, implement the <code>update_model</code> method. Note that this is different from Dyna-Q in the aforementioned way.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQPlusAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_model</span>(<span class="params">self, past_state, past_action, state, reward</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;updates the model </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        past_state  (int): s</span></span><br><span class="line"><span class="string">        past_action (int): a</span></span><br><span class="line"><span class="string">        state       (int): s&#x27;</span></span><br><span class="line"><span class="string">        reward      (int): r</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Nothing</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Recall that when adding a state-action to the model, if the agent is visiting the state</span></span><br><span class="line">    <span class="comment">#    for the first time, then the remaining actions need to be added to the model as well</span></span><br><span class="line">    <span class="comment">#    with zero reward and a transition into itself.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note: do *not* update the visitation-counts here. We will do that in `agent_step`.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># (3 lines)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> past_state <span class="keyword">not</span> <span class="keyword">in</span> self.model:</span><br><span class="line">        self.model[past_state] = &#123;past_action : (state, reward)&#125;</span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> self.actions:</span><br><span class="line">            <span class="keyword">if</span> action != past_action:</span><br><span class="line">                self.model[past_state][action] = (past_state, <span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ----------------</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.model[past_state][past_action] = (state, reward)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h3 id="test-update_model-1">Test <code>update_model()</code></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">actions = []</span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">&quot;num_states&quot;</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">&quot;discount&quot;</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">&quot;random_seed&quot;</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">&quot;planning_random_seed&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQPlusAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">agent.tau[<span class="number">0</span>][<span class="number">0</span>] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">        <span class="number">1</span>: (<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">        <span class="number">2</span>: (<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">        <span class="number">2</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">        <span class="number">3</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br></pre></td></tr></table></figure>
<p>Next, you will implement the <code>planning_step()</code> method. This will be very similar to the one you implemented in <code>DynaQAgent</code>, but here you will be adding the exploration bonus to the reward in the simulated transition.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQPlusAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">planning_step</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;performs planning, i.e. indirect RL.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Nothing</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The indirect RL step:</span></span><br><span class="line">    <span class="comment"># - Choose a state and action from the set of experiences that are stored in the model. (~2 lines)</span></span><br><span class="line">    <span class="comment"># - Query the model with this state-action pair for the predicted next state and reward.(~1 line)</span></span><br><span class="line">    <span class="comment"># - **Add the bonus to the reward** (~1 line)</span></span><br><span class="line">    <span class="comment"># - Update the action values with this simulated experience.                            (2~4 lines)</span></span><br><span class="line">    <span class="comment"># - Repeat for the required number of planning steps.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that the update equation is different for terminal and non-terminal transitions. </span></span><br><span class="line">    <span class="comment"># To differentiate between a terminal and a non-terminal next state, assume that the model stores</span></span><br><span class="line">    <span class="comment"># the terminal state as a dummy state like -1</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Important: remember you have a random number generator &#x27;planning_rand_generator&#x27; as </span></span><br><span class="line">    <span class="comment">#     a part of the class which you need to use as self.planning_rand_generator.choice()</span></span><br><span class="line">    <span class="comment">#     For the sake of reproducibility and grading, *do not* use anything else like </span></span><br><span class="line">    <span class="comment">#     np.random.choice() for performing search control.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.planning_steps):</span><br><span class="line">    </span><br><span class="line">        s = self.planning_rand_generator.choice(<span class="built_in">list</span>(self.model.keys()))</span><br><span class="line">        a = self.planning_rand_generator.choice(<span class="built_in">list</span>(self.model[s].keys()))</span><br><span class="line"></span><br><span class="line">        (next_s,r) = self.model[s][a]</span><br><span class="line">        r += self.kappa * np.sqrt(self.tau[s][a])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> next_s == -<span class="number">1</span>:</span><br><span class="line">            expect = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            expect = self.gamma * np.<span class="built_in">max</span>(self.q_values[next_s])</span><br><span class="line">        self.q_values[s,a] += self.step_size * (r + expect - self.q_values[s,a])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure>
<h3 id="test-planning_step-1">Test <code>planning_step()</code></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Test code for planning_step() ##</span></span><br><span class="line"></span><br><span class="line">actions = []</span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">&quot;num_states&quot;</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">&quot;discount&quot;</span>: <span class="number">1.0</span>, </span><br><span class="line">              <span class="string">&quot;kappa&quot;</span>: <span class="number">0.001</span>,</span><br><span class="line">              <span class="string">&quot;planning_steps&quot;</span>: <span class="number">4</span>,</span><br><span class="line">              <span class="string">&quot;random_seed&quot;</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">&quot;planning_random_seed&quot;</span>: <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQPlusAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">1</span>,-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.tau += <span class="number">1</span></span><br><span class="line">agent.tau[<span class="number">0</span>][<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.tau += <span class="number">1</span></span><br><span class="line">agent.tau[<span class="number">0</span>][<span class="number">2</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">agent.update_model(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">agent.tau += <span class="number">1</span></span><br><span class="line">agent.tau[<span class="number">2</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">agent.planning_step()</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (-<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">        <span class="number">0</span>: (<span class="number">0</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">2</span>: (<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">        <span class="number">3</span>: (<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;, </span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">0</span>: (<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">2</span>: (<span class="number">2</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">3</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.10014142</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.00036373</span>, <span class="number">0</span>, <span class="number">0.00017321</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br></pre></td></tr></table></figure>
<p>Again, before you move on to implement the rest of the agent methods, here are the couple of helper functions that you've used in the previous assessments for choosing an action using an <span class="math inline">\(\epsilon\)</span>-greedy policy.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQPlusAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span>(<span class="params">self, q_values</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;argmax with random tie-breaking</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        q_values (Numpy array): the array of action values</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        action (int): an action with the highest value</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    top = <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>)</span><br><span class="line">    ties = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(q_values)):</span><br><span class="line">        <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">            top = q_values[i]</span><br><span class="line">            ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">            ties.append(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action_egreedy</span>(<span class="params">self, state</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;returns an action using an epsilon-greedy policy w.r.t. the current action-value function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Important: assume you have a random number generator &#x27;rand_generator&#x27; as a part of the class</span></span><br><span class="line"><span class="string">                which you can use as self.rand_generator.choice() or self.rand_generator.rand()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (List): coordinates of the agent (two elements)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The action taken w.r.t. the aforementioned epsilon-greedy policy</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">        action = self.rand_generator.choice(self.actions)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        values = self.q_values[state]</span><br><span class="line">        action = self.argmax(values)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure>
<p>Now implement the rest of the agent-related methods, namely <code>agent_start</code>, <code>agent_step</code>, and <code>agent_end</code>. Again, these will be very similar to the ones in the <code>DynaQAgent</code>, but you will have to think of a way to update the counts since the last visit.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">%%add_to DynaQPlusAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_start</span>(<span class="params">self, state</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;The first method called when the experiment starts, called after</span></span><br><span class="line"><span class="string">    the environment starts.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment&#x27;s env_start function.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (int) The first action the agent takes.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># given the state, select the action using self.choose_action_egreedy(), </span></span><br><span class="line">    <span class="comment"># and save current state and action (~2 lines)</span></span><br><span class="line">    <span class="comment">### self.past_state = ?</span></span><br><span class="line">    <span class="comment">### self.past_action = ?</span></span><br><span class="line">    <span class="comment"># Note that the last-visit counts are not updated here.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.past_state = state</span><br><span class="line">    self.past_action = self.choose_action_egreedy(state)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.past_action</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, state</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;A step taken by the agent.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment&#x27;s step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">            last step</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (int) The action the agent is taking.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update the last-visited counts (~2 lines)</span></span><br><span class="line">    <span class="comment"># - Direct-RL step (1~3 lines)</span></span><br><span class="line">    <span class="comment"># - Model Update step (~1 line)</span></span><br><span class="line">    <span class="comment"># - `planning_step` (~1 line)</span></span><br><span class="line">    <span class="comment"># - Action Selection step (~1 line)</span></span><br><span class="line">    <span class="comment"># Save the current state and action before returning the action to be performed. (~2 lines)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.tau += <span class="number">1</span></span><br><span class="line">    self.tau[self.past_state,self.past_action] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    self.q_values[self.past_state,self.past_action] += self.step_size * (reward + self.gamma * np.<span class="built_in">max</span>(self.q_values[state]) - self.q_values[self.past_state,self.past_action])</span><br><span class="line">    self.update_model(self.past_state, self.past_action, state, reward)</span><br><span class="line">    self.planning_step()</span><br><span class="line">    </span><br><span class="line">    action = self.choose_action_egreedy(state)</span><br><span class="line">    self.past_state = state</span><br><span class="line">    self.past_action = action</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.past_action</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_end</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Called when the agent terminates.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">            terminal state.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Again, add the same components you added in agent_step to augment Dyna-Q into Dyna-Q+</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    self.tau += <span class="number">1</span></span><br><span class="line">    self.tau[self.past_state,self.past_action] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    self.q_values[self.past_state,self.past_action] += self.step_size * (reward  - self.q_values[self.past_state,self.past_action])</span><br><span class="line">    self.update_model(self.past_state, self.past_action, -<span class="number">1</span>, reward)</span><br><span class="line">    self.planning_step()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ----------------</span></span><br></pre></td></tr></table></figure>
<h3 id="test-agent_start-agent_step-and-agent_end-1">Test <code>agent_start()</code>, <code>agent_step()</code>, and <code>agent_end()</code></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">4</span>, </span><br><span class="line">              <span class="string">&quot;num_states&quot;</span>: <span class="number">3</span>, </span><br><span class="line">              <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">              <span class="string">&quot;discount&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">              <span class="string">&quot;kappa&quot;</span>: <span class="number">0.001</span>,</span><br><span class="line">              <span class="string">&quot;random_seed&quot;</span>: <span class="number">0</span>,</span><br><span class="line">              <span class="string">&quot;planning_steps&quot;</span>: <span class="number">4</span>,</span><br><span class="line">              <span class="string">&quot;planning_random_seed&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">agent = DynaQPlusAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>) <span class="comment"># state</span></span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.tau, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> agent.model == &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># test agent step</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_tau = np.array([</span><br><span class="line">    [<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(agent.tau == expected_tau)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.0191</span>, <span class="number">0.271</span>, <span class="number">0.0</span>, <span class="number">0.0191</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.000183847763</span>, <span class="number">0.000424264069</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;</span><br><span class="line">    <span class="number">0</span>: &#123;</span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">1</span>), </span><br><span class="line">        <span class="number">0</span>: (<span class="number">0</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">2</span>: (<span class="number">0</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">3</span>: (<span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;, </span><br><span class="line">    <span class="number">2</span>: &#123;</span><br><span class="line">        <span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">0</span>: (<span class="number">2</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">1</span>: (<span class="number">2</span>, <span class="number">0</span>), </span><br><span class="line">        <span class="number">2</span>: (<span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># test agent end</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_tau = np.array([</span><br><span class="line">    [<span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(agent.tau == expected_tau)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.0191</span>, <span class="number">0.344083848</span>, <span class="number">0</span>, <span class="number">0.0444632051</span>],</span><br><span class="line">    [<span class="number">0.0191732051</span>, <span class="number">0.19</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.000183847763</span>, <span class="number">0.000424264069</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.allclose(agent.q_values, expected_values)</span><br><span class="line"></span><br><span class="line">expected_model = &#123;<span class="number">0</span>: &#123;<span class="number">1</span>: (<span class="number">2</span>, <span class="number">1</span>), <span class="number">0</span>: (<span class="number">0</span>, <span class="number">0</span>), <span class="number">2</span>: (<span class="number">0</span>, <span class="number">0</span>), <span class="number">3</span>: (<span class="number">0</span>, <span class="number">0</span>)&#125;, <span class="number">2</span>: &#123;<span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>), <span class="number">0</span>: (<span class="number">2</span>, <span class="number">0</span>), <span class="number">1</span>: (<span class="number">2</span>, <span class="number">0</span>), <span class="number">2</span>: (<span class="number">2</span>, <span class="number">0</span>)&#125;, <span class="number">1</span>: &#123;<span class="number">1</span>: (-<span class="number">1</span>, <span class="number">1</span>), <span class="number">0</span>: (<span class="number">1</span>, <span class="number">0</span>), <span class="number">2</span>: (<span class="number">1</span>, <span class="number">0</span>), <span class="number">3</span>: (<span class="number">1</span>, <span class="number">0</span>)&#125;&#125;</span><br><span class="line"><span class="keyword">assert</span> agent.model == expected_model</span><br></pre></td></tr></table></figure>
<h3 id="experiment-dyna-q-agent-in-the-changing-environment">Experiment: Dyna-Q+ agent in the <em>changing</em> environment</h3>
<p>Okay, now we're ready to test our Dyna-Q+ agent on the Shortcut Maze. As usual, we will average the results over 30 independent runs of the experiment.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Experiment parameters</span></span><br><span class="line">experiment_parameters = &#123;</span><br><span class="line">    <span class="string">&quot;num_runs&quot;</span> : <span class="number">30</span>,                     <span class="comment"># The number of times we run the experiment</span></span><br><span class="line">    <span class="string">&quot;num_max_steps&quot;</span> : <span class="number">6000</span>,              <span class="comment"># The number of steps per experiment</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Environment parameters</span></span><br><span class="line">environment_parameters = &#123; </span><br><span class="line">    <span class="string">&quot;discount&quot;</span>: <span class="number">0.95</span>,</span><br><span class="line">    <span class="string">&quot;change_at_n&quot;</span>: <span class="number">3000</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent parameters</span></span><br><span class="line">agent_parameters = &#123;  </span><br><span class="line">    <span class="string">&quot;num_states&quot;</span> : <span class="number">54</span>,</span><br><span class="line">    <span class="string">&quot;num_actions&quot;</span> : <span class="number">4</span>, </span><br><span class="line">    <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, </span><br><span class="line">    <span class="string">&quot;step_size&quot;</span> : <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&quot;planning_steps&quot;</span> : [<span class="number">50</span>]      </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">current_env = ShortcutMazeEnvironment   <span class="comment"># The environment</span></span><br><span class="line">current_agent = DynaQPlusAgent          <span class="comment"># The agent</span></span><br><span class="line"></span><br><span class="line">data_qplus = run_experiment_with_state_visitations(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters, <span class="string">&quot;Dyna-Q+&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Planning steps :  50


100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [04:21&lt;00:00,  8.72s/it]</code></pre>
<p>Let's compare the Dyna-Q and Dyna-Q+ agents with <code>planning_steps=50</code> each.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_cumulative_reward_comparison</span>(<span class="params">data1, data2</span>):</span></span><br><span class="line"></span><br><span class="line">    cum_reward_q = data1[<span class="string">&#x27;cum_reward_all&#x27;</span>][<span class="number">2</span>]</span><br><span class="line">    cum_reward_qPlus = data2[<span class="string">&#x27;cum_reward_all&#x27;</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    plt.plot(np.mean(cum_reward_qPlus, axis=<span class="number">0</span>), label=<span class="string">&#x27;Dyna-Q+&#x27;</span>)</span><br><span class="line">    plt.plot(np.mean(cum_reward_q, axis=<span class="number">0</span>), label=<span class="string">&#x27;Dyna-Q&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.axvline(x=<span class="number">3000</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, color=<span class="string">&#x27;grey&#x27;</span>, alpha=<span class="number">0.4</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Timesteps&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Cumulative\nreward&#x27;</span>, rotation=<span class="number">0</span>, labelpad=<span class="number">60</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Average performance of Dyna-Q and Dyna-Q+ agents in the Shortcut Maze\n&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">plot_cumulative_reward_comparison(dataq, data_qplus)</span><br></pre></td></tr></table></figure>
<figure>
<img src="output_64_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<p>What do you observe? (For reference, your graph should look like <a target="_blank" rel="noopener" href="http://www.incompleteideas.net/book/RLbook2018.pdf#page=189">Figure 8.5 in Chapter 8</a> of the RL textbook)</p>
<p>The slope of the curve increases for the Dyna-Q+ curve shortly after the shortcut opens up after 3000 steps, which indicates that the rate of receiving the positive reward increases. This implies that the Dyna-Q+ agent finds the shorter path to the goal.</p>
<p>To verify this, let us plot the state-visitations of the Dyna-Q+ agent before and after the shortcut opens up.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">plot_state_visitations(data_qplus, [<span class="string">&#x27;Dyna-Q+ : State visitations before the env changes&#x27;</span>, <span class="string">&#x27;Dyna-Q+ : State visitations after the env changes&#x27;</span>], <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="output_66_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<p>What do you observe?</p>
<p>Before the shortcut opens up, like Dyna-Q, the Dyna-Q+ agent finds the sole, long path to the goal. But because the Dyna-Q+ agent keeps exploring, it succeeds in discovering the shortcut once it opens up, which leads to the goal faster. So the bonus reward heuristic is effective in helping the agent explore and find changes in the environment without degrading the performance.</p>
<h2 id="wrapping-up">Wrapping Up</h2>
<p>Congratulations! You have:</p>
<ol type="1">
<li>implemented Dyna-Q, a model-based approach to RL;</li>
<li>implemented Dyna-Q+, a variant of Dyna-Q with an exploration bonus that encourages exploration;</li>
<li>conducted scientific experiments to empirically validate the exploration/exploitation dilemma in the planning context on an environment that changes with time.</li>
</ol>
<p>Some points to ponder about: 1. At what cost does Dyna-Q+ improve over Dyna-Q? 2. In general, what is the trade-off of using model-based methods like Dyna-Q over model-free methods like Q-learning?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Planning-and-learning-with-Tabular-Methods/2020/09/29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Planning-and-learning-with-Tabular-Methods/2020/09/29/" class="post-title-link" itemprop="url">Planning and learning with Tabular Methods</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-09-29 16:09:45 / Modified: 16:19:02" itemprop="dateCreated datePublished" datetime="2020-09-29T16:09:45+08:00">2020-09-29</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Planning-and-learning-with-Tabular-Methods/2020/09/29/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Planning-and-learning-with-Tabular-Methods/2020/09/29/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We develop a unified view of reinforcement learning methods that require a model of the environment, such as dynamic programming and heuristic search, and methods that can be used without a model, such as Monte Carlo and temporal-difference methods. These are respectively called <strong>model-based</strong> and <strong>model-free</strong> reinforcement learning methods. Model-based methods rely on planning as their primary component, while model-free methods primarily rely on learning. Although there are real di‚Üµerences between these two kinds of methods, there are also great similarities.</p>
<ul>
<li>All state-space planning methods involve computing value functions as a key intermediate step toward improving the policy</li>
<li>They compute value functions by updates or backup operations applied to simulated experience.</li>
</ul>
<h2 id="model">Model</h2>
<ul>
<li>By a model of the environment we mean anything that an agent can use to predict how the environment will respond to its actions.</li>
<li>Distribution model produce a description of all possibilities and their probabilities. Sample model produce just one of the possibilities and their probabilities.</li>
<li>ÂΩìÁªôÂÆö‰∏Ä‰∏™ state Âíå‰∏Ä‰∏™ action Êó∂Ôºådistribution model ÂèØ‰ª•ÁîüÊàêÊâÄÊúâÂèØËÉΩÁöÑÁä∂ÊÄÅËΩ¨ÁßªÔºåËÄåsample modelÂè™ËÉΩÁªôÂá∫‰∏Ä‰∏™ÂèØËÉΩÁöÑÁä∂ÊÄÅËΩ¨Áßª</li>
<li>ÂΩìÁªôÂÆö‰∏Ä‰∏™ state Âíå Policy Êó∂Ôºådistribution model ÂèØ‰ª•Ëé∑ÂæóÊâÄÊúâÂèØËÉΩÁöÑ episode Âπ∂ÂæóÂà∞‰ªñ‰ª¨Âá∫Áé∞ÁöÑÊ¶ÇÁéáÔºå‰ΩÜ sample model Âè™ËÉΩÁªôÂá∫‰∏Ä‰∏™ episode</li>
</ul>
<p>ÊÄª‰πãÔºådistribution model ÊØî sample modelÂåÖÂê´Êõ¥Â§ö‰ø°ÊÅØÔºå‰ΩÜÁé∞ÂÆû‰∏≠ÂæÄÂæÄÊõ¥ÂÆπÊòìËé∑Âæósample model„ÄÇÁÆÄÂçïÊù•ËØ¥Ôºådistribution model ÂåÖÂê´‰∫ÜÊâÄÊúâÁä∂ÊÄÅÁöÑËΩ¨ÁßªÊ¶ÇÁéáÔºå‰ΩÜsample modelÊõ¥ÂÉèÊòØÁÆ°‰∏≠Á™•Ë±πÔºåÂèØËßÅ‰∏ÄÊñë„ÄÇÂú®DP‰∏≠ÔºåÊàë‰ª¨Áî®Âà∞ÁöÑÊòØdistribution modelÔºåËÄåÂú®MC‰∏≠Êàë‰ª¨Áî®Âà∞ÁöÑÊòØsample model„ÄÇmodel ÊòØÂØπÁéØÂ¢ÉÁöÑ‰∏ÄÁßçË°®ËææÊñπÂºèÔºåÔºà‰∏ç‰∏ÄÂÆöÊòØÁúüÂÆûÊàñÂÆåÂÖ®Ê≠£Á°ÆÁöÑÔºâÔºåÂèØ‰ª•Áî®Êù•‰∫ßÁîü‰ªøÁúüÁªèÈ™åÔºàsimulation experienceÔºâ„ÄÇ</p>
<h2 id="planning">Planning</h2>
<p>‰ªéModel‰∏≠ÁîüÊàêÊàñÊèêÂçáPolicy ÁöÑËÆ°ÁÆóËøáÁ®ãÁß∞‰∏∫ Planning:</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="1.png" width = "80%" height="80%">
</center>
<p>Ê≥®ÊÑèÊú¨ÊñáËÆ®ËÆ∫ÁöÑPlanningÈÉΩÊòØstate space PlanningÔºåËøôÁßçPlanningÊúâ‰∏§‰∏™ÁâπÁÇπÔºö</p>
<ul>
<li>ÈÄöËøáËÆ°ÁÆóvalues function Êù•ËøõË°åPolicy ÊèêÂçá</li>
<li>Ê†πÊçÆsimulated experienceÊù•ËÆ°ÁÆóvalue function</li>
</ul>
<p>PlanningÔºàÂ¶ÇDPÔºâ ÂíålearningÔºàÂ¶ÇMC„ÄÅTDÔºâÊñπÊ≥ïÁöÑÊ†∏ÂøÉÈÉΩÊòØÁî®backing-up Êõ¥Êñ∞ÂÖ¨ÂºèËÆ°ÁÆóvalue function ÁöÑ‰º∞ËÆ°ÂÄº„ÄÇÂå∫Âà´Âú®‰∫éPlanning ÊâÄÁî®ÁªèÈ™åÊòØÊúâÊ®°ÂûãÁîüÊàêÁöÑsimulated exprienceÔºåËÄålearning method‰ΩøÁî®ÁöÑÁªèÈ™åÊòØÁî±ÁúüÂÆûÁéØÂ¢ÉÁîüÊàêÁöÑreal exprience„ÄÇ ‰ΩÜ‰∏§ËÄÖÈÉΩÊª°Ë∂≥‰∏äËø∞state space PlanningÁªìÊûÑÔºåËøôË°®Á§∫ÂæàÂ§öÊÄùÊÉ≥ÂíåÁÆóÊ≥ïÂèØ‰ª•Áõ∏‰∫íÂÄüÈâ¥ÔºåÂú®Â∫îÁî®‰∏≠Â∏∏Â∏∏Áî® learning ‰∏≠ value function ‰º∞ËÆ°ÂÄºÁöÑÊõ¥Êñ∞ÂÖ¨ÂºèÂèñ‰ª£ Planning ‰∏≠ÁöÑ value function ‰º∞ËÆ°ÂÄºÁöÑÊõ¥Êñ∞ÂÖ¨Âºè„ÄÇ‰æãÂ¶ÇÔºåÊàë‰ª¨ÂèØ‰ª•Â∞ÜQ learning Âíå planning ÁªìÂêàÔºåÂæóÂà∞random-sample one-step tabular Q-planning ÊñπÊ≥ïÔºö</p>
<center>
<img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="2.png" width = "80%" height="80%">
</center>
<p>one-step tabular Q-learningÊúÄÁªà‰ºöÊî∂ÊïõÂà∞‰∏Ä‰∏™ÂØπÂ∫î‰∫éÁúüÂÆûÁéØÂ¢ÉÁöÑoptimal PolicyÔºåËÄå random-sample one-step tabular Q-planning ÂàôÊî∂ÊïõÂà∞‰∏Ä‰∏™ÂØπÂ∫î‰∫émodel ÁöÑoptimal Policy„ÄÇ</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Q-Learning-and-Expected-Sarsa/2020/09/29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Q-Learning-and-Expected-Sarsa/2020/09/29/" class="post-title-link" itemprop="url">Q-Learning and Expected Sarsa </a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-09-29 10:52:00" itemprop="dateCreated datePublished" datetime="2020-09-29T10:52:00+08:00">2020-09-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-10-14 17:19:36" itemprop="dateModified" datetime="2020-10-14T17:19:36+08:00">2020-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Q-Learning-and-Expected-Sarsa/2020/09/29/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Q-Learning-and-Expected-Sarsa/2020/09/29/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="assignment-2---q-learning-and-expected-sarsa">Assignment 2 - Q-Learning and Expected Sarsa</h1>
<p>Welcome to Course 2 Programming Assignment 2. In this notebook, you will:</p>
<ol type="1">
<li>Implement Q-Learning with <span class="math inline">\(\epsilon\)</span>-greedy action selection</li>
<li>Implement Expected Sarsa with <span class="math inline">\(\epsilon\)</span>-greedy action selection</li>
<li>Investigate how these two algorithms behave on Cliff World (described on page 132 of the textbook)</li>
</ol>
<p>We will provide you with the environment and infrastructure to run an experiment (called the experiment program in RL-Glue). This notebook will provide all the code you need to run your experiment and visualise learning performance.</p>
<p>This assignment will be graded automatically by comparing the behavior of your agent to our implementations of Expected Sarsa and Q-learning. The random seed will be set to avoid different behavior due to randomness. We will highlight the functions you have to use for generating random samples and the number of times these functions should be called.</p>
<h2 id="packages">Packages</h2>
<p>You will need the following libraries for this assignment. We are using: 1. numpy: the fundamental package for scientific computing with Python. 2. scipy: a Python library for scientific and technical computing. 3. matplotlib: library for plotting graphs in Python. 4. RL-Glue: library for reinforcement learning experiments.</p>
<p>DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> sem</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">import</span> cliffworld_env</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams.update(&#123;<span class="string">&#x27;font.size&#x27;</span>: <span class="number">15</span>&#125;)</span><br><span class="line">plt.rcParams.update(&#123;<span class="string">&#x27;figure.figsize&#x27;</span>: [<span class="number">10</span>,<span class="number">5</span>]&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="q-learning">Q-Learning</h2>
<p>In this section you will implement and test a Q-Learning agent with <span class="math inline">\(\epsilon\)</span>-greedy action selection (Section 6.5 in the textbook).</p>
<h3 id="implementation">Implementation</h3>
<p>Your job is to implement the updates in the methods agent_step and agent_end. We provide detailed comments in each method describing what your code should do.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningAgent</span>(<span class="params">BaseAgent</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span>(<span class="params">self, agent_init_info</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states (int): The number of states,</span></span><br><span class="line"><span class="string">            num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">            epsilon (float): The epsilon parameter for exploration,</span></span><br><span class="line"><span class="string">            step_size (float): The step-size,</span></span><br><span class="line"><span class="string">            discount (float): The discount factor,</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Store the parameters provided in agent_init_info.</span></span><br><span class="line">        self.num_actions = agent_init_info[<span class="string">&quot;num_actions&quot;</span>]</span><br><span class="line">        self.num_states = agent_init_info[<span class="string">&quot;num_states&quot;</span>]</span><br><span class="line">        self.epsilon = agent_init_info[<span class="string">&quot;epsilon&quot;</span>]</span><br><span class="line">        self.step_size = agent_init_info[<span class="string">&quot;step_size&quot;</span>]</span><br><span class="line">        self.discount = agent_init_info[<span class="string">&quot;discount&quot;</span>]</span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info[<span class="string">&quot;seed&quot;</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create an array for action-value estimates and initialize it to zero.</span></span><br><span class="line">        self.q = np.zeros((self.num_states, self.num_actions)) <span class="comment"># The array of action-value estimates.</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;The first method called when the episode starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment&#x27;s evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the first action the agent takes.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state,:]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, observation</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment&#x27;s step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">                last step.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the action the agent is taking.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state, :]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform an update</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.q[self.prev_state][self.prev_action]  += self.step_size * (reward + self.discount * np.<span class="built_in">max</span>(current_q) - \</span><br><span class="line">                                                                        self.q[self.prev_state][self.prev_action] )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Perform the last update in the episode</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.q[self.prev_state][self.prev_action] += self.step_size * (reward - self.q[self.prev_state][self.prev_action])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">argmax</span>(<span class="params">self, q_values</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;argmax with random tie-breaking</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            q_values (Numpy array): the array of action-values</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): an action with the highest value</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        top = <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>)</span><br><span class="line">        ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(q_values)):</span><br><span class="line">            <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">                top = q_values[i]</span><br><span class="line">                ties = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">                ties.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br></pre></td></tr></table></figure>
<h3 id="test">Test</h3>
<p>Run the cells below to test the implemented methods. The output of each cell should match the expected output.</p>
<p>Note that passing this test does not guarantee correct behavior on the Cliff World.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">4</span>, <span class="string">&quot;num_states&quot;</span>: <span class="number">3</span>, <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>, <span class="string">&quot;discount&quot;</span>: <span class="number">1.0</span>, <span class="string">&quot;seed&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line">agent = QLearningAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(agent.q == expected_values)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># reset the agent</span></span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.2</span>,  <span class="number">0.</span>,  <span class="number">0.</span>  ],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,   <span class="number">0.</span>,  <span class="number">0.02</span>],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,   <span class="number">0.</span>,  <span class="number">0.</span>  ],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(np.isclose(agent.q, expected_values))</span><br><span class="line"></span><br><span class="line"><span class="comment"># reset the agent</span></span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.2</span>, <span class="number">0.</span>,  <span class="number">0.</span> ],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.1</span>],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span> ],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(np.isclose(agent.q, expected_values))</span><br></pre></td></tr></table></figure>
<h1 id="expected-sarsa">Expected Sarsa</h1>
<p>In this section you will implement an Expected Sarsa agent with <span class="math inline">\(\epsilon\)</span>-greedy action selection (Section 6.6 in the textbook).</p>
<h3 id="implementation-1">Implementation</h3>
<p>Your job is to implement the updates in the methods agent_step and agent_end. We provide detailed comments in each method describing what your code should do.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExpectedSarsaAgent</span>(<span class="params">BaseAgent</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span>(<span class="params">self, agent_init_info</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states (int): The number of states,</span></span><br><span class="line"><span class="string">            num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">            epsilon (float): The epsilon parameter for exploration,</span></span><br><span class="line"><span class="string">            step_size (float): The step-size,</span></span><br><span class="line"><span class="string">            discount (float): The discount factor,</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Store the parameters provided in agent_init_info.</span></span><br><span class="line">        self.num_actions = agent_init_info[<span class="string">&quot;num_actions&quot;</span>]</span><br><span class="line">        self.num_states = agent_init_info[<span class="string">&quot;num_states&quot;</span>]</span><br><span class="line">        self.epsilon = agent_init_info[<span class="string">&quot;epsilon&quot;</span>]</span><br><span class="line">        self.step_size = agent_init_info[<span class="string">&quot;step_size&quot;</span>]</span><br><span class="line">        self.discount = agent_init_info[<span class="string">&quot;discount&quot;</span>]</span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info[<span class="string">&quot;seed&quot;</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create an array for action-value estimates and initialize it to zero.</span></span><br><span class="line">        self.q = np.zeros((self.num_states, self.num_actions)) <span class="comment"># The array of action-value estimates.</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;The first method called when the episode starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment&#x27;s evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the first action the agent takes.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state, :]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, observation</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment&#x27;s step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">                last step.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the action the agent is taking.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state,:]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform an update</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        </span><br><span class="line">        expect = (<span class="number">1</span> - self.epsilon) * np.<span class="built_in">max</span>(current_q) + self.epsilon * np.average(current_q)</span><br><span class="line">        self.q[self.prev_state][self.prev_action] += self.step_size * (reward + self.discount * expect - self.q[self.prev_state][self.prev_action])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Perform the last update in the episode</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.q[self.prev_state][self.prev_action] += self.step_size * (reward - self.q[self.prev_state][self.prev_action])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">argmax</span>(<span class="params">self, q_values</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;argmax with random tie-breaking</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            q_values (Numpy array): the array of action-values</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): an action with the highest value</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        top = <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>)</span><br><span class="line">        ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(q_values)):</span><br><span class="line">            <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">                top = q_values[i]</span><br><span class="line">                ties = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">                ties.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br></pre></td></tr></table></figure>
<h3 id="test-1">Test</h3>
<p>Run the cells below to test the implemented methods. The output of each cell should match the expected output.</p>
<p>Note that passing this test does not guarantee correct behavior on the Cliff World.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">4</span>, <span class="string">&quot;num_states&quot;</span>: <span class="number">3</span>, <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>, <span class="string">&quot;discount&quot;</span>: <span class="number">1.0</span>, <span class="string">&quot;seed&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line">agent = ExpectedSarsaAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(agent.q == expected_values)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># test agent step</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.0185</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(np.isclose(agent.q, expected_values))</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># test agent end</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.28</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.0185</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(np.isclose(agent.q, expected_values))</span><br></pre></td></tr></table></figure>
<h1 id="solving-the-cliff-world">Solving the Cliff World</h1>
<p>We described the Cliff World environment in the video "Expected Sarsa in the Cliff World" in Lesson 3. This is an undiscounted episodic task and thus we set <span class="math inline">\(\gamma\)</span>=1. The agent starts in the bottom left corner of the gridworld below and takes actions that move it in the four directions. Actions that would move the agent off of the cliff incur a reward of -100 and send the agent back to the start state. The reward for all other transitions is -1. An episode terminates when the agent reaches the bottom right corner.</p>
<p><img src="cliffworld.png" alt="Drawing" style="width: 600px;"/></p>
<p>Using the experiment program in the cell below we now compare the agents on the Cliff World environment and plot the sum of rewards during each episode for the two agents.</p>
<p>The result of this cell will be graded. If you make any changes to your algorithms, you have to run this cell again before submitting the assignment.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">agents = &#123;</span><br><span class="line">    <span class="string">&quot;Q-learning&quot;</span>: QLearningAgent,</span><br><span class="line">    <span class="string">&quot;Expected Sarsa&quot;</span>: ExpectedSarsaAgent</span><br><span class="line">&#125;</span><br><span class="line">env = cliffworld_env.Environment</span><br><span class="line">all_reward_sums = &#123;&#125; <span class="comment"># Contains sum of rewards during episode</span></span><br><span class="line">all_state_visits = &#123;&#125; <span class="comment"># Contains state visit counts during the last 10 episodes</span></span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">4</span>, <span class="string">&quot;num_states&quot;</span>: <span class="number">48</span>, <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, <span class="string">&quot;step_size&quot;</span>: <span class="number">0.5</span>, <span class="string">&quot;discount&quot;</span>: <span class="number">1.0</span>&#125;</span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line">num_runs = <span class="number">100</span> <span class="comment"># The number of runs</span></span><br><span class="line">num_episodes = <span class="number">100</span> <span class="comment"># The number of episodes in each run</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> algorithm <span class="keyword">in</span> [<span class="string">&quot;Q-learning&quot;</span>, <span class="string">&quot;Expected Sarsa&quot;</span>]:</span><br><span class="line">    all_reward_sums[algorithm] = []</span><br><span class="line">    all_state_visits[algorithm] = []</span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_runs)):</span><br><span class="line">        agent_info[<span class="string">&quot;seed&quot;</span>] = run</span><br><span class="line">        rl_glue = RLGlue(env, agents[algorithm])</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line">        reward_sums = []</span><br><span class="line">        state_visits = np.zeros(<span class="number">48</span>)</span><br><span class="line">        last_episode_total_reward = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(num_episodes):</span><br><span class="line">            <span class="keyword">if</span> episode &lt; num_episodes - <span class="number">10</span>:</span><br><span class="line">                <span class="comment"># Runs an episode</span></span><br><span class="line">                rl_glue.rl_episode(<span class="number">10000</span>) </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                <span class="comment"># Runs an episode while keeping track of visited states</span></span><br><span class="line">                state, action = rl_glue.rl_start()</span><br><span class="line">                state_visits[state] += <span class="number">1</span></span><br><span class="line">                is_terminal = <span class="literal">False</span></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal:</span><br><span class="line">                    reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">                    state_visits[state] += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">            reward_sums.append(rl_glue.rl_return() - last_episode_total_reward)</span><br><span class="line">            last_episode_total_reward = rl_glue.rl_return()</span><br><span class="line">            </span><br><span class="line">        all_reward_sums[algorithm].append(reward_sums)</span><br><span class="line">        all_state_visits[algorithm].append(state_visits)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot results</span></span><br><span class="line"><span class="keyword">for</span> algorithm <span class="keyword">in</span> [<span class="string">&quot;Q-learning&quot;</span>, <span class="string">&quot;Expected Sarsa&quot;</span>]:</span><br><span class="line">    plt.plot(np.mean(all_reward_sums[algorithm], axis=<span class="number">0</span>), label=algorithm)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Episodes&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Sum of\n rewards\n during\n episode&quot;</span>,rotation=<span class="number">0</span>, labelpad=<span class="number">40</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>,<span class="number">100</span>)</span><br><span class="line">plt.ylim(-<span class="number">30</span>,<span class="number">0</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12&lt;00:00,  8.12it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:16&lt;00:00,  6.15it/s]</code></pre>
<figure>
<img src="output_26_1.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<p>To see why these two agents behave differently, let's inspect the states they visit most. Run the cell below to generate plots showing the number of timesteps that the agents spent in each state over the last 10 episodes.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> algorithm, position <span class="keyword">in</span> [(<span class="string">&quot;Q-learning&quot;</span>, <span class="number">211</span>), (<span class="string">&quot;Expected Sarsa&quot;</span>, <span class="number">212</span>)]:</span><br><span class="line">    plt.subplot(position)</span><br><span class="line">    average_state_visits = np.array(all_state_visits[algorithm]).mean(axis=<span class="number">0</span>)</span><br><span class="line">    grid_state_visits = average_state_visits.reshape((<span class="number">4</span>,<span class="number">12</span>))</span><br><span class="line">    grid_state_visits[<span class="number">0</span>,<span class="number">1</span>:-<span class="number">1</span>] = np.nan</span><br><span class="line">    plt.pcolormesh(grid_state_visits, edgecolors=<span class="string">&#x27;gray&#x27;</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.title(algorithm)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    cm = plt.get_cmap()</span><br><span class="line">    cm.set_bad(<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplots_adjust(bottom=<span class="number">0.0</span>, right=<span class="number">0.7</span>, top=<span class="number">1.0</span>)</span><br><span class="line">    cax = plt.axes([<span class="number">0.85</span>, <span class="number">0.0</span>, <span class="number">0.075</span>, <span class="number">1.</span>])</span><br><span class="line">    </span><br><span class="line">cbar = plt.colorbar(cax=cax)</span><br><span class="line">cbar.ax.set_ylabel(<span class="string">&quot;Visits during\n the last 10\n episodes&quot;</span>, rotation=<span class="number">0</span>, labelpad=<span class="number">70</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure>
<img src="output_28_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<p>The Q-learning agent learns the optimal policy, one that moves along the cliff and reaches the goal in as few steps as possible. However, since the agent does not follow the optimal policy and uses <span class="math inline">\(\epsilon\)</span>-greedy exploration, it occasionally falls off the cliff. The Expected Sarsa agent takes exploration into account and follows a safer path.</p>
<p>Previously we used a fixed step-size of 0.5 for the agents. What happens with other step-sizes? Does this difference in performance persist?</p>
<p>In the next experiment we will try 10 different step-sizes from 0.1 to 1.0 and compare the sum of rewards per episode averaged over the first 100 episodes (similar to the interim performance curves in Figure 6.3 of the textbook). Shaded regions show standard errors.</p>
<p>This cell takes around 10 minutes to run. The result of this cell will be graded. If you make any changes to your algorithms, you have to run this cell again before submitting the assignment.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product</span><br><span class="line"></span><br><span class="line">agents = &#123;</span><br><span class="line">    <span class="string">&quot;Q-learning&quot;</span>: QLearningAgent,</span><br><span class="line">    <span class="string">&quot;Expected Sarsa&quot;</span>: ExpectedSarsaAgent</span><br><span class="line">&#125;</span><br><span class="line">env = cliffworld_env.Environment</span><br><span class="line">all_reward_sums = &#123;&#125;</span><br><span class="line">step_sizes = np.linspace(<span class="number">0.1</span>,<span class="number">1.0</span>,<span class="number">10</span>)</span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">4</span>, <span class="string">&quot;num_states&quot;</span>: <span class="number">48</span>, <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, <span class="string">&quot;discount&quot;</span>: <span class="number">1.0</span>&#125;</span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line">num_runs = <span class="number">30</span></span><br><span class="line">num_episodes = <span class="number">100</span></span><br><span class="line">all_reward_sums = &#123;&#125;</span><br><span class="line"></span><br><span class="line">algorithms = [<span class="string">&quot;Q-learning&quot;</span>, <span class="string">&quot;Expected Sarsa&quot;</span>]</span><br><span class="line">cross_product = <span class="built_in">list</span>(product(algorithms, step_sizes, <span class="built_in">range</span>(num_runs)))</span><br><span class="line"><span class="keyword">for</span> algorithm, step_size, run <span class="keyword">in</span> tqdm(cross_product):</span><br><span class="line">    <span class="keyword">if</span> (algorithm, step_size) <span class="keyword">not</span> <span class="keyword">in</span> all_reward_sums:</span><br><span class="line">        all_reward_sums[(algorithm, step_size)] = []</span><br><span class="line"></span><br><span class="line">    agent_info[<span class="string">&quot;step_size&quot;</span>] = step_size</span><br><span class="line">    agent_info[<span class="string">&quot;seed&quot;</span>] = run</span><br><span class="line">    rl_glue = RLGlue(env, agents[algorithm])</span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line">    last_episode_total_reward = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(num_episodes):</span><br><span class="line">        rl_glue.rl_episode(<span class="number">0</span>)</span><br><span class="line">    all_reward_sums[(algorithm, step_size)].append(rl_glue.rl_return()/num_episodes)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> algorithm <span class="keyword">in</span> [<span class="string">&quot;Q-learning&quot;</span>, <span class="string">&quot;Expected Sarsa&quot;</span>]:</span><br><span class="line">    algorithm_means = np.array([np.mean(all_reward_sums[(algorithm, step_size)]) <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes])</span><br><span class="line">    algorithm_stds = np.array([sem(all_reward_sums[(algorithm, step_size)]) <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes])</span><br><span class="line">    plt.plot(step_sizes, algorithm_means, marker=<span class="string">&#x27;o&#x27;</span>, linestyle=<span class="string">&#x27;solid&#x27;</span>, label=algorithm)</span><br><span class="line">    plt.fill_between(step_sizes, algorithm_means + algorithm_stds, algorithm_means - algorithm_stds, alpha=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">&quot;Step-size&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Sum of\n rewards\n per episode&quot;</span>,rotation=<span class="number">0</span>, labelpad=<span class="number">50</span>)</span><br><span class="line">plt.xticks(step_sizes)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [01:38&lt;00:00,  6.08it/s]</code></pre>
<figure>
<img src="output_30_1.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<p>Expected Sarsa shows an advantage over Q-learning in this problem across a wide range of step-sizes.</p>
<p>Congratulations! Now you have:</p>
<ul>
<li>implemented Q-Learning with <span class="math inline">\(\epsilon\)</span>-greedy action selection</li>
<li>implemented Expected Sarsa with <span class="math inline">\(\epsilon\)</span>-greedy action selection</li>
<li>investigated the behavior of these two algorithms on Cliff World</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Policy-Evaluation-in-Cliff-Walking-Environment/2020/09/28/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Policy-Evaluation-in-Cliff-Walking-Environment/2020/09/28/" class="post-title-link" itemprop="url">Policy Evaluation in Cliff Walking Environment</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-09-28 16:04:11 / Modified: 16:06:19" itemprop="dateCreated datePublished" datetime="2020-09-28T16:04:11+08:00">2020-09-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Policy-Evaluation-in-Cliff-Walking-Environment/2020/09/28/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Policy-Evaluation-in-Cliff-Walking-Environment/2020/09/28/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="assignment-policy-evaluation-in-cliff-walking-environment">Assignment: Policy Evaluation in Cliff Walking Environment</h1>
<p>Welcome to the Course 2 Module 2 Programming Assignment! In this assignment, you will implement one of the fundamental sample and bootstrapping based model free reinforcement learning agents for prediction. This is namely one that uses one-step temporal difference learning, also known as TD(0). The task is to design an agent for policy evaluation in the Cliff Walking environment. Recall that policy evaluation is the prediction problem where the goal is to accurately estimate the values of states given some policy.</p>
<h3 id="learning-objectives">Learning Objectives</h3>
<ul>
<li>Implement parts of the Cliff Walking environment, to get experience specifying MDPs [Section 1].</li>
<li>Implement an agent that uses bootstrapping and, particularly, TD(0) [Section 2].</li>
<li>Apply TD(0) to estimate value functions for different policies, i.e., run policy evaluation experiments [Section 3].</li>
</ul>
<h2 id="the-cliff-walking-environment">The Cliff Walking Environment</h2>
<p>The Cliff Walking environment is a gridworld with a discrete state space and discrete action space. The agent starts at grid cell S. The agent can move (deterministically) to the four neighboring cells by taking actions Up, Down, Left or Right. Trying to move out of the boundary results in staying in the same location. So, for example, trying to move left when at a cell on the leftmost column results in no movement at all and the agent remains in the same location. The agent receives -1 reward per step in most states, and -100 reward when falling off of the cliff. This is an episodic task; termination occurs when the agent reaches the goal grid cell G. Falling off of the cliff results in resetting to the start state, without termination.</p>
<p>The diagram below showcases the description above and also illustrates two of the policies we will be evaluating.</p>
<p><img src="cliffwalk.png" style="height:400px"></p>
<h2 id="packages.">Packages.</h2>
<p>We import the following libraries that are required for this assignment. We shall be using the following libraries: 1. jdc: Jupyter magic that allows defining classes over multiple jupyter notebook cells. 2. numpy: the fundamental package for scientific computing with Python. 3. matplotlib: the library for plotting graphs in Python. 4. RL-Glue: the library for reinforcement learning experiments. 5. BaseEnvironment, BaseAgent: the base classes from which we will inherit when creating the environment and agent classes in order for them to support the RL-Glue framework. 6. Manager: the file allowing for visualization and testing. 7. itertools.product: the function that can be used easily to compute permutations. 8. tqdm.tqdm: Provides progress bars for visualizing the status of loops.</p>
<p><strong>Please do not import other libraries</strong> ‚Äî this will break the autograder.</p>
<p><strong>NOTE: For this notebook, there is no need to make any calls to methods of random number generators. Spurious or missing calls to random number generators may affect your results.</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jdc</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> Agent <span class="keyword">import</span> BaseAgent </span><br><span class="line"><span class="keyword">from</span> Environment <span class="keyword">import</span> BaseEnvironment  </span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> manager <span class="keyword">import</span> Manager</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure>
<h2 id="section-1.-environment">Section 1. Environment</h2>
<p>In the first part of this assignment, you will get to see how the Cliff Walking environment is implemented. You will also get to implement parts of it to aid your understanding of the environment and more generally how MDPs are specified. In particular, you will implement the logic for: 1. Converting 2-dimensional coordinates to a single index for the state, 2. One of the actions (action up), and, 3. Reward and termination.</p>
<p>Given below is an annotated diagram of the environment with more details that may help in completing the tasks of this part of the assignment. Note that we will be creating a more general environment where the height and width positions can be variable but the start, goal and cliff grid cells have the same relative positions (bottom left, bottom right and the cells between the start and goal grid cells respectively).</p>
<p><img src="cliffwalk-annotated.png" style="height:400px"></p>
<p>Once you have gone through the code and begun implementing solutions, it may be a good idea to come back here and see if you can convince yourself that the diagram above is an accurate representation of the code given and the code you have written.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create empty CliffWalkEnvironment class.</span></span><br><span class="line"><span class="comment"># These methods will be filled in later cells.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CliffWalkEnvironment</span>(<span class="params">BaseEnvironment</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_init</span>(<span class="params">self, agent_info=&#123;&#125;</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_start</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_step</span>(<span class="params">self, reward, state</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_end</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_cleanup</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># helper method</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">state</span>(<span class="params">self, loc</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>
<h2 id="env_init">env_init()</h2>
<p>The first function we add to the environment is the initialization function which is called once when an environment object is created. In this function, the grid dimensions and special locations (start and goal locations and the cliff locations) are stored for easy use later.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_init</span>(<span class="params">self, env_info=&#123;&#125;</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Setup for the environment called when the experiment first starts.</span></span><br><span class="line"><span class="string">        Note:</span></span><br><span class="line"><span class="string">            Initialize a tuple with the reward, first state, boolean</span></span><br><span class="line"><span class="string">            indicating if it&#x27;s terminal.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Note, we can setup the following variables later, in env_start() as it is equivalent. </span></span><br><span class="line">        <span class="comment"># Code is left here to adhere to the note above, but these variables are initialized once more</span></span><br><span class="line">        <span class="comment"># in env_start() [See the env_start() function below.]</span></span><br><span class="line">        </span><br><span class="line">        reward = <span class="literal">None</span></span><br><span class="line">        state = <span class="literal">None</span> <span class="comment"># See Aside</span></span><br><span class="line">        termination = <span class="literal">None</span></span><br><span class="line">        self.reward_state_term = (reward, state, termination)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># AN ASIDE: Observation is a general term used in the RL-Glue files that can be interachangeably </span></span><br><span class="line">        <span class="comment"># used with the term &quot;state&quot; for our purposes and for this assignment in particular. </span></span><br><span class="line">        <span class="comment"># A difference arises in the use of the terms when we have what is called Partial Observability where </span></span><br><span class="line">        <span class="comment"># the environment may return states that may not fully represent all the information needed to </span></span><br><span class="line">        <span class="comment"># predict values or make decisions (i.e., the environment is non-Markovian.)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set the default height to 4 and width to 12 (as in the diagram given above)</span></span><br><span class="line">        self.grid_h = env_info.get(<span class="string">&quot;grid_height&quot;</span>, <span class="number">4</span>) </span><br><span class="line">        self.grid_w = env_info.get(<span class="string">&quot;grid_width&quot;</span>, <span class="number">12</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Now, we can define a frame of reference. Let positive x be towards the direction down and </span></span><br><span class="line">        <span class="comment"># positive y be towards the direction right (following the row-major NumPy convention.)</span></span><br><span class="line">        <span class="comment"># Then, keeping with the usual convention that arrays are 0-indexed, max x is then grid_h - 1 </span></span><br><span class="line">        <span class="comment"># and max y is then grid_w - 1. So, we have:</span></span><br><span class="line">        <span class="comment"># Starting location of agent is the bottom-left corner, (max x, min y). </span></span><br><span class="line">        self.start_loc = (self.grid_h - <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># Goal location is the bottom-right corner. (max x, max y).</span></span><br><span class="line">        self.goal_loc = (self.grid_h - <span class="number">1</span>, self.grid_w - <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># The cliff will contain all the cells between the start_loc and goal_loc.</span></span><br><span class="line">        self.cliff = [(self.grid_h - <span class="number">1</span>, i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, (self.grid_w - <span class="number">1</span>))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Take a look at the annotated environment diagram given in the above Jupyter Notebook cell to </span></span><br><span class="line">        <span class="comment"># verify that your understanding of the above code is correct for the default case, i.e., where </span></span><br><span class="line">        <span class="comment"># height = 4 and width = 12.</span></span><br></pre></td></tr></table></figure>
<h2 id="implement-state"><em>Implement</em> state()</h2>
<p>The agent location can be described as a two-tuple or coordinate (x, y) describing the agent‚Äôs position. However, we can convert the (x, y) tuple into a single index and provide agents with just this integer. One reason for this choice is that the spatial aspect of the problem is secondary and there is no need for the agent to know about the exact dimensions of the environment. From the agent‚Äôs viewpoint, it is just perceiving some states, accessing their corresponding values in a table, and updating them. Both the coordinate (x, y) state representation and the converted coordinate representation are thus equivalent in this sense.</p>
<p>Given a grid cell location, the state() function should return the state; a single index corresponding to the location in the grid.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Example: Suppose grid_h is 2 and grid_w is 2. Then, we can write the grid cell two-tuple or coordinate</span><br><span class="line">states as follows (following the usual 0-index convention):</span><br><span class="line">|(0, 0) (0, 1)| |0 1|</span><br><span class="line">|(1, 0) (1, 1)| |2 3|</span><br><span class="line">Assuming row-major order as NumPy does,  we can flatten the latter to get a vector [0 1 2 3].</span><br><span class="line">So, if loc = (0, 0) we return 0. While, for loc = (1, 1) we return 3.</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"><span class="comment">#GRADED FUNCTION: [state]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Modify the return statement of this function to return a correct single index as </span></span><br><span class="line"><span class="comment"># the state (see the logic for this in the previous cell.)</span></span><br><span class="line"><span class="comment"># Lines: 1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">state</span>(<span class="params">self, loc</span>):</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> loc[<span class="number">0</span>] * <span class="number">12</span> + loc[<span class="number">1</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR STATE (5 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below corresponds to the annotated diagram for the environment</span></span><br><span class="line"><span class="comment">#       given previously and is limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_state</span>():</span></span><br><span class="line">    env = CliffWalkEnvironment()</span><br><span class="line">    env.env_init(&#123;<span class="string">&quot;grid_height&quot;</span>: <span class="number">4</span>, <span class="string">&quot;grid_width&quot;</span>: <span class="number">12</span>&#125;)</span><br><span class="line">    coords_to_test = [(<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">11</span>), (<span class="number">1</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">0</span>), (<span class="number">3</span>, <span class="number">9</span>), (<span class="number">3</span>, <span class="number">11</span>)]</span><br><span class="line">    true_states = [<span class="number">0</span>, <span class="number">11</span>, <span class="number">17</span>, <span class="number">36</span>, <span class="number">45</span>, <span class="number">47</span>]</span><br><span class="line">    output_states = [env.state(coords) <span class="keyword">for</span> coords <span class="keyword">in</span> coords_to_test]</span><br><span class="line">    <span class="keyword">assert</span>(output_states == true_states)</span><br><span class="line">test_state()</span><br></pre></td></tr></table></figure>
<h2 id="env_start">env_start()</h2>
<p>In env_start(), we initialize the agent location to be the start location and return the state corresponding to it as the first state for the agent to act upon. Additionally, we also set the reward and termination terms to be 0 and False respectively as they are consistent with the notion that there is no reward nor termination before the first action is even taken.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_start</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;The first method called when the episode starts, called before the</span></span><br><span class="line"><span class="string">    agent starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The first state from the environment.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    reward = <span class="number">0</span></span><br><span class="line">    <span class="comment"># agent_loc will hold the current location of the agent</span></span><br><span class="line">    self.agent_loc = self.start_loc</span><br><span class="line">    <span class="comment"># state is the one dimensional state representation of the agent location.</span></span><br><span class="line">    state = self.state(self.agent_loc)</span><br><span class="line">    termination = <span class="literal">False</span></span><br><span class="line">    self.reward_state_term = (reward, state, termination)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.reward_state_term[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h2 id="implement-env_step"><em>Implement</em> env_step()</h2>
<p>Once an action is taken by the agent, the environment must provide a new state, reward and termination signal.</p>
<p>In the Cliff Walking environment, agents move around using a 4-cell neighborhood called the Von Neumann neighborhood (https://en.wikipedia.org/wiki/Von_Neumann_neighborhood). Thus, the agent has 4 available actions at each state. Three of the actions have been implemented for you and your first task is to implement the logic for the fourth action (Action UP).</p>
<p>Your second task for this function is to implement the reward logic. Look over the environment description given earlier in this notebook if you need a refresher for how the reward signal is defined.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"><span class="comment">#GRADED FUNCTION: [env_step]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Fill in the code for action UP and implement the logic for reward and termination.</span></span><br><span class="line"><span class="comment"># Lines: ~7.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_step</span>(<span class="params">self, action</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;A step taken by the environment.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        action: The action taken by the agent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (float, state, Boolean): a tuple of the reward, state,</span></span><br><span class="line"><span class="string">            and boolean indicating if it&#x27;s terminal.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> action == <span class="number">0</span>: <span class="comment"># UP (Task 1)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        <span class="comment"># Hint: Look at the code given for the other actions and think about the logic in them.</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>] - <span class="number">1</span>, self.agent_loc[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">0</span>] &gt;= <span class="number">0</span>:</span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">1</span>: <span class="comment"># LEFT</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>], self.agent_loc[<span class="number">1</span>] - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">1</span>] &gt;= <span class="number">0</span>: <span class="comment"># Within Bounds?</span></span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span> <span class="comment"># Stay.</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">2</span>: <span class="comment"># DOWN</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>] + <span class="number">1</span>, self.agent_loc[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">0</span>] &lt; self.grid_h: <span class="comment"># Within Bounds?</span></span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span> <span class="comment"># Stay.</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">3</span>: <span class="comment"># RIGHT</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>], self.agent_loc[<span class="number">1</span>] + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">1</span>] &lt; self.grid_w: <span class="comment"># Within Bounds?</span></span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span> <span class="comment"># Stay.</span></span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="built_in">str</span>(action) + <span class="string">&quot; not in recognized actions [0: Up, 1: Left, 2: Down, 3: Right]!&quot;</span>)</span><br><span class="line"></span><br><span class="line">    reward = -<span class="number">1</span></span><br><span class="line">    terminal = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Hint: Consider the initialization of reward and terminal variables above. Then, note the </span></span><br><span class="line">    <span class="comment"># conditional statements and comments given below and carefully ensure to set the variables reward </span></span><br><span class="line">    <span class="comment"># and terminal correctly for each case.</span></span><br><span class="line">    <span class="keyword">if</span> self.agent_loc == self.goal_loc: <span class="comment"># Reached Goal!</span></span><br><span class="line">        terminal = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">elif</span> self.agent_loc <span class="keyword">in</span> self.cliff: <span class="comment"># Fell into the cliff!</span></span><br><span class="line">        reward = -<span class="number">100</span></span><br><span class="line">        self.agent_loc = self.start_loc</span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    self.reward_state_term = (reward, self.state(self.agent_loc), terminal)</span><br><span class="line">    <span class="keyword">return</span> self.reward_state_term</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR ACTION UP (5 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is again limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_action_up</span>():</span></span><br><span class="line">    env = CliffWalkEnvironment()</span><br><span class="line">    env.env_init(&#123;<span class="string">&quot;grid_height&quot;</span>: <span class="number">4</span>, <span class="string">&quot;grid_width&quot;</span>: <span class="number">12</span>&#125;)</span><br><span class="line">    env.agent_loc = (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    env.env_step(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span>(env.agent_loc == (<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    env.agent_loc = (<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    env.env_step(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span>(env.agent_loc == (<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">test_action_up()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR REWARD &amp; TERMINATION (10 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_reward</span>():</span></span><br><span class="line">    env = CliffWalkEnvironment()</span><br><span class="line">    env.env_init(&#123;<span class="string">&quot;grid_height&quot;</span>: <span class="number">4</span>, <span class="string">&quot;grid_width&quot;</span>: <span class="number">12</span>&#125;)</span><br><span class="line">    env.agent_loc = (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    reward_state_term = env.env_step(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span>(reward_state_term[<span class="number">0</span>] == -<span class="number">1</span> <span class="keyword">and</span> reward_state_term[<span class="number">1</span>] == env.state((<span class="number">0</span>, <span class="number">0</span>)) <span class="keyword">and</span></span><br><span class="line">           reward_state_term[<span class="number">2</span>] == <span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    env.agent_loc = (<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">    reward_state_term = env.env_step(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">assert</span>(reward_state_term[<span class="number">0</span>] == -<span class="number">100</span> <span class="keyword">and</span> reward_state_term[<span class="number">1</span>] == env.state((<span class="number">3</span>, <span class="number">0</span>)) <span class="keyword">and</span></span><br><span class="line">           reward_state_term[<span class="number">2</span>] == <span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    env.agent_loc = (<span class="number">2</span>, <span class="number">11</span>)</span><br><span class="line">    reward_state_term = env.env_step(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">assert</span>(reward_state_term[<span class="number">0</span>] == -<span class="number">1</span> <span class="keyword">and</span> reward_state_term[<span class="number">1</span>] == env.state((<span class="number">3</span>, <span class="number">11</span>)) <span class="keyword">and</span></span><br><span class="line">           reward_state_term[<span class="number">2</span>] == <span class="literal">True</span>)</span><br><span class="line">test_reward()</span><br></pre></td></tr></table></figure>
<h2 id="env_cleanup">env_cleanup()</h2>
<p>There is not much cleanup to do for the Cliff Walking environment. Here, we simply reset the agent location to be the start location in this function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_cleanup</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Cleanup done after the environment ends&quot;&quot;&quot;</span></span><br><span class="line">    self.agent_loc = self.start_loc</span><br></pre></td></tr></table></figure>
<h2 id="section-2.-agent">Section 2. Agent</h2>
<p>In this second part of the assignment, you will be implementing the key updates for Temporal Difference Learning. There are two cases to consider depending on whether an action leads to a terminal state or not.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create empty TDAgent class.</span></span><br><span class="line"><span class="comment"># These methods will be filled in later cells.</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TDAgent</span>(<span class="params">BaseAgent</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span>(<span class="params">self, agent_info=&#123;&#125;</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, state</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_cleanup</span>(<span class="params">self</span>):</span>        </span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_message</span>(<span class="params">self, message</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>
<h2 id="agent_init">agent_init()</h2>
<p>As we did with the environment, we first initialize the agent once when a TDAgent object is created. In this function, we create a random number generator, seeded with the seed provided in the agent_info dictionary to get reproducible results. We also set the policy, discount and step size based on the agent_info dictionary. Finally, with a convention that the policy is always specified as a mapping from states to actions and so is an array of size (# States, # Actions), we initialize a values array of shape (# States,) to zeros.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_init</span>(<span class="params">self, agent_info=&#123;&#125;</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Setup for the agent called when the experiment first starts.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a random number generator with the provided seed to seed the agent for reproducibility.</span></span><br><span class="line">    self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">&quot;seed&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Policy will be given, recall that the goal is to accurately estimate its corresponding value function. </span></span><br><span class="line">    self.policy = agent_info.get(<span class="string">&quot;policy&quot;</span>)</span><br><span class="line">    <span class="comment"># Discount factor (gamma) to use in the updates.</span></span><br><span class="line">    self.discount = agent_info.get(<span class="string">&quot;discount&quot;</span>)</span><br><span class="line">    <span class="comment"># The learning rate or step size parameter (alpha) to use in updates.</span></span><br><span class="line">    self.step_size = agent_info.get(<span class="string">&quot;step_size&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize an array of zeros that will hold the values.</span></span><br><span class="line">    <span class="comment"># Recall that the policy can be represented as a (# States, # Actions) array. With the </span></span><br><span class="line">    <span class="comment"># assumption that this is the case, we can use the first dimension of the policy to</span></span><br><span class="line">    <span class="comment"># initialize the array for values.</span></span><br><span class="line">    self.values = np.zeros((self.policy.shape[<span class="number">0</span>],))</span><br></pre></td></tr></table></figure>
<h1 id="agent_start">agent_start()</h1>
<p>In agent_start(), we choose an action based on the initial state and policy we are evaluating. We also cache the state so that we can later update its value when we perform a Temporal Difference update. Finally, we return the action chosen so that the RL loop can continue and the environment can execute this action.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_start</span>(<span class="params">self, state</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;The first method called when the episode starts, called after</span></span><br><span class="line"><span class="string">    the environment starts.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the environment&#x27;s env_start function.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The first action the agent takes.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># The policy can be represented as a (# States, # Actions) array. So, we can use </span></span><br><span class="line">    <span class="comment"># the second dimension here when choosing an action.</span></span><br><span class="line">    action = self.rand_generator.choice(<span class="built_in">range</span>(self.policy.shape[<span class="number">1</span>]), p=self.policy[state])</span><br><span class="line">    self.last_state = state</span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure>
<h2 id="implement-agent_step"><em>Implement</em> agent_step()</h2>
<p>In agent_step(), the agent must:</p>
<ul>
<li>Perform an update to improve the value estimate of the previously visited state, and</li>
<li>Act based on the state provided by the environment.</li>
</ul>
<p>The latter of the two steps above has been implemented for you. Implement the former. Note that, unlike later in agent_end(), the episode has not yet ended in agent_step(). in other words, the previously observed state was not a terminal state.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment">#[GRADED] FUNCTION: [agent_step]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Fill in the TD-target and update.</span></span><br><span class="line"><span class="comment"># Lines: ~2.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, state</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;A step taken by the agent.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment&#x27;s step after the last action, i.e., where the agent ended up after the</span></span><br><span class="line"><span class="string">            last action</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The action the agent is taking.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Hint: We should perform an update with the last state given that we now have the reward and</span></span><br><span class="line">    <span class="comment"># next state. We break this into two steps. Recall for example that the Monte-Carlo update </span></span><br><span class="line">    <span class="comment"># had the form: V[S_t] = V[S_t] + alpha * (target - V[S_t]), where the target was the return, G_t.</span></span><br><span class="line">    target = reward + self.discount * self.values[state]</span><br><span class="line">    self.values[self.last_state] = self.values[self.last_state] + self.step_size * (target - self.values[self.last_state])</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Having updated the value for the last state, we now act based on the current </span></span><br><span class="line">    <span class="comment"># state, and set the last state to be current one as we will next be making an </span></span><br><span class="line">    <span class="comment"># update with it when agent_step is called next once the action we return from this function </span></span><br><span class="line">    <span class="comment"># is executed in the environment.</span></span><br><span class="line"></span><br><span class="line">    action = self.rand_generator.choice(<span class="built_in">range</span>(self.policy.shape[<span class="number">1</span>]), p=self.policy[state])</span><br><span class="line">    self.last_state = state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure>
<h2 id="implement-agent_end"><em>Implement</em> agent_end()</h2>
<p>Implement the TD update for the case where an action leads to a terminal state.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment">#[GRADED] FUNCTION: [agent_end]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Fill in the TD-target and update.</span></span><br><span class="line"><span class="comment"># Lines: ~2.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_end</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Run when the agent terminates.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward the agent received for entering the terminal state.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Hint: Here too, we should perform an update with the last state given that we now have the </span></span><br><span class="line">    <span class="comment"># reward. Note that in this case, the action led to termination. Once more, we break this into </span></span><br><span class="line">    <span class="comment"># two steps, computing the target and the update itself that uses the target and the </span></span><br><span class="line">    <span class="comment"># current value estimate for the state whose value we are updating.</span></span><br><span class="line">    target = reward</span><br><span class="line">    self.values[self.last_state] = self.values[self.last_state] + self.step_size * (target - self.values[self.last_state])</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>
<h2 id="agent_cleanup">agent_cleanup()</h2>
<p>In cleanup, we simply reset the last state to be None to ensure that we are not storing any states past an episode.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_cleanup</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Cleanup done after the agent ends.&quot;&quot;&quot;</span></span><br><span class="line">    self.last_state = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h2 id="agent_message">agent_message()</h2>
<p>agent_message() can generally be used to get different kinds of information about an RLGlue agent in the interaction loop of RLGlue. Here, we conditonally check for a message matching "get_values" and use it to retrieve the values table the agent has been updating over time.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_message</span>(<span class="params">self, message</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;A function used to pass information from the agent to the experiment.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        message: The message passed to the agent.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The response (or answer) to the message.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> message == <span class="string">&quot;get_values&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> self.values</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">&quot;TDAgent.agent_message(): Message not understood!&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR TD-UPDATES (20 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test belows serve as a good check in debugging your code for the TD updates. However, </span></span><br><span class="line"><span class="comment">#       as with the other tests, it is limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_td_updates</span>():</span></span><br><span class="line">    <span class="comment"># The following test checks that the TD check works for a case where the transition </span></span><br><span class="line">    <span class="comment"># garners reward -1 and does not lead to a terminal state. This is in a simple two state setting </span></span><br><span class="line">    <span class="comment"># where there is only one action. The first state&#x27;s current value estimate is 0 while the second is 1.</span></span><br><span class="line">    <span class="comment"># Note the discount and step size if you are debugging this test.</span></span><br><span class="line">    agent = TDAgent()</span><br><span class="line">    policy_list = np.array([[<span class="number">1.</span>], [<span class="number">1.</span>]])</span><br><span class="line">    agent.agent_init(&#123;<span class="string">&quot;policy&quot;</span>: np.array(policy_list), <span class="string">&quot;discount&quot;</span>: <span class="number">0.99</span>, <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line">    agent.values = np.array([<span class="number">0.</span>, <span class="number">1.</span>])</span><br><span class="line">    agent.agent_start(<span class="number">0</span>)</span><br><span class="line">    reward = -<span class="number">1</span></span><br><span class="line">    next_state = <span class="number">1</span></span><br><span class="line">    agent.agent_step(reward, next_state)</span><br><span class="line">    <span class="keyword">assert</span>(np.isclose(agent.values[<span class="number">0</span>], -<span class="number">0.001</span>) <span class="keyword">and</span> np.isclose(agent.values[<span class="number">1</span>], <span class="number">1.</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The following test checks that the TD check works for a case where the transition </span></span><br><span class="line">    <span class="comment"># garners reward -100 and lead to a terminal state. This is in a simple one state setting </span></span><br><span class="line">    <span class="comment"># where there is only one action. The state&#x27;s current value estimate is 0.</span></span><br><span class="line">    <span class="comment"># Note the discount and step size if you are debugging this test.</span></span><br><span class="line">    agent = TDAgent()</span><br><span class="line">    policy_list = np.array([[<span class="number">1.</span>]])</span><br><span class="line">    agent.agent_init(&#123;<span class="string">&quot;policy&quot;</span>: np.array(policy_list), <span class="string">&quot;discount&quot;</span>: <span class="number">0.99</span>, <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line">    agent.values = np.array([<span class="number">0.</span>])</span><br><span class="line">    agent.agent_start(<span class="number">0</span>)</span><br><span class="line">    reward = -<span class="number">100</span></span><br><span class="line">    next_state = <span class="number">0</span></span><br><span class="line">    agent.agent_end(reward)</span><br><span class="line">    <span class="keyword">assert</span>(np.isclose(agent.values[<span class="number">0</span>], -<span class="number">10</span>))</span><br><span class="line">    </span><br><span class="line">test_td_updates()</span><br></pre></td></tr></table></figure>
<h2 id="section-3.-policy-evaluation-experiments">Section 3. Policy Evaluation Experiments</h2>
<p>Finally, in this last part of the assignment, you will get to see the TD policy evaluation algorithm in action by looking at the estimated values, the per state value error and after the experiment is complete, the Mean Squared Value Error curve vs. episode number, summarizing how the value error changed over time.</p>
<p>The code below runs one run of an experiment given env_info and agent_info dictionaries. A "manager" object is created for visualizations and is used in part for the autograder. By default, the run will be for 5000 episodes. The true_values_file is specified to compare the learned value function with the values stored in the true_values_file. Plotting of the learned value function occurs by default after every 100 episodes. In addition, when true_values_file is specified, the value error per state and the root mean square value error will also be plotted.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No. </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span>(<span class="params">env_info, agent_info, </span></span></span><br><span class="line"><span class="params"><span class="function">                   num_episodes=<span class="number">5000</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                   experiment_name=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                   plot_freq=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                   true_values_file=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                   value_error_threshold=<span class="number">1e-8</span></span>):</span></span><br><span class="line">    env = CliffWalkEnvironment</span><br><span class="line">    agent = TDAgent</span><br><span class="line">    rl_glue = RLGlue(env, agent)</span><br><span class="line"></span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line">    manager = Manager(env_info, agent_info, true_values_file=true_values_file, experiment_name=experiment_name)</span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_episodes + <span class="number">1</span>):</span><br><span class="line">        rl_glue.rl_episode(<span class="number">0</span>) <span class="comment"># no step limit</span></span><br><span class="line">        <span class="keyword">if</span> episode % plot_freq == <span class="number">0</span>:</span><br><span class="line">            values = rl_glue.agent.agent_message(<span class="string">&quot;get_values&quot;</span>)</span><br><span class="line">            manager.visualize(values, episode)</span><br><span class="line"></span><br><span class="line">    values = rl_glue.agent.agent_message(<span class="string">&quot;get_values&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> true_values_file <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># Grading: The Manager will check that the values computed using your TD agent match </span></span><br><span class="line">        <span class="comment"># the true values (within some small allowance) across the states. In addition, it also</span></span><br><span class="line">        <span class="comment"># checks whether the root mean squared value error is close to 0.</span></span><br><span class="line">        manager.run_tests(values, value_error_threshold)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> values</span><br></pre></td></tr></table></figure>
<p>The cell below just runs a policy evaluation experiment with the determinstic optimal policy that strides just above the cliff. You should observe that the per state value error and RMSVE curve asymptotically go towards 0. The arrows in the four directions denote the probabilities of taking each action. This experiment is ungraded but should serve as a good test for the later experiments. The true values file provided for this experiment may help with debugging as well.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line">env_info = &#123;<span class="string">&quot;grid_height&quot;</span>: <span class="number">4</span>, <span class="string">&quot;grid_width&quot;</span>: <span class="number">12</span>, <span class="string">&quot;seed&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line">agent_info = &#123;<span class="string">&quot;discount&quot;</span>: <span class="number">1</span>, <span class="string">&quot;step_size&quot;</span>: <span class="number">0.01</span>, <span class="string">&quot;seed&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># The Optimal Policy that strides just along the cliff</span></span><br><span class="line">policy = np.ones(shape=(env_info[<span class="string">&#x27;grid_width&#x27;</span>] * env_info[<span class="string">&#x27;grid_height&#x27;</span>], <span class="number">4</span>)) * <span class="number">0.25</span></span><br><span class="line">policy[<span class="number">36</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">24</span>, <span class="number">35</span>):</span><br><span class="line">    policy[i] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">policy[<span class="number">35</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">agent_info.update(&#123;<span class="string">&quot;policy&quot;</span>: policy&#125;)</span><br><span class="line"></span><br><span class="line">true_values_file = <span class="string">&quot;optimal_policy_value_fn.npy&quot;</span></span><br><span class="line">_ = run_experiment(env_info, agent_info, num_episodes=<span class="number">5000</span>, experiment_name=<span class="string">&quot;Policy Evaluation on Optimal Policy&quot;</span>,</span><br><span class="line">                   plot_freq=<span class="number">500</span>, true_values_file=true_values_file)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;IPython.core.display.Javascript object&gt;</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The Safe Policy</span></span><br><span class="line"><span class="comment"># Hint: Fill in the array below (as done in the previous cell) based on the safe policy illustration </span></span><br><span class="line"><span class="comment"># in the environment diagram. This is the policy that strides as far as possible away from the cliff. </span></span><br><span class="line"><span class="comment"># We call it a &quot;safe&quot; policy because if the environment has any stochasticity, this policy would do a good job in </span></span><br><span class="line"><span class="comment"># keeping the agent from falling into the cliff (in contrast to the optimal policy shown before). </span></span><br><span class="line"><span class="comment"># BOILERPLATE:</span></span><br><span class="line">policy = np.ones(shape=(env_info[<span class="string">&#x27;grid_width&#x27;</span>] * env_info[<span class="string">&#x27;grid_height&#x27;</span>], <span class="number">4</span>)) * <span class="number">0.25</span></span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line">policy[<span class="number">36</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">24</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">12</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">11</span>):</span><br><span class="line">    policy[i] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">policy[<span class="number">11</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">23</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">35</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTO-GRADER TESTS FOR POLICY EVALUATION WITH SAFE POLICY</span></span><br><span class="line">agent_info.update(&#123;<span class="string">&quot;policy&quot;</span>: policy&#125;)</span><br><span class="line">v = run_experiment(env_info, agent_info,</span><br><span class="line">               experiment_name=<span class="string">&quot;Policy Evaluation On Safe Policy&quot;</span>,</span><br><span class="line">               num_episodes=<span class="number">5000</span>, plot_freq=<span class="number">500</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A Near Optimal Stochastic Policy</span></span><br><span class="line"><span class="comment"># Now, we try a stochastic policy that deviates a little from the optimal policy seen above. </span></span><br><span class="line"><span class="comment"># This means we can get different results due to randomness.</span></span><br><span class="line"><span class="comment"># We will thus average the value function estimates we get over multiple runs. </span></span><br><span class="line"><span class="comment"># This can take some time, upto about 5 minutes from previous testing. </span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The autograder will compare . Re-run this cell upon making any changes.</span></span><br><span class="line"></span><br><span class="line">env_info = &#123;<span class="string">&quot;grid_height&quot;</span>: <span class="number">4</span>, <span class="string">&quot;grid_width&quot;</span>: <span class="number">12</span>&#125;</span><br><span class="line">agent_info = &#123;<span class="string">&quot;discount&quot;</span>: <span class="number">1</span>, <span class="string">&quot;step_size&quot;</span>: <span class="number">0.01</span>&#125;</span><br><span class="line"></span><br><span class="line">policy = np.ones(shape=(env_info[<span class="string">&#x27;grid_width&#x27;</span>] * env_info[<span class="string">&#x27;grid_height&#x27;</span>], <span class="number">4</span>)) * <span class="number">0.25</span></span><br><span class="line">policy[<span class="number">36</span>] = [<span class="number">0.9</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">24</span>, <span class="number">35</span>):</span><br><span class="line">    policy[i] = [<span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.9</span>]</span><br><span class="line">policy[<span class="number">35</span>] = [<span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.9</span>, <span class="number">0.1</span>/<span class="number">3.</span>]</span><br><span class="line">agent_info.update(&#123;<span class="string">&quot;policy&quot;</span>: policy&#125;)</span><br><span class="line">agent_info.update(&#123;<span class="string">&quot;step_size&quot;</span>: <span class="number">0.01</span>&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTO-GRADER TESTS FOR POLICY EVALUATION WITH NEAR OPTIMAL STOCHASTIC POLICY (40 POINTS)</span></span><br><span class="line">arr = []</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">30</span>)):</span><br><span class="line">    env_info[<span class="string">&#x27;seed&#x27;</span>] = i</span><br><span class="line">    agent_info[<span class="string">&#x27;seed&#x27;</span>] = i</span><br><span class="line">    v = run_experiment(env_info, agent_info,</span><br><span class="line">                   experiment_name=<span class="string">&quot;Policy Evaluation On Optimal Policy&quot;</span>,</span><br><span class="line">                   num_episodes=<span class="number">5000</span>, plot_freq=<span class="number">10000</span>)</span><br><span class="line">    arr.append(v)</span><br><span class="line">average_v = np.array(arr).mean(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="wrapping-up">Wrapping Up</h2>
<p>Congratulations, you have completed assignment 2! In this assignment, we investigated a very useful concept for sample-based online learning: temporal difference. We particularly looked at the prediction problem where the goal is to find the value function corresponding to a given policy. In the next assignment, by learning the action-value function instead of the state-value function, you will get to see how temporal difference learning can be used in control as well.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Assignment-4-Chatbot/2020/09/28/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Assignment-4-Chatbot/2020/09/28/" class="post-title-link" itemprop="url">Assignment 4: Chatbot</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-09-28 13:50:01 / Modified: 17:00:27" itemprop="dateCreated datePublished" datetime="2020-09-28T13:50:01+08:00">2020-09-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Assignment-4-Chatbot/2020/09/28/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Assignment-4-Chatbot/2020/09/28/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="assignment-4-chatbot">Assignment 4: Chatbot</h1>
<p><img src = "cbot.jpg" height="400" width="400"></p>
<p>Welcome to the last assignment of Course 4. Before you get started, we want to congratulate you on getting here. It is your 16th programming assignment in this Specialization and we are very proud of you! In this assignment, you are going to use the <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2001.04451">Reformer</a>, also known as the efficient Transformer, to generate a dialogue between two bots. You will feed conversations to your model and it will learn how to understand the context of each one. Not only will it learn how to answer questions but it will also know how to ask questions if it needs more info. For example, after a customer asks for a train ticket, the chatbot can ask what time the said customer wants to leave. You can use this concept to automate call centers, hotel receptions, personal trainers, or any type of customer service. By completing this assignment, you will:</p>
<ul>
<li>Understand how the Reformer works</li>
<li>Explore the <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.00278">MultiWoz</a> dataset</li>
<li>Process the data to feed it into the model</li>
<li>Train your model</li>
<li>Generate a dialogue by feeding a question to the model</li>
</ul>
<h2 id="outline">Outline</h2>
<ul>
<li><a href="#1">Part 1: Exploring the MultiWoz dataset</a>
<ul>
<li><a href="#ex01">Exercise 01</a></li>
</ul></li>
<li><a href="#2">Part 2: Processing the data for Reformer inputs</a>
<ul>
<li><a href="#2.1">2.1 Tokenizing, batching with bucketing</a></li>
</ul></li>
<li><a href="#3">Part 3: Reversible layers</a>
<ul>
<li><a href="#ex02">Exercise 02</a></li>
<li><a href="#ex03">Exercise 03</a></li>
<li><a href="#3.1">3.1 Reversible layers and randomness</a></li>
</ul></li>
<li><a href="#4">Part 4: ReformerLM Training</a>
<ul>
<li><a href="#ex04">Exercise 04</a></li>
<li><a href="#ex05">Exercise 05</a></li>
</ul></li>
<li><a href="#5">Part 5: Decode from a pretrained model</a>
<ul>
<li><a href="#ex06">Exercise 06</a></li>
</ul></li>
</ul>
<p><a name="1"></a> # Part 1: Exploring the MultiWoz dataset</p>
<p>You will start by exploring the MultiWoz dataset. The dataset you are about to use has more than 10,000 human annotated dialogues and spans multiple domains and topics. Some dialogues include multiple domains and others include single domains. In this section, you will load and explore this dataset, as well as develop a function to extract the dialogues.</p>
<p>Let's first import the modules we will be using:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> termcolor <span class="keyword">import</span> colored</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> trax   </span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> training</span><br><span class="line">!pip <span class="built_in">list</span> | grep trax</span><br></pre></td></tr></table></figure>
<p>Let's also declare some constants we will be using in the exercises.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># filename of the MultiWOZ dialogue dataset</span></span><br><span class="line">DATA_FILE = <span class="string">&#x27;data.json&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># data directory</span></span><br><span class="line">DATA_DIR = <span class="string">&#x27;./data&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dictionary where we will load the dialogue dataset</span></span><br><span class="line">DIALOGUE_DB = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># vocabulary filename</span></span><br><span class="line">VOCAB_FILE = <span class="string">&#x27;en_32k.subword&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vocabulary file directory</span></span><br><span class="line">VOCAB_DIR = <span class="string">&#x27;data/vocabs&#x27;</span></span><br></pre></td></tr></table></figure>
<p>Let's now load the MultiWOZ 2.1 dataset. We have already provided it for you in your workspace. It is in JSON format so we should load it as such:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># help function to load a JSON file</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_json</span>(<span class="params">directory, file</span>):</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&#x27;<span class="subst">&#123;directory&#125;</span>/<span class="subst">&#123;file&#125;</span>&#x27;</span>) <span class="keyword">as</span> file: </span><br><span class="line">        db = json.load(file)</span><br><span class="line">    <span class="keyword">return</span> db</span><br><span class="line"></span><br><span class="line"><span class="comment"># load the dialogue data set into our dictionary</span></span><br><span class="line">DIALOGUE_DB = load_json(DATA_DIR, DATA_FILE)</span><br></pre></td></tr></table></figure>
<p>Let's see how many dialogues we have in the dictionary. 1 key-value pair is one dialogue so we can just get the dictionary's length.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;The number of dialogues is: <span class="subst">&#123;<span class="built_in">len</span>(DIALOGUE_DB)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>The number of dialogues is: 10438</code></pre>
<p>The dialogues are composed of multiple files and the filenames are used as keys in our dictionary. Those with multi-domain dialogues have "MUL" in their filenames while single domain dialogues have either "SNG" or "WOZ".</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print 7 keys from the dataset to see the filenames</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(DIALOGUE_DB.keys())[<span class="number">0</span>:<span class="number">7</span>]) </span><br></pre></td></tr></table></figure>
<pre><code>[&#39;SNG01856.json&#39;, &#39;SNG0129.json&#39;, &#39;PMUL1635.json&#39;, &#39;MUL2168.json&#39;, &#39;SNG0073.json&#39;, &#39;SNG01445.json&#39;, &#39;MUL2105.json&#39;]</code></pre>
<p>As you can see from the cells above, there are 10,438 conversations, each in its own file. You will train your model on all those conversations. Each file is also loaded into a dictionary and each has two keys which are the following:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get keys of the fifth file in the list above</span></span><br><span class="line"><span class="built_in">print</span>(DIALOGUE_DB[<span class="string">&#x27;SNG0073.json&#x27;</span>].keys())</span><br></pre></td></tr></table></figure>
<pre><code>dict_keys([&#39;goal&#39;, &#39;log&#39;])</code></pre>
<p>The <code>goal</code> also points to a dictionary and it contains several keys pertaining to the objectives of the conversation. For example below, we can see that the conversation will be about booking a taxi.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DIALOGUE_DB[<span class="string">&#x27;SNG0073.json&#x27;</span>][<span class="string">&#x27;goal&#x27;</span>]</span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;taxi&#39;: &#123;&#39;info&#39;: &#123;&#39;leaveAt&#39;: &#39;17:15&#39;,
   &#39;destination&#39;: &#39;pizza hut fen ditton&#39;,
   &#39;departure&#39;: &quot;saint john&#39;s college&quot;&#125;,
  &#39;reqt&#39;: [&#39;car type&#39;, &#39;phone&#39;],
  &#39;fail_info&#39;: &#123;&#125;&#125;,
 &#39;police&#39;: &#123;&#125;,
 &#39;hospital&#39;: &#123;&#125;,
 &#39;hotel&#39;: &#123;&#125;,
 &#39;attraction&#39;: &#123;&#125;,
 &#39;train&#39;: &#123;&#125;,
 &#39;message&#39;: [&quot;You want to book a &lt;span class=&#39;emphasis&#39;&gt;taxi&lt;/span&gt;. The taxi should go to &lt;span class=&#39;emphasis&#39;&gt;pizza hut fen ditton&lt;/span&gt; and should depart from &lt;span class=&#39;emphasis&#39;&gt;saint john&#39;s college&lt;/span&gt;&quot;,
  &quot;The taxi should &lt;span class=&#39;emphasis&#39;&gt;leave after 17:15&lt;/span&gt;&quot;,
  &quot;Make sure you get &lt;span class=&#39;emphasis&#39;&gt;car type&lt;/span&gt; and &lt;span class=&#39;emphasis&#39;&gt;contact number&lt;/span&gt;&quot;],
 &#39;restaurant&#39;: &#123;&#125;&#125;</code></pre>
<p>The <code>log</code> on the other hand contains the dialog. It is a list of dictionaries and each element of this list contains several descriptions as well. Let's look at an example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get first element of the log list</span></span><br><span class="line">DIALOGUE_DB[<span class="string">&#x27;SNG0073.json&#x27;</span>][<span class="string">&#x27;log&#x27;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;text&#39;: &quot;I would like a taxi from Saint John&#39;s college to Pizza Hut Fen Ditton.&quot;,
 &#39;metadata&#39;: &#123;&#125;,
 &#39;dialog_act&#39;: &#123;&#39;Taxi-Inform&#39;: [[&#39;Dest&#39;, &#39;pizza hut fen ditton&#39;],
   [&#39;Depart&#39;, &quot;saint john &#39;s college&quot;]]&#125;,
 &#39;span_info&#39;: [[&#39;Taxi-Inform&#39;, &#39;Dest&#39;, &#39;pizza hut fen ditton&#39;, 11, 14],
  [&#39;Taxi-Inform&#39;, &#39;Depart&#39;, &quot;saint john &#39;s college&quot;, 6, 9]]&#125;</code></pre>
<p>For this assignment, we are only interested in the conversation which is in the <code>text</code> field. The conversation goes back and forth between two persons. Let's call them 'Person 1' and 'Person 2'. This implies that data['SNG0073.json']['log'][0]['text'] is 'Person 1' and data['SNG0073.json']['log'][1]['text'] is 'Person 2' and so on. The even offsets are 'Person 1' and the odd offsets are 'Person 2'.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; Person 1: &#x27;</span>, DIALOGUE_DB[<span class="string">&#x27;SNG0073.json&#x27;</span>][<span class="string">&#x27;log&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;text&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; Person 2: &#x27;</span>,DIALOGUE_DB[<span class="string">&#x27;SNG0073.json&#x27;</span>][<span class="string">&#x27;log&#x27;</span>][<span class="number">1</span>][<span class="string">&#x27;text&#x27;</span>])</span><br></pre></td></tr></table></figure>
<pre><code> Person 1:  I would like a taxi from Saint John&#39;s college to Pizza Hut Fen Ditton.
 Person 2:  What time do you want to leave and what time do you want to arrive by?</code></pre>
<p><a name="ex01"></a> ### Exercise 01</p>
<p>You will now implement the <code>get_conversation()</code> function that will extract the conversations from the dataset's file.</p>
<p><strong>Instructions:</strong> Implement a function to extract conversations from the input file.<br />
As described above, the conversation is in the <code>text</code> field in each of the elements in the <code>log</code> list of the file. If the log list has <code>x</code> number of elements, then the function will get the <code>text</code> entries of each of those elements. Your function should return the conversation, prepending each field with either ' Person 1: ' if 'x' is even or ' Person 2: ' if 'x' is odd. You can use the Python modulus operator '%' to help select the even/odd entries. Important note: Do not print a newline character (i.e. <code>\n</code>) when generating the string. For example, in the code cell above, your function should output something like:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Person 1: I would like a taxi from Saint John&#x27;s college to Pizza Hut Fen Ditton. Person 2: What time do you want to leave and what time do you want to arrive by?</span><br></pre></td></tr></table></figure>
<p>and <strong>not</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Person 1:  I would like a taxi from Saint John&#x27;s college to Pizza Hut Fen Ditton.</span><br><span class="line">Person 2:  What time do you want to leave and what time do you want to arrive by?</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_conversation</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_conversation</span>(<span class="params">file, data_db</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        file (string): filename of the dialogue file saved as json</span></span><br><span class="line"><span class="string">        data_db (dict): dialogue database</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        string: A string containing the &#x27;text&#x27; fields of  data[file][&#x27;log&#x27;][x]</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize empty string</span></span><br><span class="line">    result = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get length of file&#x27;s log list</span></span><br><span class="line">    len_msg_log = <span class="built_in">len</span>(data_db[file][<span class="string">&#x27;log&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># set the delimiter strings</span></span><br><span class="line">    delimiter_1 = <span class="string">&#x27; Person 1: &#x27;</span></span><br><span class="line">    delimiter_2 = <span class="string">&#x27; Person 2: &#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over the file&#x27;s log list</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_msg_log):</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># get i&#x27;th element of file log list</span></span><br><span class="line">        cur_log = data_db[file][<span class="string">&#x27;log&#x27;</span>][i][<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># check if i is even</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>:                   </span><br><span class="line">            <span class="comment"># append the 1st delimiter string</span></span><br><span class="line">            result += delimiter_1</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="comment"># append the 2nd delimiter string</span></span><br><span class="line">            result += delimiter_2</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># append the message text from the log</span></span><br><span class="line">        result += cur_log</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line"><span class="keyword">import</span> w4_unittest</span><br><span class="line">w4_unittest.test_get_conversation(get_conversation)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">file = <span class="string">&#x27;SNG01856.json&#x27;</span></span><br><span class="line">conversation = get_conversation(file, DIALOGUE_DB)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print raw output</span></span><br><span class="line"><span class="built_in">print</span>(conversation)</span><br></pre></td></tr></table></figure>
<pre><code> Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it&#39;s cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn&#39;t able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.
Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.</code></pre>
<p><strong>Expected Result:</strong> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it&#x27;s cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn&#x27;t able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.</span><br><span class="line">Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.</span><br></pre></td></tr></table></figure></p>
<p>We can have a utility pretty print function just so we can visually follow the conversation more easily.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_conversation</span>(<span class="params">conversation</span>):</span></span><br><span class="line">    </span><br><span class="line">    delimiter_1 = <span class="string">&#x27;Person 1: &#x27;</span></span><br><span class="line">    delimiter_2 = <span class="string">&#x27;Person 2: &#x27;</span></span><br><span class="line">    </span><br><span class="line">    split_list_d1 = conversation.split(delimiter_1)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> sublist <span class="keyword">in</span> split_list_d1[<span class="number">1</span>:]:</span><br><span class="line">        split_list_d2 = sublist.split(delimiter_2)</span><br><span class="line">        <span class="built_in">print</span>(colored(<span class="string">f&#x27;Person 1: <span class="subst">&#123;split_list_d2[<span class="number">0</span>]&#125;</span>&#x27;</span>, <span class="string">&#x27;red&#x27;</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(split_list_d2) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="built_in">print</span>(colored(<span class="string">f&#x27;Person 2: <span class="subst">&#123;split_list_d2[<span class="number">1</span>]&#125;</span>&#x27;</span>, <span class="string">&#x27;green&#x27;</span>))</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">print_conversation(conversation)</span><br></pre></td></tr></table></figure>
<p>For this assignment, we will just use the outputs of the calls to <code>get_conversation</code> to train the model. But just to expound, there are also other information in the MultiWoz dataset that can be useful in other contexts. Each element of the log list has more information about it. For example, above, if you were to look at the other fields for the following, "am looking for a place to stay that has cheap price range it should be in a type of hotel", you will get the following.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DIALOGUE_DB[<span class="string">&#x27;SNG01856.json&#x27;</span>][<span class="string">&#x27;log&#x27;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;text&#39;: &#39;am looking for a place to to stay that has cheap price range it should be in a type of hotel&#39;,
 &#39;metadata&#39;: &#123;&#125;,
 &#39;dialog_act&#39;: &#123;&#39;Hotel-Inform&#39;: [[&#39;Type&#39;, &#39;hotel&#39;], [&#39;Price&#39;, &#39;cheap&#39;]]&#125;,
 &#39;span_info&#39;: [[&#39;Hotel-Inform&#39;, &#39;Type&#39;, &#39;hotel&#39;, 20, 20],
  [&#39;Hotel-Inform&#39;, &#39;Price&#39;, &#39;cheap&#39;, 10, 10]]&#125;</code></pre>
<p>The dataset also comes with hotel, hospital, taxi, train, police, and restaurant databases. For example, in case you need to call a doctor, or a hotel, or a taxi, this will allow you to automate the entire conversation. Take a look at the files accompanying the data set.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the attractions file</span></span><br><span class="line">attraction_file = <span class="built_in">open</span>(<span class="string">&#x27;data/attraction_db.json&#x27;</span>)</span><br><span class="line">attractions = json.load(attraction_file)</span><br><span class="line"><span class="built_in">print</span>(attractions[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;address&#39;: &#39;pool way, whitehill road, off newmarket road&#39;, &#39;area&#39;: &#39;east&#39;, &#39;entrance fee&#39;: &#39;?&#39;, &#39;id&#39;: &#39;1&#39;, &#39;location&#39;: [52.208789, 0.154883], &#39;name&#39;: &#39;abbey pool and astroturf pitch&#39;, &#39;openhours&#39;: &#39;?&#39;, &#39;phone&#39;: &#39;01223902088&#39;, &#39;postcode&#39;: &#39;cb58nt&#39;, &#39;pricerange&#39;: &#39;?&#39;, &#39;type&#39;: &#39;swimmingpool&#39;&#125;</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the hospital file</span></span><br><span class="line">hospital_file = <span class="built_in">open</span>(<span class="string">&#x27;data/hospital_db.json&#x27;</span>)</span><br><span class="line">hospitals = json.load(hospital_file)</span><br><span class="line"><span class="built_in">print</span>(hospitals[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;department&#39;: &#39;neurosciences critical care unit&#39;, &#39;id&#39;: 0, &#39;phone&#39;: &#39;01223216297&#39;&#125;</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the hotel file</span></span><br><span class="line">hotel_file = <span class="built_in">open</span>(<span class="string">&#x27;data/hotel_db.json&#x27;</span>)</span><br><span class="line">hotels = json.load(hotel_file)</span><br><span class="line"><span class="built_in">print</span>(hotels[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;address&#39;: &#39;124 tenison road&#39;, &#39;area&#39;: &#39;east&#39;, &#39;internet&#39;: &#39;yes&#39;, &#39;parking&#39;: &#39;no&#39;, &#39;id&#39;: &#39;0&#39;, &#39;location&#39;: [52.1963733, 0.1987426], &#39;name&#39;: &#39;a and b guest house&#39;, &#39;phone&#39;: &#39;01223315702&#39;, &#39;postcode&#39;: &#39;cb12dp&#39;, &#39;price&#39;: &#123;&#39;double&#39;: &#39;70&#39;, &#39;family&#39;: &#39;90&#39;, &#39;single&#39;: &#39;50&#39;&#125;, &#39;pricerange&#39;: &#39;moderate&#39;, &#39;stars&#39;: &#39;4&#39;, &#39;takesbookings&#39;: &#39;yes&#39;, &#39;type&#39;: &#39;guesthouse&#39;&#125;</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the police file</span></span><br><span class="line">police_file = <span class="built_in">open</span>(<span class="string">&#x27;data/police_db.json&#x27;</span>)</span><br><span class="line">police = json.load(police_file)</span><br><span class="line"><span class="built_in">print</span>(police[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;name&#39;: &#39;Parkside Police Station&#39;, &#39;address&#39;: &#39;Parkside, Cambridge&#39;, &#39;id&#39;: 0, &#39;phone&#39;: &#39;01223358966&#39;&#125;</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of a restuarant file</span></span><br><span class="line">restaurant_file = <span class="built_in">open</span>(<span class="string">&#x27;data/restaurant_db.json&#x27;</span>)</span><br><span class="line">restaurants = json.load(restaurant_file)</span><br><span class="line"><span class="built_in">print</span>(restaurants[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;address&#39;: &#39;Regent Street City Centre&#39;, &#39;area&#39;: &#39;centre&#39;, &#39;food&#39;: &#39;italian&#39;, &#39;id&#39;: &#39;19210&#39;, &#39;introduction&#39;: &#39;Pizza hut is a large chain with restaurants nationwide offering convenience pizzas pasta and salads to eat in or take away&#39;, &#39;location&#39;: [52.20103, 0.126023], &#39;name&#39;: &#39;pizza hut city centre&#39;, &#39;phone&#39;: &#39;01223323737&#39;, &#39;postcode&#39;: &#39;cb21ab&#39;, &#39;pricerange&#39;: &#39;cheap&#39;, &#39;type&#39;: &#39;restaurant&#39;&#125;</code></pre>
<p>For more information about the multiwoz 2.1 data set, please run the cell below to read the <code>ReadMe.txt</code> file. Feel free to open any other file to explore it.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/README&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    <span class="built_in">print</span>(file.read())</span><br></pre></td></tr></table></figure>
<pre><code>#####################################################
#####################################################
#  Copyright Cambridge Dialogue Systems Group, 2018 #
#####################################################
#####################################################

Dataset contains the following files:
1. data.json: the woz dialogue dataset, which contains the conversation  users and wizards, as well as a set of coarse labels for each user turn. This file contains both system and user dialogue acts annotated at the turn level. Files with multi-domain dialogues have &quot;MUL&quot; in their names. Single domain dialogues have either &quot;SNG&quot; or &quot;WOZ&quot; in their names.
2. restaurant_db.json: the Cambridge restaurant database file, containing restaurants in the Cambridge UK area and a set of attributes.
3. attraction_db.json: the Cambridge attraction database file, contining attractions in the Cambridge UK area and a set of attributes.
4. hotel_db.json: the Cambridge hotel database file, containing hotels in the Cambridge UK area and a set of attributes.
5. train_db.json: the Cambridge train (with artificial connections) database file, containing trains in the Cambridge UK area and a set of attributes.
6. hospital_db.json: the Cambridge hospital database file, contatining information about departments.
7. police_db.json: the Cambridge police station information.
8. taxi_db.json: slot-value list for taxi domain.
9. valListFile.txt: list of dialogues for validation.
10. testListFile.txt: list of dialogues for testing.
11. system_acts.json:
  There are 6 domains (&#39;Booking&#39;, &#39;Restaurant&#39;, &#39;Hotel&#39;, &#39;Attraction&#39;, &#39;Taxi&#39;, &#39;Train&#39;) and 1 dummy domain (&#39;general&#39;).
  A domain-dependent dialogue act is defined as a domain token followed by a domain-independent dialogue act, e.g. &#39;Hotel-inform&#39; means it is an &#39;inform&#39; act in the Hotel domain.
  Dialogue acts which cannot take slots, e.g., &#39;good bye&#39;, are defined under the &#39;general&#39; domain.
  A slot-value pair defined as a list with two elements. The first element is slot token and the second one is its value.
  If a dialogue act takes no slots, e.g., dialogue act &#39;offer booking&#39; for an utterance &#39;would you like to take a reservation?&#39;, its slot-value pair is [&#39;none&#39;, &#39;none&#39;]
  There are four types of values:
  1) If a slot takes a binary value, e.g., &#39;has Internet&#39; or &#39;has park&#39;, the value is either &#39;yes&#39; or &#39;no&#39;.
  2) If a slot is under the act &#39;request&#39;, e.g., &#39;request&#39; about &#39;area&#39;, the value is expressed as &#39;?&#39;.
  3) The value that appears in the utterance e.g., the name of a restaurant.
  4) If for some reason the turn does not have an annotation then it is labeled as &quot;No Annotation.&quot;
12. ontology.json: Data-based ontology containing all the values for the different slots in the domains.
13. slot_descriptions.json: A collection of human-written slot descriptions for each slot in the dataset. Each slot has at least two descriptions.
14. tokenization.md: A description of the tokenization preprocessing we had to perform to maintain consistency between the dialogue act annotations of DSTC 8 Track 1 and the existing MultiWOZ 2.0 data. </code></pre>
<p>As you can see, there are many other aspects of the MultiWoz dataset. Nonetheless, you'll see that even with just the conversations, your model will still be able to generate useful responses. This concludes our exploration of the dataset. In the next section, we will do some preprocessing before we feed it into our model for training.</p>
<p><a name="2"></a> # Part 2: Processing the data for Reformer inputs</p>
<p>You will now use the <code>get_conversation()</code> function to process the data. The Reformer expects inputs of this form:</p>
<p><strong>Person 1: Why am I so happy? Person 2: Because you are learning NLP Person 1: ... Person 2: ...</strong>*</p>
<p>And the conversation keeps going with some text. As you can see 'Person 1' and 'Person 2' act as delimiters so the model automatically recognizes the person and who is talking. It can then come up with the corresponding text responses for each person. Let's proceed to process the text in this fashion for the Reformer. First, let's grab all the conversation strings from all dialogue files and put them in a list.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the keys are the file names</span></span><br><span class="line">all_files = DIALOGUE_DB.keys()</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize empty list</span></span><br><span class="line">untokenized_data = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># loop over all files</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> all_files:</span><br><span class="line">    <span class="comment"># this is the graded function you coded</span></span><br><span class="line">    <span class="comment"># returns a string delimited by Person 1 and Person 2</span></span><br><span class="line">    result = get_conversation(file, DIALOGUE_DB)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># append to the list</span></span><br><span class="line">    untokenized_data.append(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the first element to check if it&#x27;s the same as the one we got before</span></span><br><span class="line"><span class="built_in">print</span>(untokenized_data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code> Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it&#39;s cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn&#39;t able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.
Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.</code></pre>
<p>Now let us split the list to a train and eval dataset.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># shuffle the list we generated above</span></span><br><span class="line">random.shuffle(untokenized_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define a cutoff (5% of the total length for this assignment)</span></span><br><span class="line"><span class="comment"># convert to int because we will use it as a list index</span></span><br><span class="line">cut_off = <span class="built_in">int</span>(<span class="built_in">len</span>(untokenized_data) * <span class="number">.05</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># slice the list. the last elements after the cut_off value will be the eval set. the rest is for training. </span></span><br><span class="line">train_data, eval_data = untokenized_data[:-cut_off], untokenized_data[-cut_off:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;number of conversations in the data set: <span class="subst">&#123;<span class="built_in">len</span>(untokenized_data)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;number of conversations in train set: <span class="subst">&#123;<span class="built_in">len</span>(train_data)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;number of conversations in eval set: <span class="subst">&#123;<span class="built_in">len</span>(eval_data)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>number of conversations in the data set: 10438
number of conversations in train set: 9917
number of conversations in eval set: 521</code></pre>
<p><a name="2.1"></a> ## 2.1 Tokenizing, batching with bucketing We can now proceed in generating tokenized batches of our data. Let's first define a utility generator function to yield elements from our data sets:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stream</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="comment"># loop over the entire data</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># get a random element</span></span><br><span class="line">        d = random.choice(data)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># yield a tuple pair of identical values </span></span><br><span class="line">        <span class="comment"># (i.e. our inputs to the model will also be our targets during training)</span></span><br><span class="line">        <span class="keyword">yield</span> (d, d)</span><br></pre></td></tr></table></figure>
<p>Now let's define our data pipeline for tokenizing and batching our data. As in the previous assignments, we will bucket by length and also have an upper bound on the token length.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># trax allows us to use combinators to generate our data pipeline</span></span><br><span class="line">data_pipeline = trax.data.Serial(</span><br><span class="line">    <span class="comment"># randomize the stream</span></span><br><span class="line">    trax.data.Shuffle(),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># tokenize the data</span></span><br><span class="line">    trax.data.Tokenize(vocab_dir=VOCAB_DIR,</span><br><span class="line">                       vocab_file=VOCAB_FILE),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># filter too long sequences</span></span><br><span class="line">    trax.data.FilterByLength(<span class="number">2048</span>),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># bucket by length</span></span><br><span class="line">    trax.data.BucketByLength(boundaries=[<span class="number">128</span>, <span class="number">256</span>,  <span class="number">512</span>, <span class="number">1024</span>],</span><br><span class="line">                             batch_sizes=[<span class="number">16</span>,    <span class="number">8</span>,    <span class="number">4</span>,   <span class="number">2</span>, <span class="number">1</span>]),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># add loss weights but do not add it to the padding tokens (i.e. 0)</span></span><br><span class="line">    trax.data.AddLossWeights(id_to_mask=<span class="number">0</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># apply the data pipeline to our train and eval sets</span></span><br><span class="line">train_stream = data_pipeline(stream(train_data))</span><br><span class="line">eval_stream = data_pipeline(stream(eval_data))</span><br></pre></td></tr></table></figure>
<p>Peek into the train stream.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the stream generators will yield (input, target, weights). let&#x27;s just grab the input for inspection</span></span><br><span class="line">inp, _, _ = <span class="built_in">next</span>(train_stream)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the shape. format is (batch size, token length)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input shape: &quot;</span>, inp.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># detokenize the first element</span></span><br><span class="line"><span class="built_in">print</span>(trax.data.detokenize(inp[<span class="number">0</span>], vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE))</span><br></pre></td></tr></table></figure>
<pre><code>input shape:  (4, 512)
 Person 1: I need a place to stay that has free wifi.  Person 2: There are 32 options in Cambridge, what price range are you looking for? Person 1: I&#39;m looking for something in the cheap price range, but I need it to have a 4 star rating. I don&#39;t need any parking though. Person 2: Again I have many to choose from that meet those criteria. Would you like a suggestion? Person 1: Ok, yes, if you could suggest one that comes with free parking that would be great! Person 2: I will book it for you,is there anything else I can do for you ? Person 1: I also need a Vietnamese restaurant. Person 2: My apologies it appears that I forgot to book your lodging. I recommend Alexander Bed and Breakfast, would you like me to book it for you? Person 1: Oh yes, please do. I need it for 8 people and 5 nights, beginning friday Person 2: You are booked with the reference number E9100B48. I can help you with the Vietnamese restaurant now. Do you have an area in mind? Person 1: I just want the restaurant to be in the same price range as my hotel Person 2: There is one cheap vietnamese restaurant in town. It is thanh binh. Do you want to book? Person 1: No, just provide me with the address and area for that restaurant if you could Person 2: The restaurant is located at 17 Magdalene Street City Centre in the West.  Can I help you with anything else? Person 1: Yes, will you book me a taxi to the restaurant from the hotel, please Person 2: And what time would you like that taxi? Person 1: I would like to leave the hotel by 22:15. Person 2: Your taxi service was book with a red volkswagen. The contact number is 07797935179 in case you need to contact them. Person 1: Thank you, that will be all. Person 2: You are welcome enjoy your meal. Have a good evenening</code></pre>
<p><a name="3"></a> # Part 3: Reversible layers</p>
<p>When running large deep models, you will often run out of memory as each layer allocates memory to store activations for use in backpropagation. To save this resource, you need to be able to recompute these activations during the backward pass without storing them during the forward pass. Take a look first at the leftmost diagram below.</p>
<p><img src="reversible2.PNG" height="400" width="600"></p>
<dl>
<dt>This is how the residual networks are implemented in the standard Transformer. It follows that, given <code>F()</code> is Attention and <code>G()</code> is Feed-forward(FF).</dt>
<dd>
</dd>
</dl>
<p><span class="math display">\[\begin{align}  
\mathrm{y}_\mathrm{a} &amp;= \mathrm{x} + \mathrm{F}\left(\mathrm{x}\right)\tag{1} \\
\mathrm{y}_{b}&amp;=\mathrm{y}_{a}+\mathrm{G}\left(\mathrm{y}_{a}\right)\tag{2}\\
\end{align}\]</span></p>
<p>As you can see, it requires that <span class="math inline">\(\mathrm{x}\)</span> and <span class="math inline">\(\mathrm{y}_{a}\)</span> be saved so it can be used during backpropagation. We want to avoid this to conserve memory and this is where reversible residual connections come in. They are shown in the middle and rightmost diagrams above. The key idea is that we will start with two copies of the input to the model and at each layer we will only update one of them. The activations that we <em>don't</em> update are the ones that will be used to compute the residuals.</p>
<p>Now in this reversible set up you get the following instead:</p>
<p><span class="math display">\[\begin{align}  
\mathrm{y}_{1}&amp;=\mathrm{x}_{1}+\mathrm{F}\left(\mathrm{x}_{2}\right)\tag{3}\\
\mathrm{y}_{2}&amp;=\mathrm{x}_{2}+\mathrm{G}\left(\mathrm{y}_{1}\right)\tag{4}\\
\end{align}\]</span> To recover <span class="math inline">\(\mathrm{(x_1,x_2)}\)</span> from <span class="math inline">\(\mathrm{(y_1, y_2)}\)</span></p>
<p><span class="math display">\[\begin{align}  
\mathrm{x}_{2}&amp;=\mathrm{y}_{2}-\mathrm{G}\left(\mathrm{y}_{1}\right)\tag{5}\\
\mathrm{x}_{1}&amp;=\mathrm{y}_{1}-\mathrm{F}\left(\mathrm{x}_{2}\right)\tag{6}\\
\end{align}\]</span></p>
<p>With this configuration, we're now able to run the network fully in reverse. You'll notice that during the backward pass, <span class="math inline">\(\mathrm{x2}\)</span> and <span class="math inline">\(\mathrm{x1}\)</span> can be recomputed based solely on the values of <span class="math inline">\(\mathrm{y2}\)</span> and <span class="math inline">\(\mathrm{y1}\)</span>. No need to save it during the forward pass.</p>
<p><a name="ex02"></a> ### Exercise 02 <strong>Instructions:</strong> You will implement the <code>reversible_layer_forward</code> function using equations 3 and 4 above. This function takes in the input vector <code>x</code> and the functions <code>f</code> and <code>g</code> and returns the concatenation of <span class="math inline">\(y_1 and y_2\)</span>. For this exercise, we will be splitting <code>x</code> before going through the reversible residual steps<span class="math inline">\(\mathrm{^1}\)</span>. We can then use those two vectors for the <code>reversible_layer_reverse</code> function. Utilize <code>np.concatenate()</code> to form the output being careful to match the axis of the <code>np.split()</code>.</p>
<p><span class="math inline">\(\mathrm{^1}\)</span><em>Take note that this is just for demonstrating the concept in this exercise and there are other ways of processing the input. As you'll see in the Reformer architecture later, the initial input (i.e. <code>x</code>) can instead be duplicated instead of split.</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: reversible_layer_forward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reversible_layer_forward</span>(<span class="params">x, f, g</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        x (np.array): an input vector or matrix</span></span><br><span class="line"><span class="string">        f (function): a function which operates on a vector/matrix</span></span><br><span class="line"><span class="string">        g (function): a function which operates on a vector/matrix</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        y (np.array): an output vector or matrix whose form is determined by &#x27;x&#x27;, f and g</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># split the input vector into two (* along the last axis because it is the depth dimension)</span></span><br><span class="line">    x1, x2 = np.split(x, <span class="number">2</span>, axis=-<span class="number">1</span>) </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get y1 using equation 3</span></span><br><span class="line">    y1 = x1 + f(x2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get y2 using equation 4</span></span><br><span class="line">    y2 = x2 + g(y1)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># concatenate y1 and y2 along the depth dimension. be sure output is of type np.ndarray</span></span><br><span class="line">    y = np.concatenate((y1,y2), axis = -<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_reversible_layer_forward(reversible_layer_forward)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure>
<p><a name="ex03"></a> ### Exercise 03</p>
<p>You will now implement the <code>reversible_layer_reverse</code> function which is possible because at every time step you have <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and <span class="math inline">\(y_2\)</span> and <span class="math inline">\(y_1\)</span>, along with the function <code>f</code>, and <code>g</code>. Where <code>f</code> is the attention and <code>g</code> is the feedforward. This allows you to compute equations 5 and 6.</p>
<p><strong>Instructions:</strong> Implement the <code>reversible_layer_reverse</code>. Your function takes in the output vector from <code>reversible_layer_forward</code> and functions f and g. Using equations 5 and 6 above, it computes the inputs to the layer, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. The output, x, is the concatenation of <span class="math inline">\(x_1, x_2\)</span>. Utilize <code>np.concatenate()</code> to form the output being careful to match the axis of the <code>np.split()</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: reversible_layer_reverse</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reversible_layer_reverse</span>(<span class="params">y, f, g</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        y (np.array): an input vector or matrix</span></span><br><span class="line"><span class="string">        f (function): a function which operates on a vector/matrix of the form of &#x27;y&#x27;</span></span><br><span class="line"><span class="string">        g (function): a function which operates on a vector/matrix of the form of &#x27;y&#x27;</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        y (np.array): an output vector or matrix whose form is determined by &#x27;y&#x27;, f and g</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># split the input vector into two (* along the last axis because it is the depth dimension)</span></span><br><span class="line">    y1, y2 = np.split(y, <span class="number">2</span>, axis=-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute x2 using equation 5</span></span><br><span class="line">    x2 = y2 - g(y1)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute x1 using equation 6</span></span><br><span class="line">    x1 = y1 - f(x2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># concatenate x1 and x2 along the depth dimension</span></span><br><span class="line">    x = np.concatenate((x1,x2),axis = -<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_reversible_layer_reverse(reversible_layer_reverse)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST COMMENT: assert at the end can be used in grading as well</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: x + <span class="number">2</span></span><br><span class="line">g = <span class="keyword">lambda</span> x: x * <span class="number">3</span></span><br><span class="line">input_vector = np.random.uniform(size=(<span class="number">32</span>,))</span><br><span class="line"></span><br><span class="line">output_vector = reversible_layer_forward(input_vector, f, g)</span><br><span class="line">reversed_vector = reversible_layer_reverse(output_vector, f, g)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(reversed_vector, input_vector)</span><br></pre></td></tr></table></figure>
<p><a name="3.1"></a> ## 3.1 Reversible layers and randomness</p>
<p>This is why we were learning about fastmath's random functions and keys in Course 3 Week 1. Utilizing the same key, <code>trax.fastmath.random.uniform()</code> will return the same values. This is required for the backward pass to return the correct layer inputs when random noise is introduced in the layer.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Layers like dropout have noise, so let&#x27;s simulate it here:</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: x + np.random.uniform(size=x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># See that the above doesn&#x27;t work any more:</span></span><br><span class="line">output_vector = reversible_layer_forward(input_vector, f, g)</span><br><span class="line">reversed_vector = reversible_layer_reverse(output_vector, f, g)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> <span class="keyword">not</span> np.allclose(reversed_vector, input_vector)  <span class="comment"># Fails!!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># It failed because the noise when reversing used a different random seed.</span></span><br><span class="line"></span><br><span class="line">random_seed = <span class="number">27686</span></span><br><span class="line">rng = trax.fastmath.random.get_prng(random_seed)</span><br><span class="line">f = <span class="keyword">lambda</span> x: x + trax.fastmath.random.uniform(key=rng, shape=x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># See that it works now as the same rng is used on forward and reverse.</span></span><br><span class="line">output_vector = reversible_layer_forward(input_vector, f, g)</span><br><span class="line">reversed_vector = reversible_layer_reverse(output_vector, f, g)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(reversed_vector, input_vector,  atol=<span class="number">1e-07</span>) </span><br></pre></td></tr></table></figure>
<p><a name="4"></a> # Part 4: ReformerLM Training</p>
<p>You will now proceed to training your model. Since you have already know the two main components that differentiates it from the standard Transformer, LSH in Course 1 and reversible layers above, you can just use the pre-built model already implemented in Trax. It will have this architecture:</p>
<p><img src='Reformer.jpg'></p>
<p>Similar to the Transformer you learned earlier, you want to apply an attention and feed forward layer to your inputs. For the Reformer, we improve the memory efficiency by using <strong>reversible decoder blocks</strong> and you can picture its implementation in Trax like below:</p>
<p><img src='ReversibleDecoder.png'></p>
<p>You can see that it takes the initial inputs <code>x1</code> and <code>x2</code> and does the first equation of the reversible networks you learned in Part 3. As you've also learned, the reversible residual has two equations for the forward-pass so doing just one of them will just constitute half of the reversible decoder block. Before doing the second equation (i.e. second half of the reversible residual), it first needs to swap the elements to take into account the stack semantics in Trax. It simply puts <code>x2</code> on top of the stack so it can be fed to the add block of the half-residual layer. It then swaps the two outputs again so it can be fed to the next layer of the network. All of these arrives at the two equations in Part 3 and it can be used to recompute the activations during the backward pass.</p>
<p>These are already implemented for you in Trax and in the following exercise, you'll get to practice how to call them to build your network.</p>
<p><a name="ex04"></a> ### Exercise 04 <strong>Instructions:</strong> Implement a wrapper function that returns a Reformer Language Model. You can use Trax's <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.models.html#trax.models.reformer.reformer.ReformerLM">ReformerLM</a> to do this quickly. It will have the same architecture as shown above.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReformerLM</span>(<span class="params">vocab_size=<span class="number">33000</span>, n_layers=<span class="number">2</span>, mode=<span class="string">&#x27;train&#x27;</span>, attention_type=tl.SelfAttention</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        vocab_size (int): size of the vocabulary</span></span><br><span class="line"><span class="string">        n_layers (int): number of decoder layers</span></span><br><span class="line"><span class="string">        mode (string): setting of the model which can be &#x27;train&#x27;, &#x27;eval&#x27;, or &#x27;predict&#x27; </span></span><br><span class="line"><span class="string">        attention_type(class): attention class to use </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        model (ReformerLM): a reformer language model implemented in Trax</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    <span class="comment"># initialize an instance of Trax&#x27;s ReformerLM class</span></span><br><span class="line">    model = trax.models.reformer.ReformerLM( </span><br><span class="line">        <span class="comment"># set vocab size</span></span><br><span class="line">        vocab_size = vocab_size,</span><br><span class="line">        <span class="comment"># set number of layers</span></span><br><span class="line">        n_layers = n_layers,</span><br><span class="line">        <span class="comment"># set mode</span></span><br><span class="line">        mode = mode,</span><br><span class="line">        <span class="comment"># set attention type</span></span><br><span class="line">        attention_type = attention_type</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># display the model</span></span><br><span class="line">temp_model = ReformerLM(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">str</span>(temp_model))</span><br><span class="line"></span><br><span class="line"><span class="comment"># free memory</span></span><br><span class="line"><span class="keyword">del</span> temp_model </span><br></pre></td></tr></table></figure>
<pre><code>Serial[
  ShiftRight(1)
  Embedding_train_512
  Dropout
  PositionalEncoding
  Dup_out2
  ReversibleSerial_in2_out2[
    ReversibleHalfResidualV2_in2_out2[
      Serial[
        LayerNorm
      ]
      SelfAttention
    ]
    ReversibleSwap_in2_out2
    ReversibleHalfResidualV2_in2_out2[
      Serial[
        LayerNorm
        Dense_2048
        Dropout
        FastGelu
        Dense_512
        Dropout
      ]
    ]
    ReversibleSwap_in2_out2
    ReversibleHalfResidualV2_in2_out2[
      Serial[
        LayerNorm
      ]
      SelfAttention
    ]
    ReversibleSwap_in2_out2
    ReversibleHalfResidualV2_in2_out2[
      Serial[
        LayerNorm
        Dense_2048
        Dropout
        FastGelu
        Dense_512
        Dropout
      ]
    ]
    ReversibleSwap_in2_out2
  ]
  Concatenate_in2
  LayerNorm
  Dropout
  Dense_train
  LogSoftmax
]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_ReformerLM(ReformerLM)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure>
<p><a name="ex05"></a> ### Exercise 05 You will now write a function that takes in your model and trains it.</p>
<p><strong>Instructions:</strong> Implement the <code>training_loop</code> below to train the neural network above. Here is a list of things you should do:</p>
<ul>
<li>Create <code>TrainTask</code> and <code>EvalTask</code></li>
<li>Create the training loop <code>trax.supervised.training.Loop</code></li>
<li>Pass in the following depending to train_task :
<ul>
<li><code>labeled_data=train_gen</code></li>
<li><code>loss_layer=tl.CrossEntropyLoss()</code></li>
<li><code>optimizer=trax.optimizers.Adam(0.01)</code></li>
<li><code>lr_schedule=lr_schedule</code></li>
<li><code>n_steps_per_checkpoint=10</code></li>
</ul></li>
</ul>
<p>You will be using your CrossEntropyLoss loss function with Adam optimizer. Please read the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html?highlight=adam#trax.optimizers.adam.Adam">trax</a> documentation to get a full understanding.</p>
<ul>
<li>Pass in the following to eval_task:
<ul>
<li><code>labeled_data=eval_gen</code></li>
<li><code>metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]</code></li>
</ul></li>
</ul>
<p>This function should return a <code>training.Loop</code> object. To read more about this check the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html?highlight=loop#trax.supervised.training.Loop">docs</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: train_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_loop</span>(<span class="params">ReformerLM, train_gen, eval_gen, output_dir = <span class="string">&quot;./model/&quot;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        ReformerLM:  the Reformer language model you are building</span></span><br><span class="line"><span class="string">        train_gen (generator): train data generator.</span></span><br><span class="line"><span class="string">        eval_gen (generator): Validation generator. </span></span><br><span class="line"><span class="string">        output_dir (string): Path to save the model output. Defaults to &#x27;./model/&#x27;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.supervised.training.Loop: Training loop for the model.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># use the warmup_and_rsqrt_decay learning rate schedule</span></span><br><span class="line">    lr_schedule = trax.lr.warmup_and_rsqrt_decay(</span><br><span class="line">        n_warmup_steps=<span class="number">1000</span>, max_value=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># define the train task</span></span><br><span class="line">    train_task = training.TrainTask(            </span><br><span class="line">        <span class="comment"># labeled data</span></span><br><span class="line">        labeled_data = train_gen,</span><br><span class="line">        <span class="comment"># loss layer</span></span><br><span class="line">        loss_layer = tl.CrossEntropyLoss(),</span><br><span class="line">        <span class="comment"># optimizer</span></span><br><span class="line">        optimizer=trax.optimizers.Adam(<span class="number">0.01</span>),</span><br><span class="line">        <span class="comment"># lr_schedule</span></span><br><span class="line">        lr_schedule=lr_schedule,</span><br><span class="line">        <span class="comment"># n_steps</span></span><br><span class="line">        n_steps_per_checkpoint=<span class="number">10</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># define the eval task</span></span><br><span class="line">    eval_task = training.EvalTask(                      </span><br><span class="line">        <span class="comment"># labeled data</span></span><br><span class="line">        labeled_data = eval_gen,</span><br><span class="line">        metrics = [tl.CrossEntropyLoss(), tl.Accuracy()]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    loop = training.Loop(ReformerLM(mode=<span class="string">&#x27;train&#x27;</span>),</span><br><span class="line">                         train_task,</span><br><span class="line">                         eval_tasks=[eval_task],</span><br><span class="line">                         output_dir=output_dir)</span><br><span class="line">    <span class="keyword">return</span> loop</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST COMMENT: Use the train task and eval task for grading train_model</span></span><br><span class="line">test_loop = training_loop(ReformerLM, train_stream, eval_stream)</span><br><span class="line">train_task = test_loop._task</span><br><span class="line">eval_task = test_loop._eval_task</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_task)</span><br><span class="line"><span class="built_in">print</span>(eval_task)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;trax.supervised.training.TrainTask object at 0x7fd4ddf95dd0&gt;
&lt;trax.supervised.training.EvalTask object at 0x7fd4dc2a2f50&gt;</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_tasks(train_task, eval_task)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># we will now test your function</span></span><br><span class="line">!rm -f model/model.pkl.gz</span><br><span class="line">loop = training_loop(ReformerLM, train_stream, eval_stream)</span><br><span class="line">loop.run(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Step      1: Ran 1 train steps in 58.71 secs
Step      1: train CrossEntropyLoss |  10.41530514
Step      1: eval  CrossEntropyLoss |  10.41272354
Step      1: eval          Accuracy |  0.00000000

Step     10: Ran 9 train steps in 163.46 secs
Step     10: train CrossEntropyLoss |  10.25675583
Step     10: eval  CrossEntropyLoss |  9.94296360
Step     10: eval          Accuracy |  0.11201393</code></pre>
<p><strong>Approximate Expected output:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Step      1: Ran 1 train steps in 55.73 secs</span><br><span class="line">Step      1: train CrossEntropyLoss |  10.41907787</span><br><span class="line">Step      1: eval  CrossEntropyLoss |  10.41005802</span><br><span class="line">Step      1: eval          Accuracy |  0.00000000</span><br><span class="line"></span><br><span class="line">Step     10: Ran 9 train steps in 108.21 secs</span><br><span class="line">Step     10: train CrossEntropyLoss |  10.15449715</span><br><span class="line">Step     10: eval  CrossEntropyLoss |  9.63478279</span><br><span class="line">Step     10: eval          Accuracy |  0.16350447</span><br></pre></td></tr></table></figure>
<p><a name="5"></a> # Part 5: Decode from a pretrained model</p>
<p>We will now proceed on decoding using the model architecture you just implemented. As in the previous weeks, we will be giving you a pretrained model so you can observe meaningful output during inference. You will be using the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.decoding.autoregressive_sample_stream">autoregressive_sample_stream()</a> decoding method from Trax to do fast inference. Let's define a few parameters to initialize our model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define the `predict_mem_len` and `predict_drop_len` of tl.SelfAttention</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">    <span class="comment"># number of input positions to remember in a cache when doing fast inference. </span></span><br><span class="line">    kwargs[<span class="string">&#x27;predict_mem_len&#x27;</span>] = <span class="number">120</span></span><br><span class="line">    <span class="comment"># number of input elements to drop once the fast inference input cache fills up.</span></span><br><span class="line">    kwargs[<span class="string">&#x27;predict_drop_len&#x27;</span>] = <span class="number">120</span></span><br><span class="line">    <span class="comment"># return the attention layer with the parameters defined above</span></span><br><span class="line">    <span class="keyword">return</span> tl.SelfAttention(*args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define the model using the ReformerLM function you implemented earlier.</span></span><br><span class="line">model = ReformerLM(</span><br><span class="line">    vocab_size=<span class="number">33000</span>,</span><br><span class="line">    n_layers=<span class="number">6</span>,</span><br><span class="line">    mode=<span class="string">&#x27;predict&#x27;</span>,</span><br><span class="line">    attention_type=attention,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define an input signature so we can initialize our model. shape will be (1, 1) and the data type is int32.</span></span><br><span class="line">shape11 = trax.shapes.ShapeDtype((<span class="number">1</span>, <span class="number">1</span>), dtype=np.int32)</span><br></pre></td></tr></table></figure>
<p>We can now initialize our model from a file containing the pretrained weights. We will save this starting state so we can reset the model state when we generate a new conversation. This will become clearer in the <code>generate_dialogue()</code> function later.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialize from file</span></span><br><span class="line">model.init_from_file(<span class="string">&#x27;chatbot_model1.pkl.gz&#x27;</span>,</span><br><span class="line">                     weights_only=<span class="literal">True</span>, input_signature=shape11)</span><br><span class="line"></span><br><span class="line"><span class="comment"># save the starting state</span></span><br><span class="line">STARTING_STATE = model.state</span><br></pre></td></tr></table></figure>
<p>Let's define a few utility functions as well to help us tokenize and detokenize. We can use the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.tokenize">tokenize()</a> and <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.detokenize">detokenize()</a> from <code>trax.data.tf_inputs</code> to do this.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">sentence, vocab_file, vocab_dir</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(trax.data.tokenize(<span class="built_in">iter</span>([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detokenize</span>(<span class="params">tokens, vocab_file, vocab_dir</span>):</span></span><br><span class="line">    <span class="keyword">return</span> trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)</span><br></pre></td></tr></table></figure>
<p>We are now ready to define our decoding function. This will return a generator that yields that next symbol output by the model. It will be able to predict the next words by just feeding it a starting sentence.</p>
<p><a name="ex06"></a> ### Exercise 06 <strong>Instructions:</strong> Implement the function below to return a generator that predicts the next word of the conversation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReformerLM_output_gen</span>(<span class="params">ReformerLM, start_sentence, vocab_file, vocab_dir, temperature</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        ReformerLM:  the Reformer language model you just trained</span></span><br><span class="line"><span class="string">        start_sentence (string): starting sentence of the conversation</span></span><br><span class="line"><span class="string">        vocab_file (string): vocabulary filename</span></span><br><span class="line"><span class="string">        vocab_dir (string): directory of the vocabulary file</span></span><br><span class="line"><span class="string">        temperature (float): parameter for sampling ranging from 0.0 to 1.0.</span></span><br><span class="line"><span class="string">            0.0: same as argmax, always pick the most probable token</span></span><br><span class="line"><span class="string">            1.0: sampling from the distribution (can sometimes say random things)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        generator: yields the next symbol generated by the model</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create input tokens using the the tokenize function</span></span><br><span class="line">    input_tokens = tokenize(start_sentence, vocab_file, vocab_dir)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add batch dimension to array. Convert from (n,) to (x, n) where </span></span><br><span class="line">    <span class="comment"># x is the batch size. Default is 1. (hint: you can use np.expand_dims() with axis=0)</span></span><br><span class="line">    input_tokens_with_batch = np.expand_dims(input_tokens, axis = <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># call the autoregressive_sample_stream function from trax</span></span><br><span class="line">    output_gen = trax.supervised.decoding.autoregressive_sample_stream( </span><br><span class="line">        <span class="comment"># model</span></span><br><span class="line">        ReformerLM,</span><br><span class="line">        <span class="comment"># inputs will be the tokens with batch dimension</span></span><br><span class="line">        inputs = input_tokens_with_batch,</span><br><span class="line">        <span class="comment"># temperature</span></span><br><span class="line">        temperature = temperature</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output_gen</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">WEIGHTS_FROM_FILE = ()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;weights&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    WEIGHTS_FROM_FILE = pickle.load(file)</span><br><span class="line"></span><br><span class="line">shape11 = trax.shapes.ShapeDtype((<span class="number">1</span>, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">    kwargs[<span class="string">&#x27;predict_mem_len&#x27;</span>] = <span class="number">120</span></span><br><span class="line">    kwargs[<span class="string">&#x27;predict_drop_len&#x27;</span>] = <span class="number">120</span></span><br><span class="line">    <span class="keyword">return</span> tl.SelfAttention(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">test_model = ReformerLM(vocab_size=<span class="number">5</span>, n_layers=<span class="number">1</span>, mode=<span class="string">&#x27;predict&#x27;</span>, attention_type=attention)</span><br><span class="line"></span><br><span class="line">test_output_gen = ReformerLM_output_gen(test_model, <span class="string">&quot;test&quot;</span>, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">test_model.init_weights_and_state(shape11)</span><br><span class="line"></span><br><span class="line">test_model.weights = WEIGHTS_FROM_FILE</span><br><span class="line"></span><br><span class="line">output = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    output.append(<span class="built_in">next</span>(test_output_gen)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># free memory</span></span><br><span class="line"><span class="keyword">del</span> test_model </span><br><span class="line"><span class="keyword">del</span> WEIGHTS_FROM_FILE</span><br><span class="line"><span class="keyword">del</span> test_output_gen</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure>
<pre><code>[1, 0, 4, 3, 0, 4]</code></pre>
<p><strong><em>Expected value:</em></strong></p>
<p>[1, 0, 4, 3, 0, 4]</p>
<p>Great! Now you will be able to see the model in action. The utility function below will call the generator you just implemented and will just format the output to be easier to read.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">shape11 = trax.shapes.ShapeDtype((<span class="number">1</span>, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">    kwargs[<span class="string">&#x27;predict_mem_len&#x27;</span>] = <span class="number">120</span>  <span class="comment"># max length for predictions</span></span><br><span class="line">    kwargs[<span class="string">&#x27;predict_drop_len&#x27;</span>] = <span class="number">120</span>  <span class="comment"># never drop old stuff</span></span><br><span class="line">    <span class="keyword">return</span> tl.SelfAttention(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">model = ReformerLM(</span><br><span class="line">    vocab_size=<span class="number">33000</span>,</span><br><span class="line">    n_layers=<span class="number">6</span>,</span><br><span class="line">    mode=<span class="string">&#x27;predict&#x27;</span>,</span><br><span class="line">    attention_type=attention,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.init_from_file(<span class="string">&#x27;chatbot_model1.pkl.gz&#x27;</span>,</span><br><span class="line">                     weights_only=<span class="literal">True</span>, input_signature=shape11)</span><br><span class="line"></span><br><span class="line">STARTING_STATE = model.state</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_dialogue</span>(<span class="params">ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        ReformerLM:  the Reformer language model you just trained</span></span><br><span class="line"><span class="string">        model_state (np.array): initial state of the model before decoding</span></span><br><span class="line"><span class="string">        start_sentence (string): starting sentence of the conversation</span></span><br><span class="line"><span class="string">        vocab_file (string): vocabulary filename</span></span><br><span class="line"><span class="string">        vocab_dir (string): directory of the vocabulary file</span></span><br><span class="line"><span class="string">        max_len (int): maximum number of tokens to generate </span></span><br><span class="line"><span class="string">        temperature (float): parameter for sampling ranging from 0.0 to 1.0.</span></span><br><span class="line"><span class="string">            0.0: same as argmax, always pick the most probable token</span></span><br><span class="line"><span class="string">            1.0: sampling from the distribution (can sometimes say random things)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        generator: yields the next symbol generated by the model</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># define the delimiters we used during training</span></span><br><span class="line">    delimiter_1 = <span class="string">&#x27;Person 1: &#x27;</span> </span><br><span class="line">    delimiter_2 = <span class="string">&#x27;Person 2: &#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize detokenized output</span></span><br><span class="line">    sentence = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># token counter</span></span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># output tokens. we insert a &#x27;: &#x27; for formatting</span></span><br><span class="line">    result = [tokenize(<span class="string">&#x27;: &#x27;</span>, vocab_file=vocab_file, vocab_dir=vocab_dir)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># reset the model state when starting a new dialogue</span></span><br><span class="line">    ReformerLM.state = model_state</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calls the output generator implemented earlier</span></span><br><span class="line">    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># print the starting sentence</span></span><br><span class="line">    <span class="built_in">print</span>(start_sentence.split(delimiter_2)[<span class="number">0</span>].strip())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.</span></span><br><span class="line">    <span class="keyword">for</span> o <span class="keyword">in</span> output:</span><br><span class="line">        </span><br><span class="line">        result.append(o)</span><br><span class="line">        </span><br><span class="line">        sentence = detokenize(np.concatenate(result, axis=<span class="number">0</span>), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> sentence.endswith(delimiter_1):</span><br><span class="line">            sentence = sentence.split(delimiter_1)[<span class="number">0</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;delimiter_2&#125;</span><span class="subst">&#123;sentence&#125;</span>&#x27;</span>)</span><br><span class="line">            sentence = <span class="string">&#x27;&#x27;</span></span><br><span class="line">            result.clear()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> sentence.endswith(delimiter_2):</span><br><span class="line">            sentence = sentence.split(delimiter_2)[<span class="number">0</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;delimiter_1&#125;</span><span class="subst">&#123;sentence&#125;</span>&#x27;</span>)</span><br><span class="line">            sentence = <span class="string">&#x27;&#x27;</span></span><br><span class="line">            result.clear()</span><br><span class="line"></span><br><span class="line">        counter += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> counter &gt; max_len:</span><br><span class="line">            <span class="keyword">break</span>    </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>We can now feed in different starting sentences and see how the model generates the dialogue. You can even input your own starting sentence. Just remember to ask a question that covers the topics in the Multiwoz dataset so you can generate a meaningful conversation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_sentence = <span class="string">&#x27; Person 1: Are there theatres in town? Person 2: &#x27;</span></span><br><span class="line">generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=<span class="number">120</span>, temperature=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Person 1: Are there theatres in town?
Person 2: : There are 4 theatres in town. Do you have a preference on area? 
Person 1: No, I don&#39;t care. Which one do you recommend? 
Person 2: I would recommend the Mumford Theatre. Would you like more information on it? 
Person 1: Yes, could I get the postcode and phone number please? 
Person 2: The phone number is 08451962320 and the postcode is cb11pt. The phone number is 084519/ 15/15 - would you like to book a table? </code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_sentence = <span class="string">&#x27; Person 1: Is there a hospital nearby? Person 2: &#x27;</span></span><br><span class="line">generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=<span class="number">120</span>, temperature=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Person 1: Is there a hospital nearby?
Person 2: : Addensbrookes Hospital is located at Hills Rd, Cambridge, postcode CB20QQ. Do you need anything else? 
Person 1: No, that&#39;s all I need. Thanks. 
Person 2: You&#39;re welcome. Have a good day.Good bye.
Person 1: Thanks again. Goodbye. 
Person 2: You&#39;re welcome. Have a good day.Good bye.</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_sentence = <span class="string">&#x27; Person 1: Can you book a taxi? Person 2: &#x27;</span></span><br><span class="line">generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=<span class="number">120</span>, temperature=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Person 1: Can you book a taxi?
Person 2: : I sure can. Where are you going? 
Person 1: I&#39;m going to be picked up from the city centre north b and b. 
Person 2: I have booked you a grey volkswagen. The contact number is 0783212843. 
Person 1: Thank you. That&#39;s all I need. 
Person 2: Thank you for using our services. Have a great day!k you.Good bye.
Person 1: Actually, I&#39;ry about there. </code></pre>
<p><strong>Congratulations! You just wrapped up the final assignment of this course and the entire specialization!</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Assignment-3-Question-Answering/2020/09/27/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Assignment-3-Question-Answering/2020/09/27/" class="post-title-link" itemprop="url">Assignment 3: Question Answering</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-09-27 17:13:45 / Modified: 17:15:12" itemprop="dateCreated datePublished" datetime="2020-09-27T17:13:45+08:00">2020-09-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Assignment-3-Question-Answering/2020/09/27/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Assignment-3-Question-Answering/2020/09/27/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="assignment-3-question-answering">Assignment 3: Question Answering</h1>
<p>Welcome to this week's assignment of course 4. In this you will explore question answering. You will implement the "Text to Text Transfer from Transformers" (better known as T5). Since you implemented transformers from scratch last week you will now be able to use them.</p>
<p><img src = "qa.png"></p>
<h2 id="outline">Outline</h2>
<ul>
<li><a href="#0">Overview</a></li>
<li><a href="#0">Part 0: Importing the Packages</a></li>
<li><a href="#1">Part 1: C4 Dataset</a>
<ul>
<li><a href="#1.1">1.1 Pre-Training Objective</a></li>
<li><a href="#1.2">1.2 Process C4</a>
<ul>
<li><a href="#1.2.1">1.2.1 Decode to natural language</a></li>
</ul></li>
<li><a href="#1.3">1.3 Tokenizing and Masking</a>
<ul>
<li><a href="#ex01">Exercise 01</a></li>
</ul></li>
<li><a href="#1.4">1.4 Creating the Pairs</a></li>
</ul></li>
<li><a href="#2">Part 2: Transfomer</a>
<ul>
<li><a href="#2.1">2.1 Transformer Encoder</a>
<ul>
<li><a href="#2.1.1">2.1.1 The Feedforward Block</a>
<ul>
<li><a href="#ex02">Exercise 02</a></li>
</ul></li>
<li><a href="#2.1.2">2.1.2 The Encoder Block</a>
<ul>
<li><a href="#ex03">Exercise 03</a></li>
</ul></li>
<li><a href="#2.1.3">2.1.3 The Transformer Encoder</a>
<ul>
<li><a href="#ex04">Exercise 04</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p><a name='0'></a> ### Overview</p>
<p>This assignment will be different from the two previous ones. Due to memory and time constraints of this environment you will not be able to train a model and use it for inference. Instead you will create the necessary building blocks for the transformer encoder model and will use a pretrained version of the same model in two ungraded labs after this assignment.</p>
<p>After completing these 3 (1 graded and 2 ungraded) labs you will: * Implement the code neccesary for Bidirectional Encoder Representation from Transformer (BERT). * Understand how the C4 dataset is structured. * Use a pretrained model for inference. * Understand how the "Text to Text Transfer from Transformers" or T5 model works.</p>
<p><a name='0'></a> # Part 0: Importing the Packages</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ast</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> textwrap</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> trax </span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> decoding</span><br><span class="line"></span><br><span class="line"><span class="comment"># Will come handy later.</span></span><br><span class="line">wrapper = textwrap.TextWrapper(width=<span class="number">70</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set random seed</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br></pre></td></tr></table></figure>
<p><a name='1'></a> ## Part 1: C4 Dataset</p>
<p>The <a target="_blank" rel="noopener" href="https://www.tensorflow.org/datasets/catalog/c4">C4</a> is a huge data set. For the purpose of this assignment you will use a few examples out of it which are present in <code>data.txt</code>. C4 is based on the <a target="_blank" rel="noopener" href="https://commoncrawl.org/">common crawl</a> project. Feel free to read more on their website.</p>
<p>Run the cell below to see how the examples look like.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load example jsons</span></span><br><span class="line">example_jsons = <span class="built_in">list</span>(<span class="built_in">map</span>(ast.literal_eval, <span class="built_in">open</span>(<span class="string">&#x27;data.txt&#x27;</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Printing the examples to see how the data looks like</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;example number <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>: \n\n<span class="subst">&#123;example_jsons[i]&#125;</span> \n&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>example number 1: 

&#123;&#39;content-length&#39;: b&#39;1970&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&#39;Beginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.&#39;, &#39;timestamp&#39;: b&#39;2019-04-25T12:57:54Z&#39;, &#39;url&#39;: b&#39;https://klyq.com/beginners-bbq-class-taking-place-in-missoula/&#39;&#125; 

example number 2: 

&#123;&#39;content-length&#39;: b&#39;12064&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&#39;Discussion in \&#39;Mac OS X Lion (10.7)\&#39; started by axboi87, Jan 20, 2012.\nI\&#39;ve got a 500gb internal drive and a 240gb SSD.\nWhen trying to restore using disk utility i\&#39;m given the error &quot;Not enough space on disk ____ to restore&quot;\nBut I shouldn\&#39;t have to do that!!!\nAny ideas or workarounds before resorting to the above?\nUse Carbon Copy Cloner to copy one drive to the other. I\&#39;ve done this several times going from larger HDD to smaller SSD and I wound up with a bootable SSD drive. One step you have to remember not to skip is to use Disk Utility to partition the SSD as GUID partition scheme HFS+ before doing the clone. If it came Apple Partition Scheme, even if you let CCC do the clone, the resulting drive won\&#39;t be bootable. CCC usually works in &quot;file mode&quot; and it can easily copy a larger drive (that\&#39;s mostly empty) onto a smaller drive. If you tell CCC to clone a drive you did NOT boot from, it can work in block copy mode where the destination drive must be the same size or larger than the drive you are cloning from (if I recall).\nI\&#39;ve actually done this somehow on Disk Utility several times (booting from a different drive (or even the dvd) so not running disk utility from the drive your cloning) and had it work just fine from larger to smaller bootable clone. Definitely format the drive cloning to first, as bootable Apple etc..\nThanks for pointing this out. My only experience using DU to go larger to smaller was when I was trying to make a Lion install stick and I was unable to restore InstallESD.dmg to a 4 GB USB stick but of course the reason that wouldn\&#39;t fit is there was slightly more than 4 GB of data.&#39;, &#39;timestamp&#39;: b&#39;2019-04-21T10:07:13Z&#39;, &#39;url&#39;: b&#39;https://forums.macrumors.com/threads/restore-from-larger-disk-to-smaller-disk.1311329/&#39;&#125; 

example number 3: 

&#123;&#39;content-length&#39;: b&#39;5235&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&#39;Foil plaid lycra and spandex shortall with metallic slinky insets. Attached metallic elastic belt with O-ring. Headband included. Great hip hop or jazz dance costume. Made in the USA.&#39;, &#39;timestamp&#39;: b&#39;2019-04-25T10:40:23Z&#39;, &#39;url&#39;: b&#39;https://awishcometrue.com/Catalogs/Clearance/Tweens/V1960-Find-A-Way&#39;&#125; 

example number 4: 

&#123;&#39;content-length&#39;: b&#39;4967&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&quot;How many backlinks per day for new site?\nDiscussion in &#39;Black Hat SEO&#39; started by Omoplata, Dec 3, 2010.\n1) for a newly created site, what&#39;s the max # backlinks per day I should do to be safe?\n2) how long do I have to let my site age before I can start making more blinks?\nI did about 6000 forum profiles every 24 hours for 10 days for one of my sites which had a brand new domain.\nThere is three backlinks for every of these forum profile so thats 18 000 backlinks every 24 hours and nothing happened in terms of being penalized or sandboxed. This is now maybe 3 months ago and the site is ranking on first page for a lot of my targeted keywords.\nbuild more you can in starting but do manual submission and not spammy type means manual + relevant to the post.. then after 1 month you can make a big blast..\nWow, dude, you built 18k backlinks a day on a brand new site? How quickly did you rank up? What kind of competition/searches did those keywords have?&quot;, &#39;timestamp&#39;: b&#39;2019-04-21T12:46:19Z&#39;, &#39;url&#39;: b&#39;https://www.blackhatworld.com/seo/how-many-backlinks-per-day-for-new-site.258615/&#39;&#125; 

example number 5: 

&#123;&#39;content-length&#39;: b&#39;4499&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&#39;The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what\xe2\x80\x99s included in the mill levy measure.&#39;, &#39;timestamp&#39;: b&#39;2019-04-20T14:33:21Z&#39;, &#39;url&#39;: b&#39;http://bond.dpsk12.org/category/news/&#39;&#125; </code></pre>
<p>Notice the <code>b</code> before each string? This means that this data comes as bytes rather than strings. Strings are actually lists of bytes so for the rest of the assignments the name <code>strings</code> will be used to describe the data.</p>
<p>To check this run the following cell:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">type</span>(example_jsons[<span class="number">0</span>].get(<span class="string">&#x27;text&#x27;</span>))</span><br></pre></td></tr></table></figure>
<pre><code>bytes</code></pre>
<p><a name='1.1'></a> ### 1.1 Pre-Training Objective</p>
<p><strong>Note:</strong> The word "mask" will be used throughout this assignment in context of hiding/removing word(s)</p>
<p>You will be implementing the BERT loss as shown in the following image.</p>
<p><img src = "loss.png" width="600" height = "400"></p>
<p>Assume you have the following text: <span style="color:blue"> <strong>Thank you <span style="color:red">for inviting </span> me to your party <span style="color:red">last</span> week</strong> </span></p>
<p>Now as input you will mask the words in red in the text:</p>
<p><span style="color:blue"> <strong>Input:</strong></span> Thank you <strong>X</strong> me to your party <strong>Y</strong> week.</p>
<p><span style="color:blue"><strong>Output:</strong></span> The model should predict the words(s) for <strong>X</strong> and <strong>Y</strong>.</p>
<p><strong>Z</strong> is used to represent the end.</p>
<p><a name='1.2'></a> ### 1.2 Process C4</p>
<p>C4 only has the plain string <code>text</code> field, so you will tokenize and have <code>inputs</code> and <code>targets</code> out of it for supervised learning. Given your inputs, the goal is to predict the targets during training.</p>
<p>You will now take the <code>text</code> and convert it to <code>inputs</code> and <code>targets</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Grab text field from dictionary</span></span><br><span class="line">natural_language_texts = [example_json[<span class="string">&#x27;text&#x27;</span>] <span class="keyword">for</span> example_json <span class="keyword">in</span> example_jsons]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># First text example</span></span><br><span class="line">natural_language_texts[<span class="number">4</span>]</span><br></pre></td></tr></table></figure>
<pre><code>b&#39;The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what\xe2\x80\x99s included in the mill levy measure.&#39;</code></pre>
<p><a name='1.2.1'></a> #### 1.2.1 Decode to natural language</p>
<p>The following functions will help you <code>detokenize</code> and<code>tokenize</code> the text data.</p>
<p>The <code>sentencepiece</code> vocabulary was used to convert from text to ids. This vocabulary file is loaded and used in this helper functions.</p>
<p><code>natural_language_texts</code> has the text from the examples we gave you.</p>
<p>Run the cells below to see what is going on.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Special tokens</span></span><br><span class="line">PAD, EOS, UNK = <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detokenize</span>(<span class="params">np_array</span>):</span></span><br><span class="line">    <span class="keyword">return</span> trax.data.detokenize(</span><br><span class="line">        np_array,</span><br><span class="line">        vocab_type=<span class="string">&#x27;sentencepiece&#x27;</span>,</span><br><span class="line">        vocab_file=<span class="string">&#x27;sentencepiece.model&#x27;</span>,</span><br><span class="line">        vocab_dir=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">s</span>):</span></span><br><span class="line">  <span class="comment"># The trax.data.tokenize function operates on streams,</span></span><br><span class="line">  <span class="comment"># that&#x27;s why we have to create 1-element stream with iter</span></span><br><span class="line">  <span class="comment"># and later retrieve the result with next.</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">next</span>(trax.data.tokenize(</span><br><span class="line">        <span class="built_in">iter</span>([s]),</span><br><span class="line">        vocab_type=<span class="string">&#x27;sentencepiece&#x27;</span>,</span><br><span class="line">        vocab_file=<span class="string">&#x27;sentencepiece.model&#x27;</span>,</span><br><span class="line">        vocab_dir=<span class="string">&#x27;.&#x27;</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># printing the encoding of each word to see how subwords are tokenized</span></span><br><span class="line">tokenized_text = [(tokenize(word).tolist(), word) <span class="keyword">for</span> word <span class="keyword">in</span> natural_language_texts[<span class="number">0</span>].split()]</span><br><span class="line"><span class="built_in">print</span>(tokenized_text, <span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[([12847, 277], b&#39;Beginners&#39;), ([15068], b&#39;BBQ&#39;), ([4501], b&#39;Class&#39;), ([3, 12297], b&#39;Taking&#39;), ([3399], b&#39;Place&#39;), ([16], b&#39;in&#39;), ([5964, 7115, 9, 55], b&#39;Missoula!&#39;), ([531], b&#39;Do&#39;), ([25], b&#39;you&#39;), ([241], b&#39;want&#39;), ([12], b&#39;to&#39;), ([129], b&#39;get&#39;), ([394], b&#39;better&#39;), ([44], b&#39;at&#39;), ([492], b&#39;making&#39;), ([3326], b&#39;delicious&#39;), ([15068, 58], b&#39;BBQ?&#39;), ([148], b&#39;You&#39;), ([56], b&#39;will&#39;), ([43], b&#39;have&#39;), ([8], b&#39;the&#39;), ([1004, 6], b&#39;opportunity,&#39;), ([474], b&#39;put&#39;), ([48], b&#39;this&#39;), ([30], b&#39;on&#39;), ([39], b&#39;your&#39;), ([4793], b&#39;calendar&#39;), ([230, 5], b&#39;now.&#39;), ([2721, 6], b&#39;Thursday,&#39;), ([1600], b&#39;September&#39;), ([1630, 727], b&#39;22nd&#39;), ([1715], b&#39;join&#39;), ([1150], b&#39;World&#39;), ([4501], b&#39;Class&#39;), ([15068], b&#39;BBQ&#39;), ([16127, 6], b&#39;Champion,&#39;), ([9137], b&#39;Tony&#39;), ([2659, 5595], b&#39;Balay&#39;), ([45], b&#39;from&#39;), ([301, 782, 3624], b&#39;Lonestar&#39;), ([14627, 15], b&#39;Smoke&#39;), ([12612, 277, 5], b&#39;Rangers.&#39;), ([216], b&#39;He&#39;), ([56], b&#39;will&#39;), ([36], b&#39;be&#39;), ([2119], b&#39;teaching&#39;), ([3, 9], b&#39;a&#39;), ([19529], b&#39;beginner&#39;), ([593], b&#39;level&#39;), ([853], b&#39;class&#39;), ([21], b&#39;for&#39;), ([921], b&#39;everyone&#39;), ([113], b&#39;who&#39;), ([2746], b&#39;wants&#39;), ([12], b&#39;to&#39;), ([129], b&#39;get&#39;), ([394], b&#39;better&#39;), ([28], b&#39;with&#39;), ([70], b&#39;their&#39;), ([17712], b&#39;culinary&#39;), ([1098, 5], b&#39;skills.&#39;), ([216], b&#39;He&#39;), ([56], b&#39;will&#39;), ([3884], b&#39;teach&#39;), ([25], b&#39;you&#39;), ([762], b&#39;everything&#39;), ([25], b&#39;you&#39;), ([174], b&#39;need&#39;), ([12], b&#39;to&#39;), ([214], b&#39;know&#39;), ([12], b&#39;to&#39;), ([5978], b&#39;compete&#39;), ([16], b&#39;in&#39;), ([3, 9], b&#39;a&#39;), ([3, 23405, 4547], b&#39;KCBS&#39;), ([15068], b&#39;BBQ&#39;), ([2259, 6], b&#39;competition,&#39;), ([379], b&#39;including&#39;), ([2097, 6], b&#39;techniques,&#39;), ([5459, 6], b&#39;recipes,&#39;), ([13618, 7, 6], b&#39;timelines,&#39;), ([3604], b&#39;meat&#39;), ([1801], b&#39;selection&#39;), ([11], b&#39;and&#39;), ([27856, 6], b&#39;trimming,&#39;), ([303], b&#39;plus&#39;), ([24190], b&#39;smoker&#39;), ([11], b&#39;and&#39;), ([1472], b&#39;fire&#39;), ([251, 5], b&#39;information.&#39;), ([37], b&#39;The&#39;), ([583], b&#39;cost&#39;), ([12], b&#39;to&#39;), ([36], b&#39;be&#39;), ([16], b&#39;in&#39;), ([8], b&#39;the&#39;), ([853], b&#39;class&#39;), ([19], b&#39;is&#39;), ([25264], b&#39;$35&#39;), ([399], b&#39;per&#39;), ([568, 6], b&#39;person,&#39;), ([11], b&#39;and&#39;), ([21], b&#39;for&#39;), ([21380, 7], b&#39;spectators&#39;), ([34], b&#39;it&#39;), ([19], b&#39;is&#39;), ([339, 5], b&#39;free.&#39;), ([15746, 26], b&#39;Included&#39;), ([16], b&#39;in&#39;), ([8], b&#39;the&#39;), ([583], b&#39;cost&#39;), ([56], b&#39;will&#39;), ([36], b&#39;be&#39;), ([893], b&#39;either&#39;), ([3, 9], b&#39;a&#39;), ([3, 17, 18, 9486], b&#39;t-shirt&#39;), ([42], b&#39;or&#39;), ([3, 9, 1409, 29], b&#39;apron&#39;), ([11], b&#39;and&#39;), ([25], b&#39;you&#39;), ([56], b&#39;will&#39;), ([36], b&#39;be&#39;), ([12246], b&#39;tasting&#39;), ([5977], b&#39;samples&#39;), ([13], b&#39;of&#39;), ([284], b&#39;each&#39;), ([3604], b&#39;meat&#39;), ([24], b&#39;that&#39;), ([19], b&#39;is&#39;), ([2657, 5], b&#39;prepared.&#39;)] </code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We can see that detokenize successfully undoes the tokenization</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;tokenized: <span class="subst">&#123;tokenize(<span class="string">&#x27;Beginners&#x27;</span>)&#125;</span>\ndetokenized: <span class="subst">&#123;detokenize(tokenize(<span class="string">&#x27;Beginners&#x27;</span>))&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>tokenized: [12847   277]
detokenized: Beginners</code></pre>
<p>As you can see above, you were able to take a piece of string and tokenize it.</p>
<p>Now you will create <code>input</code> and <code>target</code> pairs that will allow you to train your model. T5 uses the ids at the end of the vocab file as sentinels. For example, it will replace: - <code>vocab_size - 1</code> by <code>&lt;Z&gt;</code> - <code>vocab_size - 2</code> by <code>&lt;Y&gt;</code> - and so forth.</p>
<p>It assigns every word a <code>chr</code>.</p>
<p>The <code>pretty_decode</code> function below, which you will use in a bit, helps in handling the type when decoding. Take a look and try to understand what the function is doing.</p>
<p>Notice that: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">string.ascii_letters = <span class="string">&#x27;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ&#x27;</span></span><br></pre></td></tr></table></figure></p>
<p><strong>NOTE:</strong> Targets may have more than the 52 sentinels we replace, but this is just to give you an idea of things.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">vocab_size = trax.data.vocab_size(</span><br><span class="line">    vocab_type=<span class="string">&#x27;sentencepiece&#x27;</span>,</span><br><span class="line">    vocab_file=<span class="string">&#x27;sentencepiece.model&#x27;</span>,</span><br><span class="line">    vocab_dir=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sentinels</span>(<span class="params">vocab_size=vocab_size, display=<span class="literal">False</span></span>):</span></span><br><span class="line">    sentinels = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">reversed</span>(string.ascii_letters), <span class="number">1</span>):</span><br><span class="line">        decoded_text = detokenize([vocab_size - i]) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Sentinels, ex: &lt;Z&gt; - &lt;a&gt;</span></span><br><span class="line">        sentinels[decoded_text] = <span class="string">f&#x27;&lt;<span class="subst">&#123;char&#125;</span>&gt;&#x27;</span>    </span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> display:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;The sentinel is &lt;<span class="subst">&#123;char&#125;</span>&gt; and the decoded token is:&#x27;</span>, decoded_text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sentinels</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sentinels = get_sentinels(vocab_size, display=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>The sentinel is &lt;Z&gt; and the decoded token is: Interna»õional
The sentinel is &lt;Y&gt; and the decoded token is: erwachsene
The sentinel is &lt;X&gt; and the decoded token is: Cushion
The sentinel is &lt;W&gt; and the decoded token is: imunitar
The sentinel is &lt;V&gt; and the decoded token is: Intellectual
The sentinel is &lt;U&gt; and the decoded token is: traditi
The sentinel is &lt;T&gt; and the decoded token is: disguise
The sentinel is &lt;S&gt; and the decoded token is: exerce
The sentinel is &lt;R&gt; and the decoded token is: nourishe
The sentinel is &lt;Q&gt; and the decoded token is: predominant
The sentinel is &lt;P&gt; and the decoded token is: amiti√©
The sentinel is &lt;O&gt; and the decoded token is: erkennt
The sentinel is &lt;N&gt; and the decoded token is: dimension
The sentinel is &lt;M&gt; and the decoded token is: inf√©rieur
The sentinel is &lt;L&gt; and the decoded token is: refugi
The sentinel is &lt;K&gt; and the decoded token is: cheddar
The sentinel is &lt;J&gt; and the decoded token is: unterlieg
The sentinel is &lt;I&gt; and the decoded token is: garanteaz
The sentinel is &lt;H&gt; and the decoded token is: fƒÉcute
The sentinel is &lt;G&gt; and the decoded token is: r√©glage
The sentinel is &lt;F&gt; and the decoded token is: pedepse
The sentinel is &lt;E&gt; and the decoded token is: Germain
The sentinel is &lt;D&gt; and the decoded token is: distinctly
The sentinel is &lt;C&gt; and the decoded token is: Schraub
The sentinel is &lt;B&gt; and the decoded token is: emanat
The sentinel is &lt;A&gt; and the decoded token is: trimestre
The sentinel is &lt;z&gt; and the decoded token is: disrespect
The sentinel is &lt;y&gt; and the decoded token is: Erasmus
The sentinel is &lt;x&gt; and the decoded token is: Australia
The sentinel is &lt;w&gt; and the decoded token is: permeabil
The sentinel is &lt;v&gt; and the decoded token is: deseori
The sentinel is &lt;u&gt; and the decoded token is: manipulated
The sentinel is &lt;t&gt; and the decoded token is: sugg√©r
The sentinel is &lt;s&gt; and the decoded token is: corespund
The sentinel is &lt;r&gt; and the decoded token is: nitro
The sentinel is &lt;q&gt; and the decoded token is: oyons
The sentinel is &lt;p&gt; and the decoded token is: Account
The sentinel is &lt;o&gt; and the decoded token is: √©ch√©an
The sentinel is &lt;n&gt; and the decoded token is: laundering
The sentinel is &lt;m&gt; and the decoded token is: genealogy
The sentinel is &lt;l&gt; and the decoded token is: QuickBooks
The sentinel is &lt;k&gt; and the decoded token is: constituted
The sentinel is &lt;j&gt; and the decoded token is: Fertigung
The sentinel is &lt;i&gt; and the decoded token is: goutte
The sentinel is &lt;h&gt; and the decoded token is: regulƒÉ
The sentinel is &lt;g&gt; and the decoded token is: overwhelmingly
The sentinel is &lt;f&gt; and the decoded token is: √©merg
The sentinel is &lt;e&gt; and the decoded token is: broyeur
The sentinel is &lt;d&gt; and the decoded token is: pove»ôti
The sentinel is &lt;c&gt; and the decoded token is: emulator
The sentinel is &lt;b&gt; and the decoded token is: halloween
The sentinel is &lt;a&gt; and the decoded token is: combustibil</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretty_decode</span>(<span class="params">encoded_str_list, sentinels=sentinels</span>):</span></span><br><span class="line">    <span class="comment"># If already a string, just do the replacements.</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(encoded_str_list, (<span class="built_in">str</span>, <span class="built_in">bytes</span>)):</span><br><span class="line">        <span class="keyword">for</span> token, char <span class="keyword">in</span> sentinels.items():</span><br><span class="line">            encoded_str_list = encoded_str_list.replace(token, char)</span><br><span class="line">        <span class="keyword">return</span> encoded_str_list</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># We need to decode and then prettyfy it.</span></span><br><span class="line">    <span class="keyword">return</span> pretty_decode(detokenize(encoded_str_list))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pretty_decode(<span class="string">&quot;I want to dress up as an Intellectual this halloween.&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&#39;I want to dress up as an &lt;V&gt; this &lt;b&gt;.&#39;</code></pre>
<p>The functions above make your <code>inputs</code> and <code>targets</code> more readable. For example, you might see something like this once you implement the masking function below.</p>
<ul>
<li><span style="color:red"> Input sentence: </span> Younes and Lukasz were working together in the lab yesterday after lunch.</li>
<li><span style="color:red">Input: </span> Younes and Lukasz <strong>Z</strong> together in the <strong>Y</strong> yesterday after lunch.</li>
<li><span style="color:red">Target: </span> <strong>Z</strong> were working <strong>Y</strong> lab.</li>
</ul>
<p><a name='1.3'></a> ### 1.3 Tokenizing and Masking</p>
<p>You will now implement the <code>tokenize_and_mask</code> function. This function will allow you to tokenize and mask input words with a noise probability. We usually mask 15% of the words.</p>
<p><a name='ex01'></a> ### Exercise 01</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: tokenize_and_mask</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_and_mask</span>(<span class="params">text, vocab_size=vocab_size, noise=<span class="number">0.15</span>, </span></span></span><br><span class="line"><span class="params"><span class="function">                      randomizer=np.random.uniform, tokenize=tokenize</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Tokenizes and masks a given input.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        text (str or bytes): Text input.</span></span><br><span class="line"><span class="string">        vocab_size (int, optional): Size of the vocabulary. Defaults to vocab_size.</span></span><br><span class="line"><span class="string">        noise (float, optional): Probability of masking a token. Defaults to 0.15.</span></span><br><span class="line"><span class="string">        randomizer (function, optional): Function that generates random values. Defaults to np.random.uniform.</span></span><br><span class="line"><span class="string">        tokenize (function, optional): Tokenizer function. Defaults to tokenize.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        tuple: Tuple of lists of integers associated to inputs and targets.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># current sentinel number (starts at 0)</span></span><br><span class="line">    cur_sentinel_num = <span class="number">0</span></span><br><span class="line">    <span class="comment"># inputs</span></span><br><span class="line">    inps = []</span><br><span class="line">    <span class="comment"># targets</span></span><br><span class="line">    targs = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># prev_no_mask is True if the previous token was NOT masked, False otherwise</span></span><br><span class="line">    <span class="comment"># set prev_no_mask to True</span></span><br><span class="line">    prev_no_mask = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop through tokenized `text`</span></span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokenize(text):</span><br><span class="line">        <span class="comment"># check if the `noise` is greater than a random value (weighted coin flip)</span></span><br><span class="line">        <span class="keyword">if</span> randomizer() &lt; noise:</span><br><span class="line">            <span class="comment"># check to see if the previous token was not masked</span></span><br><span class="line">            <span class="keyword">if</span> prev_no_mask==<span class="literal">True</span>: <span class="comment"># add new masked token at end_id</span></span><br><span class="line">                <span class="comment"># number of masked tokens increases by 1</span></span><br><span class="line">                cur_sentinel_num += <span class="number">1</span></span><br><span class="line">                <span class="comment"># compute `end_id` by subtracting current sentinel value out of the total vocabulary size</span></span><br><span class="line">                end_id = vocab_size - cur_sentinel_num</span><br><span class="line">                <span class="comment"># append `end_id` at the end of the targets</span></span><br><span class="line">                targs.append(end_id)</span><br><span class="line">                <span class="comment"># append `end_id` at the end of the inputs</span></span><br><span class="line">                inps.append(end_id)</span><br><span class="line">            <span class="comment"># append `token` at the end of the targets</span></span><br><span class="line">            targs.append(token)</span><br><span class="line">            <span class="comment"># set prev_no_mask accordingly</span></span><br><span class="line">            prev_no_mask = <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># don&#x27;t have two masked tokens in a row</span></span><br><span class="line">            <span class="comment"># append `token ` at the end of the inputs</span></span><br><span class="line">            inps.append(token)</span><br><span class="line">            <span class="comment"># set prev_no_mask accordingly</span></span><br><span class="line">            prev_no_mask = <span class="literal">True</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> inps, targs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Some logic to mock a np.random value generator</span></span><br><span class="line"><span class="comment"># Needs to be in the same cell for it to always generate same output</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testing_rnd</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dummy_generator</span>():</span></span><br><span class="line">        vals = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">        cyclic_vals = itertools.cycle(vals)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">            <span class="keyword">yield</span> <span class="built_in">next</span>(cyclic_vals)</span><br><span class="line"></span><br><span class="line">    dumr = itertools.cycle(dummy_generator())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dummy_randomizer</span>():</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">next</span>(dumr)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dummy_randomizer</span><br><span class="line"></span><br><span class="line">input_str = natural_language_texts[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;input string:\n\n<span class="subst">&#123;input_str&#125;</span>\n&quot;</span>)</span><br><span class="line">inps, targs = tokenize_and_mask(input_str, randomizer=testing_rnd())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;tokenized inputs:\n\n<span class="subst">&#123;inps&#125;</span>\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;targets:\n\n<span class="subst">&#123;targs&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>input string:

b&#39;Beginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.&#39;

tokenized inputs:

[31999, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 31998, 531, 25, 241, 12, 129, 394, 44, 492, 31997, 58, 148, 56, 43, 8, 1004, 6, 474, 31996, 39, 4793, 230, 5, 2721, 6, 1600, 1630, 31995, 1150, 4501, 15068, 16127, 6, 9137, 2659, 5595, 31994, 782, 3624, 14627, 15, 12612, 277, 5, 216, 31993, 2119, 3, 9, 19529, 593, 853, 21, 921, 31992, 12, 129, 394, 28, 70, 17712, 1098, 5, 31991, 3884, 25, 762, 25, 174, 12, 214, 12, 31990, 3, 9, 3, 23405, 4547, 15068, 2259, 6, 31989, 6, 5459, 6, 13618, 7, 6, 3604, 1801, 31988, 6, 303, 24190, 11, 1472, 251, 5, 37, 31987, 36, 16, 8, 853, 19, 25264, 399, 568, 31986, 21, 21380, 7, 34, 19, 339, 5, 15746, 31985, 8, 583, 56, 36, 893, 3, 9, 3, 31984, 9486, 42, 3, 9, 1409, 29, 11, 25, 31983, 12246, 5977, 13, 284, 3604, 24, 19, 2657, 31982]

targets:

[31999, 12847, 277, 31998, 9, 55, 31997, 3326, 15068, 31996, 48, 30, 31995, 727, 1715, 31994, 45, 301, 31993, 56, 36, 31992, 113, 2746, 31991, 216, 56, 31990, 5978, 16, 31989, 379, 2097, 31988, 11, 27856, 31987, 583, 12, 31986, 6, 11, 31985, 26, 16, 31984, 17, 18, 31983, 56, 36, 31982, 5]</code></pre>
<h4 id="expected-output"><strong>Expected Output:</strong></h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">b<span class="number">&#x27;B</span>eginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You will have the opportunity, put <span class="keyword">this</span> on your calendar now. Thursday, September <span class="number">22</span>nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level <span class="class"><span class="keyword">class</span> <span class="title">for</span> <span class="title">everyone</span> <span class="title">who</span> <span class="title">wants</span> <span class="title">to</span> <span class="title">get</span> <span class="title">better</span> <span class="title">with</span> <span class="title">their</span> <span class="title">culinary</span> <span class="title">skills</span>.\<span class="title">nHe</span> <span class="title">will</span> <span class="title">teach</span> <span class="title">you</span> <span class="title">everything</span> <span class="title">you</span> <span class="title">need</span> <span class="title">to</span> <span class="title">know</span> <span class="title">to</span> <span class="title">compete</span> <span class="title">in</span> <span class="title">a</span> <span class="title">KCBS</span> <span class="title">BBQ</span> <span class="title">competition</span>, <span class="title">including</span> <span class="title">techniques</span>, <span class="title">recipes</span>, <span class="title">timelines</span>, <span class="title">meat</span> <span class="title">selection</span> <span class="title">and</span> <span class="title">trimming</span>, <span class="title">plus</span> <span class="title">smoker</span> <span class="title">and</span> <span class="title">fire</span> <span class="title">information</span>.\<span class="title">nThe</span> <span class="title">cost</span> <span class="title">to</span> <span class="title">be</span> <span class="title">in</span> <span class="title">the</span> <span class="keyword">class</span> <span class="title">is</span> $35 <span class="title">per</span> <span class="title">person</span>, <span class="title">and</span> <span class="title">for</span> <span class="title">spectators</span> <span class="title">it</span> <span class="title">is</span> <span class="title">free</span>. <span class="title">Included</span> <span class="title">in</span> <span class="title">the</span> <span class="title">cost</span> <span class="title">will</span> <span class="title">be</span> <span class="title">either</span> <span class="title">a</span> <span class="title">t</span>-<span class="title">shirt</span> <span class="title">or</span> <span class="title">apron</span> <span class="title">and</span> <span class="title">you</span> <span class="title">will</span> <span class="title">be</span> <span class="title">tasting</span> <span class="title">samples</span> <span class="title">of</span> <span class="title">each</span> <span class="title">meat</span> <span class="title">that</span> <span class="title">is</span> <span class="title">prepared</span>.&#x27;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">tokenized</span> <span class="title">inputs</span>:</span></span><br><span class="line"></span><br><span class="line">[<span class="number">31999</span>, <span class="number">15068</span>, <span class="number">4501</span>, <span class="number">3</span>, <span class="number">12297</span>, <span class="number">3399</span>, <span class="number">16</span>, <span class="number">5964</span>, <span class="number">7115</span>, <span class="number">31998</span>, <span class="number">531</span>, <span class="number">25</span>, <span class="number">241</span>, <span class="number">12</span>, <span class="number">129</span>, <span class="number">394</span>, <span class="number">44</span>, <span class="number">492</span>, <span class="number">31997</span>, <span class="number">58</span>, <span class="number">148</span>, <span class="number">56</span>, <span class="number">43</span>, <span class="number">8</span>, <span class="number">1004</span>, <span class="number">6</span>, <span class="number">474</span>, <span class="number">31996</span>, <span class="number">39</span>, <span class="number">4793</span>, <span class="number">230</span>, <span class="number">5</span>, <span class="number">2721</span>, <span class="number">6</span>, <span class="number">1600</span>, <span class="number">1630</span>, <span class="number">31995</span>, <span class="number">1150</span>, <span class="number">4501</span>, <span class="number">15068</span>, <span class="number">16127</span>, <span class="number">6</span>, <span class="number">9137</span>, <span class="number">2659</span>, <span class="number">5595</span>, <span class="number">31994</span>, <span class="number">782</span>, <span class="number">3624</span>, <span class="number">14627</span>, <span class="number">15</span>, <span class="number">12612</span>, <span class="number">277</span>, <span class="number">5</span>, <span class="number">216</span>, <span class="number">31993</span>, <span class="number">2119</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">19529</span>, <span class="number">593</span>, <span class="number">853</span>, <span class="number">21</span>, <span class="number">921</span>, <span class="number">31992</span>, <span class="number">12</span>, <span class="number">129</span>, <span class="number">394</span>, <span class="number">28</span>, <span class="number">70</span>, <span class="number">17712</span>, <span class="number">1098</span>, <span class="number">5</span>, <span class="number">31991</span>, <span class="number">3884</span>, <span class="number">25</span>, <span class="number">762</span>, <span class="number">25</span>, <span class="number">174</span>, <span class="number">12</span>, <span class="number">214</span>, <span class="number">12</span>, <span class="number">31990</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">23405</span>, <span class="number">4547</span>, <span class="number">15068</span>, <span class="number">2259</span>, <span class="number">6</span>, <span class="number">31989</span>, <span class="number">6</span>, <span class="number">5459</span>, <span class="number">6</span>, <span class="number">13618</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">3604</span>, <span class="number">1801</span>, <span class="number">31988</span>, <span class="number">6</span>, <span class="number">303</span>, <span class="number">24190</span>, <span class="number">11</span>, <span class="number">1472</span>, <span class="number">251</span>, <span class="number">5</span>, <span class="number">37</span>, <span class="number">31987</span>, <span class="number">36</span>, <span class="number">16</span>, <span class="number">8</span>, <span class="number">853</span>, <span class="number">19</span>, <span class="number">25264</span>, <span class="number">399</span>, <span class="number">568</span>, <span class="number">31986</span>, <span class="number">21</span>, <span class="number">21380</span>, <span class="number">7</span>, <span class="number">34</span>, <span class="number">19</span>, <span class="number">339</span>, <span class="number">5</span>, <span class="number">15746</span>, <span class="number">31985</span>, <span class="number">8</span>, <span class="number">583</span>, <span class="number">56</span>, <span class="number">36</span>, <span class="number">893</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">31984</span>, <span class="number">9486</span>, <span class="number">42</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">1409</span>, <span class="number">29</span>, <span class="number">11</span>, <span class="number">25</span>, <span class="number">31983</span>, <span class="number">12246</span>, <span class="number">5977</span>, <span class="number">13</span>, <span class="number">284</span>, <span class="number">3604</span>, <span class="number">24</span>, <span class="number">19</span>, <span class="number">2657</span>, <span class="number">31982</span>]</span><br><span class="line"></span><br><span class="line">targets:</span><br><span class="line"></span><br><span class="line">[<span class="number">31999</span>, <span class="number">12847</span>, <span class="number">277</span>, <span class="number">31998</span>, <span class="number">9</span>, <span class="number">55</span>, <span class="number">31997</span>, <span class="number">3326</span>, <span class="number">15068</span>, <span class="number">31996</span>, <span class="number">48</span>, <span class="number">30</span>, <span class="number">31995</span>, <span class="number">727</span>, <span class="number">1715</span>, <span class="number">31994</span>, <span class="number">45</span>, <span class="number">301</span>, <span class="number">31993</span>, <span class="number">56</span>, <span class="number">36</span>, <span class="number">31992</span>, <span class="number">113</span>, <span class="number">2746</span>, <span class="number">31991</span>, <span class="number">216</span>, <span class="number">56</span>, <span class="number">31990</span>, <span class="number">5978</span>, <span class="number">16</span>, <span class="number">31989</span>, <span class="number">379</span>, <span class="number">2097</span>, <span class="number">31988</span>, <span class="number">11</span>, <span class="number">27856</span>, <span class="number">31987</span>, <span class="number">583</span>, <span class="number">12</span>, <span class="number">31986</span>, <span class="number">6</span>, <span class="number">11</span>, <span class="number">31985</span>, <span class="number">26</span>, <span class="number">16</span>, <span class="number">31984</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">31983</span>, <span class="number">56</span>, <span class="number">36</span>, <span class="number">31982</span>, <span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<p>You will now use the inputs and the targets from the <code>tokenize_and_mask</code> function you implemented above. Take a look at the masked sentence using your <code>inps</code> and <code>targs</code> from the sentence above.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Inputs: \n\n&#x27;</span>, pretty_decode(inps))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nTargets: \n\n&#x27;</span>, pretty_decode(targs))</span><br></pre></td></tr></table></figure>
<pre><code>Inputs: 

 &lt;Z&gt; BBQ Class Taking Place in Missoul &lt;Y&gt; Do you want to get better at making &lt;X&gt;? You will have the opportunity, put &lt;W&gt; your calendar now. Thursday, September 22 &lt;V&gt; World Class BBQ Champion, Tony Balay &lt;U&gt;onestar Smoke Rangers. He &lt;T&gt; teaching a beginner level class for everyone&lt;S&gt; to get better with their culinary skills.&lt;R&gt; teach you everything you need to know to &lt;Q&gt; a KCBS BBQ competition,&lt;P&gt;, recipes, timelines, meat selection &lt;O&gt;, plus smoker and fire information. The&lt;N&gt; be in the class is $35 per person &lt;M&gt; for spectators it is free. Include &lt;L&gt; the cost will be either a  &lt;K&gt;shirt or apron and you &lt;J&gt; tasting samples of each meat that is prepared &lt;I&gt;

Targets: 

 &lt;Z&gt; Beginners &lt;Y&gt;a! &lt;X&gt; delicious BBQ &lt;W&gt; this on &lt;V&gt;nd join &lt;U&gt; from L &lt;T&gt; will be&lt;S&gt; who wants&lt;R&gt; He will &lt;Q&gt; compete in&lt;P&gt; including techniques &lt;O&gt; and trimming&lt;N&gt; cost to &lt;M&gt;, and &lt;L&gt;d in &lt;K&gt;t- &lt;J&gt; will be &lt;I&gt;.</code></pre>
<p><a name='1.4'></a> ### 1.4 Creating the Pairs</p>
<p>You will now create pairs using your dataset. You will iterate over your data and create (inp, targ) pairs using the functions that we have given you.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply tokenize_and_mask</span></span><br><span class="line">inputs_targets_pairs = [tokenize_and_mask(text) <span class="keyword">for</span> text <span class="keyword">in</span> natural_language_texts]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_input_target_pairs</span>(<span class="params">inputs_targets_pairs</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i, inp_tgt_pair <span class="keyword">in</span> <span class="built_in">enumerate</span>(inputs_targets_pairs, <span class="number">1</span>):</span><br><span class="line">        inps, tgts = inp_tgt_pair</span><br><span class="line">        inps, tgts = pretty_decode(inps), pretty_decode(tgts)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;[<span class="subst">&#123;i&#125;</span>]\n\n&#x27;</span></span><br><span class="line">              <span class="string">f&#x27;inputs:\n<span class="subst">&#123;wrapper.fill(text=inps)&#125;</span>\n\n&#x27;</span></span><br><span class="line">              <span class="string">f&#x27;targets:\n<span class="subst">&#123;wrapper.fill(text=tgts)&#125;</span>\n\n\n\n&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_input_target_pairs(inputs_targets_pairs)</span><br></pre></td></tr></table></figure>
<pre><code>[1]

inputs:
Beginners BBQ Class Taking &lt;Z&gt; in Missoul &lt;Y&gt;! Do you want to get
better at making delicious &lt;X&gt;? You will have the opportunity, &lt;W&gt;
this on &lt;V&gt; calendar now. Thursday &lt;U&gt; September 22 &lt;T&gt; join&lt;S&gt; Class
BBQ Champion, Tony Balay from Lonestar Smoke&lt;R&gt;ers &lt;Q&gt; He will be
teaching a beginner&lt;P&gt; class &lt;O&gt; everyone who wants&lt;N&gt; get better with
their &lt;M&gt; skills &lt;L&gt; He will teach &lt;K&gt; everything you need to know to
&lt;J&gt; in a KCBS BBQ &lt;I&gt; techniques, recipes, timelines, meat&lt;H&gt; and
trimming, plus smoker and fire information. The cost to be&lt;G&gt; the
class is $35 &lt;F&gt; person, and&lt;E&gt; spectators it is free. Included in the
cost will&lt;D&gt; either &lt;C&gt; t- &lt;B&gt; or apron and you will be tasting
samples &lt;A&gt; each meat that &lt;z&gt; prepared.

targets:
&lt;Z&gt; Place &lt;Y&gt;a &lt;X&gt; BBQ &lt;W&gt; put &lt;V&gt; your &lt;U&gt;, &lt;T&gt;nd&lt;S&gt; World&lt;R&gt; Rang
&lt;Q&gt;.&lt;P&gt; level &lt;O&gt; for&lt;N&gt; to &lt;M&gt; culinary &lt;L&gt;. &lt;K&gt; you &lt;J&gt; compete &lt;I&gt;
competition, including&lt;H&gt; selection&lt;G&gt; in &lt;F&gt; per&lt;E&gt; for&lt;D&gt; be&lt;C&gt;a
&lt;B&gt;shirt &lt;A&gt; of &lt;z&gt; is




[2]

inputs:
&lt;Z&gt; in &#39;Mac OS X &lt;Y&gt; (10 &lt;X&gt;7)&#39; started by axb &lt;W&gt;i87, Jan 20, 2012.
I&#39;ve got &lt;V&gt;a 500g &lt;U&gt; drive &lt;T&gt; a 240gb SSD. When trying to restore
using&lt;S&gt; utility i&#39;m given the error &quot;Not enough space on disk&lt;R&gt;____
to restore &lt;Q&gt; But I shouldn&#39;t have to do that!!! Any ideas or
work&lt;P&gt;s before &lt;O&gt;ing to the above? Use Carbon Copy Cloner to copy
one drive to the other. I&#39;&lt;N&gt; done &lt;M&gt; several times going from &lt;L&gt;D
to &lt;K&gt; SSD and I wound &lt;J&gt; a bootable SSD drive. One step you &lt;I&gt;
remember not to skip is to use Disk Utility to partition the SSD as
GUID partition scheme&lt;H&gt; doing the &lt;G&gt;ne. If it came Apple &lt;F&gt;ition
Scheme, even if&lt;E&gt; let&lt;D&gt;CC do the clone, the resulting drive&lt;C&gt; boot
&lt;B&gt;. C &lt;A&gt; usually works &lt;z&gt; &quot;file mode&quot; and it can easily copy a
larger drive (that&#39;s mostly empty &lt;y&gt; onto a smaller drive.&lt;x&gt; you&lt;w&gt;
CCC to clone a drive you did&lt;v&gt; boot&lt;u&gt;, it can work &lt;t&gt; copy mode &lt;s&gt;
destination&lt;r&gt; must be&lt;q&gt; size or larger than the drive you
are&lt;p&gt;cloning from &lt;o&gt;if &lt;n&gt; recall &lt;m&gt;ve actually done this somehow
on Disk Utility &lt;l&gt; times&lt;k&gt;booting from &lt;j&gt;a different drive (or even
the dvd)&lt;i&gt; not running disk utility from the drive your clo&lt;h&gt;ing)
and had it work just fine from larger to smaller bootable clo&lt;g&gt;.
Definitely format the drive cloning to first &lt;f&gt; as bootable Apple
etc.. Thanks for &lt;e&gt; this out. My only experience &lt;d&gt; DU to go larger
to smaller was when &lt;c&gt; trying to make  &lt;b&gt; install stick and I was
unable to restore InstallESD &lt;a&gt;dmg to a 4 GB Th√©√¢tre ofKeep the
reason that wouldn&#39;t fit isd√ºrftig was slightly moreutti GB of data.

targets:
&lt;Z&gt; Discussion &lt;Y&gt; Lion &lt;X&gt;. &lt;W&gt;o &lt;V&gt;  &lt;U&gt;b internal &lt;T&gt; and&lt;S&gt;
disk&lt;R&gt;  &lt;Q&gt;&quot;&lt;P&gt;around &lt;O&gt; resort&lt;N&gt;ve &lt;M&gt; this &lt;L&gt; larger HD &lt;K&gt;
smaller &lt;J&gt; up with &lt;I&gt; have to&lt;H&gt; HFS+ before&lt;G&gt;clo &lt;F&gt; Part&lt;E&gt;
you&lt;D&gt; C&lt;C&gt; won&#39;t be &lt;B&gt;able &lt;A&gt;CC &lt;z&gt; in &lt;y&gt;)&lt;x&gt; If&lt;w&gt; tell&lt;v&gt; NOT&lt;u&gt;
from &lt;t&gt; in block &lt;s&gt; where the&lt;r&gt; drive&lt;q&gt; the same&lt;p&gt;  &lt;o&gt; ( &lt;n&gt; I
&lt;m&gt;). I&#39; &lt;l&gt; several&lt;k&gt; ( &lt;j&gt; &lt;i&gt; so&lt;h&gt;n&lt;g&gt;ne &lt;f&gt;,&lt;e&gt;pointing &lt;d&gt;
using &lt;c&gt; I was &lt;b&gt;a Lion &lt;a&gt;. Th√©√¢tre USB stick butKeep coursed√ºrftig
thereutti than 4




[3]

inputs:
&lt;Z&gt;il plaid &lt;Y&gt;lycra &lt;X&gt; spandex shortall with metallic slinky
&lt;W&gt;sets. Attache &lt;V&gt; metallic elastic belt with O &lt;U&gt;ring. Head &lt;T&gt;
included. Great hip hop&lt;S&gt; jazz dance costume.&lt;R&gt; in the USA.

targets:
&lt;Z&gt; Fo &lt;Y&gt;  &lt;X&gt; and &lt;W&gt; in &lt;V&gt;d &lt;U&gt;- &lt;T&gt;band&lt;S&gt; or&lt;R&gt; Made




[4]

inputs:
How many backlink &lt;Z&gt; per day for new site? Discussion &lt;Y&gt; &#39;Black &lt;X&gt;
SEO&#39; started by Omoplata, Dec 3, 2010. 1) for a &lt;W&gt; created site,
what&#39;s &lt;V&gt; max &lt;U&gt;links per day I should do to be safe? 2) how &lt;T&gt; do
I have&lt;S&gt; let my site&lt;R&gt; before I can start making more blinks? I did
about 6000 forum profiles every 24 hours for 10 days for &lt;Q&gt; of my
sites&lt;P&gt; had a brand new domain. There is &lt;O&gt; backlinks for every&lt;N&gt;
these &lt;M&gt; profile so &lt;L&gt;s 18 000 backlinks every 24 hours and nothing
happened in terms of being penalized &lt;K&gt; sandboxed. This is now maybe
3 months ago &lt;J&gt; the site &lt;I&gt; ranking on first page for&lt;H&gt;a lot&lt;G&gt; my
targeted keywords. build more you can in starting &lt;F&gt; do manual
submission and not spammy&lt;E&gt; means manual +&lt;D&gt; to&lt;C&gt; post.. &lt;B&gt; after
1 month you can &lt;A&gt; a &lt;z&gt; blast.. Wow, dude, you built 18k backlink
&lt;y&gt; a day&lt;x&gt; a brand&lt;w&gt;? How quickly did&lt;v&gt; rank up? What kind of
competition/search&lt;u&gt; did &lt;t&gt; keywords have?

targets:
&lt;Z&gt;s &lt;Y&gt; in &lt;X&gt; Hat &lt;W&gt; newly &lt;V&gt; the &lt;U&gt; # back &lt;T&gt; long&lt;S&gt; to&lt;R&gt; age
&lt;Q&gt; one&lt;P&gt; which &lt;O&gt; three&lt;N&gt; of &lt;M&gt; forum &lt;L&gt; that &lt;K&gt; or &lt;J&gt; and &lt;I&gt;
is&lt;H&gt; &lt;G&gt; of &lt;F&gt; but&lt;E&gt; type&lt;D&gt; relevant&lt;C&gt; the &lt;B&gt; then &lt;A&gt; make &lt;z&gt;
big &lt;y&gt;s&lt;x&gt; on&lt;w&gt; new site&lt;v&gt; you&lt;u&gt;es &lt;t&gt; those




[5]

inputs:
The Denver Board of Education opened the 2017-18 school year with an
update &lt;Z&gt; projects that include new construction &lt;Y&gt; upgrades, heat
mitigation &lt;X&gt; quality learning environments. We &lt;W&gt; excited &lt;V&gt;
Denver students will be the beneficiaries &lt;U&gt;a four year, $572 million
General Oblig &lt;T&gt; Bond.&lt;S&gt; the passage of the bond, our construction
team has worked to schedule&lt;R&gt; projects over &lt;Q&gt; four-year term&lt;P&gt;
bond. Denver voters on Tuesday approved bond and mill funding &lt;O&gt;
for&lt;N&gt; in Denver Public Schools, agreeing to invest $572 million in
bond funding &lt;M&gt; build and improve schools and &lt;L&gt;6.6 million in
operating dollars to support proven initiatives, &lt;K&gt; as early &lt;J&gt;
Denver voters say &lt;I&gt; to bond and mill levy funding&lt;H&gt; for&lt;G&gt;PS
students and schools. Click to learn more about the details of the
voter-approved &lt;F&gt; measure. Denver voters&lt;E&gt;. 8 approved bond and mill
funding&lt;D&gt; for DPS students and schools. Learn more about what‚Äôs
included in the mill &lt;C&gt;y measure.

targets:
&lt;Z&gt; on &lt;Y&gt;, &lt;X&gt; and &lt;W&gt; are &lt;V&gt; that &lt;U&gt; of  &lt;T&gt;ation&lt;S&gt; Since&lt;R&gt; the
&lt;Q&gt; the&lt;P&gt; of the &lt;O&gt; measures&lt;N&gt; students &lt;M&gt; to &lt;L&gt; $5 &lt;K&gt; such &lt;J&gt;
literacy. &lt;I&gt; yes&lt;H&gt; support&lt;G&gt; D &lt;F&gt; bond&lt;E&gt; on Nov&lt;D&gt; measures&lt;C&gt;lev</code></pre>
<p><a name='2'></a> # Part 2: Transfomer</p>
<p>We now load a Transformer model checkpoint that has been pre-trained using the above C4 dataset and decode from it. This will save you a lot of time rather than have to train your model yourself. Later in this notebook, we will show you how to fine-tune your model.</p>
<p><img src = "fulltransformer.png" width="300" height="600"></p>
<p>Start by loading in the model. We copy the checkpoint to local dir for speed, otherwise initialization takes a very long time. Last week you implemented the decoder part for the transformer. Now you will implement the encoder part. Concretely you will implement the following.</p>
<p><img src = "encoder.png" width="300" height="600"></p>
<p><a name='2.1'></a> ### 2.1 Transformer Encoder</p>
<p>You will now implement the transformer encoder. Concretely you will implement two functions. The first function is <code>FeedForwardBlock</code>.</p>
<p><a name='2.1.1'></a> #### 2.1.1 The Feedforward Block</p>
<p>The <code>FeedForwardBlock</code> function is an important one so you will start by implementing it. To do so, you need to return a list of the following:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm"><code>tl.LayerNorm()</code></a> = layer normalization.</li>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense"><code>tl.Dense(d_ff)</code></a> = fully connected layer.</li>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu"><code>activation</code></a> = activation relu, tanh, sigmoid etc.</li>
<li><code>dropout_middle</code> = we gave you this function (don't worry about its implementation).</li>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense"><code>tl.Dense(d_model)</code></a> = fully connected layer with same dimension as the model.</li>
<li><code>dropout_final</code> = we gave you this function (don't worry about its implementation).</li>
</ul>
<p>You can always take a look at <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/">trax documentation</a> if needed.</p>
<p><strong>Instructions</strong>: Implement the feedforward part of the transformer. You will be returning a list.</p>
<p><a name='ex02'></a> ### Exercise 02</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: FeedForwardBlock</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">FeedForwardBlock</span>(<span class="params">d_model, d_ff, dropout, dropout_shared_axes, mode, activation</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns a list of layers implementing a feed-forward block.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: int:  depth of embedding</span></span><br><span class="line"><span class="string">        d_ff: int: depth of feed-forward layer</span></span><br><span class="line"><span class="string">        dropout: float: dropout rate (how much to drop out)</span></span><br><span class="line"><span class="string">        dropout_shared_axes: list of integers, axes to share dropout mask</span></span><br><span class="line"><span class="string">        mode: str: &#x27;train&#x27; or &#x27;eval&#x27;</span></span><br><span class="line"><span class="string">        activation: the non-linearity in feed-forward layer</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A list of layers which maps vectors to vectors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    dropout_middle = tl.Dropout(rate=dropout,</span><br><span class="line">                                shared_axes=dropout_shared_axes, </span><br><span class="line">                                mode=mode)</span><br><span class="line">  </span><br><span class="line">    dropout_final = tl.Dropout(rate=dropout, </span><br><span class="line">                               shared_axes=dropout_shared_axes, </span><br><span class="line">                               mode=mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    ff_block = [ </span><br><span class="line">        <span class="comment"># trax Layer normalization </span></span><br><span class="line">        tl.LayerNorm(),</span><br><span class="line">        <span class="comment"># trax Dense layer using `d_ff`</span></span><br><span class="line">        tl.Dense(d_ff),</span><br><span class="line">        <span class="comment"># activation() layer - you need to call (use parentheses) this func!</span></span><br><span class="line">        activation(),</span><br><span class="line">        <span class="comment"># dropout middle layer</span></span><br><span class="line">        dropout_middle,</span><br><span class="line">        <span class="comment"># trax Dense layer using `d_model`</span></span><br><span class="line">        tl.Dense(d_model),</span><br><span class="line">        <span class="comment"># dropout final layer</span></span><br><span class="line">        dropout_final,</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ff_block</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Print the block layout</span></span><br><span class="line">feed_forward_example = FeedForwardBlock(d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.8</span>, dropout_shared_axes=<span class="number">0</span>, mode = <span class="string">&#x27;train&#x27;</span>, activation = tl.Relu)</span><br><span class="line"><span class="built_in">print</span>(feed_forward_example)</span><br></pre></td></tr></table></figure>
<pre><code>[LayerNorm, Dense_2048, Relu, Dropout, Dense_512, Dropout]</code></pre>
<h4 id="expected-output-1"><strong>Expected Output:</strong></h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[LayerNorm, Dense_2048, Relu, Dropout, Dense_512, Dropout]</span><br></pre></td></tr></table></figure>
<p><a name='2.1.2'></a> #### 2.1.2 The Encoder Block</p>
<p>The encoder block will use the <code>FeedForwardBlock</code>.</p>
<p>You will have to build two residual connections. Inside the first residual connection you will have the <code>tl.layerNorm()</code>, <code>attention</code>, and <code>dropout_</code> layers. The second residual connection will have the <code>feed_forward</code>.</p>
<p>You will also need to implement <code>feed_forward</code>, <code>attention</code> and <code>dropout_</code> blocks.</p>
<p>So far you haven't seen the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.Attention"><code>tl.Attention()</code></a> and <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual"><code>tl.Residual()</code></a> layers so you can check the docs by clicking on them.</p>
<p><a name='ex03'></a> ### Exercise 03</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: EncoderBlock</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">EncoderBlock</span>(<span class="params">d_model, d_ff, n_heads, dropout, dropout_shared_axes,</span></span></span><br><span class="line"><span class="params"><span class="function">                  mode, ff_activation, FeedForwardBlock=FeedForwardBlock</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Returns a list of layers that implements a Transformer encoder block.</span></span><br><span class="line"><span class="string">    The input to the layer is a pair, (activations, mask), where the mask was</span></span><br><span class="line"><span class="string">    created from the original source tokens to prevent attending to the padding</span></span><br><span class="line"><span class="string">    part of the input.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model (int): depth of embedding.</span></span><br><span class="line"><span class="string">        d_ff (int): depth of feed-forward layer.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate (how much to drop out).</span></span><br><span class="line"><span class="string">        dropout_shared_axes (int): axes on which to share dropout mask.</span></span><br><span class="line"><span class="string">        mode (str): &#x27;train&#x27; or &#x27;eval&#x27;.</span></span><br><span class="line"><span class="string">        ff_activation (function): the non-linearity in feed-forward layer.</span></span><br><span class="line"><span class="string">        FeedForwardBlock (function): A function that returns the feed forward block.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        list: A list of layers that maps (activations, mask) to (activations, mask).</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Attention block</span></span><br><span class="line">    attention = tl.Attention( </span><br><span class="line">        <span class="comment"># Use dimension of the model</span></span><br><span class="line">        d_feature=d_model,</span><br><span class="line">        <span class="comment"># Set it equal to number of attention heads</span></span><br><span class="line">        n_heads=n_heads,</span><br><span class="line">        <span class="comment"># Set it equal `dropout`</span></span><br><span class="line">        dropout=dropout,</span><br><span class="line">        <span class="comment"># Set it equal `mode`</span></span><br><span class="line">        mode=mode</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Call the function `FeedForwardBlock` (implemented before) and pass in the parameters</span></span><br><span class="line">    feed_forward = FeedForwardBlock(d_model, d_ff, dropout, dropout_shared_axes, mode, ff_activation)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Dropout block</span></span><br><span class="line">    dropout_ = tl.Dropout( </span><br><span class="line">        <span class="comment"># set it equal to `dropout`</span></span><br><span class="line">        rate=dropout,</span><br><span class="line">        <span class="comment"># set it equal to the axes on which to share dropout mask</span></span><br><span class="line">        shared_axes=dropout_shared_axes,</span><br><span class="line">        <span class="comment"># set it equal to `mode`</span></span><br><span class="line">        mode=mode</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    encoder_block = [ </span><br><span class="line">        <span class="comment"># add `Residual` layer</span></span><br><span class="line">        tl.Residual(</span><br><span class="line">            <span class="comment"># add norm layer</span></span><br><span class="line">            tl.LayerNorm(),</span><br><span class="line">            <span class="comment"># add attention</span></span><br><span class="line">            attention,</span><br><span class="line">            <span class="comment"># add dropout</span></span><br><span class="line">            dropout_,</span><br><span class="line">        ),</span><br><span class="line">        <span class="comment"># add another `Residual` layer</span></span><br><span class="line">        tl.Residual(</span><br><span class="line">            <span class="comment"># add feed forward</span></span><br><span class="line">            feed_forward,</span><br><span class="line">        ),</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> encoder_block</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Print the block layout</span></span><br><span class="line">encoder_example = EncoderBlock(d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, n_heads=<span class="number">6</span>, dropout=<span class="number">0.8</span>, dropout_shared_axes=<span class="number">0</span>, mode = <span class="string">&#x27;train&#x27;</span>, ff_activation=tl.Relu)</span><br><span class="line"><span class="built_in">print</span>(encoder_example)</span><br></pre></td></tr></table></figure>
<pre><code>[Serial_in2_out2[
  Branch_in2_out3[
    None
    Serial_in2_out2[
      LayerNorm
      Serial_in2_out2[
        Dup_out2
        Dup_out2
        Serial_in4_out2[
          Parallel_in3_out3[
            Dense_512
            Dense_512
            Dense_512
          ]
          PureAttention_in4_out2
          Dense_512
        ]
      ]
      Dropout
    ]
  ]
  Add_in2
], Serial[
  Branch_out2[
    None
    Serial[
      LayerNorm
      Dense_2048
      Relu
      Dropout
      Dense_512
      Dropout
    ]
  ]
  Add_in2
]]</code></pre>
<h4 id="expected-output-2"><strong>Expected Output:</strong></h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[Serial_in2_out2[</span><br><span class="line">  Branch_in2_out3[</span><br><span class="line">    None</span><br><span class="line">    Serial_in2_out2[</span><br><span class="line">      LayerNorm</span><br><span class="line">      Serial_in2_out2[</span><br><span class="line">        Dup_out2</span><br><span class="line">        Dup_out2</span><br><span class="line">        Serial_in4_out2[</span><br><span class="line">          Parallel_in3_out3[</span><br><span class="line">            Dense_512</span><br><span class="line">            Dense_512</span><br><span class="line">            Dense_512</span><br><span class="line">          ]</span><br><span class="line">          PureAttention_in4_out2</span><br><span class="line">          Dense_512</span><br><span class="line">        ]</span><br><span class="line">      ]</span><br><span class="line">      Dropout</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">  Add_in2</span><br><span class="line">], Serial[</span><br><span class="line">  Branch_out2[</span><br><span class="line">    None</span><br><span class="line">    Serial[</span><br><span class="line">      LayerNorm</span><br><span class="line">      Dense_2048</span><br><span class="line">      Relu</span><br><span class="line">      Dropout</span><br><span class="line">      Dense_512</span><br><span class="line">      Dropout</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">  Add_in2</span><br><span class="line">]]</span><br></pre></td></tr></table></figure>
<p><a name='2.1.3'></a> ### 2.1.3 The Transformer Encoder</p>
<p>Now that you have implemented the <code>EncoderBlock</code>, it is time to build the full encoder. BERT, or Bidirectional Encoder Representations from Transformers is one such encoder.</p>
<p>You will implement its core code in the function below by using the functions you have coded so far.</p>
<p>The model takes in many hyperparameters, such as the <code>vocab_size</code>, the number of classes, the dimension of your model, etc. You want to build a generic function that will take in many parameters, so you can use it later. At the end of the day, anyone can just load in an API and call transformer, but we think it is important to make sure you understand how it is built. Let's get started.</p>
<p><strong>Instructions:</strong> For this encoder you will need a <code>positional_encoder</code> first (which is already provided) followed by <code>n_layers</code> encoder blocks, which are the same encoder blocks you previously built. Once you store the <code>n_layers</code> <code>EncoderBlock</code> in a list, you are going to encode a <code>Serial</code> layer with the following sublayers:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch"><code>tl.Branch</code></a>: helps with the branching and has the following sublayers:
<ul>
<li><code>positional_encoder</code>.</li>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.PaddingMask"><code>tl.PaddingMask()</code></a>: layer that maps integer sequences to padding masks.</li>
</ul></li>
<li>Your list of <code>EncoderBlock</code>s</li>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Select"><code>tl.Select([0], n_in=2)</code></a>: Copies, reorders, or deletes stack elements according to indices.</li>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm"><code>tl.LayerNorm()</code></a>.</li>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean"><code>tl.Mean()</code></a>: Mean along the first axis.</li>
<li><code>tl.Dense()</code> with n_units set to n_classes.</li>
<li><code>tl.LogSoftmax()</code></li>
</ul>
<p>Please refer to the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/">trax documentation</a> for further information.</p>
<p><a name='ex04'></a> ### Exercise 04</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: TransformerEncoder</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TransformerEncoder</span>(<span class="params">vocab_size=vocab_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                       n_classes=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       d_model=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       d_ff=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       n_layers=<span class="number">6</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       n_heads=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       dropout_shared_axes=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       max_len=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       mode=<span class="string">&#x27;train&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       ff_activation=tl.Relu,</span></span></span><br><span class="line"><span class="params"><span class="function">                       EncoderBlock=EncoderBlock</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Returns a Transformer encoder model.</span></span><br><span class="line"><span class="string">    The input to the model is a tensor of tokens.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab_size (int): vocab size. Defaults to vocab_size.</span></span><br><span class="line"><span class="string">        n_classes (int): how many classes on output. Defaults to 10.</span></span><br><span class="line"><span class="string">        d_model (int): depth of embedding. Defaults to 512.</span></span><br><span class="line"><span class="string">        d_ff (int): depth of feed-forward layer. Defaults to 2048.</span></span><br><span class="line"><span class="string">        n_layers (int): number of encoder/decoder layers. Defaults to 6.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads. Defaults to 8.</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate (how much to drop out). Defaults to 0.1.</span></span><br><span class="line"><span class="string">        dropout_shared_axes (int): axes on which to share dropout mask. Defaults to None.</span></span><br><span class="line"><span class="string">        max_len (int): maximum symbol length for positional encoding. Defaults to 2048.</span></span><br><span class="line"><span class="string">        mode (str): &#x27;train&#x27; or &#x27;eval&#x27;. Defaults to &#x27;train&#x27;.</span></span><br><span class="line"><span class="string">        ff_activation (function): the non-linearity in feed-forward layer. Defaults to tl.Relu.</span></span><br><span class="line"><span class="string">        EncoderBlock (function): Returns the encoder block. Defaults to EncoderBlock.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.layers.combinators.Serial: A Transformer model as a layer that maps</span></span><br><span class="line"><span class="string">        from a tensor of tokens to activations over a set of output classes.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    positional_encoder = [</span><br><span class="line">        tl.Embedding(vocab_size, d_model),</span><br><span class="line">        tl.Dropout(rate=dropout, shared_axes=dropout_shared_axes, mode=mode),</span><br><span class="line">        tl.PositionalEncoding(max_len=max_len)</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use the function `EncoderBlock` (implemented above) and pass in the parameters over `n_layers`</span></span><br><span class="line">    encoder_blocks = [EncoderBlock(d_model, d_ff, n_heads, dropout, dropout_shared_axes,</span><br><span class="line">                  mode, ff_activation, FeedForwardBlock=FeedForwardBlock) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Assemble and return the model.</span></span><br><span class="line">    <span class="keyword">return</span> tl.Serial(</span><br><span class="line">        <span class="comment"># Encode</span></span><br><span class="line">        tl.Branch(</span><br><span class="line">            <span class="comment"># Use `positional_encoder`</span></span><br><span class="line">            positional_encoder,</span><br><span class="line">            <span class="comment"># Use trax padding mask</span></span><br><span class="line">            tl.PaddingMask(),</span><br><span class="line">        ),</span><br><span class="line">        <span class="comment"># Use `encoder_blocks`</span></span><br><span class="line">        encoder_blocks,</span><br><span class="line">        <span class="comment"># Use select layer</span></span><br><span class="line">        tl.Select([<span class="number">0</span>], n_in=<span class="number">2</span>),</span><br><span class="line">        <span class="comment"># Use trax layer normalization</span></span><br><span class="line">        tl.LayerNorm(),</span><br><span class="line">        <span class="comment"># Map to output categories.</span></span><br><span class="line">        <span class="comment"># Use trax mean. set axis to 1</span></span><br><span class="line">        tl.Mean(axis = <span class="number">1</span>),</span><br><span class="line">        <span class="comment"># Use trax Dense using `n_classes`</span></span><br><span class="line">        tl.Dense(n_classes),</span><br><span class="line">        <span class="comment"># Use trax log softmax</span></span><br><span class="line">        tl.LogSoftmax(),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run this cell to see the structure of your model</span></span><br><span class="line"><span class="comment"># Only 1 layer is used to keep the output readable</span></span><br><span class="line">TransformerEncoder(n_layers=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Serial[
  Branch_out2[
    [Embedding_32000_512, Dropout, PositionalEncoding]
    PaddingMask(0)
  ]
  Serial_in2_out2[
    Branch_in2_out3[
      None
      Serial_in2_out2[
        LayerNorm
        Serial_in2_out2[
          Dup_out2
          Dup_out2
          Serial_in4_out2[
            Parallel_in3_out3[
              Dense_512
              Dense_512
              Dense_512
            ]
            PureAttention_in4_out2
            Dense_512
          ]
        ]
        Dropout
      ]
    ]
    Add_in2
  ]
  Serial[
    Branch_out2[
      None
      Serial[
        LayerNorm
        Dense_2048
        Relu
        Dropout
        Dense_512
        Dropout
      ]
    ]
    Add_in2
  ]
  Select[0]_in2
  LayerNorm
  Mean
  Dense_10
  LogSoftmax
]</code></pre>
<h4 id="expected-output-3"><strong>Expected Output:</strong></h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">Serial[</span><br><span class="line">  Branch_out2[</span><br><span class="line">    [Embedding_32000_512, Dropout, PositionalEncoding]</span><br><span class="line">    <span class="built_in">PaddingMask</span>(<span class="number">0</span>)</span><br><span class="line">  ]</span><br><span class="line">  Serial_in2_out2[</span><br><span class="line">    Branch_in2_out3[</span><br><span class="line">      None</span><br><span class="line">      Serial_in2_out2[</span><br><span class="line">        LayerNorm</span><br><span class="line">        Serial_in2_out2[</span><br><span class="line">          Dup_out2</span><br><span class="line">          Dup_out2</span><br><span class="line">          Serial_in4_out2[</span><br><span class="line">            Parallel_in3_out3[</span><br><span class="line">              Dense_512</span><br><span class="line">              Dense_512</span><br><span class="line">              Dense_512</span><br><span class="line">            ]</span><br><span class="line">            PureAttention_in4_out2</span><br><span class="line">            Dense_512</span><br><span class="line">          ]</span><br><span class="line">        ]</span><br><span class="line">        Dropout</span><br><span class="line">      ]</span><br><span class="line">    ]</span><br><span class="line">    Add_in2</span><br><span class="line">  ]</span><br><span class="line">  Serial[</span><br><span class="line">    Branch_out2[</span><br><span class="line">      None</span><br><span class="line">      Serial[</span><br><span class="line">        LayerNorm</span><br><span class="line">        Dense_2048</span><br><span class="line">        Relu</span><br><span class="line">        Dropout</span><br><span class="line">        Dense_512</span><br><span class="line">        Dropout</span><br><span class="line">      ]</span><br><span class="line">    ]</span><br><span class="line">    Add_in2</span><br><span class="line">  ]</span><br><span class="line">  Select[<span class="number">0</span>]_in2</span><br><span class="line">  LayerNorm</span><br><span class="line">  Mean</span><br><span class="line">  Dense_10</span><br><span class="line">  LogSoftmax</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p><strong>NOTE Congratulations! You have completed all of the graded functions of this assignment.</strong> Since the rest of the assignment takes a lot of time and memory to run we are providing some extra ungraded labs for you to see this model in action.</p>
<p><strong>Keep it up!</strong></p>
<p>To see this model in action continue to the next 2 ungraded labs. <strong>We strongly recommend you to try the colab versions of them as they will yield a much smoother experience.</strong> The links to the colabs can be found within the ungraded labs or if you already know how to open files within colab here are some shortcuts (if not, head to the ungraded labs which contain some extra instructions):</p>
<p><a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1EHAbMnW6u-GqYWh5r3Z8uLbz4KNpKOAv/view?usp=sharing">BERT Loss Model Colab</a></p>
<p><a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1c-8KJkTySRGqCx_JjwjvXuRBTNTqEE0N/view?usp=sharing">T5 SQuAD Model Colab</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Transformer-Summarizer/2020/09/27/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Transformer-Summarizer/2020/09/27/" class="post-title-link" itemprop="url">Transformer Summarizer</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-09-27 15:23:13 / Modified: 15:24:19" itemprop="dateCreated datePublished" datetime="2020-09-27T15:23:13+08:00">2020-09-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Transformer-Summarizer/2020/09/27/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Transformer-Summarizer/2020/09/27/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="assignment-2-transformer-summarizer">Assignment 2: Transformer Summarizer</h1>
<p>Welcome to the second assignment of course 4. In this assignment you will explore summarization using the transformer model. Yes, you will implement the transformer decoder from scratch, but we will slowly walk you through it. There are many hints in this notebook so feel free to use them as needed.</p>
<p><img src = "transformerNews.png"></p>
<h2 id="outline">Outline</h2>
<ul>
<li><a href="#0">Introduction</a></li>
<li><a href="#1">Part 1: Importing the dataset</a>
<ul>
<li><a href="#1.1">1.1 Encode &amp; Decode helper functions</a></li>
<li><a href="#1.2">1.2 Defining parameters</a></li>
<li><a href="#1.3">1.3 Exploring the data</a></li>
</ul></li>
<li><a href="#2">Part 2: Summarization with transformer</a>
<ul>
<li><a href="#2.1">2.1 Dot product attention</a>
<ul>
<li><a href="#ex01">Exercise 01</a></li>
</ul></li>
<li><a href="#2.2">2.2 Causal Attention</a>
<ul>
<li><a href="#ex02">Exercise 02</a></li>
</ul></li>
<li><a href="#2.3">2.3 Transformer decoder block</a>
<ul>
<li><a href="#ex03">Exercise 03</a></li>
</ul></li>
<li><a href="#2.4">2.4 Transformer Language model</a>
<ul>
<li><a href="#ex04">Exercise 04</a></li>
</ul></li>
</ul></li>
<li><a href="#3">Part 3: Training</a>
<ul>
<li><a href="#3.1">3.1 Training the model</a>
<ul>
<li><a href="#ex05">Exercise 05</a></li>
</ul></li>
</ul></li>
<li><a href="#4">Part 4: Evaluation</a>
<ul>
<li><a href="#4.1">4.1 Loading in a trained model</a></li>
</ul></li>
<li><a href="#5">Part 5: Testing with your own input</a>
<ul>
<li><a href="#ex06">Exercise 6</a></li>
<li><a href="#5.1">5.1 Greedy decoding</a>
<ul>
<li><a href="#ex07">Exercise 07</a></li>
</ul></li>
</ul></li>
</ul>
<p><a name='0'></a> ### Introduction</p>
<p>Summarization is an important task in natural language processing and could be useful for a consumer enterprise. For example, bots can be used to scrape articles, summarize them, and then you can use sentiment analysis to identify the sentiment about certain stocks. Anyways who wants to read an article or a long email today, when you can build a transformer to summarize text for you. Let's get started, by completing this assignment you will learn to:</p>
<ul>
<li>Use built-in functions to preprocess your data</li>
<li>Implement DotProductAttention</li>
<li>Implement Causal Attention</li>
<li>Understand how attention works</li>
<li>Build the transformer model</li>
<li>Evaluate your model</li>
<li>Summarize an article</li>
</ul>
<p>As you can tell, this model is slightly different than the ones you have already implemented. This is heavily based on attention and does not rely on sequences, which allows for parallel computing.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> textwrap</span><br><span class="line">wrapper = textwrap.TextWrapper(width=<span class="number">70</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> trax</span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> trax.fastmath <span class="keyword">import</span> numpy <span class="keyword">as</span> jnp</span><br><span class="line"></span><br><span class="line"><span class="comment"># to print the entire np array</span></span><br><span class="line">np.set_printoptions(threshold=sys.maxsize)</span><br></pre></td></tr></table></figure>
<p><a name='1'></a> ## Part 1: Importing the dataset</p>
<p>Trax makes it easy to work with Tensorflow's datasets:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This will download the dataset if no data_dir is specified.</span></span><br><span class="line"><span class="comment"># Downloading and processing can take bit of time,</span></span><br><span class="line"><span class="comment"># so we have the data already in &#x27;data/&#x27; for you</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Importing CNN/DailyMail articles dataset</span></span><br><span class="line">train_stream_fn = trax.data.TFDS(<span class="string">&#x27;cnn_dailymail&#x27;</span>,</span><br><span class="line">                                 data_dir=<span class="string">&#x27;data/&#x27;</span>,</span><br><span class="line">                                 keys=(<span class="string">&#x27;article&#x27;</span>, <span class="string">&#x27;highlights&#x27;</span>),</span><br><span class="line">                                 train=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This should be much faster as the data is downloaded already.</span></span><br><span class="line">eval_stream_fn = trax.data.TFDS(<span class="string">&#x27;cnn_dailymail&#x27;</span>,</span><br><span class="line">                                data_dir=<span class="string">&#x27;data/&#x27;</span>,</span><br><span class="line">                                keys=(<span class="string">&#x27;article&#x27;</span>, <span class="string">&#x27;highlights&#x27;</span>),</span><br><span class="line">                                train=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><a name='1.1'></a> ## 1.1 Tokenize &amp; Detokenize helper functions</p>
<p>Just like in the previous assignment, the cell above loads in the encoder for you. Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your <a target="_blank" rel="noopener" href="https://github.com/google/trax">Trax</a> models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following:</p>
<ul>
<li><span style="color:blue"> word2Ind: </span> a dictionary mapping the word to its index.</li>
<li><span style="color:blue"> ind2Word:</span> a dictionary mapping the index to its word.</li>
<li><span style="color:blue"> word2Count:</span> a dictionary mapping the word to the number of times it appears.</li>
<li><span style="color:blue"> num_words:</span> total number of words that have appeared.</li>
</ul>
<p>Since you have already implemented these in previous assignments of the specialization, we will provide you with helper functions that will do this for you. Run the cell below to get the following functions:</p>
<ul>
<li><span style="color:blue"> tokenize: </span> converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords.</li>
<li><span style="color:blue"> detokenize: </span> converts a token list to its corresponding sentence (i.e. string).</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">input_str, EOS=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Input str to features dict, ready for inference&quot;&quot;&quot;</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Use the trax.data.tokenize method. It takes streams and returns streams,</span></span><br><span class="line">    <span class="comment"># we get around it by making a 1-element stream with `iter`.</span></span><br><span class="line">    inputs =  <span class="built_in">next</span>(trax.data.tokenize(<span class="built_in">iter</span>([input_str]),</span><br><span class="line">                                      vocab_dir=<span class="string">&#x27;vocab_dir/&#x27;</span>,</span><br><span class="line">                                      vocab_file=<span class="string">&#x27;summarize32k.subword.subwords&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Mark the end of the sentence with EOS</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(inputs) + [EOS]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detokenize</span>(<span class="params">integers</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;List of ints to str&quot;&quot;&quot;</span></span><br><span class="line">  </span><br><span class="line">    s = trax.data.detokenize(integers,</span><br><span class="line">                             vocab_dir=<span class="string">&#x27;vocab_dir/&#x27;</span>,</span><br><span class="line">                             vocab_file=<span class="string">&#x27;summarize32k.subword.subwords&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> wrapper.fill(s)</span><br></pre></td></tr></table></figure>
<p><a name='1.2'></a></p>
<h2 id="preprocessing-for-language-models-concatenate-it">1.2 Preprocessing for Language Models: Concatenate It!</h2>
<p>This week you will use a language model -- Transformer Decoder -- to solve an input-output problem. As you know, language models only predict the next word, they have no notion of inputs. To create a single input suitable for a language model, we concatenate inputs with targets putting a separator in between. We also need to create a mask -- with 0s at inputs and 1s at targets -- so that the model is not penalized for mis-predicting the article and only focuses on the summary. See the preprocess function below for how this is done.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Special tokens</span></span><br><span class="line">SEP = <span class="number">0</span> <span class="comment"># Padding or separator token</span></span><br><span class="line">EOS = <span class="number">1</span> <span class="comment"># End of sentence token</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Concatenate tokenized inputs and targets using 0 as separator.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params">stream</span>):</span></span><br><span class="line">    <span class="keyword">for</span> (article, summary) <span class="keyword">in</span> stream:</span><br><span class="line">        joint = np.array(<span class="built_in">list</span>(article) + [EOS, SEP] + <span class="built_in">list</span>(summary) + [EOS])</span><br><span class="line">        mask = [<span class="number">0</span>] * (<span class="built_in">len</span>(<span class="built_in">list</span>(article)) + <span class="number">2</span>) + [<span class="number">1</span>] * (<span class="built_in">len</span>(<span class="built_in">list</span>(summary)) + <span class="number">1</span>) <span class="comment"># Accounting for EOS and SEP</span></span><br><span class="line">        <span class="keyword">yield</span> joint, joint, np.array(mask)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can combine a few data preprocessing steps into a pipeline like this.</span></span><br><span class="line">input_pipeline = trax.data.Serial(</span><br><span class="line">    <span class="comment"># Tokenizes</span></span><br><span class="line">    trax.data.Tokenize(vocab_dir=<span class="string">&#x27;vocab_dir/&#x27;</span>,</span><br><span class="line">                       vocab_file=<span class="string">&#x27;summarize32k.subword.subwords&#x27;</span>),</span><br><span class="line">    <span class="comment"># Uses function defined above</span></span><br><span class="line">    preprocess,</span><br><span class="line">    <span class="comment"># Filters out examples longer than 2048</span></span><br><span class="line">    trax.data.FilterByLength(<span class="number">2048</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply preprocessing to data streams.</span></span><br><span class="line">train_stream = input_pipeline(train_stream_fn())</span><br><span class="line">eval_stream = input_pipeline(eval_stream_fn())</span><br><span class="line"></span><br><span class="line">train_input, train_target, train_mask = <span class="built_in">next</span>(train_stream)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">sum</span>((train_input - train_target)**<span class="number">2</span>) == <span class="number">0</span>  <span class="comment"># They are the same in Language Model (LM).</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prints mask, 0s on article, 1s on summary</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Single example mask:\n\n <span class="subst">&#123;train_mask&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Single example mask:

 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prints: [Example][&lt;EOS&gt;][&lt;pad&gt;][Example Summary][&lt;EOS&gt;]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Single example:\n\n <span class="subst">&#123;detokenize(train_input)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Single example:

 By . Margot Peppers . Nigerian and Cameroonian pop star Dencia has hit
out at Lupita Nyong&#39;o for her new contract with Lancome, accusing her
of bowing to &#39;white people companies&#39;. In an angry tweet directed at
the 12 Years A Slave star, she wrote: &#39;Oh @Lupita_Nyongo cln&#39;t talk
abt the bleaching creams white people (Companies) make cuz the white
man pays her, they own her!! [sic]&#39;. The comment comes just a month
after Miss Nyong&#39;o mentioned Dencia - who has been accused of
marketing her own brand of skin-bleaching cream called Whitenicious -
in a speech about learning to value the color of her own skin. Scroll
down for video . Butting heads: Nigerian and Cameroonian pop star
Dencia has hit out at Lupita Nyong&#39;o for her new contract with
Lancome, accusing her of bowing to &#39;white people companies&#39; Fighting
words: In a tweet directed at the 12 Years A Slave star, she wrote:
&#39;Oh @Lupita_Nyongo cln&#39;t talk abt the bleaching creams white people
(Companies) make cuz the white man pays her, they own her!! [sic]&#39; The
pop star is no stranger to . controversy; in a February interview with
Ebony, she all but admitted . that Whitenicious is intended as a skin-
lightener, not as a cure for . dark spots as it claims. &#39;When . you
take that picture and you put a picture of Dencia darker, this is .
what you&#39;re telling people - the product really works,&#39; she said. &#39;And
guess what? People really want to buy it. It&#39;s what it is. I don&#39;t
really care.&#39; Given her defiant and hypocritical attitude, it&#39;s no
surprise the fiery singer was angered when Miss Nyong&#39;o called her out
in a speech at Essence&#39;s Black Women in Hollywood event on February
27. Influential: In a recent speech, Miss Nyong&#39;o read out loud a
letter from a fan who said she decided not to buy Dencia&#39;s skin-
whitening cream Whitenicious because the actress had inspired her to
love her own skin . On-screen: Miss Nyong&#39;o won an Oscar for Best
Supporting Actress for her role in 2013 film 12 Years A Slave . In her
talk, the 30-year-old opened up about how conventional standards of
beauty once affected her self-esteem, reading aloud a letter written
to her by a young girl who viewed her as a role model. &#39;Dear Lupita,&#39;
reads the letter. &#39;I think you&#39;re really lucky to be this black but
yet this successful in Hollywood overnight. I was just about to buy
Dencia&#39;s Whitenicious cream to lighten my skin when you appeared on
the world map and saved me.&#39; &#39;My heart bled a little when I read those
words,&#39; the actress said through tears, explaining how as a child,
she, too, would pray that she&#39;d one day wake up with lighter skin.
Hypocritical: Dencia is no stranger to controversy; in a February
interview with Ebony, she essentially admitted that Whitenicious is
intended as a skin-lightener, not as a cure for dark spots as it
claims . Perpetuating the problem: &#39;When you take that picture and you
put a picture of Dencia darker, this is what you&#39;re telling people -
the product really works,&#39; she said. &#39;And guess what? People really
want to buy it&#39; But while the actress saw the letter as a source of
inspiration, Dencia took it as a personal attack. After her angry
tweet at Miss Nyong&#39;o, criticism poured in, with one person tweeting:
&#39;B**** lupita is the new face of Lanc√¥me!! SHE WINS!! And you&#39;re just
TRASH [sic]&#39;. In her response, Dencia said of the cosmetics company:
&#39;But they sell bleaching cream tho [sic]&#39;. The pop star is likely
referring to Lancome&#39;s Blanc Expert range of cosmetics, which are
actually advertised as &#39;brighteners&#39; that &#39;regulate melanin production
and awaken the luminosity of the skin&#39;. And as far as Dencia&#39;s claim
that Lancome is a &#39;white people company&#39;, a quick perusal of the
website reveals that it has a number of concealers and foundations in
darker skin tones.&lt;EOS&gt;&lt;pad&gt;Dencia&#39;s comment is hypocritical
considering she recently courted controversy for marketing &#39;dark spot
remover&#39; Whitenicious, which is frequently used as a skin-whitening
cream .&lt;EOS&gt;</code></pre>
<p><a name='1.3'></a></p>
<h2 id="batching-with-bucketing">1.3 Batching with bucketing</h2>
<p>As in the previous week, we use bucketing to create batches of data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Bucketing to create batched generators.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Buckets are defined in terms of boundaries and batch sizes.</span></span><br><span class="line"><span class="comment"># Batch_sizes[i] determines the batch size for items with length &lt; boundaries[i]</span></span><br><span class="line"><span class="comment"># So below, we&#x27;ll take a batch of 16 sentences of length &lt; 128 , 8 of length &lt; 256,</span></span><br><span class="line"><span class="comment"># 4 of length &lt; 512. And so on. </span></span><br><span class="line">boundaries =  [<span class="number">128</span>, <span class="number">256</span>,  <span class="number">512</span>, <span class="number">1024</span>]</span><br><span class="line">batch_sizes = [<span class="number">16</span>,    <span class="number">8</span>,    <span class="number">4</span>,    <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the streams.</span></span><br><span class="line">train_batch_stream = trax.data.BucketByLength(</span><br><span class="line">    boundaries, batch_sizes)(train_stream)</span><br><span class="line"></span><br><span class="line">eval_batch_stream = trax.data.BucketByLength(</span><br><span class="line">    boundaries, batch_sizes)(eval_stream)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Every execution will result in generation of a different article</span></span><br><span class="line"><span class="comment"># Try running this cell multiple times to see how the length of the examples affects the batch size</span></span><br><span class="line">input_batch, _, mask_batch = <span class="built_in">next</span>(train_batch_stream)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Shape of the input_batch</span></span><br><span class="line">input_batch.shape</span><br></pre></td></tr></table></figure>
<pre><code>(2, 1024)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print corresponding integer values</span></span><br><span class="line"><span class="built_in">print</span>(input_batch[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[   27  1091    23    46  3873  1248 16013   256 11599 23297   102    68
 24308     7     5  1037  1958   320  1477   105  2557   186  4133    28
 18175  1348  1287     3  4927  7577    28  8478 10120 19134  7951   364
  7317  4990    79     2   393     2   186  8962  2995  9813  4476  3632
  2270     5     2   705     2   721 10731    16   186 17136    16   193
    54   102    41  1459   320    31 16946    47     2   119  3770   278
   355    28   622   263    78  2613     3   312  4543     4  8662  3788
  3632  2270     5     6  3048 23524     2  1210     2  1958   320  2033
   105    61     2 19134  7951   364  7317  4990    79 24810    17   213
  1091     2   931   320   213 16946    47   415 20579 20964    58  1782
   863   213  7726     2   213  7599  3938  4133    28 26719     4   752
  1480  2868   132    68   583  3898 20579 20964    58   240   197     3
  4531  9531  2959   127   132    28 27439  9275  1628  1602     3  8406
  5364    11  4927  7577    28  8478 10120 19134  7951   364  7317  4990
    79     2   393     2   497     2   186    68 24308  8962  2995  9813
  4476  3632  2270     5     2   705     2   721 10731    16   186 17136
    16   193    54   809    31   278    78  2613  7511    15  1037 20274
    21   379 21549  7150    11  9813  4476  3632  2270     5    80 18649
  1496   667   213 17136    45    78    15   882  1838   213  2439  7883
   379    27  1147     6   104     6   292   966    43 11850   213  1621
     2   931   320   213 13021     4     2    35    22   206    19  5632
   213  1018   111   213  2948   186   213 25931     4     3  2713  7801
   320    28  6105    32   922  1838   213  6350   141   102 24114    75
    78  2613   186  7511    41  2362     2    41   233  3632  2270     5
     6  3048 23524  1955    78    28 11261  1797  1782   198    25    92
  3787  3103   527 13747   320   213  7599     2   487   159   213   669
 27884     4  1622 27872   391  5977  3103   527  2918   186  1472   320
    18    46   810   132    28  2439  7726  3898   213 13021     4   127
     3    34    31 18649  3347     2   148 19134  7951   364  7317  4990
    79   186  9813  4476  3632  2270     5    18 17136    45    78    31
  5369   186 19175     5     3     9  2789    25 11203     2   412    25
   213   966   186    54  1697     3 12849    14    11  7317  4990    79
 12365   146 24810    17   213  1091  4617 27439  9275  1628  4543     4
  8662  3788  3632  2270     5     6  3048 23524     2   186   131  4133
    28 26719     4  6901   809   213    60     6  1797  6350   809   213
   414     8 12370    21    12   186   710   171   864  2362   809   213
  1610   379     9 16946    47   415  3357 15581    81     7     5  1431
  1890   163  4336  7188    20    78  3632  2270     5     6  3048 23524
     7     5   661     2    35   646    25 17926 25290 16741    20  4140
     2   213 13021     4   127     3 19134  7951   364  7317  4990    79
  1353  3873  1248 11599 23297     2 17260  8041   893   213  5627   527
    28   966     2  2439  1740  1524  7726   186 23638    16 24668 21273
   204     2   931   320  1882     3  4531  9531  2959   127 19134  7951
   364  7317  4990    79    43  9363     4   760    70    35    62    19
  2851  2754   103  1353    70  1480 22646   272  7304   132    28  1501
   809   213  1881  1610     3   305  1353   475   809   213 16946    47
   415  7411    84    78   281  3997    88   226 20934     4     3  9813
  4476  3632  2270     5  1353    43  3873  1248   966 17260  8041 16704
   464   186  2439  1740  1524  7726   186  1233   320   213  1156 10835
    78   281  1696    88   226 20934     4     3    27   924  3729    23
    46  4648  1019  3112  1859   809 18235  5333  9141 25733   812    10
     1     0  4927  7577    28  8478 10120 19134  7951   364  7317  4990
    79     2   393     2   186  8962  2995  9813  4476  3632  2270     5
     2   705     2   721  2557   102  4925  1838    28   622    78  2613
  1859 16346 27439  6774  1628   312    15  1037     2  4543     4  8662
  3788  3632  2270     5     6  3048 23524     2  1210     2  1958   320
  1477   105     2 19134  7951   364  7317  4990    79 12365 24810    17
    68 16346 27439  6774  1628   305  4133 26719     4  6901   186   864
  2362   320   423    68  1955 16346 27439  6774  1628    27  1147     6
   104     6   292  2635 11850   213  1621  2104     1     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0]</code></pre>
<p>Things to notice: - First we see the corresponding values of the words. - The first 1, which represents the <code>&lt;EOS&gt;</code> tag of the article. - Followed by a 0, which represents a <code>&lt;pad&gt;</code> tag. - After the first 0 (<code>&lt;pad&gt;</code> tag) the corresponding values are of the words that are used for the summary of the article. - The second 1 represents the <code>&lt;EOS&gt;</code> tag for the summary. - All the trailing 0s represent <code>&lt;pad&gt;</code> tags which are appended to maintain consistent length (If you don't see them then it would mean it is already of max length)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print the article and its summary</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Article:\n\n&#x27;</span>, detokenize(input_batch[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>Article:

 A woman has been charged with reckless manslaughter after her
boyfriend&#39;s mother tried to stop them fighting and suffered a fatal
heart attack. Claudia Yanira Hernandez Soriano, 25, and Juan Francisco
Martinez Rojas, 28, started punching and scratching each other after
they returned to their Bergen, New Jersey home following a party early
on Monday. When Ana Angelina Rojas-Jovel, 45, tried to break them up,
Hernandez Soriano assaulted the woman, according to the Bergen County
Prosecutor. &#39;During the assault, the victim apparently suffered a
cardiac event which resulted in her death,&#39; Prosecutor John L.
Molinelli said in a¬†statement. Fight: Claudia Yanira Hernandez
Soriano, 25, above, and her boyfriend Juan Francisco Martinez Rojas,
28, started punching and scratching each other at their home on Monday
when his mother intervened . Injured: Martinez Rojas&#39; booking shot
shows the scratches on his face from the domestic dispute . A seven-
year-old child also witnessed the fight, according to the prosecutor,
but he did not reveal the relationship between the adults and the
youngster. Police responded to a 911 call from the apartment just
after 4am on Monday and when they arrived, they found Rojas-Jovel dead
on a bedroom floor. &#39;There were no obvious signs of trauma to the
victim, however... the [couple] displayed signs of injury and appeared
to have been involved in a domestic assault,&#39; the prosecutor said. In
their booking photos, both Hernandez Soriano and Martinez Rojas have
scratches on their faces and necks. The pair were interviewed, as were
the child and other residents. Scene: Soriano allegedly then assaulted
the woman,¬†Ana Angelina Rojas-Jovel, and she suffered a cardiac arrest
at the first-floor apartment at the house (pictured) and died before
police arrived at the scene . The Bergen County Medical Examiner&#39;s
Office conducted an autopsy on Rojas-Jovel&#39;s body, but results were
pending toxicology tests, the prosecutor said. Hernandez Soriano was
charged with manslaughter, endangering the welfare of a child,
domestic violence simple assault and hindering apprehension, according
to authorities. Molinelli said Hernandez Soriano also hid evidence -
but would not detail what it was - which investigators later recovered
in a search at the crime scene. She was held at the Bergen County Jail
on $250,000 bail. Martinez Rojas was also charged with child
endangerment and domestic violence simple assault and sent to the
county jail on $75,000 bail. A court hearing has been scheduled for
Thursday morning at Hackensack Superior Court.&lt;EOS&gt;&lt;pad&gt;ClaudiaYanira
Hernandez Soriano, 25, and Juan Francisco Martinez Rojas, 28, started
fighting after returning from a party on Monday morning . When his
mother, Ana Angelina Rojas-Jovel, 45, tried to stop them, Hernandez
Soriano allegedly assaulted her . She suffered cardiac arrest and
police arrived to find her dead . A seven-year-old girl witnessed the
fight .&lt;EOS&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;</code></pre>
<p>You can see that the data has the following structure: - <span style="color:blue"> [Article] </span> -&gt; <code>&lt;EOS&gt;</code> -&gt; <code>&lt;pad&gt;</code> -&gt; <span style="color:blue"> [Article Summary] </span> -&gt; <code>&lt;EOS&gt;</code> -&gt; (possibly) multiple <code>&lt;pad&gt;</code></p>
<p>The loss is taken only on the summary using cross_entropy as loss function.</p>
<p><a name='2'></a> # Part 2: Summarization with transformer</p>
<p>Now that we have given you the data generator and have handled the preprocessing for you, it is time for you to build your own model. We saved you some time because we know you have already preprocessed data before in this specialization, so we would rather you spend your time doing the next steps.</p>
<p>You will be implementing the attention from scratch and then using it in your transformer model. Concretely, you will understand how attention works, how you use it to connect the encoder and the decoder.</p>
<p><img src="transformer_decoder_zoomin.png"></p>
<p><a name='2.1'></a> ## 2.1 Dot product attention</p>
<p>Now you will implement dot product attention which takes in a query, key, value, and a mask. It returns the output.</p>
<p><img src ="dotproduct.png"></p>
<p>Here are some helper functions that will help you create tensors and display useful information: - <code>create_tensor</code> creates a <code>jax numpy array</code> from a list of lists. - <code>display_tensor</code> prints out the shape and the actual tensor.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tensor</span>(<span class="params">t</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Create tensor from list of lists&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> jnp.array(t)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_tensor</span>(<span class="params">t, name</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Display shape and tensor&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span> shape: <span class="subst">&#123;t.shape&#125;</span>\n&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;t&#125;</span>\n&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>Before implementing it yourself, you can play around with a toy example of <code>dot product attention</code> without the softmax operation. Technically it would not be <code>dot product attention</code> without the softmax but this is done to avoid giving away too much of the answer and the idea is to display these tensors to give you a sense of how they look like.</p>
<p>The formula for attention is this one:</p>
<p><span class="math display">\[
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}+{M}\right) V\tag{1}\
\]</span></p>
<p><span class="math inline">\(d_{k}\)</span> stands for the dimension of queries and keys.</p>
<p>The <code>query</code>, <code>key</code>, <code>value</code> and <code>mask</code> vectors are provided for this example.</p>
<p>Notice that the masking is done using very negative values that will yield a similar effect to using $-$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">q = create_tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">display_tensor(q, <span class="string">&#x27;query&#x27;</span>)</span><br><span class="line">k = create_tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">display_tensor(k, <span class="string">&#x27;key&#x27;</span>)</span><br><span class="line">v = create_tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">display_tensor(v, <span class="string">&#x27;value&#x27;</span>)</span><br><span class="line">m = create_tensor([[<span class="number">0</span>, <span class="number">0</span>], [-<span class="number">1e9</span>, <span class="number">0</span>]])</span><br><span class="line">display_tensor(m, <span class="string">&#x27;mask&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>query shape: (2, 3)

[[1 0 0]
 [0 1 0]]

key shape: (2, 3)

[[1 2 3]
 [4 5 6]]

value shape: (2, 3)

[[0 1 0]
 [1 0 1]]

mask shape: (2, 2)

[[ 0.e+00  0.e+00]
 [-1.e+09  0.e+00]]</code></pre>
<p><strong>Expected Output:</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">query shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">key shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">value shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">mask shape: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">0.e+00</span>  <span class="number">0.e+00</span>]</span><br><span class="line"> [<span class="number">-1.e+09</span>  <span class="number">0.e+00</span>]]</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q_dot_k = q @ k.T / jnp.sqrt(<span class="number">3</span>)</span><br><span class="line">display_tensor(q_dot_k, <span class="string">&#x27;query dot key&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>query dot key shape: (2, 2)

[[0.57735026 2.309401  ]
 [1.1547005  2.8867514 ]]</code></pre>
<p><strong>Expected Output:</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">query dot key shape: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">0.57735026</span> <span class="number">2.309401</span>  ]</span><br><span class="line"> [<span class="number">1.1547005</span>  <span class="number">2.8867514</span> ]]</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">masked = q_dot_k + m</span><br><span class="line">display_tensor(masked, <span class="string">&#x27;masked query dot key&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>masked query dot key shape: (2, 2)

[[ 5.7735026e-01  2.3094010e+00]
 [-1.0000000e+09  2.8867514e+00]]</code></pre>
<p><strong>Expected Output:</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">masked query dot key shape: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">5.7735026e-01</span>  <span class="number">2.3094010e+00</span>]</span><br><span class="line"> [<span class="number">-1.0000000e+09</span>  <span class="number">2.8867514e+00</span>]]</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_tensor(masked @ v, <span class="string">&#x27;masked query dot key dot value&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>masked query dot key dot value shape: (2, 3)

[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]
 [ 2.8867514e+00 -1.0000000e+09  2.8867514e+00]]</code></pre>
<p><strong>Expected Output:</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">masked query dot key dot value shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">2.3094010e+00</span>  <span class="number">5.7735026e-01</span>  <span class="number">2.3094010e+00</span>]</span><br><span class="line"> [ <span class="number">2.8867514e+00</span> <span class="number">-1.0000000e+09</span>  <span class="number">2.8867514e+00</span>]]</span><br></pre></td></tr></table></figure></p>
<p>In order to use the previous dummy tensors to test some of the graded functions, a batch dimension should be added to them so they mimic the shape of real-life examples. The mask is also replaced by a version of it that resembles the one that is used by trax:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">q_with_batch = q[<span class="literal">None</span>,:]</span><br><span class="line">display_tensor(q_with_batch, <span class="string">&#x27;query with batch dim&#x27;</span>)</span><br><span class="line">k_with_batch = k[<span class="literal">None</span>,:]</span><br><span class="line">display_tensor(k_with_batch, <span class="string">&#x27;key with batch dim&#x27;</span>)</span><br><span class="line">v_with_batch = v[<span class="literal">None</span>,:]</span><br><span class="line">display_tensor(v_with_batch, <span class="string">&#x27;value with batch dim&#x27;</span>)</span><br><span class="line">m_bool = create_tensor([[<span class="literal">True</span>, <span class="literal">True</span>], [<span class="literal">False</span>, <span class="literal">True</span>]])</span><br><span class="line">display_tensor(m_bool, <span class="string">&#x27;boolean mask&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>query with batch dim shape: (1, 2, 3)

[[[1 0 0]
  [0 1 0]]]

key with batch dim shape: (1, 2, 3)

[[[1 2 3]
  [4 5 6]]]

value with batch dim shape: (1, 2, 3)

[[[0 1 0]
  [1 0 1]]]

boolean mask shape: (2, 2)

[[ True  True]
 [False  True]]</code></pre>
<p><strong>Expected Output:</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">query with batch dim shape: (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br><span class="line"></span><br><span class="line">key with batch dim shape: (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]]</span><br><span class="line"></span><br><span class="line">value with batch dim shape: (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]]]</span><br><span class="line"></span><br><span class="line">boolean mask shape: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">[[ True  True]</span><br><span class="line"> [False  True]]</span><br></pre></td></tr></table></figure></p>
<p><a name='ex01'></a> ### Exercise 01</p>
<p><strong>Instructions:</strong> Implement the dot product attention. Concretely, implement the following equation</p>
<p><span class="math display">\[
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}+{M}\right) V\tag{1}\
\]</span></p>
<p><span class="math inline">\(Q\)</span> - query, <span class="math inline">\(K\)</span> - key, <span class="math inline">\(V\)</span> - values, <span class="math inline">\(M\)</span> - mask, <span class="math inline">\({d_k}\)</span> - depth/dimension of the queries and keys (used for scaling down)</p>
<p>You can implement this formula either by <code>trax</code> numpy (trax.math.numpy) or regular <code>numpy</code> but it is recommended to use <code>jnp</code>.</p>
<p>Something to take into consideration is that within trax, the masks are tensors of <code>True/False</code> values not 0's and <span class="math inline">\(-\infty\)</span> as in the previous example. Within the graded function don't think of applying the mask by summing up matrices, instead use <code>jnp.where()</code> and treat the <strong>mask as a tensor of boolean values with <code>False</code> for values that need to be masked and True for the ones that don't.</strong></p>
<p>Also take into account that the real tensors are far more complex than the toy ones you just played with. Because of this avoid using shortened operations such as <code>@</code> for dot product or <code>.T</code> for transposing. Use <code>jnp.matmul()</code> and <code>jnp.swapaxes()</code> instead.</p>
<p>This is the self-attention block for the transformer decoder. Good luck!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: DotProductAttention</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DotProductAttention</span>(<span class="params">query, key, value, mask</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Dot product self-attention.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)</span></span><br><span class="line"><span class="string">        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)</span></span><br><span class="line"><span class="string">        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k</span></span><br><span class="line"><span class="string">        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> query.shape[-<span class="number">1</span>] == key.shape[-<span class="number">1</span>] == value.shape[-<span class="number">1</span>], <span class="string">&quot;Embedding dimensions of q, k, v aren&#x27;t all the same&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    <span class="comment"># Save depth/dimension of the query embedding for scaling down the dot product</span></span><br><span class="line">    depth = query.shape[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate scaled query key dot product according to formula above</span></span><br><span class="line">    dots = jnp.matmul(query, jnp.swapaxes(key, <span class="number">1</span>, <span class="number">2</span>)) / jnp.sqrt(depth)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Apply the mask</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># The &#x27;None&#x27; in this line does not need to be replaced</span></span><br><span class="line">        dots = jnp.where(mask, dots, jnp.full_like(dots, -<span class="number">1e9</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Softmax formula implementation</span></span><br><span class="line">    <span class="comment"># Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers</span></span><br><span class="line">    <span class="comment"># Hint: Last axis should be used and keepdims should be True</span></span><br><span class="line">    <span class="comment"># Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)</span></span><br><span class="line">    logsumexp = trax.fastmath.logsumexp(dots,axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Take exponential of dots minus logsumexp to get softmax</span></span><br><span class="line">    <span class="comment"># Use jnp.exp()</span></span><br><span class="line">    dots = jnp.exp( dots - logsumexp)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Multiply dots by value to get self-attention</span></span><br><span class="line">    <span class="comment"># Use jnp.matmul()</span></span><br><span class="line">    attention = jnp.matmul(dots, value)</span><br><span class="line"></span><br><span class="line">    <span class="comment">## END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> attention</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DotProductAttention(q_with_batch, k_with_batch, v_with_batch, m_bool)</span><br></pre></td></tr></table></figure>
<pre><code>DeviceArray([[[0.8496746 , 0.15032545, 0.8496746 ],
              [1.        , 0.        , 1.        ]]], dtype=float32)</code></pre>
<p><strong>Expected Output:</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">DeviceArray</span>([[[<span class="number">0.8496746</span> , <span class="number">0.15032545</span>, <span class="number">0.8496746</span> ],</span><br><span class="line">              [<span class="number">1.</span>        , <span class="number">0.</span>        , <span class="number">1.</span>        ]]], dtype=float32)</span><br><span class="line">```    </span><br><span class="line"></span><br><span class="line">&lt;a name=<span class="string">&#x27;2.2&#x27;</span>&gt;&lt;/a&gt;</span><br><span class="line"></span><br><span class="line">## <span class="number">2.2</span> Causal Attention</span><br><span class="line"></span><br><span class="line">Now you are going to implement causal attention: multi-headed attention with a mask to attend only to words that occurred before. </span><br><span class="line"></span><br><span class="line">&lt;img src = <span class="string">&quot;causal.png&quot;</span>&gt;</span><br><span class="line"></span><br><span class="line">In the image above, a word can see everything that is before it, but <span class="keyword">not</span> what is after it. To implement causal attention, you will have to transform vectors <span class="keyword">and</span> <span class="keyword">do</span> many reshapes. You will need to implement the functions below.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;a name=<span class="string">&#x27;ex02&#x27;</span>&gt;&lt;/a&gt;</span><br><span class="line">### Exercise <span class="number">02</span></span><br><span class="line"></span><br><span class="line">Implement the following functions that will be needed <span class="keyword">for</span> Causal Attention:</span><br><span class="line"></span><br><span class="line">- &lt;span style=<span class="string">&#x27;color:blue&#x27;</span>&gt; compute_attention_heads &lt;/span&gt;: Gets an input $x$ of <span class="built_in">dimension</span> (batch_size, seqlen, n_heads $\times$ d_head) <span class="keyword">and</span> splits the <span class="built_in">last</span> (depth) dimension <span class="keyword">and</span> stacks it to the zeroth dimension to allow matrix <span class="built_in">multiplication</span> (batch_size $\times$ n_heads, seqlen, d_head).</span><br><span class="line">- &lt;span style=<span class="string">&#x27;color:blue&#x27;</span>&gt; dot_product_self_attention &lt;/span&gt;: Creates a mask matrix with `False` values above the diagonal <span class="keyword">and</span> `True` values below <span class="keyword">and</span> calls DotProductAttention which implements dot product self attention.</span><br><span class="line">- &lt;span style=<span class="string">&#x27;color:blue&#x27;</span>&gt; compute_attention_output &lt;/span&gt;: Undoes compute_attention_heads by splitting <span class="built_in">first</span> (vertical) dimension <span class="keyword">and</span> stacking in the <span class="built_in">last</span> (depth) <span class="built_in">dimension</span> (batch_size, seqlen, n_heads $\times$ d_head). These operations <span class="built_in">concatenate</span> (stack/merge) the heads. </span><br><span class="line"></span><br><span class="line">Next there are some toy tensors which may serve to give you an idea of the data shapes <span class="keyword">and</span> opperations involved in Causal Attention. They are also useful to test out your functions! </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">tensor2d = <span class="built_in">create_tensor</span>(q)</span><br><span class="line"><span class="built_in">display_tensor</span>(tensor2d, <span class="string">&#x27;query matrix (2D tensor)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">tensor4d2b = <span class="built_in">create_tensor</span>([[q, q], [q, q]])</span><br><span class="line"><span class="built_in">display_tensor</span>(tensor4d2b, <span class="string">&#x27;batch of two (multi-head) collections of query matrices (4D tensor)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">tensor3dc = <span class="built_in">create_tensor</span>([jnp.<span class="built_in">concatenate</span>([q, q], axis = <span class="number">-1</span>)])</span><br><span class="line"><span class="built_in">display_tensor</span>(tensor3dc, <span class="string">&#x27;one batch of concatenated heads of query matrices (3d tensor)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">tensor3dc3b = <span class="built_in">create_tensor</span>([jnp.<span class="built_in">concatenate</span>([q, q], axis = <span class="number">-1</span>), jnp.<span class="built_in">concatenate</span>([q, q], axis = <span class="number">-1</span>), jnp.<span class="built_in">concatenate</span>([q, q], axis = <span class="number">-1</span>)])</span><br><span class="line"><span class="built_in">display_tensor</span>(tensor3dc3b, <span class="string">&#x27;three batches of concatenated heads of query matrices (3d tensor)&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<pre><code>query matrix (2D tensor) shape: (2, 3)

[[1 0 0]
 [0 1 0]]

batch of two (multi-head) collections of query matrices (4D tensor) shape: (2, 2, 2, 3)

[[[[1 0 0]
   [0 1 0]]

  [[1 0 0]
   [0 1 0]]]


 [[[1 0 0]
   [0 1 0]]

  [[1 0 0]
   [0 1 0]]]]

one batch of concatenated heads of query matrices (3d tensor) shape: (1, 2, 6)

[[[1 0 0 1 0 0]
  [0 1 0 0 1 0]]]

three batches of concatenated heads of query matrices (3d tensor) shape: (3, 2, 6)

[[[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]]</code></pre>
<p>It is important to know that the following 3 functions would normally be defined within the <code>CausalAttention</code> function further below.</p>
<p>However this makes these functions harder to test. Because of this, these functions are shown individually using a <code>closure</code> (when necessary) that simulates them being inside of the <code>CausalAttention</code> function. This is done because they rely on some variables that can be accessed from within <code>CausalAttention</code>.</p>
<h3 id="support-functions">Support Functions</h3>
<p><span style="color:blue"> compute_attention_heads </span>: Gets an input <span class="math inline">\(x\)</span> of dimension (batch_size, seqlen, n_heads <span class="math inline">\(\times\)</span> d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size <span class="math inline">\(\times\)</span> n_heads, seqlen, d_head).</p>
<p><strong>For the closures you only have to fill the inner function.</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: compute_attention_heads_closure</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_attention_heads_closure</span>(<span class="params">n_heads, d_head</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Function that simulates environment inside CausalAttention function.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_head (int):  dimensionality of heads.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        function: compute_attention_heads function</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_attention_heads</span>(<span class="params">x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Compute the attention heads.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Size of the x&#x27;s batch dimension</span></span><br><span class="line">        batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># Length of the sequence</span></span><br><span class="line">        <span class="comment"># Should be size of x&#x27;s first dimension without counting the batch dim</span></span><br><span class="line">        seqlen = x.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># Reshape x using jnp.reshape()</span></span><br><span class="line">        <span class="comment"># batch_size, seqlen, n_heads*d_head -&gt; batch_size, seqlen, n_heads, d_head</span></span><br><span class="line">        x = jnp.reshape(x, (batch_size, seqlen,n_heads, d_head))</span><br><span class="line">        <span class="comment"># Transpose x using jnp.transpose()</span></span><br><span class="line">        <span class="comment"># batch_size, seqlen, n_heads, d_head -&gt; batch_size, n_heads, seqlen, d_head</span></span><br><span class="line">        <span class="comment"># Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them</span></span><br><span class="line">        x = jnp.transpose(x, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">        <span class="comment"># Reshape x using jnp.reshape()</span></span><br><span class="line">        <span class="comment"># batch_size, n_heads, seqlen, d_head -&gt; batch_size*n_heads, seqlen, d_head</span></span><br><span class="line">        x = jnp.reshape(x, (batch_size*n_heads, seqlen, d_head))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> compute_attention_heads</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">display_tensor(tensor3dc3b, <span class="string">&quot;input tensor&quot;</span>)</span><br><span class="line">result_cah = compute_attention_heads_closure(<span class="number">2</span>,<span class="number">3</span>)(tensor3dc3b)</span><br><span class="line">display_tensor(result_cah, <span class="string">&quot;output tensor&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>input tensor shape: (3, 2, 6)

[[[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]]

output tensor shape: (6, 2, 3)

[[[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]]</code></pre>
<p><strong>Expected Output:</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">input tensor shape: (<span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br><span class="line"></span><br><span class="line">output tensor shape: (<span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br></pre></td></tr></table></figure></p>
<p><span style="color:blue"> dot_product_self_attention </span>: Creates a mask matrix with <code>False</code> values above the diagonal and <code>True</code> values below and calls DotProductAttention which implements dot product self attention.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: dot_product_self_attention</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dot_product_self_attention</span>(<span class="params">q, k, v</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Masked dot product self attention.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        q (jax.interpreters.xla.DeviceArray): queries.</span></span><br><span class="line"><span class="string">        k (jax.interpreters.xla.DeviceArray): keys.</span></span><br><span class="line"><span class="string">        v (jax.interpreters.xla.DeviceArray): values.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Hint: mask size should be equal to L_q. Remember that q has shape (batch_size, L_q, d)</span></span><br><span class="line">    mask_size = q.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)</span></span><br><span class="line">    <span class="comment"># Notice that 1&#x27;s and 0&#x27;s get casted to True/False by setting dtype to jnp.bool_</span></span><br><span class="line">    <span class="comment"># Use jnp.tril() - Lower triangle of an array and jnp.ones()</span></span><br><span class="line">    mask = jnp.tril(jnp.ones((<span class="number">1</span>, mask_size, mask_size), dtype=jnp.bool_), k=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> DotProductAttention(q, k, v, mask)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dot_product_self_attention(q_with_batch, k_with_batch, v_with_batch)</span><br></pre></td></tr></table></figure>
<pre><code>DeviceArray([[[0.        , 1.        , 0.        ],
              [0.8496746 , 0.15032543, 0.8496746 ]]], dtype=float32)</code></pre>
<p><strong>Expected Output:</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">DeviceArray</span>([[[<span class="number">0.</span>        , <span class="number">1.</span>        , <span class="number">0.</span>        ],</span><br><span class="line">              [<span class="number">0.8496746</span> , <span class="number">0.15032543</span>, <span class="number">0.8496746</span> ]]], dtype=float32)</span><br></pre></td></tr></table></figure></p>
<p><span style="color:blue"> compute_attention_output </span>: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads <span class="math inline">\(\times\)</span> d_head). These operations concatenate (stack/merge) the heads.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: compute_attention_output_closure</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_attention_output_closure</span>(<span class="params">n_heads, d_head</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Function that simulates environment inside CausalAttention function.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_head (int):  dimensionality of heads.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        function: compute_attention_output function</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_attention_output</span>(<span class="params">x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Compute the attention output.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Length of the sequence</span></span><br><span class="line">        <span class="comment"># Should be size of x&#x27;s first dimension without counting the batch dim</span></span><br><span class="line">        seqlen = x.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># Reshape x using jnp.reshape() to shape (batch_size, n_heads, seqlen, d_head)</span></span><br><span class="line">        x = jnp.reshape(x,(-<span class="number">1</span>, n_heads, seqlen, d_head))</span><br><span class="line">        <span class="comment"># Transpose x using jnp.transpose() to shape (batch_size, seqlen, n_heads, d_head)</span></span><br><span class="line">        x = jnp.transpose(x, (<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>)) </span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Reshape to allow to concatenate the heads</span></span><br><span class="line">        <span class="keyword">return</span> jnp.reshape(x, (-<span class="number">1</span>, seqlen, n_heads * d_head))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> compute_attention_output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">display_tensor(result_cah, <span class="string">&quot;input tensor&quot;</span>)</span><br><span class="line">result_cao = compute_attention_output_closure(<span class="number">2</span>,<span class="number">3</span>)(result_cah)</span><br><span class="line">display_tensor(result_cao, <span class="string">&quot;output tensor&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>input tensor shape: (6, 2, 3)

[[[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]]

output tensor shape: (3, 2, 6)

[[[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]]</code></pre>
<p><strong>Expected Output:</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">input tensor shape: (<span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br><span class="line"></span><br><span class="line">output tensor shape: (<span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br></pre></td></tr></table></figure></p>
<h3 id="causal-attention-function">Causal Attention Function</h3>
<p>Now it is time for you to put everything together within the <code>CausalAttention</code> or Masked multi-head attention function:</p>
<p><img src = "masked-attention.png"></p>
<p><strong>Instructions:</strong> Implement the causal attention. Your model returns the causal attention through a <span class="math inline">\(tl.Serial\)</span> with the following:</p>
<ul>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch">tl.Branch</a> </span>: consisting of 3 [tl.Dense(d_feature), ComputeAttentionHeads] to account for the queries, keys, and values.</li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn">tl.Fn</a></span>: Takes in dot_product_self_attention function and uses it to compute the dot product using <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, <span class="math inline">\(V\)</span>.</li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn">tl.Fn</a></span>: Takes in compute_attention_output_closure to allow for parallel computing.</li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">tl.Dense</a></span>: Final Dense layer, with dimension <code>d_feature</code>.</li>
</ul>
<p>Remember that in order for trax to properly handle the functions you just defined, they need to be added as layers using the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn"><code>tl.Fn()</code></a> function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: CausalAttention</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CausalAttention</span>(<span class="params">d_feature, </span></span></span><br><span class="line"><span class="params"><span class="function">                    n_heads, </span></span></span><br><span class="line"><span class="params"><span class="function">                    compute_attention_heads_closure=compute_attention_heads_closure,</span></span></span><br><span class="line"><span class="params"><span class="function">                    dot_product_self_attention=dot_product_self_attention,</span></span></span><br><span class="line"><span class="params"><span class="function">                    compute_attention_output_closure=compute_attention_output_closure,</span></span></span><br><span class="line"><span class="params"><span class="function">                    mode=<span class="string">&#x27;train&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer-style multi-headed causal attention.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_feature (int):  dimensionality of feature embedding.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">        compute_attention_heads_closure (function): Closure around compute_attention heads.</span></span><br><span class="line"><span class="string">        dot_product_self_attention (function): dot_product_self_attention function. </span></span><br><span class="line"><span class="string">        compute_attention_output_closure (function): Closure around compute_attention_output. </span></span><br><span class="line"><span class="string">        mode (str): &#x27;train&#x27; or &#x27;eval&#x27;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.layers.combinators.Serial: Multi-headed self-attention model.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> d_feature % n_heads == <span class="number">0</span></span><br><span class="line">    d_head = d_feature // n_heads</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)</span></span><br><span class="line">    <span class="comment"># Since you are dealing with closures you might need to call the outer </span></span><br><span class="line">    <span class="comment"># function with the correct parameters to get the actual uncalled function.</span></span><br><span class="line">    ComputeAttentionHeads = tl.Fn(<span class="string">&#x27;AttnHeads&#x27;</span>, compute_attention_heads_closure(n_heads, d_head), n_out=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tl.Serial(</span><br><span class="line">        tl.Branch( <span class="comment"># creates three towers for one input, takes activations and creates queries keys and values</span></span><br><span class="line">            [tl.Dense(d_feature), ComputeAttentionHeads], <span class="comment"># queries</span></span><br><span class="line">            [tl.Dense(d_feature), ComputeAttentionHeads], <span class="comment"># keys</span></span><br><span class="line">            [tl.Dense(d_feature), ComputeAttentionHeads], <span class="comment"># values</span></span><br><span class="line">        ),</span><br><span class="line">        </span><br><span class="line">        tl.Fn(<span class="string">&#x27;DotProductAttn&#x27;</span>, dot_product_self_attention, n_out=<span class="number">1</span>), <span class="comment"># takes QKV</span></span><br><span class="line">        <span class="comment"># HINT: The second argument to tl.Fn() is an uncalled function</span></span><br><span class="line">        <span class="comment"># Since you are dealing with closures you might need to call the outer </span></span><br><span class="line">        <span class="comment"># function with the correct parameters to get the actual uncalled function.</span></span><br><span class="line">        tl.Fn(<span class="string">&#x27;AttnOutput&#x27;</span>, compute_attention_output_closure(n_heads, d_head), n_out=<span class="number">1</span>), <span class="comment"># to allow for parallel</span></span><br><span class="line">        tl.Dense(d_feature) <span class="comment"># Final dense layer</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Take a look at the causal attention model</span></span><br><span class="line"><span class="built_in">print</span>(CausalAttention(d_feature=<span class="number">512</span>, n_heads=<span class="number">8</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Serial[
  Branch_out3[
    [Dense_512, AttnHeads]
    [Dense_512, AttnHeads]
    [Dense_512, AttnHeads]
  ]
  DotProductAttn_in3
  AttnOutput
  Dense_512
]</code></pre>
<p><strong>Expected Output:</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Serial[</span><br><span class="line">  Branch_out3[</span><br><span class="line">    [Dense_512, AttnHeads]</span><br><span class="line">    [Dense_512, AttnHeads]</span><br><span class="line">    [Dense_512, AttnHeads]</span><br><span class="line">  ]</span><br><span class="line">  DotProductAttn_in3</span><br><span class="line">  AttnOutput</span><br><span class="line">  Dense_512</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p>
<p><a name='2.3'></a></p>
<h2 id="transformer-decoder-block">2.3 Transformer decoder block</h2>
<p>Now that you have implemented the causal part of the transformer, you will implement the transformer decoder block. Concretely you will be implementing this image now.</p>
<p><img src = "transformer_decoder_1.png" style = "height:300px"></p>
<p>To implement this function, you will have to call the <code>CausalAttention</code> or Masked multi-head attention function you implemented above. You will have to add a feedforward which consists of:</p>
<ul>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm">tl.LayerNorm</a> </span>: used to layer normalize</li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">tl.Dense</a> </span>: the dense layer</li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu">ff_activation</a> </span>: feed forward activation (we use ReLu) here.</li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout">tl.Dropout</a> </span>: dropout layer</li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">tl.Dense</a> </span>: dense layer</li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout">tl.Dropout</a> </span>: dropout layer</li>
</ul>
<p>Finally once you implement the feedforward, you can go ahead and implement the entire block using:</p>
<ul>
<li><p><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual">tl.Residual</a> </span>: takes in the tl.LayerNorm(), causal attention block, tl.dropout.</p></li>
<li><p><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual">tl.Residual</a> </span>: takes in the feedforward block you will implement.</p></li>
</ul>
<p><a name='ex03'></a> ### Exercise 03 <strong>Instructions:</strong> Implement the transformer decoder block. Good luck!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: DecoderBlock</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DecoderBlock</span>(<span class="params">d_model, d_ff, n_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout, mode, ff_activation</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns a list of layers that implements a Transformer decoder block.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input is an activation tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model (int):  depth of embedding.</span></span><br><span class="line"><span class="string">        d_ff (int): depth of feed-forward layer.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate (how much to drop out).</span></span><br><span class="line"><span class="string">        mode (str): &#x27;train&#x27; or &#x27;eval&#x27;.</span></span><br><span class="line"><span class="string">        ff_activation (function): the non-linearity in feed-forward layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create masked multi-head attention block using CausalAttention function</span></span><br><span class="line">    causal_attention = CausalAttention( </span><br><span class="line">                        d_model,</span><br><span class="line">                        n_heads=n_heads,</span><br><span class="line">                        mode=mode</span><br><span class="line">                        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create feed-forward block (list) with two dense layers with dropout and input normalized</span></span><br><span class="line">    feed_forward = [ </span><br><span class="line">        <span class="comment"># Normalize layer inputs</span></span><br><span class="line">        tl.LayerNorm(),</span><br><span class="line">        <span class="comment"># Add first feed forward (dense) layer (don&#x27;t forget to set the correct value for n_units)</span></span><br><span class="line">        tl.Dense(d_ff),</span><br><span class="line">        <span class="comment"># Add activation function passed in as a parameter (you need to call it!)</span></span><br><span class="line">        ff_activation(), <span class="comment"># Generally ReLU</span></span><br><span class="line">        <span class="comment"># Add dropout with rate and mode specified (i.e., don&#x27;t use dropout during evaluation)</span></span><br><span class="line">        tl.Dropout(rate=dropout, mode=mode),</span><br><span class="line">        <span class="comment"># Add second feed forward layer (don&#x27;t forget to set the correct value for n_units)</span></span><br><span class="line">        tl.Dense(d_model),</span><br><span class="line">        <span class="comment"># Add dropout with rate and mode specified (i.e., don&#x27;t use dropout during evaluation)</span></span><br><span class="line">        tl.Dropout(rate=dropout, mode=mode)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks</span></span><br><span class="line">    <span class="keyword">return</span> [</span><br><span class="line">      tl.Residual(</span><br><span class="line">          <span class="comment"># Normalize layer input</span></span><br><span class="line">          tl.LayerNorm(),</span><br><span class="line">          <span class="comment"># Add causal attention block previously defined (without parentheses)</span></span><br><span class="line">          causal_attention,</span><br><span class="line">          <span class="comment"># Add dropout with rate and mode specified</span></span><br><span class="line">          tl.Dropout()</span><br><span class="line">        ),</span><br><span class="line">      tl.Residual(</span><br><span class="line">          <span class="comment"># Add feed forward block (without parentheses)</span></span><br><span class="line">          feed_forward</span><br><span class="line">        ),</span><br><span class="line">      ]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Take a look at the decoder block</span></span><br><span class="line"><span class="built_in">print</span>(DecoderBlock(d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, n_heads=<span class="number">8</span>, dropout=<span class="number">0.1</span>, mode=<span class="string">&#x27;train&#x27;</span>, ff_activation=tl.Relu))</span><br></pre></td></tr></table></figure>
<pre><code>[Serial[
  Branch_out2[
    None
    Serial[
      LayerNorm
      Serial[
        Branch_out3[
          [Dense_512, AttnHeads]
          [Dense_512, AttnHeads]
          [Dense_512, AttnHeads]
        ]
        DotProductAttn_in3
        AttnOutput
        Dense_512
      ]
      Dropout
    ]
  ]
  Add_in2
], Serial[
  Branch_out2[
    None
    Serial[
      LayerNorm
      Dense_2048
      Relu
      Dropout
      Dense_512
      Dropout
    ]
  ]
  Add_in2
]]</code></pre>
<p><strong>Expected Output:</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[Serial[</span><br><span class="line">  Branch_out2[</span><br><span class="line">    None</span><br><span class="line">    Serial[</span><br><span class="line">      LayerNorm</span><br><span class="line">      Serial[</span><br><span class="line">        Branch_out3[</span><br><span class="line">          [Dense_512, AttnHeads]</span><br><span class="line">          [Dense_512, AttnHeads]</span><br><span class="line">          [Dense_512, AttnHeads]</span><br><span class="line">        ]</span><br><span class="line">        DotProductAttn_in3</span><br><span class="line">        AttnOutput</span><br><span class="line">        Dense_512</span><br><span class="line">      ]</span><br><span class="line">      Dropout</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">  Add_in2</span><br><span class="line">], Serial[</span><br><span class="line">  Branch_out2[</span><br><span class="line">    None</span><br><span class="line">    Serial[</span><br><span class="line">      LayerNorm</span><br><span class="line">      Dense_2048</span><br><span class="line">      Relu</span><br><span class="line">      Dropout</span><br><span class="line">      Dense_512</span><br><span class="line">      Dropout</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">  Add_in2</span><br><span class="line">]]</span><br></pre></td></tr></table></figure></p>
<p><a name='2.4'></a> ## 2.4 Transformer Language Model</p>
<p>You will now bring it all together. In this part you will use all the subcomponents you previously built to make the final model. Concretely, here is the image you will be implementing. <img src = "transformer_decoder.png" style = "height:400px"></p>
<p><a name='ex04'></a> ### Exercise 04 <strong>Instructions:</strong> Previously you coded the decoder block. Now you will code the transformer language model. Here is what you will need.</p>
<ul>
<li><span style="color:blue"> positional_enconder </span>- a list containing the following layers:
<ul>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding">tl.Embedding</a></li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout">tl.Dropout</a></li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.PositionalEncoding">tl.PositionalEncoding</a></li>
</ul></li>
<li>A list of <code>n_layers</code> <span style="color:blue"> decoder blocks</span>.</li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial">tl.Serial</a>: </span> takes in the following layers or lists of layers:
<ul>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight">tl.ShiftRight</a>: </span>: shift the tensor to the right by padding on axis 1.</li>
<li><span style="color:blue"> positional_encoder </span>: encodes the text positions.</li>
<li><span style="color:blue"> decoder_blocks </span>: the ones you created.</li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm">tl.LayerNorm</a> </span>: a layer norm.</li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">tl.Dense</a> </span>: takes in the vocab_size.</li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax">tl.LogSoftmax</a> </span>: to predict.</li>
</ul></li>
</ul>
<p>Go go go!! You can do it :)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: TransformerLM</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TransformerLM</span>(<span class="params">vocab_size=<span class="number">33300</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                  d_model=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                  d_ff=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                  n_layers=<span class="number">6</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                  n_heads=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                  dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                  max_len=<span class="number">4096</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                  mode=<span class="string">&#x27;train&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                  ff_activation=tl.Relu</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns a Transformer language model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input to the model is a tensor of tokens. (This model uses only the</span></span><br><span class="line"><span class="string">    decoder part of the overall Transformer.)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab_size (int): vocab size.</span></span><br><span class="line"><span class="string">        d_model (int):  depth of embedding.</span></span><br><span class="line"><span class="string">        d_ff (int): depth of feed-forward layer.</span></span><br><span class="line"><span class="string">        n_layers (int): number of decoder layers.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate (how much to drop out).</span></span><br><span class="line"><span class="string">        max_len (int): maximum symbol length for positional encoding.</span></span><br><span class="line"><span class="string">        mode (str): &#x27;train&#x27;, &#x27;eval&#x27; or &#x27;predict&#x27;, predict mode is for fast inference.</span></span><br><span class="line"><span class="string">        ff_activation (function): the non-linearity in feed-forward layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens</span></span><br><span class="line"><span class="string">        to activations over a vocab set.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Embedding inputs and positional encoder</span></span><br><span class="line">    positional_encoder = [ </span><br><span class="line">        <span class="comment"># Add embedding layer of dimension (vocab_size, d_model)</span></span><br><span class="line">        tl.Embedding(vocab_size,d_model),</span><br><span class="line">        <span class="comment"># Use dropout with rate and mode specified</span></span><br><span class="line">        tl.Dropout(rate = dropout, mode = mode),</span><br><span class="line">        <span class="comment"># Add positional encoding layer with maximum input length and mode specified</span></span><br><span class="line">        tl.PositionalEncoding()]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create stack (list) of decoder blocks with n_layers with necessary parameters</span></span><br><span class="line">    decoder_blocks = [ </span><br><span class="line">        DecoderBlock(d_model, d_ff, n_heads,</span><br><span class="line">                 dropout, mode, ff_activation) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create the complete model as written in the figure</span></span><br><span class="line">    <span class="keyword">return</span> tl.Serial(</span><br><span class="line">        <span class="comment"># Use teacher forcing (feed output of previous step to current step)</span></span><br><span class="line">        tl.ShiftRight(), <span class="comment"># Specify the mode!</span></span><br><span class="line">        <span class="comment"># Add positional encoder</span></span><br><span class="line">        positional_encoder,</span><br><span class="line">        <span class="comment"># Add decoder blocks</span></span><br><span class="line">        decoder_blocks,</span><br><span class="line">        <span class="comment"># Normalize layer</span></span><br><span class="line">        tl.LayerNorm(),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add dense layer of vocab_size (since need to select a word to translate to)</span></span><br><span class="line">        <span class="comment"># (a.k.a., logits layer. Note: activation already set by ff_activation)</span></span><br><span class="line">        tl.Dense(vocab_size),</span><br><span class="line">        <span class="comment"># Get probabilities with Logsoftmax</span></span><br><span class="line">        tl.LogSoftmax()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Take a look at the Transformer</span></span><br><span class="line"><span class="built_in">print</span>(TransformerLM(n_layers=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Serial[
  ShiftRight(1)
  Embedding_33300_512
  Dropout
  PositionalEncoding
  Serial[
    Branch_out2[
      None
      Serial[
        LayerNorm
        Serial[
          Branch_out3[
            [Dense_512, AttnHeads]
            [Dense_512, AttnHeads]
            [Dense_512, AttnHeads]
          ]
          DotProductAttn_in3
          AttnOutput
          Dense_512
        ]
        Dropout
      ]
    ]
    Add_in2
  ]
  Serial[
    Branch_out2[
      None
      Serial[
        LayerNorm
        Dense_2048
        Relu
        Dropout
        Dense_512
        Dropout
      ]
    ]
    Add_in2
  ]
  LayerNorm
  Dense_33300
  LogSoftmax
]</code></pre>
<p><strong>Expected Output:</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">Serial[</span><br><span class="line">  <span class="built_in">ShiftRight</span>(<span class="number">1</span>)</span><br><span class="line">  Embedding_33300_512</span><br><span class="line">  Dropout</span><br><span class="line">  PositionalEncoding</span><br><span class="line">  Serial[</span><br><span class="line">    Branch_out2[</span><br><span class="line">      None</span><br><span class="line">      Serial[</span><br><span class="line">        LayerNorm</span><br><span class="line">        Serial[</span><br><span class="line">          Branch_out3[</span><br><span class="line">            [Dense_512, AttnHeads]</span><br><span class="line">            [Dense_512, AttnHeads]</span><br><span class="line">            [Dense_512, AttnHeads]</span><br><span class="line">          ]</span><br><span class="line">          DotProductAttn_in3</span><br><span class="line">          AttnOutput</span><br><span class="line">          Dense_512</span><br><span class="line">        ]</span><br><span class="line">        Dropout</span><br><span class="line">      ]</span><br><span class="line">    ]</span><br><span class="line">    Add_in2</span><br><span class="line">  ]</span><br><span class="line">  Serial[</span><br><span class="line">    Branch_out2[</span><br><span class="line">      None</span><br><span class="line">      Serial[</span><br><span class="line">        LayerNorm</span><br><span class="line">        Dense_2048</span><br><span class="line">        Relu</span><br><span class="line">        Dropout</span><br><span class="line">        Dense_512</span><br><span class="line">        Dropout</span><br><span class="line">      ]</span><br><span class="line">    ]</span><br><span class="line">    Add_in2</span><br><span class="line">  ]</span><br><span class="line">  LayerNorm</span><br><span class="line">  Dense_33300</span><br><span class="line">  LogSoftmax</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p>
<p><a name='3'></a> # Part 3: Training</p>
<p>Now you are going to train your model. As usual, you have to define the cost function, the optimizer, and decide whether you will be training it on a <code>gpu</code> or <code>cpu</code>. In this case, you will train your model on a cpu for a few steps and we will load in a pre-trained model that you can use to predict with your own words.</p>
<p><a name='3.1'></a> ### 3.1 Training the model</p>
<p>You will now write a function that takes in your model and trains it. To train your model you have to decide how many times you want to iterate over the entire data set. Each iteration is defined as an <code>epoch</code>. For each epoch, you have to go over all the data, using your training iterator.</p>
<p><a name='ex05'></a> ### Exercise 05 <strong>Instructions:</strong> Implement the <code>train_model</code> program below to train the neural network above. Here is a list of things you should do:</p>
<ul>
<li>Create the train task by calling <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask"><code>trax.supervised.training.TrainTask</code></a> and pass in the following:
<ul>
<li><span style="color:blue"> labeled_data </span> = train_gen</li>
<li><span style="color:blue"> loss_fn </span> = <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss">tl.CrossEntropyLoss()</a></li>
<li><span style="color:blue"> optimizer </span> = <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam">trax.optimizers.Adam(0.01)</a></li>
<li><span style="color:blue"> lr_schedule </span> = <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.lr_schedules.warmup_and_rsqrt_decay">lr_schedule</a></li>
</ul></li>
<li>Create the eval task by calling <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask"><code>trax.supervised.training.EvalTask</code></a> and pass in the following:
<ul>
<li><span style="color:blue"> labeled_data </span> = eval_gen</li>
<li><span style="color:blue"> metrics </span> = tl.CrossEntropyLoss() and <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy">tl.Accuracy()</a></li>
</ul></li>
<li>Create the training loop by calling <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop"><code>trax.supervised.Training.Loop</code></a> and pass in the following:
<ul>
<li><span style="color:blue"> TransformerLM </span></li>
<li><span style="color:blue"> train_task </span></li>
<li><span style="color:blue"> eval_task </span> = [eval_task]</li>
<li><span style="color:blue"> output_dir</span> = output_dir</li>
</ul></li>
</ul>
<p>You will be using a cross entropy loss, with Adam optimizer. Please read the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/index.html">Trax</a> documentation to get a full understanding.</p>
<p>The training loop that this function returns can be runned using the <code>run()</code> method by passing in the desired number of steps.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> training</span><br><span class="line"></span><br><span class="line"><span class="comment"># UNQ_C8</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: train_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_loop</span>(<span class="params">TransformerLM, train_gen, eval_gen, output_dir = <span class="string">&quot;~/model&quot;</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        TransformerLM (trax.layers.combinators.Serial): The model you are building.</span></span><br><span class="line"><span class="string">        train_gen (generator): Training stream of data.</span></span><br><span class="line"><span class="string">        eval_gen (generator): Evaluation stream of data.</span></span><br><span class="line"><span class="string">        output_dir (str): folder to save your file.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.supervised.training.Loop: Training loop.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    output_dir = os.path.expanduser(output_dir)  <span class="comment"># trainer is an object</span></span><br><span class="line">    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=<span class="number">1000</span>, max_value=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    train_task = training.TrainTask( </span><br><span class="line">      labeled_data=train_gen, <span class="comment"># The training generator</span></span><br><span class="line">      loss_layer= tl.CrossEntropyLoss(), <span class="comment"># Loss function </span></span><br><span class="line">      optimizer= trax.optimizers.Adam(<span class="number">0.01</span>), <span class="comment"># Optimizer (Don&#x27;t forget to set LR to 0.01)</span></span><br><span class="line">      lr_schedule= lr_schedule,</span><br><span class="line">      n_steps_per_checkpoint = <span class="number">10</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    eval_task = training.EvalTask( </span><br><span class="line">      labeled_data=eval_gen, <span class="comment"># The evaluation generator</span></span><br><span class="line">      metrics=[tl.CrossEntropyLoss(),tl.Accuracy()] <span class="comment"># CrossEntropyLoss and Accuracy</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    loop = training.Loop(TransformerLM(d_model=<span class="number">4</span>,</span><br><span class="line">                                       d_ff=<span class="number">16</span>,</span><br><span class="line">                                       n_layers=<span class="number">1</span>,</span><br><span class="line">                                       n_heads=<span class="number">2</span>,</span><br><span class="line">                                       mode=<span class="string">&#x27;train&#x27;</span>),</span><br><span class="line">                         train_task,</span><br><span class="line">                         eval_tasks=[eval_task],</span><br><span class="line">                         output_dir=output_dir)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loop</span><br></pre></td></tr></table></figure>
<p>Notice that the model will be trained for only 10 steps.</p>
<p>Even with this constraint the model with the original default arguments took a very long time to finish. Because of this some parameters are changed when defining the model that is fed into the training loop in the function above.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Should take around 1.5 minutes</span></span><br><span class="line">!rm -f ~/model/model.pkl.gz</span><br><span class="line">loop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream)</span><br><span class="line">loop.run(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Step      1: Ran 1 train steps in 9.11 secs
Step      1: train CrossEntropyLoss |  10.41297626
Step      1: eval  CrossEntropyLoss |  10.41586781
Step      1: eval          Accuracy |  0.00000000

Step     10: Ran 9 train steps in 58.21 secs
Step     10: train CrossEntropyLoss |  10.41278458
Step     10: eval  CrossEntropyLoss |  10.41440201
Step     10: eval          Accuracy |  0.00000000</code></pre>
<p><a name='4'></a> # Part 4: Evaluation</p>
<p><a name='4.1'></a> ### 4.1 Loading in a trained model</p>
<p>In this part you will evaluate by loading in an almost exact version of the model you coded, but we trained it for you to save you time. Please run the cell below to load in the model.</p>
<p>As you may have already noticed the model that you trained and the pretrained model share the same overall architecture but they have different values for some of the parameters:</p>
<p><code>Original (pretrained) model:</code></p>
<pre><code>TransformerLM(vocab_size=33300, d_model=512, d_ff=2048, n_layers=6, n_heads=8, 
               dropout=0.1, max_len=4096, ff_activation=tl.Relu)
               </code></pre>
<p><code>Your model:</code></p>
<pre><code>TransformerLM(d_model=4, d_ff=16, n_layers=1, n_heads=2)</code></pre>
<p><strong>Only the parameters shown for your model were changed. The others stayed the same.</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get the model architecture</span></span><br><span class="line">model = TransformerLM(mode=<span class="string">&#x27;eval&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pre-trained weights</span></span><br><span class="line">model.init_from_file(<span class="string">&#x27;model.pkl.gz&#x27;</span>, weights_only=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><a name='5'></a> # Part 5: Testing with your own input</p>
<p>You will now test your input. You are going to implement greedy decoding. This consists of two functions. The first one allows you to identify the next symbol. It gets the argmax of the output of your model and then returns that index.</p>
<p><a name='ex06'></a> ### Exercise 06 <strong>Instructions:</strong> Implement the next symbol function that takes in the cur_output_tokens and the trained model to return the index of the next word.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C9</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_symbol</span>(<span class="params">cur_output_tokens, model</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns the next symbol for a given sentence.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.</span></span><br><span class="line"><span class="string">        model (trax.layers.combinators.Serial): The transformer model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        int: tokenized symbol.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># current output tokens length</span></span><br><span class="line">    token_length = <span class="built_in">len</span>(cur_output_tokens)</span><br><span class="line">    <span class="comment"># calculate the minimum power of 2 big enough to store token_length</span></span><br><span class="line">    <span class="comment"># HINT: use np.ceil() and np.log2()</span></span><br><span class="line">    <span class="comment"># add 1 to token_length so np.log2() doesn&#x27;t receive 0 when token_length is 0</span></span><br><span class="line">    padded_length = <span class="number">2</span>**<span class="built_in">int</span>(np.ceil(np.log2(token_length + <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Fill cur_output_tokens with 0&#x27;s until it reaches padded_length</span></span><br><span class="line">    padded = cur_output_tokens + [<span class="number">0</span>] * (padded_length - token_length)</span><br><span class="line">    padded_with_batch = np.array(padded)[<span class="literal">None</span>, :] <span class="comment"># Don&#x27;t replace this &#x27;None&#x27;! This is a way of setting the batch dim</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># model expects a tuple containing two padded tensors (with batch)</span></span><br><span class="line">    output, _ = model((padded_with_batch, padded_with_batch)) </span><br><span class="line">    <span class="comment"># HINT: output has shape (1, padded_length, vocab_size)</span></span><br><span class="line">    <span class="comment"># To get log_probs you need to index output with 0 in the first dim</span></span><br><span class="line">    <span class="comment"># token_length in the second dim and all of the entries for the last dim.</span></span><br><span class="line">    log_probs = output[<span class="number">0</span>, token_length, :]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">int</span>(np.argmax(log_probs))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test it out!</span></span><br><span class="line">sentence_test_nxt_symbl = <span class="string">&quot;I want to fly in the sky.&quot;</span></span><br><span class="line">detokenize([next_symbol(tokenize(sentence_test_nxt_symbl)+[<span class="number">0</span>], model)])</span><br></pre></td></tr></table></figure>
<pre><code>&#39;The&#39;</code></pre>
<p><strong>Expected Output:</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;The&#x27;</span></span><br></pre></td></tr></table></figure></p>
<p><a name='5.1'></a> ### 5.1 Greedy decoding</p>
<p>Now you will implement the greedy_decode algorithm that will call the <code>next_symbol</code> function. It takes in the input_sentence, the trained model and returns the decoded sentence.</p>
<p><a name='ex07'></a> ### Exercise 07</p>
<p><strong>Instructions</strong>: Implement the greedy_decode algorithm.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C10</span></span><br><span class="line"><span class="comment"># Decoding functions.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greedy_decode</span>(<span class="params">input_sentence, model</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Greedy decode function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        input_sentence (string): a sentence or article.</span></span><br><span class="line"><span class="string">        model (trax.layers.combinators.Serial): Transformer model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        string: summary of the input.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    <span class="comment"># Use tokenize()</span></span><br><span class="line">    cur_output_tokens = tokenize(input_sentence) + [<span class="number">0</span>]</span><br><span class="line">    generated_output = [] </span><br><span class="line">    cur_output = <span class="number">0</span> </span><br><span class="line">    EOS = <span class="number">1</span> </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> cur_output != EOS:</span><br><span class="line">        <span class="comment"># Get next symbol</span></span><br><span class="line">        cur_output = next_symbol(cur_output_tokens, model)</span><br><span class="line">        <span class="comment"># Append next symbol to original sentence</span></span><br><span class="line">        cur_output_tokens.append(cur_output)</span><br><span class="line">        <span class="comment"># Append next symbol to generated sentence</span></span><br><span class="line">        generated_output.append(cur_output)</span><br><span class="line">        <span class="built_in">print</span>(detokenize(generated_output))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> detokenize(generated_output)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test it out on a sentence!</span></span><br><span class="line">test_sentence = <span class="string">&quot;It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips.&quot;</span></span><br><span class="line"><span class="built_in">print</span>(wrapper.fill(test_sentence), <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(greedy_decode(test_sentence, model))</span><br></pre></td></tr></table></figure>
<pre><code>It was a sunny day when I went to the market to buy some flowers. But
I only found roses, not tulips. 

:
: I
: I just
: I just found
: I just found ros
: I just found roses
: I just found roses,
: I just found roses, not
: I just found roses, not tu
: I just found roses, not tulips
: I just found roses, not tulips
: I just found roses, not tulips.
: I just found roses, not tulips.&lt;EOS&gt;
: I just found roses, not tulips.&lt;EOS&gt;</code></pre>
<p><strong>Expected Output:</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">:</span><br><span class="line">: I</span><br><span class="line">: I just</span><br><span class="line">: I just found</span><br><span class="line">: I just found ros</span><br><span class="line">: I just found roses</span><br><span class="line">: I just found roses,</span><br><span class="line">: I just found roses, <span class="keyword">not</span></span><br><span class="line">: I just found roses, <span class="keyword">not</span> tu</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips.</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips.&lt;EOS&gt;</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips.&lt;EOS&gt;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test it out with a whole article!</span></span><br><span class="line">article = <span class="string">&quot;It‚Äôs the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‚ÄòTebowing‚Äô craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the &#x27;Tebowing&#x27; craze at the school was blocking the hallway and presenting a safety hazard to students.&quot;</span></span><br><span class="line"><span class="built_in">print</span>(wrapper.fill(article), <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(greedy_decode(article, model))</span><br></pre></td></tr></table></figure>
<p><strong>Expected Output:</strong> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Jordan</span><br><span class="line">Jordan Ful</span><br><span class="line">Jordan Fulcol</span><br><span class="line">Jordan Fulcoly</span><br><span class="line">Jordan Fulcoly,</span><br><span class="line">Jordan Fulcoly, Wayne</span><br><span class="line">Jordan Fulcoly, Wayne Dre</span><br><span class="line">Jordan Fulcoly, Wayne Drexe</span><br><span class="line">Jordan Fulcoly, Wayne Drexel</span><br><span class="line">Jordan Fulcoly, Wayne Drexel,</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line"></span><br><span class="line">Final summary:</span><br><span class="line"></span><br><span class="line">Jordan Fulcoly, Wayne Drexel, Tyler Carroll <span class="keyword">and</span> Connor Carroll were</span><br><span class="line">suspended <span class="keyword">for</span> one day. Four students were suspended <span class="keyword">for</span> one day</span><br><span class="line">because they allegedly did <span class="keyword">not</span> heed to warnings that the <span class="string">&#x27;Tebowing&#x27;</span></span><br><span class="line">craze was blocking the hallway <span class="keyword">and</span> presenting a safety hazard to</span><br><span class="line">students.&lt;EOS&gt;</span><br></pre></td></tr></table></figure></p>
<p><strong>Congratulations on finishing this week's assignment!</strong> You did a lot of work and now you should have a better understanding of the encoder part of Transformers and how Transformers can be used for text summarization.</p>
<p><strong>Keep it up!</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Using-RL-to-Solve-Blackjack/2020/09/17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Using-RL-to-Solve-Blackjack/2020/09/17/" class="post-title-link" itemprop="url">Using RL to Solve Blackjack</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-09-17 17:44:04" itemprop="dateCreated datePublished" datetime="2020-09-17T17:44:04+08:00">2020-09-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-31 18:04:17" itemprop="dateModified" datetime="2021-12-31T18:04:17+08:00">2021-12-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Using-RL-to-Solve-Blackjack/2020/09/17/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Using-RL-to-Solve-Blackjack/2020/09/17/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Âº∫ÂåñÂ≠¶‰π†‚Äî‚ÄîMC(ËíôÁâπÂç°Ê¥õ)Áé©21ÁÇπÊâëÂÖãÊ∏∏Êàè</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/Using-RL-to-Solve-Blackjack/2020/09/17/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Generalized-Policy-Iteration/2020/09/10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Generalized-Policy-Iteration/2020/09/10/" class="post-title-link" itemprop="url">Generalized Policy Iteration</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-09-10 14:26:25" itemprop="dateCreated datePublished" datetime="2020-09-10T14:26:25+08:00">2020-09-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-31 15:48:52" itemprop="dateModified" datetime="2021-12-31T15:48:52+08:00">2021-12-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Generalized-Policy-Iteration/2020/09/10/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Generalized-Policy-Iteration/2020/09/10/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Generalized Policy Iteration</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/Generalized-Policy-Iteration/2020/09/10/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">268</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub ‚Üí https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail ‚Üí mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 ‚Äì 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'en' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
