<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PingFang SC:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruochi.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="RUOCHI.AI">
<meta property="og:url" content="https://zhangruochi.com/page/2/index.html">
<meta property="og:site_name" content="RUOCHI.AI">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Ruochi Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhangruochi.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>RUOCHI.AI</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RUOCHI.AI</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Planning-and-learning-with-Tabular-Methods/2020/09/29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Planning-and-learning-with-Tabular-Methods/2020/09/29/" class="post-title-link" itemprop="url">Planning and learning with Tabular Methods</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-09-29 16:09:45 / Modified: 16:19:02" itemprop="dateCreated datePublished" datetime="2020-09-29T16:09:45+08:00">2020-09-29</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Planning-and-learning-with-Tabular-Methods/2020/09/29/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Planning-and-learning-with-Tabular-Methods/2020/09/29/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We develop a unified view of reinforcement learning methods that require a model of the environment, such as dynamic programming and heuristic search, and methods that can be used without a model, such as Monte Carlo and temporal-difference methods. These are respectively called <strong>model-based</strong> and <strong>model-free</strong> reinforcement learning methods. Model-based methods rely on planning as their primary component, while model-free methods primarily rely on learning. Although there are real di↵erences between these two kinds of methods, there are also great similarities.</p>
<ul>
<li>All state-space planning methods involve computing value functions as a key intermediate step toward improving the policy</li>
<li>They compute value functions by updates or backup operations applied to simulated experience.</li>
</ul>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><ul>
<li>By a model of the environment we mean anything that an agent can use to predict how the environment will respond to its actions.</li>
<li>Distribution model produce a description of all possibilities and their probabilities. Sample model produce just one of the possibilities and their probabilities.</li>
<li>当给定一个 state 和一个 action 时，distribution model 可以生成所有可能的状态转移，而sample model只能给出一个可能的状态转移</li>
<li>当给定一个 state 和 Policy 时，distribution model 可以获得所有可能的 episode 并得到他们出现的概率，但 sample model 只能给出一个 episode</li>
</ul>
<p>总之，distribution model 比 sample model包含更多信息，但现实中往往更容易获得sample model。简单来说，distribution model 包含了所有状态的转移概率，但sample model更像是管中窥豹，可见一斑。在DP中，我们用到的是distribution model，而在MC中我们用到的是sample model。model 是对环境的一种表达方式，（不一定是真实或完全正确的），可以用来产生仿真经验（simulation experience）。</p>
<h2 id="Planning"><a href="#Planning" class="headerlink" title="Planning"></a>Planning</h2><p>从Model中生成或提升Policy 的计算过程称为 Planning:</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="1.png" width = "80%" height="80%">
</center>

<p>注意本文讨论的Planning都是state space Planning，这种Planning有两个特点：</p>
<ul>
<li>通过计算values function 来进行Policy 提升</li>
<li>根据simulated experience来计算value function</li>
</ul>
<p>Planning（如DP） 和learning（如MC、TD）方法的核心都是用backing-up 更新公式计算value function 的估计值。区别在于Planning 所用经验是有模型生成的simulated exprience，而learning method使用的经验是由真实环境生成的real exprience。  但两者都满足上述state space Planning结构，这表示很多思想和算法可以相互借鉴，在应用中常常用 learning 中 value function 估计值的更新公式取代 Planning 中的 value function 估计值的更新公式。例如，我们可以将Q learning 和 planning 结合，得到random-sample one-step tabular Q-planning 方法：</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="2.png" width = "80%" height="80%">
</center>

<p>one-step tabular Q-learning最终会收敛到一个对应于真实环境的optimal Policy，而 random-sample one-step tabular Q-planning 则收敛到一个对应于model 的optimal Policy。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Q-Learning-and-Expected-Sarsa/2020/09/29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Q-Learning-and-Expected-Sarsa/2020/09/29/" class="post-title-link" itemprop="url">Q-Learning and Expected Sarsa </a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-09-29 10:52:00" itemprop="dateCreated datePublished" datetime="2020-09-29T10:52:00+08:00">2020-09-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-10-14 17:19:36" itemprop="dateModified" datetime="2020-10-14T17:19:36+08:00">2020-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Q-Learning-and-Expected-Sarsa/2020/09/29/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Q-Learning-and-Expected-Sarsa/2020/09/29/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Assignment-2-Q-Learning-and-Expected-Sarsa"><a href="#Assignment-2-Q-Learning-and-Expected-Sarsa" class="headerlink" title="Assignment 2 - Q-Learning and Expected Sarsa"></a>Assignment 2 - Q-Learning and Expected Sarsa</h1><p>Welcome to Course 2 Programming Assignment 2. In this notebook, you will:</p>
<ol>
<li>Implement Q-Learning with $\epsilon$-greedy action selection</li>
<li>Implement Expected Sarsa with $\epsilon$-greedy action selection</li>
<li>Investigate how these two algorithms behave on Cliff World (described on page 132 of the textbook)</li>
</ol>
<p>We will provide you with the environment and infrastructure to run an experiment (called the experiment program in RL-Glue). This notebook will provide all the code you need to run your experiment and visualise learning performance.</p>
<p>This assignment will be graded automatically by comparing the behavior of your agent to our implementations of Expected Sarsa and Q-learning. The random seed will be set to avoid different behavior due to randomness. We will highlight the functions you have to use for generating random samples and the number of times these functions should be called. </p>
<h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h2><p>You will need the following libraries for this assignment. We are using:</p>
<ol>
<li>numpy: the fundamental package for scientific computing with Python.</li>
<li>scipy: a Python library for scientific and technical computing.</li>
<li>matplotlib: library for plotting graphs in Python.</li>
<li>RL-Glue: library for reinforcement learning experiments.</li>
</ol>
<p>DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> sem</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="keyword">from</span> agent <span class="keyword">import</span> BaseAgent</span><br><span class="line"><span class="keyword">import</span> cliffworld_env</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams.update(&#123;<span class="string">&#x27;font.size&#x27;</span>: <span class="number">15</span>&#125;)</span><br><span class="line">plt.rcParams.update(&#123;<span class="string">&#x27;figure.figsize&#x27;</span>: [<span class="number">10</span>,<span class="number">5</span>]&#125;)</span><br></pre></td></tr></table></figure>

<h2 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><p>In this section you will implement and test a Q-Learning agent with $\epsilon$-greedy action selection (Section 6.5 in the textbook). </p>
<h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>Your job is to implement the updates in the methods agent_step and agent_end. We provide detailed comments in each method describing what your code should do.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningAgent</span>(<span class="params">BaseAgent</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span>(<span class="params">self, agent_init_info</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states (int): The number of states,</span></span><br><span class="line"><span class="string">            num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">            epsilon (float): The epsilon parameter for exploration,</span></span><br><span class="line"><span class="string">            step_size (float): The step-size,</span></span><br><span class="line"><span class="string">            discount (float): The discount factor,</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Store the parameters provided in agent_init_info.</span></span><br><span class="line">        self.num_actions = agent_init_info[<span class="string">&quot;num_actions&quot;</span>]</span><br><span class="line">        self.num_states = agent_init_info[<span class="string">&quot;num_states&quot;</span>]</span><br><span class="line">        self.epsilon = agent_init_info[<span class="string">&quot;epsilon&quot;</span>]</span><br><span class="line">        self.step_size = agent_init_info[<span class="string">&quot;step_size&quot;</span>]</span><br><span class="line">        self.discount = agent_init_info[<span class="string">&quot;discount&quot;</span>]</span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info[<span class="string">&quot;seed&quot;</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create an array for action-value estimates and initialize it to zero.</span></span><br><span class="line">        self.q = np.zeros((self.num_states, self.num_actions)) <span class="comment"># The array of action-value estimates.</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;The first method called when the episode starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment&#x27;s evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the first action the agent takes.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state,:]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, observation</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment&#x27;s step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">                last step.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the action the agent is taking.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state, :]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform an update</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.q[self.prev_state][self.prev_action]  += self.step_size * (reward + self.discount * np.<span class="built_in">max</span>(current_q) - \</span><br><span class="line">                                                                        self.q[self.prev_state][self.prev_action] )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Perform the last update in the episode</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.q[self.prev_state][self.prev_action] += self.step_size * (reward - self.q[self.prev_state][self.prev_action])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">argmax</span>(<span class="params">self, q_values</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;argmax with random tie-breaking</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            q_values (Numpy array): the array of action-values</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): an action with the highest value</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        top = <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>)</span><br><span class="line">        ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(q_values)):</span><br><span class="line">            <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">                top = q_values[i]</span><br><span class="line">                ties = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">                ties.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br></pre></td></tr></table></figure>

<h3 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h3><p>Run the cells below to test the implemented methods. The output of each cell should match the expected output.</p>
<p>Note that passing this test does not guarantee correct behavior on the Cliff World.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">4</span>, <span class="string">&quot;num_states&quot;</span>: <span class="number">3</span>, <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>, <span class="string">&quot;discount&quot;</span>: <span class="number">1.0</span>, <span class="string">&quot;seed&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line">agent = QLearningAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(agent.q == expected_values)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># reset the agent</span></span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.2</span>,  <span class="number">0.</span>,  <span class="number">0.</span>  ],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,   <span class="number">0.</span>,  <span class="number">0.02</span>],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,   <span class="number">0.</span>,  <span class="number">0.</span>  ],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(np.isclose(agent.q, expected_values))</span><br><span class="line"></span><br><span class="line"><span class="comment"># reset the agent</span></span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.2</span>, <span class="number">0.</span>,  <span class="number">0.</span> ],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.1</span>],</span><br><span class="line">    [<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span> ],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(np.isclose(agent.q, expected_values))</span><br></pre></td></tr></table></figure>

<h1 id="Expected-Sarsa"><a href="#Expected-Sarsa" class="headerlink" title="Expected Sarsa"></a>Expected Sarsa</h1><p>In this section you will implement an Expected Sarsa agent with $\epsilon$-greedy action selection (Section 6.6 in the textbook). </p>
<h3 id="Implementation-1"><a href="#Implementation-1" class="headerlink" title="Implementation"></a>Implementation</h3><p>Your job is to implement the updates in the methods agent_step and agent_end. We provide detailed comments in each method describing what your code should do.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExpectedSarsaAgent</span>(<span class="params">BaseAgent</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span>(<span class="params">self, agent_init_info</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Setup for the agent called when the experiment first starts.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            num_states (int): The number of states,</span></span><br><span class="line"><span class="string">            num_actions (int): The number of actions,</span></span><br><span class="line"><span class="string">            epsilon (float): The epsilon parameter for exploration,</span></span><br><span class="line"><span class="string">            step_size (float): The step-size,</span></span><br><span class="line"><span class="string">            discount (float): The discount factor,</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Store the parameters provided in agent_init_info.</span></span><br><span class="line">        self.num_actions = agent_init_info[<span class="string">&quot;num_actions&quot;</span>]</span><br><span class="line">        self.num_states = agent_init_info[<span class="string">&quot;num_states&quot;</span>]</span><br><span class="line">        self.epsilon = agent_init_info[<span class="string">&quot;epsilon&quot;</span>]</span><br><span class="line">        self.step_size = agent_init_info[<span class="string">&quot;step_size&quot;</span>]</span><br><span class="line">        self.discount = agent_init_info[<span class="string">&quot;discount&quot;</span>]</span><br><span class="line">        self.rand_generator = np.random.RandomState(agent_info[<span class="string">&quot;seed&quot;</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Create an array for action-value estimates and initialize it to zero.</span></span><br><span class="line">        self.q = np.zeros((self.num_states, self.num_actions)) <span class="comment"># The array of action-value estimates.</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;The first method called when the episode starts, called after</span></span><br><span class="line"><span class="string">        the environment starts.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment&#x27;s evn_start function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the first action the agent takes.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state, :]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, observation</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;A step taken by the agent.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">            observation (int): the state observation from the</span></span><br><span class="line"><span class="string">                environment&#x27;s step based on where the agent ended up after the</span></span><br><span class="line"><span class="string">                last step.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): the action the agent is taking.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Choose action using epsilon greedy.</span></span><br><span class="line">        state = observation</span><br><span class="line">        current_q = self.q[state,:]</span><br><span class="line">        <span class="keyword">if</span> self.rand_generator.rand() &lt; self.epsilon:</span><br><span class="line">            action = self.rand_generator.randint(self.num_actions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = self.argmax(current_q)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform an update</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        </span><br><span class="line">        expect = (<span class="number">1</span> - self.epsilon) * np.<span class="built_in">max</span>(current_q) + self.epsilon * np.average(current_q)</span><br><span class="line">        self.q[self.prev_state][self.prev_action] += self.step_size * (reward + self.discount * expect - self.q[self.prev_state][self.prev_action])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">        self.prev_state = state</span><br><span class="line">        self.prev_action = action</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Run when the agent terminates.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            reward (float): the reward the agent received for entering the</span></span><br><span class="line"><span class="string">                terminal state.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Perform the last update in the episode</span></span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        <span class="comment"># your code here</span></span><br><span class="line">        self.q[self.prev_state][self.prev_action] += self.step_size * (reward - self.q[self.prev_state][self.prev_action])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># --------------------------</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">argmax</span>(<span class="params">self, q_values</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;argmax with random tie-breaking</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            q_values (Numpy array): the array of action-values</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            action (int): an action with the highest value</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        top = <span class="built_in">float</span>(<span class="string">&quot;-inf&quot;</span>)</span><br><span class="line">        ties = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(q_values)):</span><br><span class="line">            <span class="keyword">if</span> q_values[i] &gt; top:</span><br><span class="line">                top = q_values[i]</span><br><span class="line">                ties = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> q_values[i] == top:</span><br><span class="line">                ties.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.rand_generator.choice(ties)</span><br></pre></td></tr></table></figure>

<h3 id="Test-1"><a href="#Test-1" class="headerlink" title="Test"></a>Test</h3><p>Run the cells below to test the implemented methods. The output of each cell should match the expected output.</p>
<p>Note that passing this test does not guarantee correct behavior on the Cliff World.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">4</span>, <span class="string">&quot;num_states&quot;</span>: <span class="number">3</span>, <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>, <span class="string">&quot;discount&quot;</span>: <span class="number">1.0</span>, <span class="string">&quot;seed&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line">agent = ExpectedSarsaAgent()</span><br><span class="line">agent.agent_init(agent_info)</span><br><span class="line"></span><br><span class="line">action = agent.agent_start(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(agent.q == expected_values)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># test agent step</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">3</span></span><br><span class="line"></span><br><span class="line">action = agent.agent_step(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> action == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.0185</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(np.isclose(agent.q, expected_values))</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># test agent end</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"></span><br><span class="line">agent.agent_end(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">expected_values = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.28</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.0185</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(np.isclose(agent.q, expected_values))</span><br></pre></td></tr></table></figure>

<h1 id="Solving-the-Cliff-World"><a href="#Solving-the-Cliff-World" class="headerlink" title="Solving the Cliff World"></a>Solving the Cliff World</h1><p>We described the Cliff World environment in the video “Expected Sarsa in the Cliff World” in Lesson 3. This is an undiscounted episodic task and thus we set $\gamma$=1. The agent starts in the bottom left corner of the gridworld below and takes actions that move it in the four directions. Actions that would move the agent off of the cliff incur a reward of -100 and send the agent back to the start state. The reward for all other transitions is -1. An episode terminates when the agent reaches the bottom right corner. </p>
<img src="cliffworld.png" alt="Drawing" style="width: 600px;"/>


<p>Using the experiment program in the cell below we now compare the agents on the Cliff World environment and plot the sum of rewards during each episode for the two agents.</p>
<p>The result of this cell will be graded. If you make any changes to your algorithms, you have to run this cell again before submitting the assignment.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">agents = &#123;</span><br><span class="line">    <span class="string">&quot;Q-learning&quot;</span>: QLearningAgent,</span><br><span class="line">    <span class="string">&quot;Expected Sarsa&quot;</span>: ExpectedSarsaAgent</span><br><span class="line">&#125;</span><br><span class="line">env = cliffworld_env.Environment</span><br><span class="line">all_reward_sums = &#123;&#125; <span class="comment"># Contains sum of rewards during episode</span></span><br><span class="line">all_state_visits = &#123;&#125; <span class="comment"># Contains state visit counts during the last 10 episodes</span></span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">4</span>, <span class="string">&quot;num_states&quot;</span>: <span class="number">48</span>, <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, <span class="string">&quot;step_size&quot;</span>: <span class="number">0.5</span>, <span class="string">&quot;discount&quot;</span>: <span class="number">1.0</span>&#125;</span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line">num_runs = <span class="number">100</span> <span class="comment"># The number of runs</span></span><br><span class="line">num_episodes = <span class="number">100</span> <span class="comment"># The number of episodes in each run</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> algorithm <span class="keyword">in</span> [<span class="string">&quot;Q-learning&quot;</span>, <span class="string">&quot;Expected Sarsa&quot;</span>]:</span><br><span class="line">    all_reward_sums[algorithm] = []</span><br><span class="line">    all_state_visits[algorithm] = []</span><br><span class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_runs)):</span><br><span class="line">        agent_info[<span class="string">&quot;seed&quot;</span>] = run</span><br><span class="line">        rl_glue = RLGlue(env, agents[algorithm])</span><br><span class="line">        rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line">        reward_sums = []</span><br><span class="line">        state_visits = np.zeros(<span class="number">48</span>)</span><br><span class="line">        last_episode_total_reward = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(num_episodes):</span><br><span class="line">            <span class="keyword">if</span> episode &lt; num_episodes - <span class="number">10</span>:</span><br><span class="line">                <span class="comment"># Runs an episode</span></span><br><span class="line">                rl_glue.rl_episode(<span class="number">10000</span>) </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                <span class="comment"># Runs an episode while keeping track of visited states</span></span><br><span class="line">                state, action = rl_glue.rl_start()</span><br><span class="line">                state_visits[state] += <span class="number">1</span></span><br><span class="line">                is_terminal = <span class="literal">False</span></span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">not</span> is_terminal:</span><br><span class="line">                    reward, state, action, is_terminal = rl_glue.rl_step()</span><br><span class="line">                    state_visits[state] += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">            reward_sums.append(rl_glue.rl_return() - last_episode_total_reward)</span><br><span class="line">            last_episode_total_reward = rl_glue.rl_return()</span><br><span class="line">            </span><br><span class="line">        all_reward_sums[algorithm].append(reward_sums)</span><br><span class="line">        all_state_visits[algorithm].append(state_visits)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot results</span></span><br><span class="line"><span class="keyword">for</span> algorithm <span class="keyword">in</span> [<span class="string">&quot;Q-learning&quot;</span>, <span class="string">&quot;Expected Sarsa&quot;</span>]:</span><br><span class="line">    plt.plot(np.mean(all_reward_sums[algorithm], axis=<span class="number">0</span>), label=algorithm)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Episodes&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Sum of\n rewards\n during\n episode&quot;</span>,rotation=<span class="number">0</span>, labelpad=<span class="number">40</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>,<span class="number">100</span>)</span><br><span class="line">plt.ylim(-<span class="number">30</span>,<span class="number">0</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 100/100 [00:12&lt;00:00,  8.12it/s]
100%|██████████| 100/100 [00:16&lt;00:00,  6.15it/s]
</code></pre>
<p><img src="output_26_1.png" alt="png"></p>
<p>To see why these two agents behave differently, let’s inspect the states they visit most. Run the cell below to generate plots showing the number of timesteps that the agents spent in each state over the last 10 episodes.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> algorithm, position <span class="keyword">in</span> [(<span class="string">&quot;Q-learning&quot;</span>, <span class="number">211</span>), (<span class="string">&quot;Expected Sarsa&quot;</span>, <span class="number">212</span>)]:</span><br><span class="line">    plt.subplot(position)</span><br><span class="line">    average_state_visits = np.array(all_state_visits[algorithm]).mean(axis=<span class="number">0</span>)</span><br><span class="line">    grid_state_visits = average_state_visits.reshape((<span class="number">4</span>,<span class="number">12</span>))</span><br><span class="line">    grid_state_visits[<span class="number">0</span>,<span class="number">1</span>:-<span class="number">1</span>] = np.nan</span><br><span class="line">    plt.pcolormesh(grid_state_visits, edgecolors=<span class="string">&#x27;gray&#x27;</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.title(algorithm)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    cm = plt.get_cmap()</span><br><span class="line">    cm.set_bad(<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplots_adjust(bottom=<span class="number">0.0</span>, right=<span class="number">0.7</span>, top=<span class="number">1.0</span>)</span><br><span class="line">    cax = plt.axes([<span class="number">0.85</span>, <span class="number">0.0</span>, <span class="number">0.075</span>, <span class="number">1.</span>])</span><br><span class="line">    </span><br><span class="line">cbar = plt.colorbar(cax=cax)</span><br><span class="line">cbar.ax.set_ylabel(<span class="string">&quot;Visits during\n the last 10\n episodes&quot;</span>, rotation=<span class="number">0</span>, labelpad=<span class="number">70</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_28_0.png" alt="png"></p>
<p>The Q-learning agent learns the optimal policy, one that moves along the cliff and reaches the goal in as few steps as possible. However, since the agent does not follow the optimal policy and uses $\epsilon$-greedy exploration, it occasionally falls off the cliff. The Expected Sarsa agent takes exploration into account and follows a safer path.</p>
<p>Previously we used a fixed step-size of 0.5 for the agents. What happens with other step-sizes? Does this difference in performance persist?</p>
<p>In the next experiment we will try 10 different step-sizes from 0.1 to 1.0 and compare the sum of rewards per episode averaged over the first 100 episodes (similar to the interim performance curves in Figure 6.3 of the textbook). Shaded regions show standard errors.</p>
<p>This cell takes around 10 minutes to run. The result of this cell will be graded. If you make any changes to your algorithms, you have to run this cell again before submitting the assignment.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product</span><br><span class="line"></span><br><span class="line">agents = &#123;</span><br><span class="line">    <span class="string">&quot;Q-learning&quot;</span>: QLearningAgent,</span><br><span class="line">    <span class="string">&quot;Expected Sarsa&quot;</span>: ExpectedSarsaAgent</span><br><span class="line">&#125;</span><br><span class="line">env = cliffworld_env.Environment</span><br><span class="line">all_reward_sums = &#123;&#125;</span><br><span class="line">step_sizes = np.linspace(<span class="number">0.1</span>,<span class="number">1.0</span>,<span class="number">10</span>)</span><br><span class="line">agent_info = &#123;<span class="string">&quot;num_actions&quot;</span>: <span class="number">4</span>, <span class="string">&quot;num_states&quot;</span>: <span class="number">48</span>, <span class="string">&quot;epsilon&quot;</span>: <span class="number">0.1</span>, <span class="string">&quot;discount&quot;</span>: <span class="number">1.0</span>&#125;</span><br><span class="line">env_info = &#123;&#125;</span><br><span class="line">num_runs = <span class="number">30</span></span><br><span class="line">num_episodes = <span class="number">100</span></span><br><span class="line">all_reward_sums = &#123;&#125;</span><br><span class="line"></span><br><span class="line">algorithms = [<span class="string">&quot;Q-learning&quot;</span>, <span class="string">&quot;Expected Sarsa&quot;</span>]</span><br><span class="line">cross_product = <span class="built_in">list</span>(product(algorithms, step_sizes, <span class="built_in">range</span>(num_runs)))</span><br><span class="line"><span class="keyword">for</span> algorithm, step_size, run <span class="keyword">in</span> tqdm(cross_product):</span><br><span class="line">    <span class="keyword">if</span> (algorithm, step_size) <span class="keyword">not</span> <span class="keyword">in</span> all_reward_sums:</span><br><span class="line">        all_reward_sums[(algorithm, step_size)] = []</span><br><span class="line"></span><br><span class="line">    agent_info[<span class="string">&quot;step_size&quot;</span>] = step_size</span><br><span class="line">    agent_info[<span class="string">&quot;seed&quot;</span>] = run</span><br><span class="line">    rl_glue = RLGlue(env, agents[algorithm])</span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line">    last_episode_total_reward = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(num_episodes):</span><br><span class="line">        rl_glue.rl_episode(<span class="number">0</span>)</span><br><span class="line">    all_reward_sums[(algorithm, step_size)].append(rl_glue.rl_return()/num_episodes)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> algorithm <span class="keyword">in</span> [<span class="string">&quot;Q-learning&quot;</span>, <span class="string">&quot;Expected Sarsa&quot;</span>]:</span><br><span class="line">    algorithm_means = np.array([np.mean(all_reward_sums[(algorithm, step_size)]) <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes])</span><br><span class="line">    algorithm_stds = np.array([sem(all_reward_sums[(algorithm, step_size)]) <span class="keyword">for</span> step_size <span class="keyword">in</span> step_sizes])</span><br><span class="line">    plt.plot(step_sizes, algorithm_means, marker=<span class="string">&#x27;o&#x27;</span>, linestyle=<span class="string">&#x27;solid&#x27;</span>, label=algorithm)</span><br><span class="line">    plt.fill_between(step_sizes, algorithm_means + algorithm_stds, algorithm_means - algorithm_stds, alpha=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">&quot;Step-size&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Sum of\n rewards\n per episode&quot;</span>,rotation=<span class="number">0</span>, labelpad=<span class="number">50</span>)</span><br><span class="line">plt.xticks(step_sizes)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>100%|██████████| 600/600 [01:38&lt;00:00,  6.08it/s]
</code></pre>
<p><img src="output_30_1.png" alt="png"></p>
<p>Expected Sarsa shows an advantage over Q-learning in this problem across a wide range of step-sizes.</p>
<p>Congratulations! Now you have:</p>
<ul>
<li>implemented Q-Learning with $\epsilon$-greedy action selection</li>
<li>implemented Expected Sarsa with $\epsilon$-greedy action selection</li>
<li>investigated the behavior of these two algorithms on Cliff World</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Policy-Evaluation-in-Cliff-Walking-Environment/2020/09/28/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Policy-Evaluation-in-Cliff-Walking-Environment/2020/09/28/" class="post-title-link" itemprop="url">Policy Evaluation in Cliff Walking Environment</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-09-28 16:04:11 / Modified: 16:06:19" itemprop="dateCreated datePublished" datetime="2020-09-28T16:04:11+08:00">2020-09-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Policy-Evaluation-in-Cliff-Walking-Environment/2020/09/28/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Policy-Evaluation-in-Cliff-Walking-Environment/2020/09/28/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Assignment-Policy-Evaluation-in-Cliff-Walking-Environment"><a href="#Assignment-Policy-Evaluation-in-Cliff-Walking-Environment" class="headerlink" title="Assignment: Policy Evaluation in Cliff Walking Environment"></a>Assignment: Policy Evaluation in Cliff Walking Environment</h1><p>Welcome to the Course 2 Module 2 Programming Assignment! In this assignment, you will implement one of the fundamental sample and bootstrapping based model free reinforcement learning agents for prediction. This is namely one that uses one-step temporal difference learning, also known as TD(0). The task is to design an agent for policy evaluation in the Cliff Walking environment. Recall that policy evaluation is the prediction problem where the goal is to accurately estimate the values of states given some policy.</p>
<h3 id="Learning-Objectives"><a href="#Learning-Objectives" class="headerlink" title="Learning Objectives"></a>Learning Objectives</h3><ul>
<li>Implement parts of the Cliff Walking environment, to get experience specifying MDPs [Section 1].</li>
<li>Implement an agent that uses bootstrapping and, particularly, TD(0) [Section 2].</li>
<li>Apply TD(0) to estimate value functions for different policies, i.e., run policy evaluation experiments [Section 3].</li>
</ul>
<h2 id="The-Cliff-Walking-Environment"><a href="#The-Cliff-Walking-Environment" class="headerlink" title="The Cliff Walking Environment"></a>The Cliff Walking Environment</h2><p>The Cliff Walking environment is a gridworld with a discrete state space and discrete action space. The agent starts at grid cell S. The agent can move (deterministically) to the four neighboring cells by taking actions Up, Down, Left or Right. Trying to move out of the boundary results in staying in the same location. So, for example, trying to move left when at a cell on the leftmost column results in no movement at all and the agent remains in the same location. The agent receives -1 reward per step in most states, and -100 reward when falling off of the cliff. This is an episodic task; termination occurs when the agent reaches the goal grid cell G. Falling off of the cliff results in resetting to the start state, without termination.</p>
<p>The diagram below showcases the description above and also illustrates two of the policies we will be evaluating.</p>
<img src="cliffwalk.png" style="height:400px">

<h2 id="Packages"><a href="#Packages" class="headerlink" title="Packages."></a>Packages.</h2><p>We import the following libraries that are required for this assignment. We shall be using the following libraries:</p>
<ol>
<li>jdc: Jupyter magic that allows defining classes over multiple jupyter notebook cells.</li>
<li>numpy: the fundamental package for scientific computing with Python.</li>
<li>matplotlib: the library for plotting graphs in Python.</li>
<li>RL-Glue: the library for reinforcement learning experiments.</li>
<li>BaseEnvironment, BaseAgent: the base classes from which we will inherit when creating the environment and agent classes in order for them to support the RL-Glue framework.</li>
<li>Manager: the file allowing for visualization and testing.</li>
<li>itertools.product: the function that can be used easily to compute permutations.</li>
<li>tqdm.tqdm: Provides progress bars for visualizing the status of loops.</li>
</ol>
<p><strong>Please do not import other libraries</strong> — this will break the autograder.</p>
<p><strong>NOTE: For this notebook, there is no need to make any calls to methods of random number generators. Spurious or missing calls to random number generators may affect your results.</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> jdc</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> rl_glue <span class="keyword">import</span> RLGlue</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> Agent <span class="keyword">import</span> BaseAgent </span><br><span class="line"><span class="keyword">from</span> Environment <span class="keyword">import</span> BaseEnvironment  </span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> manager <span class="keyword">import</span> Manager</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> product</span><br><span class="line"><span class="comment"># --</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure>

<h2 id="Section-1-Environment"><a href="#Section-1-Environment" class="headerlink" title="Section 1. Environment"></a>Section 1. Environment</h2><p>In the first part of this assignment, you will get to see how the Cliff Walking environment is implemented. You will also get to implement parts of it to aid your understanding of the environment and more generally how MDPs are specified. In particular, you will implement the logic for:</p>
<ol>
<li>Converting 2-dimensional coordinates to a single index for the state,</li>
<li>One of the actions (action up), and,</li>
<li>Reward and termination.</li>
</ol>
<p>Given below is an annotated diagram of the environment with more details that may help in completing the tasks of this part of the assignment. Note that we will be creating a more general environment where the height and width positions can be variable but the start, goal and cliff grid cells have the same relative positions (bottom left, bottom right and the cells between the start and goal grid cells respectively).</p>
<img src="cliffwalk-annotated.png" style="height:400px">

<p>Once you have gone through the code and begun implementing solutions, it may be a good idea to come back here and see if you can convince yourself that the diagram above is an accurate representation of the code given and the code you have written.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create empty CliffWalkEnvironment class.</span></span><br><span class="line"><span class="comment"># These methods will be filled in later cells.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CliffWalkEnvironment</span>(<span class="params">BaseEnvironment</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_init</span>(<span class="params">self, agent_info=&#123;&#125;</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_start</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_step</span>(<span class="params">self, reward, state</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_end</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">env_cleanup</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># helper method</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">state</span>(<span class="params">self, loc</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>

<h2 id="env-init"><a href="#env-init" class="headerlink" title="env_init()"></a>env_init()</h2><p>The first function we add to the environment is the initialization function which is called once when an environment object is created. In this function, the grid dimensions and special locations (start and goal locations and the cliff locations) are stored for easy use later.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_init</span>(<span class="params">self, env_info=&#123;&#125;</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Setup for the environment called when the experiment first starts.</span></span><br><span class="line"><span class="string">        Note:</span></span><br><span class="line"><span class="string">            Initialize a tuple with the reward, first state, boolean</span></span><br><span class="line"><span class="string">            indicating if it&#x27;s terminal.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Note, we can setup the following variables later, in env_start() as it is equivalent. </span></span><br><span class="line">        <span class="comment"># Code is left here to adhere to the note above, but these variables are initialized once more</span></span><br><span class="line">        <span class="comment"># in env_start() [See the env_start() function below.]</span></span><br><span class="line">        </span><br><span class="line">        reward = <span class="literal">None</span></span><br><span class="line">        state = <span class="literal">None</span> <span class="comment"># See Aside</span></span><br><span class="line">        termination = <span class="literal">None</span></span><br><span class="line">        self.reward_state_term = (reward, state, termination)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># AN ASIDE: Observation is a general term used in the RL-Glue files that can be interachangeably </span></span><br><span class="line">        <span class="comment"># used with the term &quot;state&quot; for our purposes and for this assignment in particular. </span></span><br><span class="line">        <span class="comment"># A difference arises in the use of the terms when we have what is called Partial Observability where </span></span><br><span class="line">        <span class="comment"># the environment may return states that may not fully represent all the information needed to </span></span><br><span class="line">        <span class="comment"># predict values or make decisions (i.e., the environment is non-Markovian.)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set the default height to 4 and width to 12 (as in the diagram given above)</span></span><br><span class="line">        self.grid_h = env_info.get(<span class="string">&quot;grid_height&quot;</span>, <span class="number">4</span>) </span><br><span class="line">        self.grid_w = env_info.get(<span class="string">&quot;grid_width&quot;</span>, <span class="number">12</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Now, we can define a frame of reference. Let positive x be towards the direction down and </span></span><br><span class="line">        <span class="comment"># positive y be towards the direction right (following the row-major NumPy convention.)</span></span><br><span class="line">        <span class="comment"># Then, keeping with the usual convention that arrays are 0-indexed, max x is then grid_h - 1 </span></span><br><span class="line">        <span class="comment"># and max y is then grid_w - 1. So, we have:</span></span><br><span class="line">        <span class="comment"># Starting location of agent is the bottom-left corner, (max x, min y). </span></span><br><span class="line">        self.start_loc = (self.grid_h - <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># Goal location is the bottom-right corner. (max x, max y).</span></span><br><span class="line">        self.goal_loc = (self.grid_h - <span class="number">1</span>, self.grid_w - <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># The cliff will contain all the cells between the start_loc and goal_loc.</span></span><br><span class="line">        self.cliff = [(self.grid_h - <span class="number">1</span>, i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, (self.grid_w - <span class="number">1</span>))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Take a look at the annotated environment diagram given in the above Jupyter Notebook cell to </span></span><br><span class="line">        <span class="comment"># verify that your understanding of the above code is correct for the default case, i.e., where </span></span><br><span class="line">        <span class="comment"># height = 4 and width = 12.</span></span><br></pre></td></tr></table></figure>

<h2 id="Implement-state"><a href="#Implement-state" class="headerlink" title="Implement state()"></a><em>Implement</em> state()</h2><p>The agent location can be described as a two-tuple or coordinate (x, y) describing the agent’s position.<br>However, we can convert the (x, y) tuple into a single index and provide agents with just this integer.<br>One reason for this choice is that the spatial aspect of the problem is secondary and there is no need<br>for the agent to know about the exact dimensions of the environment.<br>From the agent’s viewpoint, it is just perceiving some states, accessing their corresponding values<br>in a table, and updating them. Both the coordinate (x, y) state representation and the converted coordinate representation are thus equivalent in this sense.</p>
<p>Given a grid cell location, the state() function should return the state; a single index corresponding to the location in the grid.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Example: Suppose grid_h is 2 and grid_w is 2. Then, we can write the grid cell two-tuple or coordinate</span><br><span class="line">states as follows (following the usual 0-index convention):</span><br><span class="line">|(0, 0) (0, 1)| |0 1|</span><br><span class="line">|(1, 0) (1, 1)| |2 3|</span><br><span class="line">Assuming row-major order as NumPy does,  we can flatten the latter to get a vector [0 1 2 3].</span><br><span class="line">So, if loc = (0, 0) we return 0. While, for loc = (1, 1) we return 3.</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"><span class="comment">#GRADED FUNCTION: [state]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Modify the return statement of this function to return a correct single index as </span></span><br><span class="line"><span class="comment"># the state (see the logic for this in the previous cell.)</span></span><br><span class="line"><span class="comment"># Lines: 1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">state</span>(<span class="params">self, loc</span>):</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> loc[<span class="number">0</span>] * <span class="number">12</span> + loc[<span class="number">1</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR STATE (5 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below corresponds to the annotated diagram for the environment</span></span><br><span class="line"><span class="comment">#       given previously and is limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_state</span>():</span></span><br><span class="line">    env = CliffWalkEnvironment()</span><br><span class="line">    env.env_init(&#123;<span class="string">&quot;grid_height&quot;</span>: <span class="number">4</span>, <span class="string">&quot;grid_width&quot;</span>: <span class="number">12</span>&#125;)</span><br><span class="line">    coords_to_test = [(<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">11</span>), (<span class="number">1</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">0</span>), (<span class="number">3</span>, <span class="number">9</span>), (<span class="number">3</span>, <span class="number">11</span>)]</span><br><span class="line">    true_states = [<span class="number">0</span>, <span class="number">11</span>, <span class="number">17</span>, <span class="number">36</span>, <span class="number">45</span>, <span class="number">47</span>]</span><br><span class="line">    output_states = [env.state(coords) <span class="keyword">for</span> coords <span class="keyword">in</span> coords_to_test]</span><br><span class="line">    <span class="keyword">assert</span>(output_states == true_states)</span><br><span class="line">test_state()</span><br></pre></td></tr></table></figure>

<h2 id="env-start"><a href="#env-start" class="headerlink" title="env_start()"></a>env_start()</h2><p>In env_start(), we initialize the agent location to be the start location and return the state corresponding to it as the first state for the agent to act upon. Additionally, we also set the reward and termination terms to be 0 and False respectively as they are consistent with the notion that there is no reward nor termination before the first action is even taken.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_start</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;The first method called when the episode starts, called before the</span></span><br><span class="line"><span class="string">    agent starts.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The first state from the environment.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    reward = <span class="number">0</span></span><br><span class="line">    <span class="comment"># agent_loc will hold the current location of the agent</span></span><br><span class="line">    self.agent_loc = self.start_loc</span><br><span class="line">    <span class="comment"># state is the one dimensional state representation of the agent location.</span></span><br><span class="line">    state = self.state(self.agent_loc)</span><br><span class="line">    termination = <span class="literal">False</span></span><br><span class="line">    self.reward_state_term = (reward, state, termination)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.reward_state_term[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<h2 id="Implement-env-step"><a href="#Implement-env-step" class="headerlink" title="Implement env_step()"></a><em>Implement</em> env_step()</h2><p>Once an action is taken by the agent, the environment must provide a new state, reward and termination signal. </p>
<p>In the Cliff Walking environment, agents move around using a 4-cell neighborhood called the Von Neumann neighborhood (<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Von_Neumann_neighborhood">https://en.wikipedia.org/wiki/Von_Neumann_neighborhood</a>). Thus, the agent has 4 available actions at each state. Three of the actions have been implemented for you and your first task is to implement the logic for the fourth action (Action UP).</p>
<p>Your second task for this function is to implement the reward logic. Look over the environment description given earlier in this notebook if you need a refresher for how the reward signal is defined.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"><span class="comment">#GRADED FUNCTION: [env_step]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Fill in the code for action UP and implement the logic for reward and termination.</span></span><br><span class="line"><span class="comment"># Lines: ~7.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_step</span>(<span class="params">self, action</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;A step taken by the environment.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        action: The action taken by the agent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (float, state, Boolean): a tuple of the reward, state,</span></span><br><span class="line"><span class="string">            and boolean indicating if it&#x27;s terminal.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> action == <span class="number">0</span>: <span class="comment"># UP (Task 1)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        <span class="comment"># Hint: Look at the code given for the other actions and think about the logic in them.</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>] - <span class="number">1</span>, self.agent_loc[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">0</span>] &gt;= <span class="number">0</span>:</span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">1</span>: <span class="comment"># LEFT</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>], self.agent_loc[<span class="number">1</span>] - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">1</span>] &gt;= <span class="number">0</span>: <span class="comment"># Within Bounds?</span></span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span> <span class="comment"># Stay.</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">2</span>: <span class="comment"># DOWN</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>] + <span class="number">1</span>, self.agent_loc[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">0</span>] &lt; self.grid_h: <span class="comment"># Within Bounds?</span></span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span> <span class="comment"># Stay.</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">3</span>: <span class="comment"># RIGHT</span></span><br><span class="line">        possible_next_loc = (self.agent_loc[<span class="number">0</span>], self.agent_loc[<span class="number">1</span>] + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> possible_next_loc[<span class="number">1</span>] &lt; self.grid_w: <span class="comment"># Within Bounds?</span></span><br><span class="line">            self.agent_loc = possible_next_loc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span> <span class="comment"># Stay.</span></span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="built_in">str</span>(action) + <span class="string">&quot; not in recognized actions [0: Up, 1: Left, 2: Down, 3: Right]!&quot;</span>)</span><br><span class="line"></span><br><span class="line">    reward = -<span class="number">1</span></span><br><span class="line">    terminal = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Hint: Consider the initialization of reward and terminal variables above. Then, note the </span></span><br><span class="line">    <span class="comment"># conditional statements and comments given below and carefully ensure to set the variables reward </span></span><br><span class="line">    <span class="comment"># and terminal correctly for each case.</span></span><br><span class="line">    <span class="keyword">if</span> self.agent_loc == self.goal_loc: <span class="comment"># Reached Goal!</span></span><br><span class="line">        terminal = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">elif</span> self.agent_loc <span class="keyword">in</span> self.cliff: <span class="comment"># Fell into the cliff!</span></span><br><span class="line">        reward = -<span class="number">100</span></span><br><span class="line">        self.agent_loc = self.start_loc</span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    self.reward_state_term = (reward, self.state(self.agent_loc), terminal)</span><br><span class="line">    <span class="keyword">return</span> self.reward_state_term</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR ACTION UP (5 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is again limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_action_up</span>():</span></span><br><span class="line">    env = CliffWalkEnvironment()</span><br><span class="line">    env.env_init(&#123;<span class="string">&quot;grid_height&quot;</span>: <span class="number">4</span>, <span class="string">&quot;grid_width&quot;</span>: <span class="number">12</span>&#125;)</span><br><span class="line">    env.agent_loc = (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    env.env_step(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span>(env.agent_loc == (<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    env.agent_loc = (<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    env.env_step(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span>(env.agent_loc == (<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">test_action_up()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR REWARD &amp; TERMINATION (10 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test below is limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_reward</span>():</span></span><br><span class="line">    env = CliffWalkEnvironment()</span><br><span class="line">    env.env_init(&#123;<span class="string">&quot;grid_height&quot;</span>: <span class="number">4</span>, <span class="string">&quot;grid_width&quot;</span>: <span class="number">12</span>&#125;)</span><br><span class="line">    env.agent_loc = (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    reward_state_term = env.env_step(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span>(reward_state_term[<span class="number">0</span>] == -<span class="number">1</span> <span class="keyword">and</span> reward_state_term[<span class="number">1</span>] == env.state((<span class="number">0</span>, <span class="number">0</span>)) <span class="keyword">and</span></span><br><span class="line">           reward_state_term[<span class="number">2</span>] == <span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    env.agent_loc = (<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">    reward_state_term = env.env_step(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">assert</span>(reward_state_term[<span class="number">0</span>] == -<span class="number">100</span> <span class="keyword">and</span> reward_state_term[<span class="number">1</span>] == env.state((<span class="number">3</span>, <span class="number">0</span>)) <span class="keyword">and</span></span><br><span class="line">           reward_state_term[<span class="number">2</span>] == <span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    env.agent_loc = (<span class="number">2</span>, <span class="number">11</span>)</span><br><span class="line">    reward_state_term = env.env_step(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">assert</span>(reward_state_term[<span class="number">0</span>] == -<span class="number">1</span> <span class="keyword">and</span> reward_state_term[<span class="number">1</span>] == env.state((<span class="number">3</span>, <span class="number">11</span>)) <span class="keyword">and</span></span><br><span class="line">           reward_state_term[<span class="number">2</span>] == <span class="literal">True</span>)</span><br><span class="line">test_reward()</span><br></pre></td></tr></table></figure>

<h2 id="env-cleanup"><a href="#env-cleanup" class="headerlink" title="env_cleanup()"></a>env_cleanup()</h2><p>There is not much cleanup to do for the Cliff Walking environment. Here, we simply reset the agent location to be the start location in this function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%%add_to CliffWalkEnvironment</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">env_cleanup</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Cleanup done after the environment ends&quot;&quot;&quot;</span></span><br><span class="line">    self.agent_loc = self.start_loc</span><br></pre></td></tr></table></figure>

<h2 id="Section-2-Agent"><a href="#Section-2-Agent" class="headerlink" title="Section 2. Agent"></a>Section 2. Agent</h2><p>In this second part of the assignment, you will be implementing the key updates for Temporal Difference Learning. There are two cases to consider depending on whether an action leads to a terminal state or not.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create empty TDAgent class.</span></span><br><span class="line"><span class="comment"># These methods will be filled in later cells.</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TDAgent</span>(<span class="params">BaseAgent</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_init</span>(<span class="params">self, agent_info=&#123;&#125;</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_start</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, state</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_end</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_cleanup</span>(<span class="params">self</span>):</span>        </span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">agent_message</span>(<span class="params">self, message</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>

<h2 id="agent-init"><a href="#agent-init" class="headerlink" title="agent_init()"></a>agent_init()</h2><p>As we did with the environment, we first initialize the agent once when a TDAgent object is created. In this function, we create a random number generator, seeded with the seed provided in the agent_info dictionary to get reproducible results. We also set the policy, discount and step size based on the agent_info dictionary. Finally, with a convention that the policy is always specified as a mapping from states to actions and so is an array of size (# States, # Actions), we initialize a values array of shape (# States,) to zeros.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_init</span>(<span class="params">self, agent_info=&#123;&#125;</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Setup for the agent called when the experiment first starts.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a random number generator with the provided seed to seed the agent for reproducibility.</span></span><br><span class="line">    self.rand_generator = np.random.RandomState(agent_info.get(<span class="string">&quot;seed&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Policy will be given, recall that the goal is to accurately estimate its corresponding value function. </span></span><br><span class="line">    self.policy = agent_info.get(<span class="string">&quot;policy&quot;</span>)</span><br><span class="line">    <span class="comment"># Discount factor (gamma) to use in the updates.</span></span><br><span class="line">    self.discount = agent_info.get(<span class="string">&quot;discount&quot;</span>)</span><br><span class="line">    <span class="comment"># The learning rate or step size parameter (alpha) to use in updates.</span></span><br><span class="line">    self.step_size = agent_info.get(<span class="string">&quot;step_size&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize an array of zeros that will hold the values.</span></span><br><span class="line">    <span class="comment"># Recall that the policy can be represented as a (# States, # Actions) array. With the </span></span><br><span class="line">    <span class="comment"># assumption that this is the case, we can use the first dimension of the policy to</span></span><br><span class="line">    <span class="comment"># initialize the array for values.</span></span><br><span class="line">    self.values = np.zeros((self.policy.shape[<span class="number">0</span>],))</span><br></pre></td></tr></table></figure>

<h1 id="agent-start"><a href="#agent-start" class="headerlink" title="agent_start()"></a>agent_start()</h1><p>In agent_start(), we choose an action based on the initial state and policy we are evaluating. We also cache the state so that we can later update its value when we perform a Temporal Difference update. Finally, we return the action chosen so that the RL loop can continue and the environment can execute this action.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_start</span>(<span class="params">self, state</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;The first method called when the episode starts, called after</span></span><br><span class="line"><span class="string">    the environment starts.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the environment&#x27;s env_start function.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The first action the agent takes.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># The policy can be represented as a (# States, # Actions) array. So, we can use </span></span><br><span class="line">    <span class="comment"># the second dimension here when choosing an action.</span></span><br><span class="line">    action = self.rand_generator.choice(<span class="built_in">range</span>(self.policy.shape[<span class="number">1</span>]), p=self.policy[state])</span><br><span class="line">    self.last_state = state</span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure>

<h2 id="Implement-agent-step"><a href="#Implement-agent-step" class="headerlink" title="Implement agent_step()"></a><em>Implement</em> agent_step()</h2><p>In agent_step(), the agent must:</p>
<ul>
<li>Perform an update to improve the value estimate of the previously visited state, and</li>
<li>Act based on the state provided by the environment.</li>
</ul>
<p>The latter of the two steps above has been implemented for you. Implement the former. Note that, unlike later in agent_end(), the episode has not yet ended in agent_step(). in other words, the previously observed state was not a terminal state.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment">#[GRADED] FUNCTION: [agent_step]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Fill in the TD-target and update.</span></span><br><span class="line"><span class="comment"># Lines: ~2.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_step</span>(<span class="params">self, reward, state</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;A step taken by the agent.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward received for taking the last action taken</span></span><br><span class="line"><span class="string">        state (Numpy array): the state from the</span></span><br><span class="line"><span class="string">            environment&#x27;s step after the last action, i.e., where the agent ended up after the</span></span><br><span class="line"><span class="string">            last action</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The action the agent is taking.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Hint: We should perform an update with the last state given that we now have the reward and</span></span><br><span class="line">    <span class="comment"># next state. We break this into two steps. Recall for example that the Monte-Carlo update </span></span><br><span class="line">    <span class="comment"># had the form: V[S_t] = V[S_t] + alpha * (target - V[S_t]), where the target was the return, G_t.</span></span><br><span class="line">    target = reward + self.discount * self.values[state]</span><br><span class="line">    self.values[self.last_state] = self.values[self.last_state] + self.step_size * (target - self.values[self.last_state])</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Having updated the value for the last state, we now act based on the current </span></span><br><span class="line">    <span class="comment"># state, and set the last state to be current one as we will next be making an </span></span><br><span class="line">    <span class="comment"># update with it when agent_step is called next once the action we return from this function </span></span><br><span class="line">    <span class="comment"># is executed in the environment.</span></span><br><span class="line"></span><br><span class="line">    action = self.rand_generator.choice(<span class="built_in">range</span>(self.policy.shape[<span class="number">1</span>]), p=self.policy[state])</span><br><span class="line">    self.last_state = state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure>

<h2 id="Implement-agent-end"><a href="#Implement-agent-end" class="headerlink" title="Implement agent_end()"></a><em>Implement</em> agent_end()</h2><p>Implement the TD update for the case where an action leads to a terminal state.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment">#[GRADED] FUNCTION: [agent_end]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: Yes. Fill in the TD-target and update.</span></span><br><span class="line"><span class="comment"># Lines: ~2.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_end</span>(<span class="params">self, reward</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Run when the agent terminates.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        reward (float): the reward the agent received for entering the terminal state.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Hint: Here too, we should perform an update with the last state given that we now have the </span></span><br><span class="line">    <span class="comment"># reward. Note that in this case, the action led to termination. Once more, we break this into </span></span><br><span class="line">    <span class="comment"># two steps, computing the target and the update itself that uses the target and the </span></span><br><span class="line">    <span class="comment"># current value estimate for the state whose value we are updating.</span></span><br><span class="line">    target = reward</span><br><span class="line">    self.values[self.last_state] = self.values[self.last_state] + self.step_size * (target - self.values[self.last_state])</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>

<h2 id="agent-cleanup"><a href="#agent-cleanup" class="headerlink" title="agent_cleanup()"></a>agent_cleanup()</h2><p>In cleanup, we simply reset the last state to be None to ensure that we are not storing any states past an episode.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_cleanup</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Cleanup done after the agent ends.&quot;&quot;&quot;</span></span><br><span class="line">    self.last_state = <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<h2 id="agent-message"><a href="#agent-message" class="headerlink" title="agent_message()"></a>agent_message()</h2><p>agent_message() can generally be used to get different kinds of information about an RLGlue agent in the interaction loop of RLGlue. Here, we conditonally check for a message matching “get_values” and use it to retrieve the values table the agent has been updating over time.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%%add_to TDAgent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_message</span>(<span class="params">self, message</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;A function used to pass information from the agent to the experiment.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        message: The message passed to the agent.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The response (or answer) to the message.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> message == <span class="string">&quot;get_values&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> self.values</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">&quot;TDAgent.agent_message(): Message not understood!&quot;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTOGRADER TESTS FOR TD-UPDATES (20 POINTS)</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The test belows serve as a good check in debugging your code for the TD updates. However, </span></span><br><span class="line"><span class="comment">#       as with the other tests, it is limited in scope. Hidden tests are used in the autograder.</span></span><br><span class="line"><span class="comment">#       You may wish to run other tests to check your implementation.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_td_updates</span>():</span></span><br><span class="line">    <span class="comment"># The following test checks that the TD check works for a case where the transition </span></span><br><span class="line">    <span class="comment"># garners reward -1 and does not lead to a terminal state. This is in a simple two state setting </span></span><br><span class="line">    <span class="comment"># where there is only one action. The first state&#x27;s current value estimate is 0 while the second is 1.</span></span><br><span class="line">    <span class="comment"># Note the discount and step size if you are debugging this test.</span></span><br><span class="line">    agent = TDAgent()</span><br><span class="line">    policy_list = np.array([[<span class="number">1.</span>], [<span class="number">1.</span>]])</span><br><span class="line">    agent.agent_init(&#123;<span class="string">&quot;policy&quot;</span>: np.array(policy_list), <span class="string">&quot;discount&quot;</span>: <span class="number">0.99</span>, <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line">    agent.values = np.array([<span class="number">0.</span>, <span class="number">1.</span>])</span><br><span class="line">    agent.agent_start(<span class="number">0</span>)</span><br><span class="line">    reward = -<span class="number">1</span></span><br><span class="line">    next_state = <span class="number">1</span></span><br><span class="line">    agent.agent_step(reward, next_state)</span><br><span class="line">    <span class="keyword">assert</span>(np.isclose(agent.values[<span class="number">0</span>], -<span class="number">0.001</span>) <span class="keyword">and</span> np.isclose(agent.values[<span class="number">1</span>], <span class="number">1.</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The following test checks that the TD check works for a case where the transition </span></span><br><span class="line">    <span class="comment"># garners reward -100 and lead to a terminal state. This is in a simple one state setting </span></span><br><span class="line">    <span class="comment"># where there is only one action. The state&#x27;s current value estimate is 0.</span></span><br><span class="line">    <span class="comment"># Note the discount and step size if you are debugging this test.</span></span><br><span class="line">    agent = TDAgent()</span><br><span class="line">    policy_list = np.array([[<span class="number">1.</span>]])</span><br><span class="line">    agent.agent_init(&#123;<span class="string">&quot;policy&quot;</span>: np.array(policy_list), <span class="string">&quot;discount&quot;</span>: <span class="number">0.99</span>, <span class="string">&quot;step_size&quot;</span>: <span class="number">0.1</span>&#125;)</span><br><span class="line">    agent.values = np.array([<span class="number">0.</span>])</span><br><span class="line">    agent.agent_start(<span class="number">0</span>)</span><br><span class="line">    reward = -<span class="number">100</span></span><br><span class="line">    next_state = <span class="number">0</span></span><br><span class="line">    agent.agent_end(reward)</span><br><span class="line">    <span class="keyword">assert</span>(np.isclose(agent.values[<span class="number">0</span>], -<span class="number">10</span>))</span><br><span class="line">    </span><br><span class="line">test_td_updates()</span><br></pre></td></tr></table></figure>

<h2 id="Section-3-Policy-Evaluation-Experiments"><a href="#Section-3-Policy-Evaluation-Experiments" class="headerlink" title="Section 3. Policy Evaluation Experiments"></a>Section 3. Policy Evaluation Experiments</h2><p>Finally, in this last part of the assignment, you will get to see the TD policy evaluation algorithm in action by looking at the estimated values, the per state value error and after the experiment is complete, the Mean Squared Value Error curve vs. episode number, summarizing how the value error changed over time.</p>
<p>The code below runs one run of an experiment given env_info and agent_info dictionaries. A “manager” object is created for visualizations and is used in part for the autograder. By default, the run will be for 5000 episodes. The true_values_file is specified to compare the learned value function with the values stored in the true_values_file. Plotting of the learned value  function occurs by default after every 100 episodes. In addition, when true_values_file is specified, the value error per state and the root mean square value error will also be plotted.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib notebook</span><br><span class="line"></span><br><span class="line"><span class="comment"># Work Required: No. </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span>(<span class="params">env_info, agent_info, </span></span></span><br><span class="line"><span class="params"><span class="function">                   num_episodes=<span class="number">5000</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                   experiment_name=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                   plot_freq=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                   true_values_file=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                   value_error_threshold=<span class="number">1e-8</span></span>):</span></span><br><span class="line">    env = CliffWalkEnvironment</span><br><span class="line">    agent = TDAgent</span><br><span class="line">    rl_glue = RLGlue(env, agent)</span><br><span class="line"></span><br><span class="line">    rl_glue.rl_init(agent_info, env_info)</span><br><span class="line"></span><br><span class="line">    manager = Manager(env_info, agent_info, true_values_file=true_values_file, experiment_name=experiment_name)</span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_episodes + <span class="number">1</span>):</span><br><span class="line">        rl_glue.rl_episode(<span class="number">0</span>) <span class="comment"># no step limit</span></span><br><span class="line">        <span class="keyword">if</span> episode % plot_freq == <span class="number">0</span>:</span><br><span class="line">            values = rl_glue.agent.agent_message(<span class="string">&quot;get_values&quot;</span>)</span><br><span class="line">            manager.visualize(values, episode)</span><br><span class="line"></span><br><span class="line">    values = rl_glue.agent.agent_message(<span class="string">&quot;get_values&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> true_values_file <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># Grading: The Manager will check that the values computed using your TD agent match </span></span><br><span class="line">        <span class="comment"># the true values (within some small allowance) across the states. In addition, it also</span></span><br><span class="line">        <span class="comment"># checks whether the root mean squared value error is close to 0.</span></span><br><span class="line">        manager.run_tests(values, value_error_threshold)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> values</span><br></pre></td></tr></table></figure>

<p>The cell below just runs a policy evaluation experiment with the determinstic optimal policy that strides just above the cliff. You should observe that the per state value error and RMSVE curve asymptotically go towards 0. The arrows in the four directions denote the probabilities of taking each action. This experiment is ungraded but should serve as a good test for the later experiments. The true values file provided for this experiment may help with debugging as well.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line">env_info = &#123;<span class="string">&quot;grid_height&quot;</span>: <span class="number">4</span>, <span class="string">&quot;grid_width&quot;</span>: <span class="number">12</span>, <span class="string">&quot;seed&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line">agent_info = &#123;<span class="string">&quot;discount&quot;</span>: <span class="number">1</span>, <span class="string">&quot;step_size&quot;</span>: <span class="number">0.01</span>, <span class="string">&quot;seed&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># The Optimal Policy that strides just along the cliff</span></span><br><span class="line">policy = np.ones(shape=(env_info[<span class="string">&#x27;grid_width&#x27;</span>] * env_info[<span class="string">&#x27;grid_height&#x27;</span>], <span class="number">4</span>)) * <span class="number">0.25</span></span><br><span class="line">policy[<span class="number">36</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">24</span>, <span class="number">35</span>):</span><br><span class="line">    policy[i] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">policy[<span class="number">35</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">agent_info.update(&#123;<span class="string">&quot;policy&quot;</span>: policy&#125;)</span><br><span class="line"></span><br><span class="line">true_values_file = <span class="string">&quot;optimal_policy_value_fn.npy&quot;</span></span><br><span class="line">_ = run_experiment(env_info, agent_info, num_episodes=<span class="number">5000</span>, experiment_name=<span class="string">&quot;Policy Evaluation on Optimal Policy&quot;</span>,</span><br><span class="line">                   plot_freq=<span class="number">500</span>, true_values_file=true_values_file)</span><br></pre></td></tr></table></figure>


<pre><code>&lt;IPython.core.display.Javascript object&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The Safe Policy</span></span><br><span class="line"><span class="comment"># Hint: Fill in the array below (as done in the previous cell) based on the safe policy illustration </span></span><br><span class="line"><span class="comment"># in the environment diagram. This is the policy that strides as far as possible away from the cliff. </span></span><br><span class="line"><span class="comment"># We call it a &quot;safe&quot; policy because if the environment has any stochasticity, this policy would do a good job in </span></span><br><span class="line"><span class="comment"># keeping the agent from falling into the cliff (in contrast to the optimal policy shown before). </span></span><br><span class="line"><span class="comment"># BOILERPLATE:</span></span><br><span class="line">policy = np.ones(shape=(env_info[<span class="string">&#x27;grid_width&#x27;</span>] * env_info[<span class="string">&#x27;grid_height&#x27;</span>], <span class="number">4</span>)) * <span class="number">0.25</span></span><br><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line">policy[<span class="number">36</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">24</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">12</span>] = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">11</span>):</span><br><span class="line">    policy[i] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">policy[<span class="number">11</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">23</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">policy[<span class="number">35</span>] = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTO-GRADER TESTS FOR POLICY EVALUATION WITH SAFE POLICY</span></span><br><span class="line">agent_info.update(&#123;<span class="string">&quot;policy&quot;</span>: policy&#125;)</span><br><span class="line">v = run_experiment(env_info, agent_info,</span><br><span class="line">               experiment_name=<span class="string">&quot;Policy Evaluation On Safe Policy&quot;</span>,</span><br><span class="line">               num_episodes=<span class="number">5000</span>, plot_freq=<span class="number">500</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Do not modify this cell!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A Near Optimal Stochastic Policy</span></span><br><span class="line"><span class="comment"># Now, we try a stochastic policy that deviates a little from the optimal policy seen above. </span></span><br><span class="line"><span class="comment"># This means we can get different results due to randomness.</span></span><br><span class="line"><span class="comment"># We will thus average the value function estimates we get over multiple runs. </span></span><br><span class="line"><span class="comment"># This can take some time, upto about 5 minutes from previous testing. </span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> The autograder will compare . Re-run this cell upon making any changes.</span></span><br><span class="line"></span><br><span class="line">env_info = &#123;<span class="string">&quot;grid_height&quot;</span>: <span class="number">4</span>, <span class="string">&quot;grid_width&quot;</span>: <span class="number">12</span>&#125;</span><br><span class="line">agent_info = &#123;<span class="string">&quot;discount&quot;</span>: <span class="number">1</span>, <span class="string">&quot;step_size&quot;</span>: <span class="number">0.01</span>&#125;</span><br><span class="line"></span><br><span class="line">policy = np.ones(shape=(env_info[<span class="string">&#x27;grid_width&#x27;</span>] * env_info[<span class="string">&#x27;grid_height&#x27;</span>], <span class="number">4</span>)) * <span class="number">0.25</span></span><br><span class="line">policy[<span class="number">36</span>] = [<span class="number">0.9</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">24</span>, <span class="number">35</span>):</span><br><span class="line">    policy[i] = [<span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.9</span>]</span><br><span class="line">policy[<span class="number">35</span>] = [<span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.1</span>/<span class="number">3.</span>, <span class="number">0.9</span>, <span class="number">0.1</span>/<span class="number">3.</span>]</span><br><span class="line">agent_info.update(&#123;<span class="string">&quot;policy&quot;</span>: policy&#125;)</span><br><span class="line">agent_info.update(&#123;<span class="string">&quot;step_size&quot;</span>: <span class="number">0.01</span>&#125;)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### AUTO-GRADER TESTS FOR POLICY EVALUATION WITH NEAR OPTIMAL STOCHASTIC POLICY (40 POINTS)</span></span><br><span class="line">arr = []</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">30</span>)):</span><br><span class="line">    env_info[<span class="string">&#x27;seed&#x27;</span>] = i</span><br><span class="line">    agent_info[<span class="string">&#x27;seed&#x27;</span>] = i</span><br><span class="line">    v = run_experiment(env_info, agent_info,</span><br><span class="line">                   experiment_name=<span class="string">&quot;Policy Evaluation On Optimal Policy&quot;</span>,</span><br><span class="line">                   num_episodes=<span class="number">5000</span>, plot_freq=<span class="number">10000</span>)</span><br><span class="line">    arr.append(v)</span><br><span class="line">average_v = np.array(arr).mean(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h2><p>Congratulations, you have completed assignment 2! In this assignment, we investigated a very useful concept for sample-based online learning: temporal difference. We particularly looked at the prediction problem where the goal is to find the value function corresponding to a given policy. In the next assignment, by learning the action-value function instead of the state-value function, you will get to see how temporal difference learning can be used in control as well.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Assignment-4-Chatbot/2020/09/28/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Assignment-4-Chatbot/2020/09/28/" class="post-title-link" itemprop="url">Assignment 4: Chatbot</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-09-28 13:50:01 / Modified: 17:00:27" itemprop="dateCreated datePublished" datetime="2020-09-28T13:50:01+08:00">2020-09-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Assignment-4-Chatbot/2020/09/28/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Assignment-4-Chatbot/2020/09/28/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Assignment-4-Chatbot"><a href="#Assignment-4-Chatbot" class="headerlink" title="Assignment 4: Chatbot"></a>Assignment 4: Chatbot</h1><img src = "cbot.jpg" height="400" width="400"> 

<p>Welcome to the last assignment of Course 4. Before you get started, we want to congratulate you on getting here. It is your 16th programming assignment in this Specialization and we are very proud of you! In this assignment, you are going to use the <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2001.04451">Reformer</a>, also known as the efficient Transformer, to generate a dialogue between two bots. You will feed conversations to your model and it will learn how to understand the context of each one. Not only will it learn how to answer questions but it will also know how to ask questions if it needs more info. For example, after a customer asks for a train ticket, the chatbot can ask what time the said customer wants to leave. You can use this concept to automate call centers, hotel receptions, personal trainers, or any type of customer service. By completing this assignment, you will:</p>
<ul>
<li>Understand how the Reformer works</li>
<li>Explore the <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.00278">MultiWoz</a> dataset</li>
<li>Process the data to feed it into the model</li>
<li>Train your model</li>
<li>Generate a dialogue by feeding a question to the model</li>
</ul>
<h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li><a href="#1">Part 1:   Exploring the MultiWoz dataset</a><ul>
<li><a href="#ex01">Exercise 01</a></li>
</ul>
</li>
<li><a href="#2">Part 2:   Processing the data for Reformer inputs</a><ul>
<li><a href="#2.1">2.1   Tokenizing, batching with bucketing</a></li>
</ul>
</li>
<li><a href="#3">Part 3:   Reversible layers</a><ul>
<li><a href="#ex02">Exercise 02</a></li>
<li><a href="#ex03">Exercise 03</a></li>
<li><a href="#3.1">3.1   Reversible layers and randomness</a></li>
</ul>
</li>
<li><a href="#4">Part 4:   ReformerLM Training</a><ul>
<li><a href="#ex04">Exercise 04</a></li>
<li><a href="#ex05">Exercise 05</a></li>
</ul>
</li>
<li><a href="#5">Part 5:   Decode from a pretrained model</a><ul>
<li><a href="#ex06">Exercise 06</a></li>
</ul>
</li>
</ul>
<p><a name="1"></a></p>
<h1 id="Part-1-Exploring-the-MultiWoz-dataset"><a href="#Part-1-Exploring-the-MultiWoz-dataset" class="headerlink" title="Part 1:   Exploring the MultiWoz dataset"></a>Part 1:   Exploring the MultiWoz dataset</h1><p>You will start by exploring the MultiWoz dataset. The dataset you are about to use has more than 10,000 human annotated dialogues and spans multiple domains and topics. Some dialogues include multiple domains and others include single domains. In this section, you will load and explore this dataset, as well as develop a function to extract the dialogues.</p>
<p>Let’s first import the modules we will be using:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> termcolor <span class="keyword">import</span> colored</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> trax   </span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> training</span><br><span class="line">!pip <span class="built_in">list</span> | grep trax</span><br></pre></td></tr></table></figure>

<p>Let’s also declare some constants we will be using in the exercises.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># filename of the MultiWOZ dialogue dataset</span></span><br><span class="line">DATA_FILE = <span class="string">&#x27;data.json&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># data directory</span></span><br><span class="line">DATA_DIR = <span class="string">&#x27;./data&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dictionary where we will load the dialogue dataset</span></span><br><span class="line">DIALOGUE_DB = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># vocabulary filename</span></span><br><span class="line">VOCAB_FILE = <span class="string">&#x27;en_32k.subword&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vocabulary file directory</span></span><br><span class="line">VOCAB_DIR = <span class="string">&#x27;data/vocabs&#x27;</span></span><br></pre></td></tr></table></figure>

<p>Let’s now load the MultiWOZ 2.1 dataset. We have already provided it for you in your workspace. It is in JSON format so we should load it as such:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># help function to load a JSON file</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_json</span>(<span class="params">directory, file</span>):</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&#x27;<span class="subst">&#123;directory&#125;</span>/<span class="subst">&#123;file&#125;</span>&#x27;</span>) <span class="keyword">as</span> file: </span><br><span class="line">        db = json.load(file)</span><br><span class="line">    <span class="keyword">return</span> db</span><br><span class="line"></span><br><span class="line"><span class="comment"># load the dialogue data set into our dictionary</span></span><br><span class="line">DIALOGUE_DB = load_json(DATA_DIR, DATA_FILE)</span><br></pre></td></tr></table></figure>

<p>Let’s see how many dialogues we have in the dictionary. 1 key-value pair is one dialogue so we can just get the dictionary’s length.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;The number of dialogues is: <span class="subst">&#123;<span class="built_in">len</span>(DIALOGUE_DB)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The number of dialogues is: 10438
</code></pre>
<p>The dialogues are composed of multiple files and the filenames are used as keys in our dictionary. Those with multi-domain dialogues have “MUL” in their filenames while single domain dialogues have either “SNG” or “WOZ”.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print 7 keys from the dataset to see the filenames</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(DIALOGUE_DB.keys())[<span class="number">0</span>:<span class="number">7</span>]) </span><br></pre></td></tr></table></figure>

<pre><code>[&#39;SNG01856.json&#39;, &#39;SNG0129.json&#39;, &#39;PMUL1635.json&#39;, &#39;MUL2168.json&#39;, &#39;SNG0073.json&#39;, &#39;SNG01445.json&#39;, &#39;MUL2105.json&#39;]
</code></pre>
<p>As you can see from the cells above, there are 10,438 conversations, each in its own file.  You will train your model on all those conversations. Each file is also loaded into a dictionary and each has two keys which are the following:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get keys of the fifth file in the list above</span></span><br><span class="line"><span class="built_in">print</span>(DIALOGUE_DB[<span class="string">&#x27;SNG0073.json&#x27;</span>].keys())</span><br></pre></td></tr></table></figure>

<pre><code>dict_keys([&#39;goal&#39;, &#39;log&#39;])
</code></pre>
<p>The <code>goal</code> also points to a dictionary and it contains several keys pertaining to the objectives of the conversation. For example below, we can see that the conversation will be about booking a taxi.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DIALOGUE_DB[<span class="string">&#x27;SNG0073.json&#x27;</span>][<span class="string">&#x27;goal&#x27;</span>]</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;taxi&#39;: &#123;&#39;info&#39;: &#123;&#39;leaveAt&#39;: &#39;17:15&#39;,
   &#39;destination&#39;: &#39;pizza hut fen ditton&#39;,
   &#39;departure&#39;: &quot;saint john&#39;s college&quot;&#125;,
  &#39;reqt&#39;: [&#39;car type&#39;, &#39;phone&#39;],
  &#39;fail_info&#39;: &#123;&#125;&#125;,
 &#39;police&#39;: &#123;&#125;,
 &#39;hospital&#39;: &#123;&#125;,
 &#39;hotel&#39;: &#123;&#125;,
 &#39;attraction&#39;: &#123;&#125;,
 &#39;train&#39;: &#123;&#125;,
 &#39;message&#39;: [&quot;You want to book a &lt;span class=&#39;emphasis&#39;&gt;taxi&lt;/span&gt;. The taxi should go to &lt;span class=&#39;emphasis&#39;&gt;pizza hut fen ditton&lt;/span&gt; and should depart from &lt;span class=&#39;emphasis&#39;&gt;saint john&#39;s college&lt;/span&gt;&quot;,
  &quot;The taxi should &lt;span class=&#39;emphasis&#39;&gt;leave after 17:15&lt;/span&gt;&quot;,
  &quot;Make sure you get &lt;span class=&#39;emphasis&#39;&gt;car type&lt;/span&gt; and &lt;span class=&#39;emphasis&#39;&gt;contact number&lt;/span&gt;&quot;],
 &#39;restaurant&#39;: &#123;&#125;&#125;
</code></pre>
<p>The <code>log</code> on the other hand contains the dialog. It is a list of dictionaries and each element of this list contains several descriptions as well. Let’s look at an example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get first element of the log list</span></span><br><span class="line">DIALOGUE_DB[<span class="string">&#x27;SNG0073.json&#x27;</span>][<span class="string">&#x27;log&#x27;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;text&#39;: &quot;I would like a taxi from Saint John&#39;s college to Pizza Hut Fen Ditton.&quot;,
 &#39;metadata&#39;: &#123;&#125;,
 &#39;dialog_act&#39;: &#123;&#39;Taxi-Inform&#39;: [[&#39;Dest&#39;, &#39;pizza hut fen ditton&#39;],
   [&#39;Depart&#39;, &quot;saint john &#39;s college&quot;]]&#125;,
 &#39;span_info&#39;: [[&#39;Taxi-Inform&#39;, &#39;Dest&#39;, &#39;pizza hut fen ditton&#39;, 11, 14],
  [&#39;Taxi-Inform&#39;, &#39;Depart&#39;, &quot;saint john &#39;s college&quot;, 6, 9]]&#125;
</code></pre>
<p>For this assignment, we are only interested in the conversation which is in the <code>text</code> field.<br>The conversation goes back and forth between two persons. Let’s call them ‘Person 1’ and ‘Person 2’. This implies that<br>data[‘SNG0073.json’][‘log’][0][‘text’] is ‘Person 1’ and<br>data[‘SNG0073.json’][‘log’][1][‘text’] is ‘Person 2’ and so on. The even offsets are ‘Person 1’ and the odd offsets are ‘Person 2’.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; Person 1: &#x27;</span>, DIALOGUE_DB[<span class="string">&#x27;SNG0073.json&#x27;</span>][<span class="string">&#x27;log&#x27;</span>][<span class="number">0</span>][<span class="string">&#x27;text&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; Person 2: &#x27;</span>,DIALOGUE_DB[<span class="string">&#x27;SNG0073.json&#x27;</span>][<span class="string">&#x27;log&#x27;</span>][<span class="number">1</span>][<span class="string">&#x27;text&#x27;</span>])</span><br></pre></td></tr></table></figure>

<pre><code> Person 1:  I would like a taxi from Saint John&#39;s college to Pizza Hut Fen Ditton.
 Person 2:  What time do you want to leave and what time do you want to arrive by?
</code></pre>
<p><a name="ex01"></a></p>
<h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p>You will now implement the <code>get_conversation()</code> function that will extract the conversations from the dataset’s file.</p>
<p><strong>Instructions:</strong> Implement a function to extract conversations from the input file.<br>As described above, the conversation is in the <code>text</code> field in each of the elements in the <code>log</code> list of the file. If the log list has <code>x</code> number of elements, then the function will get the <code>text</code> entries of each of those elements. Your function should return the conversation, prepending each field with either ‘ Person 1: ‘ if ‘x’ is even or ‘ Person 2: ‘ if ‘x’ is odd. You can use the Python modulus operator ‘%’ to help select the even/odd entries. Important note: Do not print a newline character (i.e. <code>\n</code>) when generating the string. For example, in the code cell above, your function should output something like:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Person 1: I would like a taxi from Saint John&#x27;s college to Pizza Hut Fen Ditton. Person 2: What time do you want to leave and what time do you want to arrive by?</span><br></pre></td></tr></table></figure>

<p>and <strong>not</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Person 1:  I would like a taxi from Saint John&#x27;s college to Pizza Hut Fen Ditton.</span><br><span class="line">Person 2:  What time do you want to leave and what time do you want to arrive by?</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: get_conversation</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_conversation</span>(<span class="params">file, data_db</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        file (string): filename of the dialogue file saved as json</span></span><br><span class="line"><span class="string">        data_db (dict): dialogue database</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        string: A string containing the &#x27;text&#x27; fields of  data[file][&#x27;log&#x27;][x]</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize empty string</span></span><br><span class="line">    result = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get length of file&#x27;s log list</span></span><br><span class="line">    len_msg_log = <span class="built_in">len</span>(data_db[file][<span class="string">&#x27;log&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># set the delimiter strings</span></span><br><span class="line">    delimiter_1 = <span class="string">&#x27; Person 1: &#x27;</span></span><br><span class="line">    delimiter_2 = <span class="string">&#x27; Person 2: &#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over the file&#x27;s log list</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_msg_log):</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># get i&#x27;th element of file log list</span></span><br><span class="line">        cur_log = data_db[file][<span class="string">&#x27;log&#x27;</span>][i][<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># check if i is even</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>:                   </span><br><span class="line">            <span class="comment"># append the 1st delimiter string</span></span><br><span class="line">            result += delimiter_1</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="comment"># append the 2nd delimiter string</span></span><br><span class="line">            result += delimiter_2</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># append the message text from the log</span></span><br><span class="line">        result += cur_log</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line"><span class="keyword">import</span> w4_unittest</span><br><span class="line">w4_unittest.test_get_conversation(get_conversation)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">file = <span class="string">&#x27;SNG01856.json&#x27;</span></span><br><span class="line">conversation = get_conversation(file, DIALOGUE_DB)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print raw output</span></span><br><span class="line"><span class="built_in">print</span>(conversation)</span><br></pre></td></tr></table></figure>

<pre><code> Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it&#39;s cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn&#39;t able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.
Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.
</code></pre>
<p><strong>Expected Result:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it&#x27;s cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn&#x27;t able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.</span><br><span class="line">Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.</span><br></pre></td></tr></table></figure>

<p>We can have a utility pretty print function just so we can visually follow the conversation more easily.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_conversation</span>(<span class="params">conversation</span>):</span></span><br><span class="line">    </span><br><span class="line">    delimiter_1 = <span class="string">&#x27;Person 1: &#x27;</span></span><br><span class="line">    delimiter_2 = <span class="string">&#x27;Person 2: &#x27;</span></span><br><span class="line">    </span><br><span class="line">    split_list_d1 = conversation.split(delimiter_1)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> sublist <span class="keyword">in</span> split_list_d1[<span class="number">1</span>:]:</span><br><span class="line">        split_list_d2 = sublist.split(delimiter_2)</span><br><span class="line">        <span class="built_in">print</span>(colored(<span class="string">f&#x27;Person 1: <span class="subst">&#123;split_list_d2[<span class="number">0</span>]&#125;</span>&#x27;</span>, <span class="string">&#x27;red&#x27;</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(split_list_d2) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="built_in">print</span>(colored(<span class="string">f&#x27;Person 2: <span class="subst">&#123;split_list_d2[<span class="number">1</span>]&#125;</span>&#x27;</span>, <span class="string">&#x27;green&#x27;</span>))</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">print_conversation(conversation)</span><br></pre></td></tr></table></figure>

<p>For this assignment, we will just use the outputs of the calls to <code>get_conversation</code> to train the model. But just to expound, there are also other information in the MultiWoz dataset that can be useful in other contexts. Each element of the log list has more information about it. For example, above, if you were to look at the other fields for the following, “am looking for a place to stay that has cheap price range it should be in a type of hotel”, you will get the following. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DIALOGUE_DB[<span class="string">&#x27;SNG01856.json&#x27;</span>][<span class="string">&#x27;log&#x27;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>




<pre><code>&#123;&#39;text&#39;: &#39;am looking for a place to to stay that has cheap price range it should be in a type of hotel&#39;,
 &#39;metadata&#39;: &#123;&#125;,
 &#39;dialog_act&#39;: &#123;&#39;Hotel-Inform&#39;: [[&#39;Type&#39;, &#39;hotel&#39;], [&#39;Price&#39;, &#39;cheap&#39;]]&#125;,
 &#39;span_info&#39;: [[&#39;Hotel-Inform&#39;, &#39;Type&#39;, &#39;hotel&#39;, 20, 20],
  [&#39;Hotel-Inform&#39;, &#39;Price&#39;, &#39;cheap&#39;, 10, 10]]&#125;
</code></pre>
<p>The dataset also comes with hotel, hospital, taxi, train, police, and restaurant databases. For example, in case you need to call a doctor, or a hotel, or a taxi, this will allow you to automate the entire conversation. Take a look at the files accompanying the data set.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the attractions file</span></span><br><span class="line">attraction_file = <span class="built_in">open</span>(<span class="string">&#x27;data/attraction_db.json&#x27;</span>)</span><br><span class="line">attractions = json.load(attraction_file)</span><br><span class="line"><span class="built_in">print</span>(attractions[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;address&#39;: &#39;pool way, whitehill road, off newmarket road&#39;, &#39;area&#39;: &#39;east&#39;, &#39;entrance fee&#39;: &#39;?&#39;, &#39;id&#39;: &#39;1&#39;, &#39;location&#39;: [52.208789, 0.154883], &#39;name&#39;: &#39;abbey pool and astroturf pitch&#39;, &#39;openhours&#39;: &#39;?&#39;, &#39;phone&#39;: &#39;01223902088&#39;, &#39;postcode&#39;: &#39;cb58nt&#39;, &#39;pricerange&#39;: &#39;?&#39;, &#39;type&#39;: &#39;swimmingpool&#39;&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the hospital file</span></span><br><span class="line">hospital_file = <span class="built_in">open</span>(<span class="string">&#x27;data/hospital_db.json&#x27;</span>)</span><br><span class="line">hospitals = json.load(hospital_file)</span><br><span class="line"><span class="built_in">print</span>(hospitals[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;department&#39;: &#39;neurosciences critical care unit&#39;, &#39;id&#39;: 0, &#39;phone&#39;: &#39;01223216297&#39;&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the hotel file</span></span><br><span class="line">hotel_file = <span class="built_in">open</span>(<span class="string">&#x27;data/hotel_db.json&#x27;</span>)</span><br><span class="line">hotels = json.load(hotel_file)</span><br><span class="line"><span class="built_in">print</span>(hotels[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;address&#39;: &#39;124 tenison road&#39;, &#39;area&#39;: &#39;east&#39;, &#39;internet&#39;: &#39;yes&#39;, &#39;parking&#39;: &#39;no&#39;, &#39;id&#39;: &#39;0&#39;, &#39;location&#39;: [52.1963733, 0.1987426], &#39;name&#39;: &#39;a and b guest house&#39;, &#39;phone&#39;: &#39;01223315702&#39;, &#39;postcode&#39;: &#39;cb12dp&#39;, &#39;price&#39;: &#123;&#39;double&#39;: &#39;70&#39;, &#39;family&#39;: &#39;90&#39;, &#39;single&#39;: &#39;50&#39;&#125;, &#39;pricerange&#39;: &#39;moderate&#39;, &#39;stars&#39;: &#39;4&#39;, &#39;takesbookings&#39;: &#39;yes&#39;, &#39;type&#39;: &#39;guesthouse&#39;&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of the police file</span></span><br><span class="line">police_file = <span class="built_in">open</span>(<span class="string">&#x27;data/police_db.json&#x27;</span>)</span><br><span class="line">police = json.load(police_file)</span><br><span class="line"><span class="built_in">print</span>(police[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;name&#39;: &#39;Parkside Police Station&#39;, &#39;address&#39;: &#39;Parkside, Cambridge&#39;, &#39;id&#39;: 0, &#39;phone&#39;: &#39;01223358966&#39;&#125;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># this is an example of a restuarant file</span></span><br><span class="line">restaurant_file = <span class="built_in">open</span>(<span class="string">&#x27;data/restaurant_db.json&#x27;</span>)</span><br><span class="line">restaurants = json.load(restaurant_file)</span><br><span class="line"><span class="built_in">print</span>(restaurants[<span class="number">0</span>]) <span class="comment"># feel free to index into other indices</span></span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;address&#39;: &#39;Regent Street City Centre&#39;, &#39;area&#39;: &#39;centre&#39;, &#39;food&#39;: &#39;italian&#39;, &#39;id&#39;: &#39;19210&#39;, &#39;introduction&#39;: &#39;Pizza hut is a large chain with restaurants nationwide offering convenience pizzas pasta and salads to eat in or take away&#39;, &#39;location&#39;: [52.20103, 0.126023], &#39;name&#39;: &#39;pizza hut city centre&#39;, &#39;phone&#39;: &#39;01223323737&#39;, &#39;postcode&#39;: &#39;cb21ab&#39;, &#39;pricerange&#39;: &#39;cheap&#39;, &#39;type&#39;: &#39;restaurant&#39;&#125;
</code></pre>
<p>For more information about the multiwoz 2.1 data set, please run the cell below to read the <code>ReadMe.txt</code> file. Feel free to open any other file to explore it. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/README&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    <span class="built_in">print</span>(file.read())</span><br></pre></td></tr></table></figure>

<pre><code>#####################################################
#####################################################
#  Copyright Cambridge Dialogue Systems Group, 2018 #
#####################################################
#####################################################

Dataset contains the following files:
1. data.json: the woz dialogue dataset, which contains the conversation  users and wizards, as well as a set of coarse labels for each user turn. This file contains both system and user dialogue acts annotated at the turn level. Files with multi-domain dialogues have &quot;MUL&quot; in their names. Single domain dialogues have either &quot;SNG&quot; or &quot;WOZ&quot; in their names.
2. restaurant_db.json: the Cambridge restaurant database file, containing restaurants in the Cambridge UK area and a set of attributes.
3. attraction_db.json: the Cambridge attraction database file, contining attractions in the Cambridge UK area and a set of attributes.
4. hotel_db.json: the Cambridge hotel database file, containing hotels in the Cambridge UK area and a set of attributes.
5. train_db.json: the Cambridge train (with artificial connections) database file, containing trains in the Cambridge UK area and a set of attributes.
6. hospital_db.json: the Cambridge hospital database file, contatining information about departments.
7. police_db.json: the Cambridge police station information.
8. taxi_db.json: slot-value list for taxi domain.
9. valListFile.txt: list of dialogues for validation.
10. testListFile.txt: list of dialogues for testing.
11. system_acts.json:
  There are 6 domains (&#39;Booking&#39;, &#39;Restaurant&#39;, &#39;Hotel&#39;, &#39;Attraction&#39;, &#39;Taxi&#39;, &#39;Train&#39;) and 1 dummy domain (&#39;general&#39;).
  A domain-dependent dialogue act is defined as a domain token followed by a domain-independent dialogue act, e.g. &#39;Hotel-inform&#39; means it is an &#39;inform&#39; act in the Hotel domain.
  Dialogue acts which cannot take slots, e.g., &#39;good bye&#39;, are defined under the &#39;general&#39; domain.
  A slot-value pair defined as a list with two elements. The first element is slot token and the second one is its value.
  If a dialogue act takes no slots, e.g., dialogue act &#39;offer booking&#39; for an utterance &#39;would you like to take a reservation?&#39;, its slot-value pair is [&#39;none&#39;, &#39;none&#39;]
  There are four types of values:
  1) If a slot takes a binary value, e.g., &#39;has Internet&#39; or &#39;has park&#39;, the value is either &#39;yes&#39; or &#39;no&#39;.
  2) If a slot is under the act &#39;request&#39;, e.g., &#39;request&#39; about &#39;area&#39;, the value is expressed as &#39;?&#39;.
  3) The value that appears in the utterance e.g., the name of a restaurant.
  4) If for some reason the turn does not have an annotation then it is labeled as &quot;No Annotation.&quot;
12. ontology.json: Data-based ontology containing all the values for the different slots in the domains.
13. slot_descriptions.json: A collection of human-written slot descriptions for each slot in the dataset. Each slot has at least two descriptions.
14. tokenization.md: A description of the tokenization preprocessing we had to perform to maintain consistency between the dialogue act annotations of DSTC 8 Track 1 and the existing MultiWOZ 2.0 data. 
</code></pre>
<p>As you can see, there are many other aspects of the MultiWoz dataset. Nonetheless, you’ll see that even with just the conversations, your model will still be able to generate useful responses. This concludes our exploration of the dataset. In the next section, we will do some preprocessing before we feed it into our model for training.</p>
<p><a name="2"></a></p>
<h1 id="Part-2-Processing-the-data-for-Reformer-inputs"><a href="#Part-2-Processing-the-data-for-Reformer-inputs" class="headerlink" title="Part 2:   Processing the data for Reformer inputs"></a>Part 2:   Processing the data for Reformer inputs</h1><p>You will now use the <code>get_conversation()</code> function to process the data. The Reformer expects inputs of this form: </p>
<p><strong>Person 1: Why am I so happy? Person 2: Because you are learning NLP Person 1: … Person 2: …</strong>*</p>
<p>And the conversation keeps going with some text. As you can see ‘Person 1’ and ‘Person 2’ act as delimiters so the model automatically recognizes the person and who is talking. It can then come up with the corresponding text responses for each person. Let’s proceed to process the text in this fashion for the Reformer. First, let’s grab all the conversation strings from all dialogue files and put them in a list.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the keys are the file names</span></span><br><span class="line">all_files = DIALOGUE_DB.keys()</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize empty list</span></span><br><span class="line">untokenized_data = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># loop over all files</span></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> all_files:</span><br><span class="line">    <span class="comment"># this is the graded function you coded</span></span><br><span class="line">    <span class="comment"># returns a string delimited by Person 1 and Person 2</span></span><br><span class="line">    result = get_conversation(file, DIALOGUE_DB)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># append to the list</span></span><br><span class="line">    untokenized_data.append(result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the first element to check if it&#x27;s the same as the one we got before</span></span><br><span class="line"><span class="built_in">print</span>(untokenized_data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<pre><code> Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it&#39;s cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn&#39;t able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.
Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.
</code></pre>
<p>Now let us split the list to a train and eval dataset.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># shuffle the list we generated above</span></span><br><span class="line">random.shuffle(untokenized_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define a cutoff (5% of the total length for this assignment)</span></span><br><span class="line"><span class="comment"># convert to int because we will use it as a list index</span></span><br><span class="line">cut_off = <span class="built_in">int</span>(<span class="built_in">len</span>(untokenized_data) * <span class="number">.05</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># slice the list. the last elements after the cut_off value will be the eval set. the rest is for training. </span></span><br><span class="line">train_data, eval_data = untokenized_data[:-cut_off], untokenized_data[-cut_off:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;number of conversations in the data set: <span class="subst">&#123;<span class="built_in">len</span>(untokenized_data)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;number of conversations in train set: <span class="subst">&#123;<span class="built_in">len</span>(train_data)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;number of conversations in eval set: <span class="subst">&#123;<span class="built_in">len</span>(eval_data)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>number of conversations in the data set: 10438
number of conversations in train set: 9917
number of conversations in eval set: 521
</code></pre>
<p><a name="2.1"></a></p>
<h2 id="2-1-Tokenizing-batching-with-bucketing"><a href="#2-1-Tokenizing-batching-with-bucketing" class="headerlink" title="2.1   Tokenizing, batching with bucketing"></a>2.1   Tokenizing, batching with bucketing</h2><p>We can now proceed in generating tokenized batches of our data. Let’s first define a utility generator function to yield elements from our data sets:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stream</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="comment"># loop over the entire data</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># get a random element</span></span><br><span class="line">        d = random.choice(data)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># yield a tuple pair of identical values </span></span><br><span class="line">        <span class="comment"># (i.e. our inputs to the model will also be our targets during training)</span></span><br><span class="line">        <span class="keyword">yield</span> (d, d)</span><br></pre></td></tr></table></figure>

<p>Now let’s define our data pipeline for tokenizing and batching our data. As in the previous assignments, we will bucket by length and also have an upper bound on the token length.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># trax allows us to use combinators to generate our data pipeline</span></span><br><span class="line">data_pipeline = trax.data.Serial(</span><br><span class="line">    <span class="comment"># randomize the stream</span></span><br><span class="line">    trax.data.Shuffle(),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># tokenize the data</span></span><br><span class="line">    trax.data.Tokenize(vocab_dir=VOCAB_DIR,</span><br><span class="line">                       vocab_file=VOCAB_FILE),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># filter too long sequences</span></span><br><span class="line">    trax.data.FilterByLength(<span class="number">2048</span>),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># bucket by length</span></span><br><span class="line">    trax.data.BucketByLength(boundaries=[<span class="number">128</span>, <span class="number">256</span>,  <span class="number">512</span>, <span class="number">1024</span>],</span><br><span class="line">                             batch_sizes=[<span class="number">16</span>,    <span class="number">8</span>,    <span class="number">4</span>,   <span class="number">2</span>, <span class="number">1</span>]),</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># add loss weights but do not add it to the padding tokens (i.e. 0)</span></span><br><span class="line">    trax.data.AddLossWeights(id_to_mask=<span class="number">0</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># apply the data pipeline to our train and eval sets</span></span><br><span class="line">train_stream = data_pipeline(stream(train_data))</span><br><span class="line">eval_stream = data_pipeline(stream(eval_data))</span><br></pre></td></tr></table></figure>

<p>Peek into the train stream.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the stream generators will yield (input, target, weights). let&#x27;s just grab the input for inspection</span></span><br><span class="line">inp, _, _ = <span class="built_in">next</span>(train_stream)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the shape. format is (batch size, token length)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input shape: &quot;</span>, inp.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># detokenize the first element</span></span><br><span class="line"><span class="built_in">print</span>(trax.data.detokenize(inp[<span class="number">0</span>], vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE))</span><br></pre></td></tr></table></figure>

<pre><code>input shape:  (4, 512)
 Person 1: I need a place to stay that has free wifi.  Person 2: There are 32 options in Cambridge, what price range are you looking for? Person 1: I&#39;m looking for something in the cheap price range, but I need it to have a 4 star rating. I don&#39;t need any parking though. Person 2: Again I have many to choose from that meet those criteria. Would you like a suggestion? Person 1: Ok, yes, if you could suggest one that comes with free parking that would be great! Person 2: I will book it for you,is there anything else I can do for you ? Person 1: I also need a Vietnamese restaurant. Person 2: My apologies it appears that I forgot to book your lodging. I recommend Alexander Bed and Breakfast, would you like me to book it for you? Person 1: Oh yes, please do. I need it for 8 people and 5 nights, beginning friday Person 2: You are booked with the reference number E9100B48. I can help you with the Vietnamese restaurant now. Do you have an area in mind? Person 1: I just want the restaurant to be in the same price range as my hotel Person 2: There is one cheap vietnamese restaurant in town. It is thanh binh. Do you want to book? Person 1: No, just provide me with the address and area for that restaurant if you could Person 2: The restaurant is located at 17 Magdalene Street City Centre in the West.  Can I help you with anything else? Person 1: Yes, will you book me a taxi to the restaurant from the hotel, please Person 2: And what time would you like that taxi? Person 1: I would like to leave the hotel by 22:15. Person 2: Your taxi service was book with a red volkswagen. The contact number is 07797935179 in case you need to contact them. Person 1: Thank you, that will be all. Person 2: You are welcome enjoy your meal. Have a good evenening
</code></pre>
<p><a name="3"></a></p>
<h1 id="Part-3-Reversible-layers"><a href="#Part-3-Reversible-layers" class="headerlink" title="Part 3:   Reversible layers"></a>Part 3:   Reversible layers</h1><p>When running large deep models, you will often run out of memory as each layer allocates memory to store activations for use in backpropagation. To save this resource, you need to be able to recompute these activations during the backward pass without storing them during the forward pass. Take a look first at the leftmost diagram below. </p>
<img src="reversible2.PNG" height="400" width="600">

<p>This is how the residual networks are implemented in the standard Transformer. It follows that, given <code>F()</code> is Attention and <code>G()</code> is Feed-forward(FF).<br>: </p>
<p>\begin{align}<br>\mathrm{y}<em>\mathrm{a} &amp;= \mathrm{x} + \mathrm{F}\left(\mathrm{x}\right)\tag{1} \<br>\mathrm{y}</em>{b}&amp;=\mathrm{y}<em>{a}+\mathrm{G}\left(\mathrm{y}</em>{a}\right)\tag{2}\<br>\end{align}</p>
<p>As you can see, it requires that $\mathrm{x}$ and $\mathrm{y}_{a}$ be saved so it can be used during backpropagation. We want to avoid this to conserve memory and this is where reversible residual connections come in. They are shown in the middle and rightmost diagrams above. The key idea is that we will start with two copies of the input to the model and at each layer we will only update one of them. The activations that we <em>don’t</em> update are the ones that will be used to compute the residuals. </p>
<p>Now in this reversible set up you get the following instead: </p>
<p>\begin{align}<br>\mathrm{y}<em>{1}&amp;=\mathrm{x}</em>{1}+\mathrm{F}\left(\mathrm{x}<em>{2}\right)\tag{3}\<br>\mathrm{y}</em>{2}&amp;=\mathrm{x}<em>{2}+\mathrm{G}\left(\mathrm{y}</em>{1}\right)\tag{4}\<br>\end{align}<br>To recover $\mathrm{(x_1,x_2)}$ from $\mathrm{(y_1, y_2)}$ </p>
<p>\begin{align}<br>\mathrm{x}<em>{2}&amp;=\mathrm{y}</em>{2}-\mathrm{G}\left(\mathrm{y}<em>{1}\right)\tag{5}\<br>\mathrm{x}</em>{1}&amp;=\mathrm{y}<em>{1}-\mathrm{F}\left(\mathrm{x}</em>{2}\right)\tag{6}\<br>\end{align}</p>
<p>With this configuration, we’re now able to run the network fully in reverse. You’ll notice that during the backward pass, $\mathrm{x2}$ and $\mathrm{x1}$ can be recomputed based solely on the values of $\mathrm{y2}$ and $\mathrm{y1}$. No need to save it during the forward pass.</p>
<p><a name="ex02"></a></p>
<h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><p><strong>Instructions:</strong> You will implement the <code>reversible_layer_forward</code> function using equations 3 and 4 above. This function takes in the input vector <code>x</code> and the functions <code>f</code> and <code>g</code> and returns the concatenation of $y_1 and y_2$. For this exercise, we will be splitting <code>x</code> before going through the reversible residual steps$\mathrm{^1}$.  We can then use those two vectors for the <code>reversible_layer_reverse</code> function. Utilize <code>np.concatenate()</code> to form the output being careful to match the axis of the <code>np.split()</code>.</p>
<p>$\mathrm{^1}$<em>Take note that this is just for demonstrating the concept in this exercise and there are other ways of processing the input. As you’ll see in the Reformer architecture later, the initial input (i.e. <code>x</code>) can instead be duplicated instead of split.</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: reversible_layer_forward</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reversible_layer_forward</span>(<span class="params">x, f, g</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        x (np.array): an input vector or matrix</span></span><br><span class="line"><span class="string">        f (function): a function which operates on a vector/matrix</span></span><br><span class="line"><span class="string">        g (function): a function which operates on a vector/matrix</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        y (np.array): an output vector or matrix whose form is determined by &#x27;x&#x27;, f and g</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># split the input vector into two (* along the last axis because it is the depth dimension)</span></span><br><span class="line">    x1, x2 = np.split(x, <span class="number">2</span>, axis=-<span class="number">1</span>) </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get y1 using equation 3</span></span><br><span class="line">    y1 = x1 + f(x2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get y2 using equation 4</span></span><br><span class="line">    y2 = x2 + g(y1)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># concatenate y1 and y2 along the depth dimension. be sure output is of type np.ndarray</span></span><br><span class="line">    y = np.concatenate((y1,y2), axis = -<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_reversible_layer_forward(reversible_layer_forward)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure>


<p><a name="ex03"></a></p>
<h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p>You will now implement the <code>reversible_layer_reverse</code> function  which is possible because at every time step you have $x_1$ and $x_2$ and $y_2$ and $y_1$, along with the function <code>f</code>, and <code>g</code>. Where <code>f</code> is the attention and <code>g</code> is the feedforward. This allows you to compute equations 5 and 6.</p>
<p><strong>Instructions:</strong> Implement the <code>reversible_layer_reverse</code>. Your function takes in the output vector from  <code>reversible_layer_forward</code> and functions f and g. Using equations 5 and 6 above, it computes the inputs to the layer,  $x_1$ and $x_2$.  The output, x, is the concatenation of  $x_1, x_2$. Utilize <code>np.concatenate()</code>  to form the output being careful to match the axis of the <code>np.split()</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: reversible_layer_reverse</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reversible_layer_reverse</span>(<span class="params">y, f, g</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        y (np.array): an input vector or matrix</span></span><br><span class="line"><span class="string">        f (function): a function which operates on a vector/matrix of the form of &#x27;y&#x27;</span></span><br><span class="line"><span class="string">        g (function): a function which operates on a vector/matrix of the form of &#x27;y&#x27;</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        y (np.array): an output vector or matrix whose form is determined by &#x27;y&#x27;, f and g</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># split the input vector into two (* along the last axis because it is the depth dimension)</span></span><br><span class="line">    y1, y2 = np.split(y, <span class="number">2</span>, axis=-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute x2 using equation 5</span></span><br><span class="line">    x2 = y2 - g(y1)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute x1 using equation 6</span></span><br><span class="line">    x1 = y1 - f(x2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># concatenate x1 and x2 along the depth dimension</span></span><br><span class="line">    x = np.concatenate((x1,x2),axis = -<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_reversible_layer_reverse(reversible_layer_reverse)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST COMMENT: assert at the end can be used in grading as well</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: x + <span class="number">2</span></span><br><span class="line">g = <span class="keyword">lambda</span> x: x * <span class="number">3</span></span><br><span class="line">input_vector = np.random.uniform(size=(<span class="number">32</span>,))</span><br><span class="line"></span><br><span class="line">output_vector = reversible_layer_forward(input_vector, f, g)</span><br><span class="line">reversed_vector = reversible_layer_reverse(output_vector, f, g)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(reversed_vector, input_vector)</span><br></pre></td></tr></table></figure>

<p><a name="3.1"></a></p>
<h2 id="3-1-Reversible-layers-and-randomness"><a href="#3-1-Reversible-layers-and-randomness" class="headerlink" title="3.1   Reversible layers and randomness"></a>3.1   Reversible layers and randomness</h2><p>This is why we were learning about fastmath’s random functions and keys in Course 3 Week 1. Utilizing the same key, <code>trax.fastmath.random.uniform()</code> will return the same values. This is required for the backward pass to return the correct layer inputs when random noise is introduced in the layer.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Layers like dropout have noise, so let&#x27;s simulate it here:</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: x + np.random.uniform(size=x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># See that the above doesn&#x27;t work any more:</span></span><br><span class="line">output_vector = reversible_layer_forward(input_vector, f, g)</span><br><span class="line">reversed_vector = reversible_layer_reverse(output_vector, f, g)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> <span class="keyword">not</span> np.allclose(reversed_vector, input_vector)  <span class="comment"># Fails!!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># It failed because the noise when reversing used a different random seed.</span></span><br><span class="line"></span><br><span class="line">random_seed = <span class="number">27686</span></span><br><span class="line">rng = trax.fastmath.random.get_prng(random_seed)</span><br><span class="line">f = <span class="keyword">lambda</span> x: x + trax.fastmath.random.uniform(key=rng, shape=x.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># See that it works now as the same rng is used on forward and reverse.</span></span><br><span class="line">output_vector = reversible_layer_forward(input_vector, f, g)</span><br><span class="line">reversed_vector = reversible_layer_reverse(output_vector, f, g)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(reversed_vector, input_vector,  atol=<span class="number">1e-07</span>) </span><br></pre></td></tr></table></figure>

<p><a name="4"></a></p>
<h1 id="Part-4-ReformerLM-Training"><a href="#Part-4-ReformerLM-Training" class="headerlink" title="Part 4:   ReformerLM Training"></a>Part 4:   ReformerLM Training</h1><p>You will now proceed to training your model. Since you have already know the two main components that differentiates it from the standard Transformer, LSH in Course 1 and reversible layers above, you can just use the pre-built model already implemented in Trax. It will have this architecture:</p>
<img src='Reformer.jpg'>

<p>Similar to the Transformer you learned earlier, you want to apply an attention and feed forward layer to your inputs. For the Reformer, we improve the memory efficiency by using <strong>reversible decoder blocks</strong> and you can picture its implementation in Trax like below:</p>
<img src='ReversibleDecoder.png'>

<p>You can see that it takes the initial inputs <code>x1</code> and <code>x2</code> and does the first equation of the reversible networks you learned in Part 3. As you’ve also learned, the reversible residual has two equations for the forward-pass so doing just one of them will just constitute half of the reversible decoder block. Before doing the second equation (i.e. second half of the reversible residual), it first needs to swap the elements to take into account the stack semantics in Trax. It simply puts <code>x2</code> on top of the stack so it can be fed to the add block of the half-residual layer. It then swaps the two outputs again so it can be fed to the next layer of the network. All of these arrives at the two equations in Part 3 and it can be used to recompute the activations during the backward pass.</p>
<p>These are already implemented for you in Trax and in the following exercise, you’ll get to practice how to call them to build your network.</p>
<p><a name="ex04"></a></p>
<h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p><strong>Instructions:</strong> Implement a wrapper function that returns a Reformer Language Model. You can use Trax’s <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.models.html#trax.models.reformer.reformer.ReformerLM">ReformerLM</a> to do this quickly. It will have the same architecture as shown above.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReformerLM</span>(<span class="params">vocab_size=<span class="number">33000</span>, n_layers=<span class="number">2</span>, mode=<span class="string">&#x27;train&#x27;</span>, attention_type=tl.SelfAttention</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args: </span></span><br><span class="line"><span class="string">        vocab_size (int): size of the vocabulary</span></span><br><span class="line"><span class="string">        n_layers (int): number of decoder layers</span></span><br><span class="line"><span class="string">        mode (string): setting of the model which can be &#x27;train&#x27;, &#x27;eval&#x27;, or &#x27;predict&#x27; </span></span><br><span class="line"><span class="string">        attention_type(class): attention class to use </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">        model (ReformerLM): a reformer language model implemented in Trax</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    <span class="comment"># initialize an instance of Trax&#x27;s ReformerLM class</span></span><br><span class="line">    model = trax.models.reformer.ReformerLM( </span><br><span class="line">        <span class="comment"># set vocab size</span></span><br><span class="line">        vocab_size = vocab_size,</span><br><span class="line">        <span class="comment"># set number of layers</span></span><br><span class="line">        n_layers = n_layers,</span><br><span class="line">        <span class="comment"># set mode</span></span><br><span class="line">        mode = mode,</span><br><span class="line">        <span class="comment"># set attention type</span></span><br><span class="line">        attention_type = attention_type</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># display the model</span></span><br><span class="line">temp_model = ReformerLM(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">str</span>(temp_model))</span><br><span class="line"></span><br><span class="line"><span class="comment"># free memory</span></span><br><span class="line"><span class="keyword">del</span> temp_model </span><br></pre></td></tr></table></figure>

<pre><code>Serial[
  ShiftRight(1)
  Embedding_train_512
  Dropout
  PositionalEncoding
  Dup_out2
  ReversibleSerial_in2_out2[
    ReversibleHalfResidualV2_in2_out2[
      Serial[
        LayerNorm
      ]
      SelfAttention
    ]
    ReversibleSwap_in2_out2
    ReversibleHalfResidualV2_in2_out2[
      Serial[
        LayerNorm
        Dense_2048
        Dropout
        FastGelu
        Dense_512
        Dropout
      ]
    ]
    ReversibleSwap_in2_out2
    ReversibleHalfResidualV2_in2_out2[
      Serial[
        LayerNorm
      ]
      SelfAttention
    ]
    ReversibleSwap_in2_out2
    ReversibleHalfResidualV2_in2_out2[
      Serial[
        LayerNorm
        Dense_2048
        Dropout
        FastGelu
        Dense_512
        Dropout
      ]
    ]
    ReversibleSwap_in2_out2
  ]
  Concatenate_in2
  LayerNorm
  Dropout
  Dense_train
  LogSoftmax
]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_ReformerLM(ReformerLM)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure>


<p><a name="ex05"></a></p>
<h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p>You will now write a function that takes in your model and trains it. </p>
<p><strong>Instructions:</strong> Implement the <code>training_loop</code> below to train the neural network above. Here is a list of things you should do:</p>
<ul>
<li>Create <code>TrainTask</code> and <code>EvalTask</code></li>
<li>Create the training loop <code>trax.supervised.training.Loop</code></li>
<li>Pass in the following depending to train_task :<ul>
<li><code>labeled_data=train_gen</code></li>
<li><code>loss_layer=tl.CrossEntropyLoss()</code></li>
<li><code>optimizer=trax.optimizers.Adam(0.01)</code></li>
<li><code>lr_schedule=lr_schedule</code></li>
<li><code>n_steps_per_checkpoint=10</code>  </li>
</ul>
</li>
</ul>
<p>You will be using your CrossEntropyLoss loss function with Adam optimizer. Please read the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html?highlight=adam#trax.optimizers.adam.Adam">trax</a> documentation to get a full understanding. </p>
<ul>
<li>Pass in the following to eval_task:<ul>
<li><code>labeled_data=eval_gen</code></li>
<li><code>metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]</code></li>
</ul>
</li>
</ul>
<p>This function should return a <code>training.Loop</code> object. To read more about this check the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html?highlight=loop#trax.supervised.training.Loop">docs</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: train_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_loop</span>(<span class="params">ReformerLM, train_gen, eval_gen, output_dir = <span class="string">&quot;./model/&quot;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        ReformerLM:  the Reformer language model you are building</span></span><br><span class="line"><span class="string">        train_gen (generator): train data generator.</span></span><br><span class="line"><span class="string">        eval_gen (generator): Validation generator. </span></span><br><span class="line"><span class="string">        output_dir (string): Path to save the model output. Defaults to &#x27;./model/&#x27;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.supervised.training.Loop: Training loop for the model.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># use the warmup_and_rsqrt_decay learning rate schedule</span></span><br><span class="line">    lr_schedule = trax.lr.warmup_and_rsqrt_decay(</span><br><span class="line">        n_warmup_steps=<span class="number">1000</span>, max_value=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># define the train task</span></span><br><span class="line">    train_task = training.TrainTask(            </span><br><span class="line">        <span class="comment"># labeled data</span></span><br><span class="line">        labeled_data = train_gen,</span><br><span class="line">        <span class="comment"># loss layer</span></span><br><span class="line">        loss_layer = tl.CrossEntropyLoss(),</span><br><span class="line">        <span class="comment"># optimizer</span></span><br><span class="line">        optimizer=trax.optimizers.Adam(<span class="number">0.01</span>),</span><br><span class="line">        <span class="comment"># lr_schedule</span></span><br><span class="line">        lr_schedule=lr_schedule,</span><br><span class="line">        <span class="comment"># n_steps</span></span><br><span class="line">        n_steps_per_checkpoint=<span class="number">10</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># define the eval task</span></span><br><span class="line">    eval_task = training.EvalTask(                      </span><br><span class="line">        <span class="comment"># labeled data</span></span><br><span class="line">        labeled_data = eval_gen,</span><br><span class="line">        metrics = [tl.CrossEntropyLoss(), tl.Accuracy()]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    loop = training.Loop(ReformerLM(mode=<span class="string">&#x27;train&#x27;</span>),</span><br><span class="line">                         train_task,</span><br><span class="line">                         eval_tasks=[eval_task],</span><br><span class="line">                         output_dir=output_dir)</span><br><span class="line">    <span class="keyword">return</span> loop</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNIT TEST COMMENT: Use the train task and eval task for grading train_model</span></span><br><span class="line">test_loop = training_loop(ReformerLM, train_stream, eval_stream)</span><br><span class="line">train_task = test_loop._task</span><br><span class="line">eval_task = test_loop._eval_task</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_task)</span><br><span class="line"><span class="built_in">print</span>(eval_task)</span><br></pre></td></tr></table></figure>

<pre><code>&lt;trax.supervised.training.TrainTask object at 0x7fd4ddf95dd0&gt;
&lt;trax.supervised.training.EvalTask object at 0x7fd4dc2a2f50&gt;
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line">w4_unittest.test_tasks(train_task, eval_task)</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># we will now test your function</span></span><br><span class="line">!rm -f model/model.pkl.gz</span><br><span class="line">loop = training_loop(ReformerLM, train_stream, eval_stream)</span><br><span class="line">loop.run(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Step      1: Ran 1 train steps in 58.71 secs
Step      1: train CrossEntropyLoss |  10.41530514
Step      1: eval  CrossEntropyLoss |  10.41272354
Step      1: eval          Accuracy |  0.00000000

Step     10: Ran 9 train steps in 163.46 secs
Step     10: train CrossEntropyLoss |  10.25675583
Step     10: eval  CrossEntropyLoss |  9.94296360
Step     10: eval          Accuracy |  0.11201393
</code></pre>
<p><strong>Approximate Expected output:</strong>  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Step      1: Ran 1 train steps in 55.73 secs</span><br><span class="line">Step      1: train CrossEntropyLoss |  10.41907787</span><br><span class="line">Step      1: eval  CrossEntropyLoss |  10.41005802</span><br><span class="line">Step      1: eval          Accuracy |  0.00000000</span><br><span class="line"></span><br><span class="line">Step     10: Ran 9 train steps in 108.21 secs</span><br><span class="line">Step     10: train CrossEntropyLoss |  10.15449715</span><br><span class="line">Step     10: eval  CrossEntropyLoss |  9.63478279</span><br><span class="line">Step     10: eval          Accuracy |  0.16350447</span><br></pre></td></tr></table></figure>

<p><a name="5"></a></p>
<h1 id="Part-5-Decode-from-a-pretrained-model"><a href="#Part-5-Decode-from-a-pretrained-model" class="headerlink" title="Part 5:   Decode from a pretrained model"></a>Part 5:   Decode from a pretrained model</h1><p>We will now proceed on decoding using the model architecture you just implemented. As in the previous weeks, we will be giving you a pretrained model so you can observe meaningful output during inference. You will be using the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.decoding.autoregressive_sample_stream">autoregressive_sample_stream()</a> decoding method from Trax to do fast inference. Let’s define a few parameters to initialize our model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define the `predict_mem_len` and `predict_drop_len` of tl.SelfAttention</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">    <span class="comment"># number of input positions to remember in a cache when doing fast inference. </span></span><br><span class="line">    kwargs[<span class="string">&#x27;predict_mem_len&#x27;</span>] = <span class="number">120</span></span><br><span class="line">    <span class="comment"># number of input elements to drop once the fast inference input cache fills up.</span></span><br><span class="line">    kwargs[<span class="string">&#x27;predict_drop_len&#x27;</span>] = <span class="number">120</span></span><br><span class="line">    <span class="comment"># return the attention layer with the parameters defined above</span></span><br><span class="line">    <span class="keyword">return</span> tl.SelfAttention(*args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define the model using the ReformerLM function you implemented earlier.</span></span><br><span class="line">model = ReformerLM(</span><br><span class="line">    vocab_size=<span class="number">33000</span>,</span><br><span class="line">    n_layers=<span class="number">6</span>,</span><br><span class="line">    mode=<span class="string">&#x27;predict&#x27;</span>,</span><br><span class="line">    attention_type=attention,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define an input signature so we can initialize our model. shape will be (1, 1) and the data type is int32.</span></span><br><span class="line">shape11 = trax.shapes.ShapeDtype((<span class="number">1</span>, <span class="number">1</span>), dtype=np.int32)</span><br></pre></td></tr></table></figure>

<p>We can now initialize our model from a file containing the pretrained weights. We will save this starting state so we can reset the model state when we generate a new conversation. This will become clearer in the <code>generate_dialogue()</code> function later.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialize from file</span></span><br><span class="line">model.init_from_file(<span class="string">&#x27;chatbot_model1.pkl.gz&#x27;</span>,</span><br><span class="line">                     weights_only=<span class="literal">True</span>, input_signature=shape11)</span><br><span class="line"></span><br><span class="line"><span class="comment"># save the starting state</span></span><br><span class="line">STARTING_STATE = model.state</span><br></pre></td></tr></table></figure>

<p>Let’s define a few utility functions as well to help us tokenize and detokenize. We can use the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.tokenize">tokenize()</a> and <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.detokenize">detokenize()</a> from <code>trax.data.tf_inputs</code> to do this.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">sentence, vocab_file, vocab_dir</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(trax.data.tokenize(<span class="built_in">iter</span>([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detokenize</span>(<span class="params">tokens, vocab_file, vocab_dir</span>):</span></span><br><span class="line">    <span class="keyword">return</span> trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)</span><br></pre></td></tr></table></figure>

<p>We are now ready to define our decoding function. This will return a generator that yields that next symbol output by the model. It will be able to predict the next words by just feeding it a starting sentence.</p>
<p><a name="ex06"></a></p>
<h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p><strong>Instructions:</strong> Implement the function below to return a generator that predicts the next word of the conversation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReformerLM_output_gen</span>(<span class="params">ReformerLM, start_sentence, vocab_file, vocab_dir, temperature</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        ReformerLM:  the Reformer language model you just trained</span></span><br><span class="line"><span class="string">        start_sentence (string): starting sentence of the conversation</span></span><br><span class="line"><span class="string">        vocab_file (string): vocabulary filename</span></span><br><span class="line"><span class="string">        vocab_dir (string): directory of the vocabulary file</span></span><br><span class="line"><span class="string">        temperature (float): parameter for sampling ranging from 0.0 to 1.0.</span></span><br><span class="line"><span class="string">            0.0: same as argmax, always pick the most probable token</span></span><br><span class="line"><span class="string">            1.0: sampling from the distribution (can sometimes say random things)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        generator: yields the next symbol generated by the model</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create input tokens using the the tokenize function</span></span><br><span class="line">    input_tokens = tokenize(start_sentence, vocab_file, vocab_dir)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add batch dimension to array. Convert from (n,) to (x, n) where </span></span><br><span class="line">    <span class="comment"># x is the batch size. Default is 1. (hint: you can use np.expand_dims() with axis=0)</span></span><br><span class="line">    input_tokens_with_batch = np.expand_dims(input_tokens, axis = <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># call the autoregressive_sample_stream function from trax</span></span><br><span class="line">    output_gen = trax.supervised.decoding.autoregressive_sample_stream( </span><br><span class="line">        <span class="comment"># model</span></span><br><span class="line">        ReformerLM,</span><br><span class="line">        <span class="comment"># inputs will be the tokens with batch dimension</span></span><br><span class="line">        inputs = input_tokens_with_batch,</span><br><span class="line">        <span class="comment"># temperature</span></span><br><span class="line">        temperature = temperature</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output_gen</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BEGIN UNIT TEST</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">WEIGHTS_FROM_FILE = ()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;weights&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    WEIGHTS_FROM_FILE = pickle.load(file)</span><br><span class="line"></span><br><span class="line">shape11 = trax.shapes.ShapeDtype((<span class="number">1</span>, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">    kwargs[<span class="string">&#x27;predict_mem_len&#x27;</span>] = <span class="number">120</span></span><br><span class="line">    kwargs[<span class="string">&#x27;predict_drop_len&#x27;</span>] = <span class="number">120</span></span><br><span class="line">    <span class="keyword">return</span> tl.SelfAttention(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">test_model = ReformerLM(vocab_size=<span class="number">5</span>, n_layers=<span class="number">1</span>, mode=<span class="string">&#x27;predict&#x27;</span>, attention_type=attention)</span><br><span class="line"></span><br><span class="line">test_output_gen = ReformerLM_output_gen(test_model, <span class="string">&quot;test&quot;</span>, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">test_model.init_weights_and_state(shape11)</span><br><span class="line"></span><br><span class="line">test_model.weights = WEIGHTS_FROM_FILE</span><br><span class="line"></span><br><span class="line">output = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    output.append(<span class="built_in">next</span>(test_output_gen)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># free memory</span></span><br><span class="line"><span class="keyword">del</span> test_model </span><br><span class="line"><span class="keyword">del</span> WEIGHTS_FROM_FILE</span><br><span class="line"><span class="keyword">del</span> test_output_gen</span><br><span class="line"><span class="comment"># END UNIT TEST</span></span><br></pre></td></tr></table></figure>

<pre><code>[1, 0, 4, 3, 0, 4]
</code></pre>
<p><em><strong>Expected value:</strong></em></p>
<p>[1, 0, 4, 3, 0, 4]</p>
<p>Great! Now you will be able to see the model in action. The utility function below will call the generator you just implemented and will just format the output to be easier to read. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">shape11 = trax.shapes.ShapeDtype((<span class="number">1</span>, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">    kwargs[<span class="string">&#x27;predict_mem_len&#x27;</span>] = <span class="number">120</span>  <span class="comment"># max length for predictions</span></span><br><span class="line">    kwargs[<span class="string">&#x27;predict_drop_len&#x27;</span>] = <span class="number">120</span>  <span class="comment"># never drop old stuff</span></span><br><span class="line">    <span class="keyword">return</span> tl.SelfAttention(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">model = ReformerLM(</span><br><span class="line">    vocab_size=<span class="number">33000</span>,</span><br><span class="line">    n_layers=<span class="number">6</span>,</span><br><span class="line">    mode=<span class="string">&#x27;predict&#x27;</span>,</span><br><span class="line">    attention_type=attention,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.init_from_file(<span class="string">&#x27;chatbot_model1.pkl.gz&#x27;</span>,</span><br><span class="line">                     weights_only=<span class="literal">True</span>, input_signature=shape11)</span><br><span class="line"></span><br><span class="line">STARTING_STATE = model.state</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_dialogue</span>(<span class="params">ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        ReformerLM:  the Reformer language model you just trained</span></span><br><span class="line"><span class="string">        model_state (np.array): initial state of the model before decoding</span></span><br><span class="line"><span class="string">        start_sentence (string): starting sentence of the conversation</span></span><br><span class="line"><span class="string">        vocab_file (string): vocabulary filename</span></span><br><span class="line"><span class="string">        vocab_dir (string): directory of the vocabulary file</span></span><br><span class="line"><span class="string">        max_len (int): maximum number of tokens to generate </span></span><br><span class="line"><span class="string">        temperature (float): parameter for sampling ranging from 0.0 to 1.0.</span></span><br><span class="line"><span class="string">            0.0: same as argmax, always pick the most probable token</span></span><br><span class="line"><span class="string">            1.0: sampling from the distribution (can sometimes say random things)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        generator: yields the next symbol generated by the model</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># define the delimiters we used during training</span></span><br><span class="line">    delimiter_1 = <span class="string">&#x27;Person 1: &#x27;</span> </span><br><span class="line">    delimiter_2 = <span class="string">&#x27;Person 2: &#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize detokenized output</span></span><br><span class="line">    sentence = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># token counter</span></span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># output tokens. we insert a &#x27;: &#x27; for formatting</span></span><br><span class="line">    result = [tokenize(<span class="string">&#x27;: &#x27;</span>, vocab_file=vocab_file, vocab_dir=vocab_dir)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># reset the model state when starting a new dialogue</span></span><br><span class="line">    ReformerLM.state = model_state</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calls the output generator implemented earlier</span></span><br><span class="line">    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># print the starting sentence</span></span><br><span class="line">    <span class="built_in">print</span>(start_sentence.split(delimiter_2)[<span class="number">0</span>].strip())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.</span></span><br><span class="line">    <span class="keyword">for</span> o <span class="keyword">in</span> output:</span><br><span class="line">        </span><br><span class="line">        result.append(o)</span><br><span class="line">        </span><br><span class="line">        sentence = detokenize(np.concatenate(result, axis=<span class="number">0</span>), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> sentence.endswith(delimiter_1):</span><br><span class="line">            sentence = sentence.split(delimiter_1)[<span class="number">0</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;delimiter_2&#125;</span><span class="subst">&#123;sentence&#125;</span>&#x27;</span>)</span><br><span class="line">            sentence = <span class="string">&#x27;&#x27;</span></span><br><span class="line">            result.clear()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">elif</span> sentence.endswith(delimiter_2):</span><br><span class="line">            sentence = sentence.split(delimiter_2)[<span class="number">0</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;delimiter_1&#125;</span><span class="subst">&#123;sentence&#125;</span>&#x27;</span>)</span><br><span class="line">            sentence = <span class="string">&#x27;&#x27;</span></span><br><span class="line">            result.clear()</span><br><span class="line"></span><br><span class="line">        counter += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> counter &gt; max_len:</span><br><span class="line">            <span class="keyword">break</span>    </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>We can now feed in different starting sentences and see how the model generates the dialogue. You can even input your own starting sentence. Just remember to ask a question that covers the topics in the Multiwoz dataset so you can generate a meaningful conversation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_sentence = <span class="string">&#x27; Person 1: Are there theatres in town? Person 2: &#x27;</span></span><br><span class="line">generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=<span class="number">120</span>, temperature=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Person 1: Are there theatres in town?
Person 2: : There are 4 theatres in town. Do you have a preference on area? 
Person 1: No, I don&#39;t care. Which one do you recommend? 
Person 2: I would recommend the Mumford Theatre. Would you like more information on it? 
Person 1: Yes, could I get the postcode and phone number please? 
Person 2: The phone number is 08451962320 and the postcode is cb11pt. The phone number is 084519/ 15/15 - would you like to book a table? 
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_sentence = <span class="string">&#x27; Person 1: Is there a hospital nearby? Person 2: &#x27;</span></span><br><span class="line">generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=<span class="number">120</span>, temperature=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Person 1: Is there a hospital nearby?
Person 2: : Addensbrookes Hospital is located at Hills Rd, Cambridge, postcode CB20QQ. Do you need anything else? 
Person 1: No, that&#39;s all I need. Thanks. 
Person 2: You&#39;re welcome. Have a good day.Good bye.
Person 1: Thanks again. Goodbye. 
Person 2: You&#39;re welcome. Have a good day.Good bye.
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_sentence = <span class="string">&#x27; Person 1: Can you book a taxi? Person 2: &#x27;</span></span><br><span class="line">generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=<span class="number">120</span>, temperature=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Person 1: Can you book a taxi?
Person 2: : I sure can. Where are you going? 
Person 1: I&#39;m going to be picked up from the city centre north b and b. 
Person 2: I have booked you a grey volkswagen. The contact number is 0783212843. 
Person 1: Thank you. That&#39;s all I need. 
Person 2: Thank you for using our services. Have a great day!k you.Good bye.
Person 1: Actually, I&#39;ry about there. 
</code></pre>
<p><strong>Congratulations! You just wrapped up the final assignment of this course and the entire specialization!</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Assignment-3-Question-Answering/2020/09/27/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Assignment-3-Question-Answering/2020/09/27/" class="post-title-link" itemprop="url">Assignment 3: Question Answering</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-09-27 17:13:45 / Modified: 17:15:12" itemprop="dateCreated datePublished" datetime="2020-09-27T17:13:45+08:00">2020-09-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Assignment-3-Question-Answering/2020/09/27/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Assignment-3-Question-Answering/2020/09/27/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Assignment-3-Question-Answering"><a href="#Assignment-3-Question-Answering" class="headerlink" title="Assignment 3: Question Answering"></a>Assignment 3: Question Answering</h1><p>Welcome to this week’s assignment of course 4. In this you will explore question answering. You will implement the “Text to Text Transfer from Transformers” (better known as T5). Since you implemented transformers from scratch last week you will now be able to use them. </p>
<img src = "qa.png"> 

<h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li><a href="#0">Overview</a></li>
<li><a href="#0">Part 0: Importing the Packages</a></li>
<li><a href="#1">Part 1: C4 Dataset</a><ul>
<li><a href="#1.1">1.1 Pre-Training Objective</a></li>
<li><a href="#1.2">1.2 Process C4</a><ul>
<li><a href="#1.2.1">1.2.1 Decode to natural language</a></li>
</ul>
</li>
<li><a href="#1.3">1.3 Tokenizing and Masking</a><ul>
<li><a href="#ex01">Exercise 01</a></li>
</ul>
</li>
<li><a href="#1.4">1.4 Creating the Pairs</a></li>
</ul>
</li>
<li><a href="#2">Part 2: Transfomer</a><ul>
<li><a href="#2.1">2.1 Transformer Encoder</a><ul>
<li><a href="#2.1.1">2.1.1 The Feedforward Block</a><ul>
<li><a href="#ex02">Exercise 02</a></li>
</ul>
</li>
<li><a href="#2.1.2">2.1.2 The Encoder Block</a><ul>
<li><a href="#ex03">Exercise 03</a></li>
</ul>
</li>
<li><a href="#2.1.3">2.1.3 The Transformer Encoder</a>            <ul>
<li><a href="#ex04">Exercise 04</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><a name='0'></a></p>
<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>This assignment will be different from the two previous ones. Due to memory and time constraints of this environment you will not be able to train a model and use it for inference. Instead you will create the necessary building blocks for the transformer encoder model and will use a pretrained version of the same model in two ungraded labs after this assignment.</p>
<p>After completing these 3 (1 graded and 2 ungraded) labs you will:</p>
<ul>
<li>Implement the code neccesary for Bidirectional Encoder Representation from Transformer (BERT).</li>
<li>Understand how the C4 dataset is structured.</li>
<li>Use a pretrained model for inference.</li>
<li>Understand how the “Text to Text Transfer from Transformers” or T5 model works. </li>
</ul>
<p><a name='0'></a></p>
<h1 id="Part-0-Importing-the-Packages"><a href="#Part-0-Importing-the-Packages" class="headerlink" title="Part 0: Importing the Packages"></a>Part 0: Importing the Packages</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ast</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> textwrap</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> trax </span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> decoding</span><br><span class="line"></span><br><span class="line"><span class="comment"># Will come handy later.</span></span><br><span class="line">wrapper = textwrap.TextWrapper(width=<span class="number">70</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set random seed</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<p><a name='1'></a></p>
<h2 id="Part-1-C4-Dataset"><a href="#Part-1-C4-Dataset" class="headerlink" title="Part 1: C4 Dataset"></a>Part 1: C4 Dataset</h2><p>The <a target="_blank" rel="noopener" href="https://www.tensorflow.org/datasets/catalog/c4">C4</a> is a huge data set. For the purpose of this assignment you will use a few examples out of it which are present in <code>data.txt</code>. C4 is based on the <a target="_blank" rel="noopener" href="https://commoncrawl.org/">common crawl</a> project. Feel free to read more on their website. </p>
<p>Run the cell below to see how the examples look like. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load example jsons</span></span><br><span class="line">example_jsons = <span class="built_in">list</span>(<span class="built_in">map</span>(ast.literal_eval, <span class="built_in">open</span>(<span class="string">&#x27;data.txt&#x27;</span>)))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Printing the examples to see how the data looks like</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;example number <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>: \n\n<span class="subst">&#123;example_jsons[i]&#125;</span> \n&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>example number 1: 

&#123;&#39;content-length&#39;: b&#39;1970&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&#39;Beginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.&#39;, &#39;timestamp&#39;: b&#39;2019-04-25T12:57:54Z&#39;, &#39;url&#39;: b&#39;https://klyq.com/beginners-bbq-class-taking-place-in-missoula/&#39;&#125; 

example number 2: 

&#123;&#39;content-length&#39;: b&#39;12064&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&#39;Discussion in \&#39;Mac OS X Lion (10.7)\&#39; started by axboi87, Jan 20, 2012.\nI\&#39;ve got a 500gb internal drive and a 240gb SSD.\nWhen trying to restore using disk utility i\&#39;m given the error &quot;Not enough space on disk ____ to restore&quot;\nBut I shouldn\&#39;t have to do that!!!\nAny ideas or workarounds before resorting to the above?\nUse Carbon Copy Cloner to copy one drive to the other. I\&#39;ve done this several times going from larger HDD to smaller SSD and I wound up with a bootable SSD drive. One step you have to remember not to skip is to use Disk Utility to partition the SSD as GUID partition scheme HFS+ before doing the clone. If it came Apple Partition Scheme, even if you let CCC do the clone, the resulting drive won\&#39;t be bootable. CCC usually works in &quot;file mode&quot; and it can easily copy a larger drive (that\&#39;s mostly empty) onto a smaller drive. If you tell CCC to clone a drive you did NOT boot from, it can work in block copy mode where the destination drive must be the same size or larger than the drive you are cloning from (if I recall).\nI\&#39;ve actually done this somehow on Disk Utility several times (booting from a different drive (or even the dvd) so not running disk utility from the drive your cloning) and had it work just fine from larger to smaller bootable clone. Definitely format the drive cloning to first, as bootable Apple etc..\nThanks for pointing this out. My only experience using DU to go larger to smaller was when I was trying to make a Lion install stick and I was unable to restore InstallESD.dmg to a 4 GB USB stick but of course the reason that wouldn\&#39;t fit is there was slightly more than 4 GB of data.&#39;, &#39;timestamp&#39;: b&#39;2019-04-21T10:07:13Z&#39;, &#39;url&#39;: b&#39;https://forums.macrumors.com/threads/restore-from-larger-disk-to-smaller-disk.1311329/&#39;&#125; 

example number 3: 

&#123;&#39;content-length&#39;: b&#39;5235&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&#39;Foil plaid lycra and spandex shortall with metallic slinky insets. Attached metallic elastic belt with O-ring. Headband included. Great hip hop or jazz dance costume. Made in the USA.&#39;, &#39;timestamp&#39;: b&#39;2019-04-25T10:40:23Z&#39;, &#39;url&#39;: b&#39;https://awishcometrue.com/Catalogs/Clearance/Tweens/V1960-Find-A-Way&#39;&#125; 

example number 4: 

&#123;&#39;content-length&#39;: b&#39;4967&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&quot;How many backlinks per day for new site?\nDiscussion in &#39;Black Hat SEO&#39; started by Omoplata, Dec 3, 2010.\n1) for a newly created site, what&#39;s the max # backlinks per day I should do to be safe?\n2) how long do I have to let my site age before I can start making more blinks?\nI did about 6000 forum profiles every 24 hours for 10 days for one of my sites which had a brand new domain.\nThere is three backlinks for every of these forum profile so thats 18 000 backlinks every 24 hours and nothing happened in terms of being penalized or sandboxed. This is now maybe 3 months ago and the site is ranking on first page for a lot of my targeted keywords.\nbuild more you can in starting but do manual submission and not spammy type means manual + relevant to the post.. then after 1 month you can make a big blast..\nWow, dude, you built 18k backlinks a day on a brand new site? How quickly did you rank up? What kind of competition/searches did those keywords have?&quot;, &#39;timestamp&#39;: b&#39;2019-04-21T12:46:19Z&#39;, &#39;url&#39;: b&#39;https://www.blackhatworld.com/seo/how-many-backlinks-per-day-for-new-site.258615/&#39;&#125; 

example number 5: 

&#123;&#39;content-length&#39;: b&#39;4499&#39;, &#39;content-type&#39;: b&#39;text/plain&#39;, &#39;text&#39;: b&#39;The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what\xe2\x80\x99s included in the mill levy measure.&#39;, &#39;timestamp&#39;: b&#39;2019-04-20T14:33:21Z&#39;, &#39;url&#39;: b&#39;http://bond.dpsk12.org/category/news/&#39;&#125; 
</code></pre>
<p>Notice the <code>b</code> before each string? This means that this data comes as bytes rather than strings. Strings are actually lists of bytes so for the rest of the assignments the name <code>strings</code> will be used to describe the data. </p>
<p>To check this run the following cell:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">type</span>(example_jsons[<span class="number">0</span>].get(<span class="string">&#x27;text&#x27;</span>))</span><br></pre></td></tr></table></figure>




<pre><code>bytes
</code></pre>
<p><a name='1.1'></a></p>
<h3 id="1-1-Pre-Training-Objective"><a href="#1-1-Pre-Training-Objective" class="headerlink" title="1.1 Pre-Training Objective"></a>1.1 Pre-Training Objective</h3><p><strong>Note:</strong> The word “mask” will be used throughout this assignment in context of hiding/removing word(s)</p>
<p>You will be implementing the BERT loss as shown in the following image. </p>
<img src = "loss.png" width="600" height = "400">

<p>Assume you have the following text: <span style = "color:blue"> <strong>Thank you <span style = "color:red">for inviting </span> me to your party <span style = "color:red">last</span>  week</strong> </span> </p>
<p>Now as input you will mask the words in red in the text: </p>
<p><span style = "color:blue"> <strong>Input:</strong></span> Thank you  <strong>X</strong> me to your party <strong>Y</strong> week.</p>
<p><span style = "color:blue"><strong>Output:</strong></span> The model should predict the words(s) for <strong>X</strong> and <strong>Y</strong>. </p>
<p><strong>Z</strong> is used to represent the end.</p>
<p><a name='1.2'></a></p>
<h3 id="1-2-Process-C4"><a href="#1-2-Process-C4" class="headerlink" title="1.2 Process C4"></a>1.2 Process C4</h3><p>C4 only has the plain string <code>text</code> field, so you will tokenize and have <code>inputs</code> and <code>targets</code> out of it for supervised learning. Given your inputs, the goal is to predict the targets during training. </p>
<p>You will now take the <code>text</code> and convert it to <code>inputs</code> and <code>targets</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Grab text field from dictionary</span></span><br><span class="line">natural_language_texts = [example_json[<span class="string">&#x27;text&#x27;</span>] <span class="keyword">for</span> example_json <span class="keyword">in</span> example_jsons]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># First text example</span></span><br><span class="line">natural_language_texts[<span class="number">4</span>]</span><br></pre></td></tr></table></figure>




<pre><code>b&#39;The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what\xe2\x80\x99s included in the mill levy measure.&#39;
</code></pre>
<p><a name='1.2.1'></a></p>
<h4 id="1-2-1-Decode-to-natural-language"><a href="#1-2-1-Decode-to-natural-language" class="headerlink" title="1.2.1 Decode to natural language"></a>1.2.1 Decode to natural language</h4><p>The following functions will help you <code>detokenize</code> and<code>tokenize</code> the text data.  </p>
<p>The <code>sentencepiece</code> vocabulary was used to convert from text to ids. This vocabulary file is loaded and used in this helper functions.</p>
<p><code>natural_language_texts</code> has the text from the examples we gave you. </p>
<p>Run the cells below to see what is going on. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Special tokens</span></span><br><span class="line">PAD, EOS, UNK = <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detokenize</span>(<span class="params">np_array</span>):</span></span><br><span class="line">    <span class="keyword">return</span> trax.data.detokenize(</span><br><span class="line">        np_array,</span><br><span class="line">        vocab_type=<span class="string">&#x27;sentencepiece&#x27;</span>,</span><br><span class="line">        vocab_file=<span class="string">&#x27;sentencepiece.model&#x27;</span>,</span><br><span class="line">        vocab_dir=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">s</span>):</span></span><br><span class="line">  <span class="comment"># The trax.data.tokenize function operates on streams,</span></span><br><span class="line">  <span class="comment"># that&#x27;s why we have to create 1-element stream with iter</span></span><br><span class="line">  <span class="comment"># and later retrieve the result with next.</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">next</span>(trax.data.tokenize(</span><br><span class="line">        <span class="built_in">iter</span>([s]),</span><br><span class="line">        vocab_type=<span class="string">&#x27;sentencepiece&#x27;</span>,</span><br><span class="line">        vocab_file=<span class="string">&#x27;sentencepiece.model&#x27;</span>,</span><br><span class="line">        vocab_dir=<span class="string">&#x27;.&#x27;</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># printing the encoding of each word to see how subwords are tokenized</span></span><br><span class="line">tokenized_text = [(tokenize(word).tolist(), word) <span class="keyword">for</span> word <span class="keyword">in</span> natural_language_texts[<span class="number">0</span>].split()]</span><br><span class="line"><span class="built_in">print</span>(tokenized_text, <span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>[([12847, 277], b&#39;Beginners&#39;), ([15068], b&#39;BBQ&#39;), ([4501], b&#39;Class&#39;), ([3, 12297], b&#39;Taking&#39;), ([3399], b&#39;Place&#39;), ([16], b&#39;in&#39;), ([5964, 7115, 9, 55], b&#39;Missoula!&#39;), ([531], b&#39;Do&#39;), ([25], b&#39;you&#39;), ([241], b&#39;want&#39;), ([12], b&#39;to&#39;), ([129], b&#39;get&#39;), ([394], b&#39;better&#39;), ([44], b&#39;at&#39;), ([492], b&#39;making&#39;), ([3326], b&#39;delicious&#39;), ([15068, 58], b&#39;BBQ?&#39;), ([148], b&#39;You&#39;), ([56], b&#39;will&#39;), ([43], b&#39;have&#39;), ([8], b&#39;the&#39;), ([1004, 6], b&#39;opportunity,&#39;), ([474], b&#39;put&#39;), ([48], b&#39;this&#39;), ([30], b&#39;on&#39;), ([39], b&#39;your&#39;), ([4793], b&#39;calendar&#39;), ([230, 5], b&#39;now.&#39;), ([2721, 6], b&#39;Thursday,&#39;), ([1600], b&#39;September&#39;), ([1630, 727], b&#39;22nd&#39;), ([1715], b&#39;join&#39;), ([1150], b&#39;World&#39;), ([4501], b&#39;Class&#39;), ([15068], b&#39;BBQ&#39;), ([16127, 6], b&#39;Champion,&#39;), ([9137], b&#39;Tony&#39;), ([2659, 5595], b&#39;Balay&#39;), ([45], b&#39;from&#39;), ([301, 782, 3624], b&#39;Lonestar&#39;), ([14627, 15], b&#39;Smoke&#39;), ([12612, 277, 5], b&#39;Rangers.&#39;), ([216], b&#39;He&#39;), ([56], b&#39;will&#39;), ([36], b&#39;be&#39;), ([2119], b&#39;teaching&#39;), ([3, 9], b&#39;a&#39;), ([19529], b&#39;beginner&#39;), ([593], b&#39;level&#39;), ([853], b&#39;class&#39;), ([21], b&#39;for&#39;), ([921], b&#39;everyone&#39;), ([113], b&#39;who&#39;), ([2746], b&#39;wants&#39;), ([12], b&#39;to&#39;), ([129], b&#39;get&#39;), ([394], b&#39;better&#39;), ([28], b&#39;with&#39;), ([70], b&#39;their&#39;), ([17712], b&#39;culinary&#39;), ([1098, 5], b&#39;skills.&#39;), ([216], b&#39;He&#39;), ([56], b&#39;will&#39;), ([3884], b&#39;teach&#39;), ([25], b&#39;you&#39;), ([762], b&#39;everything&#39;), ([25], b&#39;you&#39;), ([174], b&#39;need&#39;), ([12], b&#39;to&#39;), ([214], b&#39;know&#39;), ([12], b&#39;to&#39;), ([5978], b&#39;compete&#39;), ([16], b&#39;in&#39;), ([3, 9], b&#39;a&#39;), ([3, 23405, 4547], b&#39;KCBS&#39;), ([15068], b&#39;BBQ&#39;), ([2259, 6], b&#39;competition,&#39;), ([379], b&#39;including&#39;), ([2097, 6], b&#39;techniques,&#39;), ([5459, 6], b&#39;recipes,&#39;), ([13618, 7, 6], b&#39;timelines,&#39;), ([3604], b&#39;meat&#39;), ([1801], b&#39;selection&#39;), ([11], b&#39;and&#39;), ([27856, 6], b&#39;trimming,&#39;), ([303], b&#39;plus&#39;), ([24190], b&#39;smoker&#39;), ([11], b&#39;and&#39;), ([1472], b&#39;fire&#39;), ([251, 5], b&#39;information.&#39;), ([37], b&#39;The&#39;), ([583], b&#39;cost&#39;), ([12], b&#39;to&#39;), ([36], b&#39;be&#39;), ([16], b&#39;in&#39;), ([8], b&#39;the&#39;), ([853], b&#39;class&#39;), ([19], b&#39;is&#39;), ([25264], b&#39;$35&#39;), ([399], b&#39;per&#39;), ([568, 6], b&#39;person,&#39;), ([11], b&#39;and&#39;), ([21], b&#39;for&#39;), ([21380, 7], b&#39;spectators&#39;), ([34], b&#39;it&#39;), ([19], b&#39;is&#39;), ([339, 5], b&#39;free.&#39;), ([15746, 26], b&#39;Included&#39;), ([16], b&#39;in&#39;), ([8], b&#39;the&#39;), ([583], b&#39;cost&#39;), ([56], b&#39;will&#39;), ([36], b&#39;be&#39;), ([893], b&#39;either&#39;), ([3, 9], b&#39;a&#39;), ([3, 17, 18, 9486], b&#39;t-shirt&#39;), ([42], b&#39;or&#39;), ([3, 9, 1409, 29], b&#39;apron&#39;), ([11], b&#39;and&#39;), ([25], b&#39;you&#39;), ([56], b&#39;will&#39;), ([36], b&#39;be&#39;), ([12246], b&#39;tasting&#39;), ([5977], b&#39;samples&#39;), ([13], b&#39;of&#39;), ([284], b&#39;each&#39;), ([3604], b&#39;meat&#39;), ([24], b&#39;that&#39;), ([19], b&#39;is&#39;), ([2657, 5], b&#39;prepared.&#39;)] 
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We can see that detokenize successfully undoes the tokenization</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;tokenized: <span class="subst">&#123;tokenize(<span class="string">&#x27;Beginners&#x27;</span>)&#125;</span>\ndetokenized: <span class="subst">&#123;detokenize(tokenize(<span class="string">&#x27;Beginners&#x27;</span>))&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>tokenized: [12847   277]
detokenized: Beginners
</code></pre>
<p>As you can see above, you were able to take a piece of string and tokenize it. </p>
<p>Now you will create <code>input</code> and <code>target</code> pairs that will allow you to train your model. T5 uses the ids at the end of the vocab file as sentinels. For example, it will replace: </p>
<ul>
<li><code>vocab_size - 1</code> by <code>&lt;Z&gt;</code></li>
<li><code>vocab_size - 2</code> by <code>&lt;Y&gt;</code></li>
<li>and so forth. </li>
</ul>
<p>It assigns every word a <code>chr</code>.</p>
<p>The <code>pretty_decode</code> function below, which you will use in a bit, helps in handling the type when decoding. Take a look and try to understand what the function is doing.</p>
<p>Notice that:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">string.ascii_letters = <span class="string">&#x27;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ&#x27;</span></span><br></pre></td></tr></table></figure>

<p><strong>NOTE:</strong> Targets may have more than the 52 sentinels we replace, but this is just to give you an idea of things.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">vocab_size = trax.data.vocab_size(</span><br><span class="line">    vocab_type=<span class="string">&#x27;sentencepiece&#x27;</span>,</span><br><span class="line">    vocab_file=<span class="string">&#x27;sentencepiece.model&#x27;</span>,</span><br><span class="line">    vocab_dir=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sentinels</span>(<span class="params">vocab_size=vocab_size, display=<span class="literal">False</span></span>):</span></span><br><span class="line">    sentinels = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">reversed</span>(string.ascii_letters), <span class="number">1</span>):</span><br><span class="line">        decoded_text = detokenize([vocab_size - i]) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Sentinels, ex: &lt;Z&gt; - &lt;a&gt;</span></span><br><span class="line">        sentinels[decoded_text] = <span class="string">f&#x27;&lt;<span class="subst">&#123;char&#125;</span>&gt;&#x27;</span>    </span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> display:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;The sentinel is &lt;<span class="subst">&#123;char&#125;</span>&gt; and the decoded token is:&#x27;</span>, decoded_text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sentinels</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sentinels = get_sentinels(vocab_size, display=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The sentinel is &lt;Z&gt; and the decoded token is: Internațional
The sentinel is &lt;Y&gt; and the decoded token is: erwachsene
The sentinel is &lt;X&gt; and the decoded token is: Cushion
The sentinel is &lt;W&gt; and the decoded token is: imunitar
The sentinel is &lt;V&gt; and the decoded token is: Intellectual
The sentinel is &lt;U&gt; and the decoded token is: traditi
The sentinel is &lt;T&gt; and the decoded token is: disguise
The sentinel is &lt;S&gt; and the decoded token is: exerce
The sentinel is &lt;R&gt; and the decoded token is: nourishe
The sentinel is &lt;Q&gt; and the decoded token is: predominant
The sentinel is &lt;P&gt; and the decoded token is: amitié
The sentinel is &lt;O&gt; and the decoded token is: erkennt
The sentinel is &lt;N&gt; and the decoded token is: dimension
The sentinel is &lt;M&gt; and the decoded token is: inférieur
The sentinel is &lt;L&gt; and the decoded token is: refugi
The sentinel is &lt;K&gt; and the decoded token is: cheddar
The sentinel is &lt;J&gt; and the decoded token is: unterlieg
The sentinel is &lt;I&gt; and the decoded token is: garanteaz
The sentinel is &lt;H&gt; and the decoded token is: făcute
The sentinel is &lt;G&gt; and the decoded token is: réglage
The sentinel is &lt;F&gt; and the decoded token is: pedepse
The sentinel is &lt;E&gt; and the decoded token is: Germain
The sentinel is &lt;D&gt; and the decoded token is: distinctly
The sentinel is &lt;C&gt; and the decoded token is: Schraub
The sentinel is &lt;B&gt; and the decoded token is: emanat
The sentinel is &lt;A&gt; and the decoded token is: trimestre
The sentinel is &lt;z&gt; and the decoded token is: disrespect
The sentinel is &lt;y&gt; and the decoded token is: Erasmus
The sentinel is &lt;x&gt; and the decoded token is: Australia
The sentinel is &lt;w&gt; and the decoded token is: permeabil
The sentinel is &lt;v&gt; and the decoded token is: deseori
The sentinel is &lt;u&gt; and the decoded token is: manipulated
The sentinel is &lt;t&gt; and the decoded token is: suggér
The sentinel is &lt;s&gt; and the decoded token is: corespund
The sentinel is &lt;r&gt; and the decoded token is: nitro
The sentinel is &lt;q&gt; and the decoded token is: oyons
The sentinel is &lt;p&gt; and the decoded token is: Account
The sentinel is &lt;o&gt; and the decoded token is: échéan
The sentinel is &lt;n&gt; and the decoded token is: laundering
The sentinel is &lt;m&gt; and the decoded token is: genealogy
The sentinel is &lt;l&gt; and the decoded token is: QuickBooks
The sentinel is &lt;k&gt; and the decoded token is: constituted
The sentinel is &lt;j&gt; and the decoded token is: Fertigung
The sentinel is &lt;i&gt; and the decoded token is: goutte
The sentinel is &lt;h&gt; and the decoded token is: regulă
The sentinel is &lt;g&gt; and the decoded token is: overwhelmingly
The sentinel is &lt;f&gt; and the decoded token is: émerg
The sentinel is &lt;e&gt; and the decoded token is: broyeur
The sentinel is &lt;d&gt; and the decoded token is: povești
The sentinel is &lt;c&gt; and the decoded token is: emulator
The sentinel is &lt;b&gt; and the decoded token is: halloween
The sentinel is &lt;a&gt; and the decoded token is: combustibil
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretty_decode</span>(<span class="params">encoded_str_list, sentinels=sentinels</span>):</span></span><br><span class="line">    <span class="comment"># If already a string, just do the replacements.</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(encoded_str_list, (<span class="built_in">str</span>, <span class="built_in">bytes</span>)):</span><br><span class="line">        <span class="keyword">for</span> token, char <span class="keyword">in</span> sentinels.items():</span><br><span class="line">            encoded_str_list = encoded_str_list.replace(token, char)</span><br><span class="line">        <span class="keyword">return</span> encoded_str_list</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># We need to decode and then prettyfy it.</span></span><br><span class="line">    <span class="keyword">return</span> pretty_decode(detokenize(encoded_str_list))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pretty_decode(<span class="string">&quot;I want to dress up as an Intellectual this halloween.&quot;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&#39;I want to dress up as an &lt;V&gt; this &lt;b&gt;.&#39;
</code></pre>
<p>The functions above make your <code>inputs</code> and <code>targets</code> more readable. For example, you might see something like this once you implement the masking function below. </p>
<ul>
<li><span style="color:red"> Input sentence: </span> Younes and Lukasz were working together in the lab yesterday after lunch. </li>
<li><span style="color:red">Input: </span> Younes and Lukasz  <strong>Z</strong> together in the <strong>Y</strong> yesterday after lunch.</li>
<li><span style="color:red">Target: </span> <strong>Z</strong> were working <strong>Y</strong> lab.</li>
</ul>
<p><a name='1.3'></a></p>
<h3 id="1-3-Tokenizing-and-Masking"><a href="#1-3-Tokenizing-and-Masking" class="headerlink" title="1.3 Tokenizing and Masking"></a>1.3 Tokenizing and Masking</h3><p>You will now implement the <code>tokenize_and_mask</code> function. This function  will allow you to tokenize and mask input words with a noise probability. We usually mask 15% of the words.</p>
<p><a name='ex01'></a></p>
<h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: tokenize_and_mask</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_and_mask</span>(<span class="params">text, vocab_size=vocab_size, noise=<span class="number">0.15</span>, </span></span></span><br><span class="line"><span class="params"><span class="function">                      randomizer=np.random.uniform, tokenize=tokenize</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Tokenizes and masks a given input.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        text (str or bytes): Text input.</span></span><br><span class="line"><span class="string">        vocab_size (int, optional): Size of the vocabulary. Defaults to vocab_size.</span></span><br><span class="line"><span class="string">        noise (float, optional): Probability of masking a token. Defaults to 0.15.</span></span><br><span class="line"><span class="string">        randomizer (function, optional): Function that generates random values. Defaults to np.random.uniform.</span></span><br><span class="line"><span class="string">        tokenize (function, optional): Tokenizer function. Defaults to tokenize.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        tuple: Tuple of lists of integers associated to inputs and targets.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># current sentinel number (starts at 0)</span></span><br><span class="line">    cur_sentinel_num = <span class="number">0</span></span><br><span class="line">    <span class="comment"># inputs</span></span><br><span class="line">    inps = []</span><br><span class="line">    <span class="comment"># targets</span></span><br><span class="line">    targs = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># prev_no_mask is True if the previous token was NOT masked, False otherwise</span></span><br><span class="line">    <span class="comment"># set prev_no_mask to True</span></span><br><span class="line">    prev_no_mask = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop through tokenized `text`</span></span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokenize(text):</span><br><span class="line">        <span class="comment"># check if the `noise` is greater than a random value (weighted coin flip)</span></span><br><span class="line">        <span class="keyword">if</span> randomizer() &lt; noise:</span><br><span class="line">            <span class="comment"># check to see if the previous token was not masked</span></span><br><span class="line">            <span class="keyword">if</span> prev_no_mask==<span class="literal">True</span>: <span class="comment"># add new masked token at end_id</span></span><br><span class="line">                <span class="comment"># number of masked tokens increases by 1</span></span><br><span class="line">                cur_sentinel_num += <span class="number">1</span></span><br><span class="line">                <span class="comment"># compute `end_id` by subtracting current sentinel value out of the total vocabulary size</span></span><br><span class="line">                end_id = vocab_size - cur_sentinel_num</span><br><span class="line">                <span class="comment"># append `end_id` at the end of the targets</span></span><br><span class="line">                targs.append(end_id)</span><br><span class="line">                <span class="comment"># append `end_id` at the end of the inputs</span></span><br><span class="line">                inps.append(end_id)</span><br><span class="line">            <span class="comment"># append `token` at the end of the targets</span></span><br><span class="line">            targs.append(token)</span><br><span class="line">            <span class="comment"># set prev_no_mask accordingly</span></span><br><span class="line">            prev_no_mask = <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># don&#x27;t have two masked tokens in a row</span></span><br><span class="line">            <span class="comment"># append `token ` at the end of the inputs</span></span><br><span class="line">            inps.append(token)</span><br><span class="line">            <span class="comment"># set prev_no_mask accordingly</span></span><br><span class="line">            prev_no_mask = <span class="literal">True</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> inps, targs</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Some logic to mock a np.random value generator</span></span><br><span class="line"><span class="comment"># Needs to be in the same cell for it to always generate same output</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testing_rnd</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dummy_generator</span>():</span></span><br><span class="line">        vals = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">        cyclic_vals = itertools.cycle(vals)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">            <span class="keyword">yield</span> <span class="built_in">next</span>(cyclic_vals)</span><br><span class="line"></span><br><span class="line">    dumr = itertools.cycle(dummy_generator())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dummy_randomizer</span>():</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">next</span>(dumr)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dummy_randomizer</span><br><span class="line"></span><br><span class="line">input_str = natural_language_texts[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;input string:\n\n<span class="subst">&#123;input_str&#125;</span>\n&quot;</span>)</span><br><span class="line">inps, targs = tokenize_and_mask(input_str, randomizer=testing_rnd())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;tokenized inputs:\n\n<span class="subst">&#123;inps&#125;</span>\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;targets:\n\n<span class="subst">&#123;targs&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>input string:

b&#39;Beginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.&#39;

tokenized inputs:

[31999, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 31998, 531, 25, 241, 12, 129, 394, 44, 492, 31997, 58, 148, 56, 43, 8, 1004, 6, 474, 31996, 39, 4793, 230, 5, 2721, 6, 1600, 1630, 31995, 1150, 4501, 15068, 16127, 6, 9137, 2659, 5595, 31994, 782, 3624, 14627, 15, 12612, 277, 5, 216, 31993, 2119, 3, 9, 19529, 593, 853, 21, 921, 31992, 12, 129, 394, 28, 70, 17712, 1098, 5, 31991, 3884, 25, 762, 25, 174, 12, 214, 12, 31990, 3, 9, 3, 23405, 4547, 15068, 2259, 6, 31989, 6, 5459, 6, 13618, 7, 6, 3604, 1801, 31988, 6, 303, 24190, 11, 1472, 251, 5, 37, 31987, 36, 16, 8, 853, 19, 25264, 399, 568, 31986, 21, 21380, 7, 34, 19, 339, 5, 15746, 31985, 8, 583, 56, 36, 893, 3, 9, 3, 31984, 9486, 42, 3, 9, 1409, 29, 11, 25, 31983, 12246, 5977, 13, 284, 3604, 24, 19, 2657, 31982]

targets:

[31999, 12847, 277, 31998, 9, 55, 31997, 3326, 15068, 31996, 48, 30, 31995, 727, 1715, 31994, 45, 301, 31993, 56, 36, 31992, 113, 2746, 31991, 216, 56, 31990, 5978, 16, 31989, 379, 2097, 31988, 11, 27856, 31987, 583, 12, 31986, 6, 11, 31985, 26, 16, 31984, 17, 18, 31983, 56, 36, 31982, 5]
</code></pre>
<h4 id="Expected-Output"><a href="#Expected-Output" class="headerlink" title="Expected Output:"></a><strong>Expected Output:</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">b<span class="number">&#x27;B</span>eginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You will have the opportunity, put <span class="keyword">this</span> on your calendar now. Thursday, September <span class="number">22</span>nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level <span class="class"><span class="keyword">class</span> <span class="title">for</span> <span class="title">everyone</span> <span class="title">who</span> <span class="title">wants</span> <span class="title">to</span> <span class="title">get</span> <span class="title">better</span> <span class="title">with</span> <span class="title">their</span> <span class="title">culinary</span> <span class="title">skills</span>.\<span class="title">nHe</span> <span class="title">will</span> <span class="title">teach</span> <span class="title">you</span> <span class="title">everything</span> <span class="title">you</span> <span class="title">need</span> <span class="title">to</span> <span class="title">know</span> <span class="title">to</span> <span class="title">compete</span> <span class="title">in</span> <span class="title">a</span> <span class="title">KCBS</span> <span class="title">BBQ</span> <span class="title">competition</span>, <span class="title">including</span> <span class="title">techniques</span>, <span class="title">recipes</span>, <span class="title">timelines</span>, <span class="title">meat</span> <span class="title">selection</span> <span class="title">and</span> <span class="title">trimming</span>, <span class="title">plus</span> <span class="title">smoker</span> <span class="title">and</span> <span class="title">fire</span> <span class="title">information</span>.\<span class="title">nThe</span> <span class="title">cost</span> <span class="title">to</span> <span class="title">be</span> <span class="title">in</span> <span class="title">the</span> <span class="keyword">class</span> <span class="title">is</span> $35 <span class="title">per</span> <span class="title">person</span>, <span class="title">and</span> <span class="title">for</span> <span class="title">spectators</span> <span class="title">it</span> <span class="title">is</span> <span class="title">free</span>. <span class="title">Included</span> <span class="title">in</span> <span class="title">the</span> <span class="title">cost</span> <span class="title">will</span> <span class="title">be</span> <span class="title">either</span> <span class="title">a</span> <span class="title">t</span>-<span class="title">shirt</span> <span class="title">or</span> <span class="title">apron</span> <span class="title">and</span> <span class="title">you</span> <span class="title">will</span> <span class="title">be</span> <span class="title">tasting</span> <span class="title">samples</span> <span class="title">of</span> <span class="title">each</span> <span class="title">meat</span> <span class="title">that</span> <span class="title">is</span> <span class="title">prepared</span>.&#x27;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">tokenized</span> <span class="title">inputs</span>:</span></span><br><span class="line"></span><br><span class="line">[<span class="number">31999</span>, <span class="number">15068</span>, <span class="number">4501</span>, <span class="number">3</span>, <span class="number">12297</span>, <span class="number">3399</span>, <span class="number">16</span>, <span class="number">5964</span>, <span class="number">7115</span>, <span class="number">31998</span>, <span class="number">531</span>, <span class="number">25</span>, <span class="number">241</span>, <span class="number">12</span>, <span class="number">129</span>, <span class="number">394</span>, <span class="number">44</span>, <span class="number">492</span>, <span class="number">31997</span>, <span class="number">58</span>, <span class="number">148</span>, <span class="number">56</span>, <span class="number">43</span>, <span class="number">8</span>, <span class="number">1004</span>, <span class="number">6</span>, <span class="number">474</span>, <span class="number">31996</span>, <span class="number">39</span>, <span class="number">4793</span>, <span class="number">230</span>, <span class="number">5</span>, <span class="number">2721</span>, <span class="number">6</span>, <span class="number">1600</span>, <span class="number">1630</span>, <span class="number">31995</span>, <span class="number">1150</span>, <span class="number">4501</span>, <span class="number">15068</span>, <span class="number">16127</span>, <span class="number">6</span>, <span class="number">9137</span>, <span class="number">2659</span>, <span class="number">5595</span>, <span class="number">31994</span>, <span class="number">782</span>, <span class="number">3624</span>, <span class="number">14627</span>, <span class="number">15</span>, <span class="number">12612</span>, <span class="number">277</span>, <span class="number">5</span>, <span class="number">216</span>, <span class="number">31993</span>, <span class="number">2119</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">19529</span>, <span class="number">593</span>, <span class="number">853</span>, <span class="number">21</span>, <span class="number">921</span>, <span class="number">31992</span>, <span class="number">12</span>, <span class="number">129</span>, <span class="number">394</span>, <span class="number">28</span>, <span class="number">70</span>, <span class="number">17712</span>, <span class="number">1098</span>, <span class="number">5</span>, <span class="number">31991</span>, <span class="number">3884</span>, <span class="number">25</span>, <span class="number">762</span>, <span class="number">25</span>, <span class="number">174</span>, <span class="number">12</span>, <span class="number">214</span>, <span class="number">12</span>, <span class="number">31990</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">23405</span>, <span class="number">4547</span>, <span class="number">15068</span>, <span class="number">2259</span>, <span class="number">6</span>, <span class="number">31989</span>, <span class="number">6</span>, <span class="number">5459</span>, <span class="number">6</span>, <span class="number">13618</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">3604</span>, <span class="number">1801</span>, <span class="number">31988</span>, <span class="number">6</span>, <span class="number">303</span>, <span class="number">24190</span>, <span class="number">11</span>, <span class="number">1472</span>, <span class="number">251</span>, <span class="number">5</span>, <span class="number">37</span>, <span class="number">31987</span>, <span class="number">36</span>, <span class="number">16</span>, <span class="number">8</span>, <span class="number">853</span>, <span class="number">19</span>, <span class="number">25264</span>, <span class="number">399</span>, <span class="number">568</span>, <span class="number">31986</span>, <span class="number">21</span>, <span class="number">21380</span>, <span class="number">7</span>, <span class="number">34</span>, <span class="number">19</span>, <span class="number">339</span>, <span class="number">5</span>, <span class="number">15746</span>, <span class="number">31985</span>, <span class="number">8</span>, <span class="number">583</span>, <span class="number">56</span>, <span class="number">36</span>, <span class="number">893</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">31984</span>, <span class="number">9486</span>, <span class="number">42</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">1409</span>, <span class="number">29</span>, <span class="number">11</span>, <span class="number">25</span>, <span class="number">31983</span>, <span class="number">12246</span>, <span class="number">5977</span>, <span class="number">13</span>, <span class="number">284</span>, <span class="number">3604</span>, <span class="number">24</span>, <span class="number">19</span>, <span class="number">2657</span>, <span class="number">31982</span>]</span><br><span class="line"></span><br><span class="line">targets:</span><br><span class="line"></span><br><span class="line">[<span class="number">31999</span>, <span class="number">12847</span>, <span class="number">277</span>, <span class="number">31998</span>, <span class="number">9</span>, <span class="number">55</span>, <span class="number">31997</span>, <span class="number">3326</span>, <span class="number">15068</span>, <span class="number">31996</span>, <span class="number">48</span>, <span class="number">30</span>, <span class="number">31995</span>, <span class="number">727</span>, <span class="number">1715</span>, <span class="number">31994</span>, <span class="number">45</span>, <span class="number">301</span>, <span class="number">31993</span>, <span class="number">56</span>, <span class="number">36</span>, <span class="number">31992</span>, <span class="number">113</span>, <span class="number">2746</span>, <span class="number">31991</span>, <span class="number">216</span>, <span class="number">56</span>, <span class="number">31990</span>, <span class="number">5978</span>, <span class="number">16</span>, <span class="number">31989</span>, <span class="number">379</span>, <span class="number">2097</span>, <span class="number">31988</span>, <span class="number">11</span>, <span class="number">27856</span>, <span class="number">31987</span>, <span class="number">583</span>, <span class="number">12</span>, <span class="number">31986</span>, <span class="number">6</span>, <span class="number">11</span>, <span class="number">31985</span>, <span class="number">26</span>, <span class="number">16</span>, <span class="number">31984</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">31983</span>, <span class="number">56</span>, <span class="number">36</span>, <span class="number">31982</span>, <span class="number">5</span>]</span><br></pre></td></tr></table></figure>

<p>You will now use the inputs and the targets from the <code>tokenize_and_mask</code> function you implemented above. Take a look at the masked sentence using your <code>inps</code> and <code>targs</code> from the sentence above. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Inputs: \n\n&#x27;</span>, pretty_decode(inps))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nTargets: \n\n&#x27;</span>, pretty_decode(targs))</span><br></pre></td></tr></table></figure>

<pre><code>Inputs: 

 &lt;Z&gt; BBQ Class Taking Place in Missoul &lt;Y&gt; Do you want to get better at making &lt;X&gt;? You will have the opportunity, put &lt;W&gt; your calendar now. Thursday, September 22 &lt;V&gt; World Class BBQ Champion, Tony Balay &lt;U&gt;onestar Smoke Rangers. He &lt;T&gt; teaching a beginner level class for everyone&lt;S&gt; to get better with their culinary skills.&lt;R&gt; teach you everything you need to know to &lt;Q&gt; a KCBS BBQ competition,&lt;P&gt;, recipes, timelines, meat selection &lt;O&gt;, plus smoker and fire information. The&lt;N&gt; be in the class is $35 per person &lt;M&gt; for spectators it is free. Include &lt;L&gt; the cost will be either a  &lt;K&gt;shirt or apron and you &lt;J&gt; tasting samples of each meat that is prepared &lt;I&gt;

Targets: 

 &lt;Z&gt; Beginners &lt;Y&gt;a! &lt;X&gt; delicious BBQ &lt;W&gt; this on &lt;V&gt;nd join &lt;U&gt; from L &lt;T&gt; will be&lt;S&gt; who wants&lt;R&gt; He will &lt;Q&gt; compete in&lt;P&gt; including techniques &lt;O&gt; and trimming&lt;N&gt; cost to &lt;M&gt;, and &lt;L&gt;d in &lt;K&gt;t- &lt;J&gt; will be &lt;I&gt;.
</code></pre>
<p><a name='1.4'></a></p>
<h3 id="1-4-Creating-the-Pairs"><a href="#1-4-Creating-the-Pairs" class="headerlink" title="1.4 Creating the Pairs"></a>1.4 Creating the Pairs</h3><p>You will now create pairs using your dataset. You will iterate over your data and create (inp, targ) pairs using the functions that we have given you. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply tokenize_and_mask</span></span><br><span class="line">inputs_targets_pairs = [tokenize_and_mask(text) <span class="keyword">for</span> text <span class="keyword">in</span> natural_language_texts]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_input_target_pairs</span>(<span class="params">inputs_targets_pairs</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i, inp_tgt_pair <span class="keyword">in</span> <span class="built_in">enumerate</span>(inputs_targets_pairs, <span class="number">1</span>):</span><br><span class="line">        inps, tgts = inp_tgt_pair</span><br><span class="line">        inps, tgts = pretty_decode(inps), pretty_decode(tgts)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;[<span class="subst">&#123;i&#125;</span>]\n\n&#x27;</span></span><br><span class="line">              <span class="string">f&#x27;inputs:\n<span class="subst">&#123;wrapper.fill(text=inps)&#125;</span>\n\n&#x27;</span></span><br><span class="line">              <span class="string">f&#x27;targets:\n<span class="subst">&#123;wrapper.fill(text=tgts)&#125;</span>\n\n\n\n&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_input_target_pairs(inputs_targets_pairs)</span><br></pre></td></tr></table></figure>

<pre><code>[1]

inputs:
Beginners BBQ Class Taking &lt;Z&gt; in Missoul &lt;Y&gt;! Do you want to get
better at making delicious &lt;X&gt;? You will have the opportunity, &lt;W&gt;
this on &lt;V&gt; calendar now. Thursday &lt;U&gt; September 22 &lt;T&gt; join&lt;S&gt; Class
BBQ Champion, Tony Balay from Lonestar Smoke&lt;R&gt;ers &lt;Q&gt; He will be
teaching a beginner&lt;P&gt; class &lt;O&gt; everyone who wants&lt;N&gt; get better with
their &lt;M&gt; skills &lt;L&gt; He will teach &lt;K&gt; everything you need to know to
&lt;J&gt; in a KCBS BBQ &lt;I&gt; techniques, recipes, timelines, meat&lt;H&gt; and
trimming, plus smoker and fire information. The cost to be&lt;G&gt; the
class is $35 &lt;F&gt; person, and&lt;E&gt; spectators it is free. Included in the
cost will&lt;D&gt; either &lt;C&gt; t- &lt;B&gt; or apron and you will be tasting
samples &lt;A&gt; each meat that &lt;z&gt; prepared.

targets:
&lt;Z&gt; Place &lt;Y&gt;a &lt;X&gt; BBQ &lt;W&gt; put &lt;V&gt; your &lt;U&gt;, &lt;T&gt;nd&lt;S&gt; World&lt;R&gt; Rang
&lt;Q&gt;.&lt;P&gt; level &lt;O&gt; for&lt;N&gt; to &lt;M&gt; culinary &lt;L&gt;. &lt;K&gt; you &lt;J&gt; compete &lt;I&gt;
competition, including&lt;H&gt; selection&lt;G&gt; in &lt;F&gt; per&lt;E&gt; for&lt;D&gt; be&lt;C&gt;a
&lt;B&gt;shirt &lt;A&gt; of &lt;z&gt; is




[2]

inputs:
&lt;Z&gt; in &#39;Mac OS X &lt;Y&gt; (10 &lt;X&gt;7)&#39; started by axb &lt;W&gt;i87, Jan 20, 2012.
I&#39;ve got &lt;V&gt;a 500g &lt;U&gt; drive &lt;T&gt; a 240gb SSD. When trying to restore
using&lt;S&gt; utility i&#39;m given the error &quot;Not enough space on disk&lt;R&gt;____
to restore &lt;Q&gt; But I shouldn&#39;t have to do that!!! Any ideas or
work&lt;P&gt;s before &lt;O&gt;ing to the above? Use Carbon Copy Cloner to copy
one drive to the other. I&#39;&lt;N&gt; done &lt;M&gt; several times going from &lt;L&gt;D
to &lt;K&gt; SSD and I wound &lt;J&gt; a bootable SSD drive. One step you &lt;I&gt;
remember not to skip is to use Disk Utility to partition the SSD as
GUID partition scheme&lt;H&gt; doing the &lt;G&gt;ne. If it came Apple &lt;F&gt;ition
Scheme, even if&lt;E&gt; let&lt;D&gt;CC do the clone, the resulting drive&lt;C&gt; boot
&lt;B&gt;. C &lt;A&gt; usually works &lt;z&gt; &quot;file mode&quot; and it can easily copy a
larger drive (that&#39;s mostly empty &lt;y&gt; onto a smaller drive.&lt;x&gt; you&lt;w&gt;
CCC to clone a drive you did&lt;v&gt; boot&lt;u&gt;, it can work &lt;t&gt; copy mode &lt;s&gt;
destination&lt;r&gt; must be&lt;q&gt; size or larger than the drive you
are&lt;p&gt;cloning from &lt;o&gt;if &lt;n&gt; recall &lt;m&gt;ve actually done this somehow
on Disk Utility &lt;l&gt; times&lt;k&gt;booting from &lt;j&gt;a different drive (or even
the dvd)&lt;i&gt; not running disk utility from the drive your clo&lt;h&gt;ing)
and had it work just fine from larger to smaller bootable clo&lt;g&gt;.
Definitely format the drive cloning to first &lt;f&gt; as bootable Apple
etc.. Thanks for &lt;e&gt; this out. My only experience &lt;d&gt; DU to go larger
to smaller was when &lt;c&gt; trying to make  &lt;b&gt; install stick and I was
unable to restore InstallESD &lt;a&gt;dmg to a 4 GB Théâtre ofKeep the
reason that wouldn&#39;t fit isdürftig was slightly moreutti GB of data.

targets:
&lt;Z&gt; Discussion &lt;Y&gt; Lion &lt;X&gt;. &lt;W&gt;o &lt;V&gt;  &lt;U&gt;b internal &lt;T&gt; and&lt;S&gt;
disk&lt;R&gt;  &lt;Q&gt;&quot;&lt;P&gt;around &lt;O&gt; resort&lt;N&gt;ve &lt;M&gt; this &lt;L&gt; larger HD &lt;K&gt;
smaller &lt;J&gt; up with &lt;I&gt; have to&lt;H&gt; HFS+ before&lt;G&gt;clo &lt;F&gt; Part&lt;E&gt;
you&lt;D&gt; C&lt;C&gt; won&#39;t be &lt;B&gt;able &lt;A&gt;CC &lt;z&gt; in &lt;y&gt;)&lt;x&gt; If&lt;w&gt; tell&lt;v&gt; NOT&lt;u&gt;
from &lt;t&gt; in block &lt;s&gt; where the&lt;r&gt; drive&lt;q&gt; the same&lt;p&gt;  &lt;o&gt; ( &lt;n&gt; I
&lt;m&gt;). I&#39; &lt;l&gt; several&lt;k&gt; ( &lt;j&gt; &lt;i&gt; so&lt;h&gt;n&lt;g&gt;ne &lt;f&gt;,&lt;e&gt;pointing &lt;d&gt;
using &lt;c&gt; I was &lt;b&gt;a Lion &lt;a&gt;. Théâtre USB stick butKeep coursedürftig
thereutti than 4




[3]

inputs:
&lt;Z&gt;il plaid &lt;Y&gt;lycra &lt;X&gt; spandex shortall with metallic slinky
&lt;W&gt;sets. Attache &lt;V&gt; metallic elastic belt with O &lt;U&gt;ring. Head &lt;T&gt;
included. Great hip hop&lt;S&gt; jazz dance costume.&lt;R&gt; in the USA.

targets:
&lt;Z&gt; Fo &lt;Y&gt;  &lt;X&gt; and &lt;W&gt; in &lt;V&gt;d &lt;U&gt;- &lt;T&gt;band&lt;S&gt; or&lt;R&gt; Made




[4]

inputs:
How many backlink &lt;Z&gt; per day for new site? Discussion &lt;Y&gt; &#39;Black &lt;X&gt;
SEO&#39; started by Omoplata, Dec 3, 2010. 1) for a &lt;W&gt; created site,
what&#39;s &lt;V&gt; max &lt;U&gt;links per day I should do to be safe? 2) how &lt;T&gt; do
I have&lt;S&gt; let my site&lt;R&gt; before I can start making more blinks? I did
about 6000 forum profiles every 24 hours for 10 days for &lt;Q&gt; of my
sites&lt;P&gt; had a brand new domain. There is &lt;O&gt; backlinks for every&lt;N&gt;
these &lt;M&gt; profile so &lt;L&gt;s 18 000 backlinks every 24 hours and nothing
happened in terms of being penalized &lt;K&gt; sandboxed. This is now maybe
3 months ago &lt;J&gt; the site &lt;I&gt; ranking on first page for&lt;H&gt;a lot&lt;G&gt; my
targeted keywords. build more you can in starting &lt;F&gt; do manual
submission and not spammy&lt;E&gt; means manual +&lt;D&gt; to&lt;C&gt; post.. &lt;B&gt; after
1 month you can &lt;A&gt; a &lt;z&gt; blast.. Wow, dude, you built 18k backlink
&lt;y&gt; a day&lt;x&gt; a brand&lt;w&gt;? How quickly did&lt;v&gt; rank up? What kind of
competition/search&lt;u&gt; did &lt;t&gt; keywords have?

targets:
&lt;Z&gt;s &lt;Y&gt; in &lt;X&gt; Hat &lt;W&gt; newly &lt;V&gt; the &lt;U&gt; # back &lt;T&gt; long&lt;S&gt; to&lt;R&gt; age
&lt;Q&gt; one&lt;P&gt; which &lt;O&gt; three&lt;N&gt; of &lt;M&gt; forum &lt;L&gt; that &lt;K&gt; or &lt;J&gt; and &lt;I&gt;
is&lt;H&gt; &lt;G&gt; of &lt;F&gt; but&lt;E&gt; type&lt;D&gt; relevant&lt;C&gt; the &lt;B&gt; then &lt;A&gt; make &lt;z&gt;
big &lt;y&gt;s&lt;x&gt; on&lt;w&gt; new site&lt;v&gt; you&lt;u&gt;es &lt;t&gt; those




[5]

inputs:
The Denver Board of Education opened the 2017-18 school year with an
update &lt;Z&gt; projects that include new construction &lt;Y&gt; upgrades, heat
mitigation &lt;X&gt; quality learning environments. We &lt;W&gt; excited &lt;V&gt;
Denver students will be the beneficiaries &lt;U&gt;a four year, $572 million
General Oblig &lt;T&gt; Bond.&lt;S&gt; the passage of the bond, our construction
team has worked to schedule&lt;R&gt; projects over &lt;Q&gt; four-year term&lt;P&gt;
bond. Denver voters on Tuesday approved bond and mill funding &lt;O&gt;
for&lt;N&gt; in Denver Public Schools, agreeing to invest $572 million in
bond funding &lt;M&gt; build and improve schools and &lt;L&gt;6.6 million in
operating dollars to support proven initiatives, &lt;K&gt; as early &lt;J&gt;
Denver voters say &lt;I&gt; to bond and mill levy funding&lt;H&gt; for&lt;G&gt;PS
students and schools. Click to learn more about the details of the
voter-approved &lt;F&gt; measure. Denver voters&lt;E&gt;. 8 approved bond and mill
funding&lt;D&gt; for DPS students and schools. Learn more about what’s
included in the mill &lt;C&gt;y measure.

targets:
&lt;Z&gt; on &lt;Y&gt;, &lt;X&gt; and &lt;W&gt; are &lt;V&gt; that &lt;U&gt; of  &lt;T&gt;ation&lt;S&gt; Since&lt;R&gt; the
&lt;Q&gt; the&lt;P&gt; of the &lt;O&gt; measures&lt;N&gt; students &lt;M&gt; to &lt;L&gt; $5 &lt;K&gt; such &lt;J&gt;
literacy. &lt;I&gt; yes&lt;H&gt; support&lt;G&gt; D &lt;F&gt; bond&lt;E&gt; on Nov&lt;D&gt; measures&lt;C&gt;lev
</code></pre>
<p><a name='2'></a></p>
<h1 id="Part-2-Transfomer"><a href="#Part-2-Transfomer" class="headerlink" title="Part 2: Transfomer"></a>Part 2: Transfomer</h1><p>We now load a Transformer model checkpoint that has been pre-trained using the above C4 dataset and decode from it. This will save you a lot of time rather than have to train your model yourself. Later in this notebook, we will show you how to fine-tune your model.</p>
<img src = "fulltransformer.png" width="300" height="600">

<p>Start by loading in the model. We copy the checkpoint to local dir for speed, otherwise initialization takes a very long time. Last week you implemented the decoder part for the transformer. Now you will implement the encoder part. Concretely you will implement the following. </p>
<img src = "encoder.png" width="300" height="600">



<p><a name='2.1'></a></p>
<h3 id="2-1-Transformer-Encoder"><a href="#2-1-Transformer-Encoder" class="headerlink" title="2.1 Transformer Encoder"></a>2.1 Transformer Encoder</h3><p>You will now implement the transformer encoder. Concretely you will implement two functions. The first function is <code>FeedForwardBlock</code>.</p>
<p><a name='2.1.1'></a></p>
<h4 id="2-1-1-The-Feedforward-Block"><a href="#2-1-1-The-Feedforward-Block" class="headerlink" title="2.1.1 The Feedforward Block"></a>2.1.1 The Feedforward Block</h4><p>The <code>FeedForwardBlock</code> function is an important one so you will start by implementing it. To do so, you need to return a list of the following: </p>
<ul>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm"><code>tl.LayerNorm()</code></a> = layer normalization.</li>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense"><code>tl.Dense(d_ff)</code></a> = fully connected layer.</li>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu"><code>activation</code></a> = activation relu, tanh, sigmoid etc. </li>
<li><code>dropout_middle</code> = we gave you this function (don’t worry about its implementation).</li>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense"><code>tl.Dense(d_model)</code></a> = fully connected layer with same dimension as the model.</li>
<li><code>dropout_final</code> = we gave you this function (don’t worry about its implementation).</li>
</ul>
<p>You can always take a look at <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/">trax documentation</a> if needed.</p>
<p><strong>Instructions</strong>: Implement the feedforward part of the transformer. You will be returning a list. </p>
<p><a name='ex02'></a></p>
<h3 id="Exercise-02"><a href="#Exercise-02" class="headerlink" title="Exercise 02"></a>Exercise 02</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: FeedForwardBlock</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">FeedForwardBlock</span>(<span class="params">d_model, d_ff, dropout, dropout_shared_axes, mode, activation</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns a list of layers implementing a feed-forward block.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: int:  depth of embedding</span></span><br><span class="line"><span class="string">        d_ff: int: depth of feed-forward layer</span></span><br><span class="line"><span class="string">        dropout: float: dropout rate (how much to drop out)</span></span><br><span class="line"><span class="string">        dropout_shared_axes: list of integers, axes to share dropout mask</span></span><br><span class="line"><span class="string">        mode: str: &#x27;train&#x27; or &#x27;eval&#x27;</span></span><br><span class="line"><span class="string">        activation: the non-linearity in feed-forward layer</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A list of layers which maps vectors to vectors.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    dropout_middle = tl.Dropout(rate=dropout,</span><br><span class="line">                                shared_axes=dropout_shared_axes, </span><br><span class="line">                                mode=mode)</span><br><span class="line">  </span><br><span class="line">    dropout_final = tl.Dropout(rate=dropout, </span><br><span class="line">                               shared_axes=dropout_shared_axes, </span><br><span class="line">                               mode=mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    ff_block = [ </span><br><span class="line">        <span class="comment"># trax Layer normalization </span></span><br><span class="line">        tl.LayerNorm(),</span><br><span class="line">        <span class="comment"># trax Dense layer using `d_ff`</span></span><br><span class="line">        tl.Dense(d_ff),</span><br><span class="line">        <span class="comment"># activation() layer - you need to call (use parentheses) this func!</span></span><br><span class="line">        activation(),</span><br><span class="line">        <span class="comment"># dropout middle layer</span></span><br><span class="line">        dropout_middle,</span><br><span class="line">        <span class="comment"># trax Dense layer using `d_model`</span></span><br><span class="line">        tl.Dense(d_model),</span><br><span class="line">        <span class="comment"># dropout final layer</span></span><br><span class="line">        dropout_final,</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ff_block</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Print the block layout</span></span><br><span class="line">feed_forward_example = FeedForwardBlock(d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.8</span>, dropout_shared_axes=<span class="number">0</span>, mode = <span class="string">&#x27;train&#x27;</span>, activation = tl.Relu)</span><br><span class="line"><span class="built_in">print</span>(feed_forward_example)</span><br></pre></td></tr></table></figure>

<pre><code>[LayerNorm, Dense_2048, Relu, Dropout, Dense_512, Dropout]
</code></pre>
<h4 id="Expected-Output-1"><a href="#Expected-Output-1" class="headerlink" title="Expected Output:"></a><strong>Expected Output:</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[LayerNorm, Dense_2048, Relu, Dropout, Dense_512, Dropout]</span><br></pre></td></tr></table></figure>

<p><a name='2.1.2'></a></p>
<h4 id="2-1-2-The-Encoder-Block"><a href="#2-1-2-The-Encoder-Block" class="headerlink" title="2.1.2 The Encoder Block"></a>2.1.2 The Encoder Block</h4><p>The encoder block will use the <code>FeedForwardBlock</code>. </p>
<p>You will have to build two residual connections. Inside the first residual connection you will have the <code>tl.layerNorm()</code>, <code>attention</code>, and <code>dropout_</code> layers. The second residual connection will have the <code>feed_forward</code>.  </p>
<p>You will also need to implement <code>feed_forward</code>, <code>attention</code> and <code>dropout_</code> blocks. </p>
<p>So far you haven’t seen the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.Attention"><code>tl.Attention()</code></a> and <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual"><code>tl.Residual()</code></a> layers so you can check the docs by clicking on them.</p>
<p><a name='ex03'></a></p>
<h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: EncoderBlock</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">EncoderBlock</span>(<span class="params">d_model, d_ff, n_heads, dropout, dropout_shared_axes,</span></span></span><br><span class="line"><span class="params"><span class="function">                  mode, ff_activation, FeedForwardBlock=FeedForwardBlock</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Returns a list of layers that implements a Transformer encoder block.</span></span><br><span class="line"><span class="string">    The input to the layer is a pair, (activations, mask), where the mask was</span></span><br><span class="line"><span class="string">    created from the original source tokens to prevent attending to the padding</span></span><br><span class="line"><span class="string">    part of the input.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model (int): depth of embedding.</span></span><br><span class="line"><span class="string">        d_ff (int): depth of feed-forward layer.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate (how much to drop out).</span></span><br><span class="line"><span class="string">        dropout_shared_axes (int): axes on which to share dropout mask.</span></span><br><span class="line"><span class="string">        mode (str): &#x27;train&#x27; or &#x27;eval&#x27;.</span></span><br><span class="line"><span class="string">        ff_activation (function): the non-linearity in feed-forward layer.</span></span><br><span class="line"><span class="string">        FeedForwardBlock (function): A function that returns the feed forward block.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        list: A list of layers that maps (activations, mask) to (activations, mask).</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Attention block</span></span><br><span class="line">    attention = tl.Attention( </span><br><span class="line">        <span class="comment"># Use dimension of the model</span></span><br><span class="line">        d_feature=d_model,</span><br><span class="line">        <span class="comment"># Set it equal to number of attention heads</span></span><br><span class="line">        n_heads=n_heads,</span><br><span class="line">        <span class="comment"># Set it equal `dropout`</span></span><br><span class="line">        dropout=dropout,</span><br><span class="line">        <span class="comment"># Set it equal `mode`</span></span><br><span class="line">        mode=mode</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Call the function `FeedForwardBlock` (implemented before) and pass in the parameters</span></span><br><span class="line">    feed_forward = FeedForwardBlock(d_model, d_ff, dropout, dropout_shared_axes, mode, ff_activation)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Dropout block</span></span><br><span class="line">    dropout_ = tl.Dropout( </span><br><span class="line">        <span class="comment"># set it equal to `dropout`</span></span><br><span class="line">        rate=dropout,</span><br><span class="line">        <span class="comment"># set it equal to the axes on which to share dropout mask</span></span><br><span class="line">        shared_axes=dropout_shared_axes,</span><br><span class="line">        <span class="comment"># set it equal to `mode`</span></span><br><span class="line">        mode=mode</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    encoder_block = [ </span><br><span class="line">        <span class="comment"># add `Residual` layer</span></span><br><span class="line">        tl.Residual(</span><br><span class="line">            <span class="comment"># add norm layer</span></span><br><span class="line">            tl.LayerNorm(),</span><br><span class="line">            <span class="comment"># add attention</span></span><br><span class="line">            attention,</span><br><span class="line">            <span class="comment"># add dropout</span></span><br><span class="line">            dropout_,</span><br><span class="line">        ),</span><br><span class="line">        <span class="comment"># add another `Residual` layer</span></span><br><span class="line">        tl.Residual(</span><br><span class="line">            <span class="comment"># add feed forward</span></span><br><span class="line">            feed_forward,</span><br><span class="line">        ),</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> encoder_block</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Print the block layout</span></span><br><span class="line">encoder_example = EncoderBlock(d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, n_heads=<span class="number">6</span>, dropout=<span class="number">0.8</span>, dropout_shared_axes=<span class="number">0</span>, mode = <span class="string">&#x27;train&#x27;</span>, ff_activation=tl.Relu)</span><br><span class="line"><span class="built_in">print</span>(encoder_example)</span><br></pre></td></tr></table></figure>

<pre><code>[Serial_in2_out2[
  Branch_in2_out3[
    None
    Serial_in2_out2[
      LayerNorm
      Serial_in2_out2[
        Dup_out2
        Dup_out2
        Serial_in4_out2[
          Parallel_in3_out3[
            Dense_512
            Dense_512
            Dense_512
          ]
          PureAttention_in4_out2
          Dense_512
        ]
      ]
      Dropout
    ]
  ]
  Add_in2
], Serial[
  Branch_out2[
    None
    Serial[
      LayerNorm
      Dense_2048
      Relu
      Dropout
      Dense_512
      Dropout
    ]
  ]
  Add_in2
]]
</code></pre>
<h4 id="Expected-Output-2"><a href="#Expected-Output-2" class="headerlink" title="Expected Output:"></a><strong>Expected Output:</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[Serial_in2_out2[</span><br><span class="line">  Branch_in2_out3[</span><br><span class="line">    None</span><br><span class="line">    Serial_in2_out2[</span><br><span class="line">      LayerNorm</span><br><span class="line">      Serial_in2_out2[</span><br><span class="line">        Dup_out2</span><br><span class="line">        Dup_out2</span><br><span class="line">        Serial_in4_out2[</span><br><span class="line">          Parallel_in3_out3[</span><br><span class="line">            Dense_512</span><br><span class="line">            Dense_512</span><br><span class="line">            Dense_512</span><br><span class="line">          ]</span><br><span class="line">          PureAttention_in4_out2</span><br><span class="line">          Dense_512</span><br><span class="line">        ]</span><br><span class="line">      ]</span><br><span class="line">      Dropout</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">  Add_in2</span><br><span class="line">], Serial[</span><br><span class="line">  Branch_out2[</span><br><span class="line">    None</span><br><span class="line">    Serial[</span><br><span class="line">      LayerNorm</span><br><span class="line">      Dense_2048</span><br><span class="line">      Relu</span><br><span class="line">      Dropout</span><br><span class="line">      Dense_512</span><br><span class="line">      Dropout</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">  Add_in2</span><br><span class="line">]]</span><br></pre></td></tr></table></figure>

<p><a name='2.1.3'></a></p>
<h3 id="2-1-3-The-Transformer-Encoder"><a href="#2-1-3-The-Transformer-Encoder" class="headerlink" title="2.1.3 The Transformer Encoder"></a>2.1.3 The Transformer Encoder</h3><p>Now that you have implemented the <code>EncoderBlock</code>, it is time to build the full encoder. BERT, or Bidirectional Encoder Representations from Transformers is one such encoder. </p>
<p>You will implement its core code in the function below by using the functions you have coded so far. </p>
<p>The model takes in many hyperparameters, such as the <code>vocab_size</code>, the number of classes, the dimension of your model, etc. You want to build a generic function that will take in many parameters, so you can use it later. At the end of the day, anyone can just load in an API and call transformer, but we think it is important to make sure you understand how it is built. Let’s get started. </p>
<p><strong>Instructions:</strong> For this encoder you will need a <code>positional_encoder</code> first (which is already provided) followed by <code>n_layers</code> encoder blocks, which are the same encoder blocks you previously built. Once you store the <code>n_layers</code> <code>EncoderBlock</code> in a list, you are going to encode a <code>Serial</code> layer with the following sublayers: </p>
<ul>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch"><code>tl.Branch</code></a>: helps with the branching and has the following sublayers:<ul>
<li><code>positional_encoder</code>.</li>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.PaddingMask"><code>tl.PaddingMask()</code></a>: layer that maps integer sequences to padding masks.</li>
</ul>
</li>
<li>Your list of <code>EncoderBlock</code>s</li>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Select"><code>tl.Select([0], n_in=2)</code></a>:  Copies, reorders, or deletes stack elements according to indices.</li>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm"><code>tl.LayerNorm()</code></a>.</li>
<li><a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean"><code>tl.Mean()</code></a>: Mean along the first axis.</li>
<li><code>tl.Dense()</code> with n_units set to n_classes. </li>
<li><code>tl.LogSoftmax()</code>   </li>
</ul>
<p>Please refer to the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/">trax documentation</a> for further information. </p>
<p><a name='ex04'></a></p>
<h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: TransformerEncoder</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TransformerEncoder</span>(<span class="params">vocab_size=vocab_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                       n_classes=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       d_model=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       d_ff=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       n_layers=<span class="number">6</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       n_heads=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       dropout_shared_axes=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       max_len=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       mode=<span class="string">&#x27;train&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                       ff_activation=tl.Relu,</span></span></span><br><span class="line"><span class="params"><span class="function">                       EncoderBlock=EncoderBlock</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Returns a Transformer encoder model.</span></span><br><span class="line"><span class="string">    The input to the model is a tensor of tokens.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab_size (int): vocab size. Defaults to vocab_size.</span></span><br><span class="line"><span class="string">        n_classes (int): how many classes on output. Defaults to 10.</span></span><br><span class="line"><span class="string">        d_model (int): depth of embedding. Defaults to 512.</span></span><br><span class="line"><span class="string">        d_ff (int): depth of feed-forward layer. Defaults to 2048.</span></span><br><span class="line"><span class="string">        n_layers (int): number of encoder/decoder layers. Defaults to 6.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads. Defaults to 8.</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate (how much to drop out). Defaults to 0.1.</span></span><br><span class="line"><span class="string">        dropout_shared_axes (int): axes on which to share dropout mask. Defaults to None.</span></span><br><span class="line"><span class="string">        max_len (int): maximum symbol length for positional encoding. Defaults to 2048.</span></span><br><span class="line"><span class="string">        mode (str): &#x27;train&#x27; or &#x27;eval&#x27;. Defaults to &#x27;train&#x27;.</span></span><br><span class="line"><span class="string">        ff_activation (function): the non-linearity in feed-forward layer. Defaults to tl.Relu.</span></span><br><span class="line"><span class="string">        EncoderBlock (function): Returns the encoder block. Defaults to EncoderBlock.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.layers.combinators.Serial: A Transformer model as a layer that maps</span></span><br><span class="line"><span class="string">        from a tensor of tokens to activations over a set of output classes.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    positional_encoder = [</span><br><span class="line">        tl.Embedding(vocab_size, d_model),</span><br><span class="line">        tl.Dropout(rate=dropout, shared_axes=dropout_shared_axes, mode=mode),</span><br><span class="line">        tl.PositionalEncoding(max_len=max_len)</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; WITH YOUR CODE) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use the function `EncoderBlock` (implemented above) and pass in the parameters over `n_layers`</span></span><br><span class="line">    encoder_blocks = [EncoderBlock(d_model, d_ff, n_heads, dropout, dropout_shared_axes,</span><br><span class="line">                  mode, ff_activation, FeedForwardBlock=FeedForwardBlock) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Assemble and return the model.</span></span><br><span class="line">    <span class="keyword">return</span> tl.Serial(</span><br><span class="line">        <span class="comment"># Encode</span></span><br><span class="line">        tl.Branch(</span><br><span class="line">            <span class="comment"># Use `positional_encoder`</span></span><br><span class="line">            positional_encoder,</span><br><span class="line">            <span class="comment"># Use trax padding mask</span></span><br><span class="line">            tl.PaddingMask(),</span><br><span class="line">        ),</span><br><span class="line">        <span class="comment"># Use `encoder_blocks`</span></span><br><span class="line">        encoder_blocks,</span><br><span class="line">        <span class="comment"># Use select layer</span></span><br><span class="line">        tl.Select([<span class="number">0</span>], n_in=<span class="number">2</span>),</span><br><span class="line">        <span class="comment"># Use trax layer normalization</span></span><br><span class="line">        tl.LayerNorm(),</span><br><span class="line">        <span class="comment"># Map to output categories.</span></span><br><span class="line">        <span class="comment"># Use trax mean. set axis to 1</span></span><br><span class="line">        tl.Mean(axis = <span class="number">1</span>),</span><br><span class="line">        <span class="comment"># Use trax Dense using `n_classes`</span></span><br><span class="line">        tl.Dense(n_classes),</span><br><span class="line">        <span class="comment"># Use trax log softmax</span></span><br><span class="line">        tl.LogSoftmax(),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run this cell to see the structure of your model</span></span><br><span class="line"><span class="comment"># Only 1 layer is used to keep the output readable</span></span><br><span class="line">TransformerEncoder(n_layers=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>Serial[
  Branch_out2[
    [Embedding_32000_512, Dropout, PositionalEncoding]
    PaddingMask(0)
  ]
  Serial_in2_out2[
    Branch_in2_out3[
      None
      Serial_in2_out2[
        LayerNorm
        Serial_in2_out2[
          Dup_out2
          Dup_out2
          Serial_in4_out2[
            Parallel_in3_out3[
              Dense_512
              Dense_512
              Dense_512
            ]
            PureAttention_in4_out2
            Dense_512
          ]
        ]
        Dropout
      ]
    ]
    Add_in2
  ]
  Serial[
    Branch_out2[
      None
      Serial[
        LayerNorm
        Dense_2048
        Relu
        Dropout
        Dense_512
        Dropout
      ]
    ]
    Add_in2
  ]
  Select[0]_in2
  LayerNorm
  Mean
  Dense_10
  LogSoftmax
]
</code></pre>
<h4 id="Expected-Output-3"><a href="#Expected-Output-3" class="headerlink" title="Expected Output:"></a><strong>Expected Output:</strong></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">Serial[</span><br><span class="line">  Branch_out2[</span><br><span class="line">    [Embedding_32000_512, Dropout, PositionalEncoding]</span><br><span class="line">    <span class="built_in">PaddingMask</span>(<span class="number">0</span>)</span><br><span class="line">  ]</span><br><span class="line">  Serial_in2_out2[</span><br><span class="line">    Branch_in2_out3[</span><br><span class="line">      None</span><br><span class="line">      Serial_in2_out2[</span><br><span class="line">        LayerNorm</span><br><span class="line">        Serial_in2_out2[</span><br><span class="line">          Dup_out2</span><br><span class="line">          Dup_out2</span><br><span class="line">          Serial_in4_out2[</span><br><span class="line">            Parallel_in3_out3[</span><br><span class="line">              Dense_512</span><br><span class="line">              Dense_512</span><br><span class="line">              Dense_512</span><br><span class="line">            ]</span><br><span class="line">            PureAttention_in4_out2</span><br><span class="line">            Dense_512</span><br><span class="line">          ]</span><br><span class="line">        ]</span><br><span class="line">        Dropout</span><br><span class="line">      ]</span><br><span class="line">    ]</span><br><span class="line">    Add_in2</span><br><span class="line">  ]</span><br><span class="line">  Serial[</span><br><span class="line">    Branch_out2[</span><br><span class="line">      None</span><br><span class="line">      Serial[</span><br><span class="line">        LayerNorm</span><br><span class="line">        Dense_2048</span><br><span class="line">        Relu</span><br><span class="line">        Dropout</span><br><span class="line">        Dense_512</span><br><span class="line">        Dropout</span><br><span class="line">      ]</span><br><span class="line">    ]</span><br><span class="line">    Add_in2</span><br><span class="line">  ]</span><br><span class="line">  Select[<span class="number">0</span>]_in2</span><br><span class="line">  LayerNorm</span><br><span class="line">  Mean</span><br><span class="line">  Dense_10</span><br><span class="line">  LogSoftmax</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p><strong>NOTE Congratulations! You have completed all of the graded functions of this assignment.</strong> Since the rest of the assignment takes a lot of time and memory to run we are providing some extra ungraded labs for you to see this model in action.</p>
<p><strong>Keep it up!</strong></p>
<p>To see this model in action continue to the next 2 ungraded labs. <strong>We strongly recommend you to try the colab versions of them as they will yield a much smoother experience.</strong> The links to the colabs can be found within the ungraded labs or if you already know how to open files within colab here are some shortcuts (if not, head to the ungraded labs which contain some extra instructions):</p>
<p><a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1EHAbMnW6u-GqYWh5r3Z8uLbz4KNpKOAv/view?usp=sharing">BERT Loss Model Colab</a></p>
<p><a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1c-8KJkTySRGqCx_JjwjvXuRBTNTqEE0N/view?usp=sharing">T5 SQuAD Model Colab</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Transformer-Summarizer/2020/09/27/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Transformer-Summarizer/2020/09/27/" class="post-title-link" itemprop="url">Transformer Summarizer</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-09-27 15:23:13 / Modified: 15:24:19" itemprop="dateCreated datePublished" datetime="2020-09-27T15:23:13+08:00">2020-09-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Transformer-Summarizer/2020/09/27/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Transformer-Summarizer/2020/09/27/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Assignment-2-Transformer-Summarizer"><a href="#Assignment-2-Transformer-Summarizer" class="headerlink" title="Assignment 2: Transformer Summarizer"></a>Assignment 2: Transformer Summarizer</h1><p>Welcome to the second assignment of course 4. In this assignment you will explore summarization using the transformer model. Yes, you will implement the transformer decoder from scratch, but we will slowly walk you through it. There are many hints in this notebook so feel free to use them as needed. </p>
<img src = "transformerNews.png">



<h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li><a href="#0">Introduction</a></li>
<li><a href="#1">Part 1: Importing the dataset</a><ul>
<li><a href="#1.1">1.1 Encode &amp; Decode helper functions</a></li>
<li><a href="#1.2">1.2 Defining parameters</a></li>
<li><a href="#1.3">1.3 Exploring the data</a></li>
</ul>
</li>
<li><a href="#2">Part 2: Summarization with transformer</a><ul>
<li><a href="#2.1">2.1 Dot product attention</a><ul>
<li><a href="#ex01">Exercise 01</a></li>
</ul>
</li>
<li><a href="#2.2">2.2 Causal Attention</a><ul>
<li><a href="#ex02">Exercise 02</a></li>
</ul>
</li>
<li><a href="#2.3">2.3 Transformer decoder block</a><ul>
<li><a href="#ex03">Exercise 03</a></li>
</ul>
</li>
<li><a href="#2.4">2.4 Transformer Language model</a><ul>
<li><a href="#ex04">Exercise 04</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#3">Part 3: Training</a><ul>
<li><a href="#3.1">3.1 Training the model</a><ul>
<li><a href="#ex05">Exercise 05</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#4">Part 4: Evaluation</a><ul>
<li><a href="#4.1">4.1 Loading in a trained model</a></li>
</ul>
</li>
<li><a href="#5">Part 5: Testing with your own input</a> <ul>
<li><a href="#ex06">Exercise 6</a></li>
<li><a href="#5.1">5.1 Greedy decoding</a><ul>
<li><a href="#ex07">Exercise 07</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><a name='0'></a></p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Summarization is an important task in natural language processing and could be useful for a consumer enterprise. For example, bots can be used to scrape articles, summarize them, and then you can use sentiment analysis to identify the sentiment about certain stocks. Anyways who wants to read an article or a long email today, when you can build a transformer to summarize text for you. Let’s get started, by completing this assignment you will learn to:  </p>
<ul>
<li>Use built-in functions to preprocess your data</li>
<li>Implement DotProductAttention</li>
<li>Implement Causal Attention</li>
<li>Understand how attention works</li>
<li>Build the transformer model</li>
<li>Evaluate your model</li>
<li>Summarize an article</li>
</ul>
<p>As you can tell, this model is slightly different than the ones you have already implemented. This is heavily based on attention and does not rely on sequences, which allows for parallel computing. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> textwrap</span><br><span class="line">wrapper = textwrap.TextWrapper(width=<span class="number">70</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> trax</span><br><span class="line"><span class="keyword">from</span> trax <span class="keyword">import</span> layers <span class="keyword">as</span> tl</span><br><span class="line"><span class="keyword">from</span> trax.fastmath <span class="keyword">import</span> numpy <span class="keyword">as</span> jnp</span><br><span class="line"></span><br><span class="line"><span class="comment"># to print the entire np array</span></span><br><span class="line">np.set_printoptions(threshold=sys.maxsize)</span><br></pre></td></tr></table></figure>

<p><a name='1'></a></p>
<h2 id="Part-1-Importing-the-dataset"><a href="#Part-1-Importing-the-dataset" class="headerlink" title="Part 1: Importing the dataset"></a>Part 1: Importing the dataset</h2><p>Trax makes it easy to work with Tensorflow’s datasets:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This will download the dataset if no data_dir is specified.</span></span><br><span class="line"><span class="comment"># Downloading and processing can take bit of time,</span></span><br><span class="line"><span class="comment"># so we have the data already in &#x27;data/&#x27; for you</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Importing CNN/DailyMail articles dataset</span></span><br><span class="line">train_stream_fn = trax.data.TFDS(<span class="string">&#x27;cnn_dailymail&#x27;</span>,</span><br><span class="line">                                 data_dir=<span class="string">&#x27;data/&#x27;</span>,</span><br><span class="line">                                 keys=(<span class="string">&#x27;article&#x27;</span>, <span class="string">&#x27;highlights&#x27;</span>),</span><br><span class="line">                                 train=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This should be much faster as the data is downloaded already.</span></span><br><span class="line">eval_stream_fn = trax.data.TFDS(<span class="string">&#x27;cnn_dailymail&#x27;</span>,</span><br><span class="line">                                data_dir=<span class="string">&#x27;data/&#x27;</span>,</span><br><span class="line">                                keys=(<span class="string">&#x27;article&#x27;</span>, <span class="string">&#x27;highlights&#x27;</span>),</span><br><span class="line">                                train=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p><a name='1.1'></a></p>
<h2 id="1-1-Tokenize-amp-Detokenize-helper-functions"><a href="#1-1-Tokenize-amp-Detokenize-helper-functions" class="headerlink" title="1.1 Tokenize &amp; Detokenize helper functions"></a>1.1 Tokenize &amp; Detokenize helper functions</h2><p>Just like in the previous assignment, the cell above loads in the encoder for you. Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your <a target="_blank" rel="noopener" href="https://github.com/google/trax">Trax</a> models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following: </p>
<ul>
<li><span style='color:blue'> word2Ind: </span> a dictionary mapping the word to its index.</li>
<li><span style='color:blue'> ind2Word:</span> a dictionary mapping the index to its word.</li>
<li><span style='color:blue'> word2Count:</span> a dictionary mapping the word to the number of times it appears. </li>
<li><span style='color:blue'> num_words:</span> total number of words that have appeared. </li>
</ul>
<p>Since you have already implemented these in previous assignments of the specialization, we will provide you with helper functions that will do this for you. Run the cell below to get the following functions:</p>
<ul>
<li><span style='color:blue'> tokenize: </span> converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords.</li>
<li><span style='color:blue'> detokenize: </span> converts a token list to its corresponding sentence (i.e. string).</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">input_str, EOS=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Input str to features dict, ready for inference&quot;&quot;&quot;</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Use the trax.data.tokenize method. It takes streams and returns streams,</span></span><br><span class="line">    <span class="comment"># we get around it by making a 1-element stream with `iter`.</span></span><br><span class="line">    inputs =  <span class="built_in">next</span>(trax.data.tokenize(<span class="built_in">iter</span>([input_str]),</span><br><span class="line">                                      vocab_dir=<span class="string">&#x27;vocab_dir/&#x27;</span>,</span><br><span class="line">                                      vocab_file=<span class="string">&#x27;summarize32k.subword.subwords&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Mark the end of the sentence with EOS</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(inputs) + [EOS]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detokenize</span>(<span class="params">integers</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;List of ints to str&quot;&quot;&quot;</span></span><br><span class="line">  </span><br><span class="line">    s = trax.data.detokenize(integers,</span><br><span class="line">                             vocab_dir=<span class="string">&#x27;vocab_dir/&#x27;</span>,</span><br><span class="line">                             vocab_file=<span class="string">&#x27;summarize32k.subword.subwords&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> wrapper.fill(s)</span><br></pre></td></tr></table></figure>

<p><a name='1.2'></a></p>
<h2 id="1-2-Preprocessing-for-Language-Models-Concatenate-It"><a href="#1-2-Preprocessing-for-Language-Models-Concatenate-It" class="headerlink" title="1.2 Preprocessing for Language Models: Concatenate It!"></a>1.2 Preprocessing for Language Models: Concatenate It!</h2><p>This week you will use a language model – Transformer Decoder – to solve<br>an input-output problem. As you know, language models only predict the next<br>word, they have no notion of inputs. To create a single input suitable for<br>a language model, we concatenate inputs with targets putting a separator<br>in between. We also need to create a mask – with 0s at inputs and 1s at targets – so that the model is not penalized for mis-predicting the article and only focuses on the summary. See the preprocess function below for how this is done.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Special tokens</span></span><br><span class="line">SEP = <span class="number">0</span> <span class="comment"># Padding or separator token</span></span><br><span class="line">EOS = <span class="number">1</span> <span class="comment"># End of sentence token</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Concatenate tokenized inputs and targets using 0 as separator.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params">stream</span>):</span></span><br><span class="line">    <span class="keyword">for</span> (article, summary) <span class="keyword">in</span> stream:</span><br><span class="line">        joint = np.array(<span class="built_in">list</span>(article) + [EOS, SEP] + <span class="built_in">list</span>(summary) + [EOS])</span><br><span class="line">        mask = [<span class="number">0</span>] * (<span class="built_in">len</span>(<span class="built_in">list</span>(article)) + <span class="number">2</span>) + [<span class="number">1</span>] * (<span class="built_in">len</span>(<span class="built_in">list</span>(summary)) + <span class="number">1</span>) <span class="comment"># Accounting for EOS and SEP</span></span><br><span class="line">        <span class="keyword">yield</span> joint, joint, np.array(mask)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can combine a few data preprocessing steps into a pipeline like this.</span></span><br><span class="line">input_pipeline = trax.data.Serial(</span><br><span class="line">    <span class="comment"># Tokenizes</span></span><br><span class="line">    trax.data.Tokenize(vocab_dir=<span class="string">&#x27;vocab_dir/&#x27;</span>,</span><br><span class="line">                       vocab_file=<span class="string">&#x27;summarize32k.subword.subwords&#x27;</span>),</span><br><span class="line">    <span class="comment"># Uses function defined above</span></span><br><span class="line">    preprocess,</span><br><span class="line">    <span class="comment"># Filters out examples longer than 2048</span></span><br><span class="line">    trax.data.FilterByLength(<span class="number">2048</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply preprocessing to data streams.</span></span><br><span class="line">train_stream = input_pipeline(train_stream_fn())</span><br><span class="line">eval_stream = input_pipeline(eval_stream_fn())</span><br><span class="line"></span><br><span class="line">train_input, train_target, train_mask = <span class="built_in">next</span>(train_stream)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">sum</span>((train_input - train_target)**<span class="number">2</span>) == <span class="number">0</span>  <span class="comment"># They are the same in Language Model (LM).</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prints mask, 0s on article, 1s on summary</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Single example mask:\n\n <span class="subst">&#123;train_mask&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Single example mask:

 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prints: [Example][&lt;EOS&gt;][&lt;pad&gt;][Example Summary][&lt;EOS&gt;]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Single example:\n\n <span class="subst">&#123;detokenize(train_input)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Single example:

 By . Margot Peppers . Nigerian and Cameroonian pop star Dencia has hit
out at Lupita Nyong&#39;o for her new contract with Lancome, accusing her
of bowing to &#39;white people companies&#39;. In an angry tweet directed at
the 12 Years A Slave star, she wrote: &#39;Oh @Lupita_Nyongo cln&#39;t talk
abt the bleaching creams white people (Companies) make cuz the white
man pays her, they own her!! [sic]&#39;. The comment comes just a month
after Miss Nyong&#39;o mentioned Dencia - who has been accused of
marketing her own brand of skin-bleaching cream called Whitenicious -
in a speech about learning to value the color of her own skin. Scroll
down for video . Butting heads: Nigerian and Cameroonian pop star
Dencia has hit out at Lupita Nyong&#39;o for her new contract with
Lancome, accusing her of bowing to &#39;white people companies&#39; Fighting
words: In a tweet directed at the 12 Years A Slave star, she wrote:
&#39;Oh @Lupita_Nyongo cln&#39;t talk abt the bleaching creams white people
(Companies) make cuz the white man pays her, they own her!! [sic]&#39; The
pop star is no stranger to . controversy; in a February interview with
Ebony, she all but admitted . that Whitenicious is intended as a skin-
lightener, not as a cure for . dark spots as it claims. &#39;When . you
take that picture and you put a picture of Dencia darker, this is .
what you&#39;re telling people - the product really works,&#39; she said. &#39;And
guess what? People really want to buy it. It&#39;s what it is. I don&#39;t
really care.&#39; Given her defiant and hypocritical attitude, it&#39;s no
surprise the fiery singer was angered when Miss Nyong&#39;o called her out
in a speech at Essence&#39;s Black Women in Hollywood event on February
27. Influential: In a recent speech, Miss Nyong&#39;o read out loud a
letter from a fan who said she decided not to buy Dencia&#39;s skin-
whitening cream Whitenicious because the actress had inspired her to
love her own skin . On-screen: Miss Nyong&#39;o won an Oscar for Best
Supporting Actress for her role in 2013 film 12 Years A Slave . In her
talk, the 30-year-old opened up about how conventional standards of
beauty once affected her self-esteem, reading aloud a letter written
to her by a young girl who viewed her as a role model. &#39;Dear Lupita,&#39;
reads the letter. &#39;I think you&#39;re really lucky to be this black but
yet this successful in Hollywood overnight. I was just about to buy
Dencia&#39;s Whitenicious cream to lighten my skin when you appeared on
the world map and saved me.&#39; &#39;My heart bled a little when I read those
words,&#39; the actress said through tears, explaining how as a child,
she, too, would pray that she&#39;d one day wake up with lighter skin.
Hypocritical: Dencia is no stranger to controversy; in a February
interview with Ebony, she essentially admitted that Whitenicious is
intended as a skin-lightener, not as a cure for dark spots as it
claims . Perpetuating the problem: &#39;When you take that picture and you
put a picture of Dencia darker, this is what you&#39;re telling people -
the product really works,&#39; she said. &#39;And guess what? People really
want to buy it&#39; But while the actress saw the letter as a source of
inspiration, Dencia took it as a personal attack. After her angry
tweet at Miss Nyong&#39;o, criticism poured in, with one person tweeting:
&#39;B**** lupita is the new face of Lancôme!! SHE WINS!! And you&#39;re just
TRASH [sic]&#39;. In her response, Dencia said of the cosmetics company:
&#39;But they sell bleaching cream tho [sic]&#39;. The pop star is likely
referring to Lancome&#39;s Blanc Expert range of cosmetics, which are
actually advertised as &#39;brighteners&#39; that &#39;regulate melanin production
and awaken the luminosity of the skin&#39;. And as far as Dencia&#39;s claim
that Lancome is a &#39;white people company&#39;, a quick perusal of the
website reveals that it has a number of concealers and foundations in
darker skin tones.&lt;EOS&gt;&lt;pad&gt;Dencia&#39;s comment is hypocritical
considering she recently courted controversy for marketing &#39;dark spot
remover&#39; Whitenicious, which is frequently used as a skin-whitening
cream .&lt;EOS&gt;
</code></pre>
<p><a name='1.3'></a></p>
<h2 id="1-3-Batching-with-bucketing"><a href="#1-3-Batching-with-bucketing" class="headerlink" title="1.3 Batching with bucketing"></a>1.3 Batching with bucketing</h2><p>As in the previous week, we use bucketing to create batches of data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Bucketing to create batched generators.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Buckets are defined in terms of boundaries and batch sizes.</span></span><br><span class="line"><span class="comment"># Batch_sizes[i] determines the batch size for items with length &lt; boundaries[i]</span></span><br><span class="line"><span class="comment"># So below, we&#x27;ll take a batch of 16 sentences of length &lt; 128 , 8 of length &lt; 256,</span></span><br><span class="line"><span class="comment"># 4 of length &lt; 512. And so on. </span></span><br><span class="line">boundaries =  [<span class="number">128</span>, <span class="number">256</span>,  <span class="number">512</span>, <span class="number">1024</span>]</span><br><span class="line">batch_sizes = [<span class="number">16</span>,    <span class="number">8</span>,    <span class="number">4</span>,    <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the streams.</span></span><br><span class="line">train_batch_stream = trax.data.BucketByLength(</span><br><span class="line">    boundaries, batch_sizes)(train_stream)</span><br><span class="line"></span><br><span class="line">eval_batch_stream = trax.data.BucketByLength(</span><br><span class="line">    boundaries, batch_sizes)(eval_stream)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Every execution will result in generation of a different article</span></span><br><span class="line"><span class="comment"># Try running this cell multiple times to see how the length of the examples affects the batch size</span></span><br><span class="line">input_batch, _, mask_batch = <span class="built_in">next</span>(train_batch_stream)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Shape of the input_batch</span></span><br><span class="line">input_batch.shape</span><br></pre></td></tr></table></figure>




<pre><code>(2, 1024)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print corresponding integer values</span></span><br><span class="line"><span class="built_in">print</span>(input_batch[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[   27  1091    23    46  3873  1248 16013   256 11599 23297   102    68
 24308     7     5  1037  1958   320  1477   105  2557   186  4133    28
 18175  1348  1287     3  4927  7577    28  8478 10120 19134  7951   364
  7317  4990    79     2   393     2   186  8962  2995  9813  4476  3632
  2270     5     2   705     2   721 10731    16   186 17136    16   193
    54   102    41  1459   320    31 16946    47     2   119  3770   278
   355    28   622   263    78  2613     3   312  4543     4  8662  3788
  3632  2270     5     6  3048 23524     2  1210     2  1958   320  2033
   105    61     2 19134  7951   364  7317  4990    79 24810    17   213
  1091     2   931   320   213 16946    47   415 20579 20964    58  1782
   863   213  7726     2   213  7599  3938  4133    28 26719     4   752
  1480  2868   132    68   583  3898 20579 20964    58   240   197     3
  4531  9531  2959   127   132    28 27439  9275  1628  1602     3  8406
  5364    11  4927  7577    28  8478 10120 19134  7951   364  7317  4990
    79     2   393     2   497     2   186    68 24308  8962  2995  9813
  4476  3632  2270     5     2   705     2   721 10731    16   186 17136
    16   193    54   809    31   278    78  2613  7511    15  1037 20274
    21   379 21549  7150    11  9813  4476  3632  2270     5    80 18649
  1496   667   213 17136    45    78    15   882  1838   213  2439  7883
   379    27  1147     6   104     6   292   966    43 11850   213  1621
     2   931   320   213 13021     4     2    35    22   206    19  5632
   213  1018   111   213  2948   186   213 25931     4     3  2713  7801
   320    28  6105    32   922  1838   213  6350   141   102 24114    75
    78  2613   186  7511    41  2362     2    41   233  3632  2270     5
     6  3048 23524  1955    78    28 11261  1797  1782   198    25    92
  3787  3103   527 13747   320   213  7599     2   487   159   213   669
 27884     4  1622 27872   391  5977  3103   527  2918   186  1472   320
    18    46   810   132    28  2439  7726  3898   213 13021     4   127
     3    34    31 18649  3347     2   148 19134  7951   364  7317  4990
    79   186  9813  4476  3632  2270     5    18 17136    45    78    31
  5369   186 19175     5     3     9  2789    25 11203     2   412    25
   213   966   186    54  1697     3 12849    14    11  7317  4990    79
 12365   146 24810    17   213  1091  4617 27439  9275  1628  4543     4
  8662  3788  3632  2270     5     6  3048 23524     2   186   131  4133
    28 26719     4  6901   809   213    60     6  1797  6350   809   213
   414     8 12370    21    12   186   710   171   864  2362   809   213
  1610   379     9 16946    47   415  3357 15581    81     7     5  1431
  1890   163  4336  7188    20    78  3632  2270     5     6  3048 23524
     7     5   661     2    35   646    25 17926 25290 16741    20  4140
     2   213 13021     4   127     3 19134  7951   364  7317  4990    79
  1353  3873  1248 11599 23297     2 17260  8041   893   213  5627   527
    28   966     2  2439  1740  1524  7726   186 23638    16 24668 21273
   204     2   931   320  1882     3  4531  9531  2959   127 19134  7951
   364  7317  4990    79    43  9363     4   760    70    35    62    19
  2851  2754   103  1353    70  1480 22646   272  7304   132    28  1501
   809   213  1881  1610     3   305  1353   475   809   213 16946    47
   415  7411    84    78   281  3997    88   226 20934     4     3  9813
  4476  3632  2270     5  1353    43  3873  1248   966 17260  8041 16704
   464   186  2439  1740  1524  7726   186  1233   320   213  1156 10835
    78   281  1696    88   226 20934     4     3    27   924  3729    23
    46  4648  1019  3112  1859   809 18235  5333  9141 25733   812    10
     1     0  4927  7577    28  8478 10120 19134  7951   364  7317  4990
    79     2   393     2   186  8962  2995  9813  4476  3632  2270     5
     2   705     2   721  2557   102  4925  1838    28   622    78  2613
  1859 16346 27439  6774  1628   312    15  1037     2  4543     4  8662
  3788  3632  2270     5     6  3048 23524     2  1210     2  1958   320
  1477   105     2 19134  7951   364  7317  4990    79 12365 24810    17
    68 16346 27439  6774  1628   305  4133 26719     4  6901   186   864
  2362   320   423    68  1955 16346 27439  6774  1628    27  1147     6
   104     6   292  2635 11850   213  1621  2104     1     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0]
</code></pre>
<p>Things to notice:</p>
<ul>
<li>First we see the corresponding values of the words.</li>
<li>The first 1, which represents the <code>&lt;EOS&gt;</code> tag of the article.</li>
<li>Followed by a 0, which represents a <code>&lt;pad&gt;</code> tag.</li>
<li>After the first 0 (<code>&lt;pad&gt;</code> tag) the corresponding values are of the words that are used for the summary of the article.</li>
<li>The second 1 represents the <code>&lt;EOS&gt;</code> tag for the summary.</li>
<li>All the trailing 0s represent <code>&lt;pad&gt;</code> tags which are appended to maintain consistent length (If you don’t see them then it would mean it is already of max length)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print the article and its summary</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Article:\n\n&#x27;</span>, detokenize(input_batch[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>Article:

 A woman has been charged with reckless manslaughter after her
boyfriend&#39;s mother tried to stop them fighting and suffered a fatal
heart attack. Claudia Yanira Hernandez Soriano, 25, and Juan Francisco
Martinez Rojas, 28, started punching and scratching each other after
they returned to their Bergen, New Jersey home following a party early
on Monday. When Ana Angelina Rojas-Jovel, 45, tried to break them up,
Hernandez Soriano assaulted the woman, according to the Bergen County
Prosecutor. &#39;During the assault, the victim apparently suffered a
cardiac event which resulted in her death,&#39; Prosecutor John L.
Molinelli said in a statement. Fight: Claudia Yanira Hernandez
Soriano, 25, above, and her boyfriend Juan Francisco Martinez Rojas,
28, started punching and scratching each other at their home on Monday
when his mother intervened . Injured: Martinez Rojas&#39; booking shot
shows the scratches on his face from the domestic dispute . A seven-
year-old child also witnessed the fight, according to the prosecutor,
but he did not reveal the relationship between the adults and the
youngster. Police responded to a 911 call from the apartment just
after 4am on Monday and when they arrived, they found Rojas-Jovel dead
on a bedroom floor. &#39;There were no obvious signs of trauma to the
victim, however... the [couple] displayed signs of injury and appeared
to have been involved in a domestic assault,&#39; the prosecutor said. In
their booking photos, both Hernandez Soriano and Martinez Rojas have
scratches on their faces and necks. The pair were interviewed, as were
the child and other residents. Scene: Soriano allegedly then assaulted
the woman, Ana Angelina Rojas-Jovel, and she suffered a cardiac arrest
at the first-floor apartment at the house (pictured) and died before
police arrived at the scene . The Bergen County Medical Examiner&#39;s
Office conducted an autopsy on Rojas-Jovel&#39;s body, but results were
pending toxicology tests, the prosecutor said. Hernandez Soriano was
charged with manslaughter, endangering the welfare of a child,
domestic violence simple assault and hindering apprehension, according
to authorities. Molinelli said Hernandez Soriano also hid evidence -
but would not detail what it was - which investigators later recovered
in a search at the crime scene. She was held at the Bergen County Jail
on $250,000 bail. Martinez Rojas was also charged with child
endangerment and domestic violence simple assault and sent to the
county jail on $75,000 bail. A court hearing has been scheduled for
Thursday morning at Hackensack Superior Court.&lt;EOS&gt;&lt;pad&gt;ClaudiaYanira
Hernandez Soriano, 25, and Juan Francisco Martinez Rojas, 28, started
fighting after returning from a party on Monday morning . When his
mother, Ana Angelina Rojas-Jovel, 45, tried to stop them, Hernandez
Soriano allegedly assaulted her . She suffered cardiac arrest and
police arrived to find her dead . A seven-year-old girl witnessed the
fight .&lt;EOS&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pa
d&gt;&lt;pad&gt;
</code></pre>
<p>You can see that the data has the following structure:</p>
<ul>
<li><span style='color:blue'> [Article] </span> -&gt; <code>&lt;EOS&gt;</code> -&gt; <code>&lt;pad&gt;</code> -&gt; <span style='color:blue'> [Article Summary] </span> -&gt; <code>&lt;EOS&gt;</code> -&gt; (possibly) multiple <code>&lt;pad&gt;</code></li>
</ul>
<p>The loss is taken only on the summary using cross_entropy as loss function. </p>
<p><a name='2'></a></p>
<h1 id="Part-2-Summarization-with-transformer"><a href="#Part-2-Summarization-with-transformer" class="headerlink" title="Part 2: Summarization with transformer"></a>Part 2: Summarization with transformer</h1><p>Now that we have given you the data generator and have handled the preprocessing for you, it is time for you to build your own model. We saved you some time because we know you have already preprocessed data before in this specialization, so we would rather you spend your time doing the next steps. </p>
<p>You will be implementing the attention from scratch and then using it in your transformer model. Concretely, you will understand how attention works, how you use it to connect the encoder and the decoder.</p>
<img src="transformer_decoder_zoomin.png">

<p><a name='2.1'></a></p>
<h2 id="2-1-Dot-product-attention"><a href="#2-1-Dot-product-attention" class="headerlink" title="2.1 Dot product attention"></a>2.1 Dot product attention</h2><p>Now you will implement dot product attention which takes in a query, key, value, and a mask. It returns the output. </p>
<img src ="dotproduct.png">


<p>Here are some helper functions that will help you create tensors and display useful information:</p>
<ul>
<li><code>create_tensor</code>  creates a <code>jax numpy array</code> from a list of lists.</li>
<li><code>display_tensor</code> prints out the shape and the actual tensor.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tensor</span>(<span class="params">t</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Create tensor from list of lists&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> jnp.array(t)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_tensor</span>(<span class="params">t, name</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Display shape and tensor&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span> shape: <span class="subst">&#123;t.shape&#125;</span>\n&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;t&#125;</span>\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>Before implementing it yourself, you can play around with a toy example of <code>dot product attention</code> without the softmax  operation. Technically it would not be <code>dot product attention</code> without the softmax but this is done to avoid giving away too much of the answer and the idea is to display these tensors to give you a sense of how they look like.</p>
<p>The formula for attention is this one:</p>
<p>$$<br>\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}+{M}\right) V\tag{1}<br>$$</p>
<p>$d_{k}$ stands for the dimension of queries and keys.</p>
<p>The <code>query</code>, <code>key</code>, <code>value</code> and <code>mask</code> vectors are provided for this example.</p>
<p>Notice that the masking is done using very negative values that will yield a similar effect to using $-\infty $. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">q = create_tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">display_tensor(q, <span class="string">&#x27;query&#x27;</span>)</span><br><span class="line">k = create_tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">display_tensor(k, <span class="string">&#x27;key&#x27;</span>)</span><br><span class="line">v = create_tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">display_tensor(v, <span class="string">&#x27;value&#x27;</span>)</span><br><span class="line">m = create_tensor([[<span class="number">0</span>, <span class="number">0</span>], [-<span class="number">1e9</span>, <span class="number">0</span>]])</span><br><span class="line">display_tensor(m, <span class="string">&#x27;mask&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>query shape: (2, 3)

[[1 0 0]
 [0 1 0]]

key shape: (2, 3)

[[1 2 3]
 [4 5 6]]

value shape: (2, 3)

[[0 1 0]
 [1 0 1]]

mask shape: (2, 2)

[[ 0.e+00  0.e+00]
 [-1.e+09  0.e+00]]
</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">query shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">key shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line"></span><br><span class="line">value shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">mask shape: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">0.e+00</span>  <span class="number">0.e+00</span>]</span><br><span class="line"> [<span class="number">-1.e+09</span>  <span class="number">0.e+00</span>]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q_dot_k = q @ k.T / jnp.sqrt(<span class="number">3</span>)</span><br><span class="line">display_tensor(q_dot_k, <span class="string">&#x27;query dot key&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>query dot key shape: (2, 2)

[[0.57735026 2.309401  ]
 [1.1547005  2.8867514 ]]
</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">query dot key shape: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">[[<span class="number">0.57735026</span> <span class="number">2.309401</span>  ]</span><br><span class="line"> [<span class="number">1.1547005</span>  <span class="number">2.8867514</span> ]]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">masked = q_dot_k + m</span><br><span class="line">display_tensor(masked, <span class="string">&#x27;masked query dot key&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>masked query dot key shape: (2, 2)

[[ 5.7735026e-01  2.3094010e+00]
 [-1.0000000e+09  2.8867514e+00]]
</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">masked query dot key shape: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">5.7735026e-01</span>  <span class="number">2.3094010e+00</span>]</span><br><span class="line"> [<span class="number">-1.0000000e+09</span>  <span class="number">2.8867514e+00</span>]]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_tensor(masked @ v, <span class="string">&#x27;masked query dot key dot value&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>masked query dot key dot value shape: (2, 3)

[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]
 [ 2.8867514e+00 -1.0000000e+09  2.8867514e+00]]
</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">masked query dot key dot value shape: (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">2.3094010e+00</span>  <span class="number">5.7735026e-01</span>  <span class="number">2.3094010e+00</span>]</span><br><span class="line"> [ <span class="number">2.8867514e+00</span> <span class="number">-1.0000000e+09</span>  <span class="number">2.8867514e+00</span>]]</span><br></pre></td></tr></table></figure>

<p>In order to use the previous dummy tensors to test some of the graded functions, a batch dimension should be added to them so they mimic the shape of real-life examples. The mask is also replaced by a version of it that resembles the one that is used by trax:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">q_with_batch = q[<span class="literal">None</span>,:]</span><br><span class="line">display_tensor(q_with_batch, <span class="string">&#x27;query with batch dim&#x27;</span>)</span><br><span class="line">k_with_batch = k[<span class="literal">None</span>,:]</span><br><span class="line">display_tensor(k_with_batch, <span class="string">&#x27;key with batch dim&#x27;</span>)</span><br><span class="line">v_with_batch = v[<span class="literal">None</span>,:]</span><br><span class="line">display_tensor(v_with_batch, <span class="string">&#x27;value with batch dim&#x27;</span>)</span><br><span class="line">m_bool = create_tensor([[<span class="literal">True</span>, <span class="literal">True</span>], [<span class="literal">False</span>, <span class="literal">True</span>]])</span><br><span class="line">display_tensor(m_bool, <span class="string">&#x27;boolean mask&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>query with batch dim shape: (1, 2, 3)

[[[1 0 0]
  [0 1 0]]]

key with batch dim shape: (1, 2, 3)

[[[1 2 3]
  [4 5 6]]]

value with batch dim shape: (1, 2, 3)

[[[0 1 0]
  [1 0 1]]]

boolean mask shape: (2, 2)

[[ True  True]
 [False  True]]
</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">query with batch dim shape: (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br><span class="line"></span><br><span class="line">key with batch dim shape: (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]]</span><br><span class="line"></span><br><span class="line">value with batch dim shape: (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]]]</span><br><span class="line"></span><br><span class="line">boolean mask shape: (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">[[ True  True]</span><br><span class="line"> [False  True]]</span><br></pre></td></tr></table></figure>

<p><a name='ex01'></a></p>
<h3 id="Exercise-01"><a href="#Exercise-01" class="headerlink" title="Exercise 01"></a>Exercise 01</h3><p><strong>Instructions:</strong> Implement the dot product attention. Concretely, implement the following equation</p>
<p>$$<br>\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}+{M}\right) V\tag{1}<br>$$</p>
<p>$Q$ - query,<br>$K$ - key,<br>$V$ - values,<br>$M$ - mask,<br>${d_k}$ - depth/dimension of the queries and keys (used for scaling down)</p>
<p>You can implement this formula either by <code>trax</code> numpy (trax.math.numpy) or regular <code>numpy</code> but it is recommended to use <code>jnp</code>.</p>
<p>Something to take into consideration is that within trax, the masks are tensors of <code>True/False</code> values not 0’s and $-\infty$ as in the previous example. Within the graded function don’t think of applying the mask by summing up matrices, instead use <code>jnp.where()</code> and treat the <strong>mask as a tensor of boolean values with <code>False</code> for values that need to be masked and True for the ones that don’t.</strong></p>
<p>Also take into account that the real tensors are far more complex than the toy ones you just played with. Because of this avoid using shortened operations such as <code>@</code> for dot product or <code>.T</code> for transposing. Use <code>jnp.matmul()</code> and <code>jnp.swapaxes()</code> instead.</p>
<p>This is the self-attention block for the transformer decoder. Good luck!  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C1</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: DotProductAttention</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DotProductAttention</span>(<span class="params">query, key, value, mask</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Dot product self-attention.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)</span></span><br><span class="line"><span class="string">        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)</span></span><br><span class="line"><span class="string">        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k</span></span><br><span class="line"><span class="string">        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> query.shape[-<span class="number">1</span>] == key.shape[-<span class="number">1</span>] == value.shape[-<span class="number">1</span>], <span class="string">&quot;Embedding dimensions of q, k, v aren&#x27;t all the same&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    <span class="comment"># Save depth/dimension of the query embedding for scaling down the dot product</span></span><br><span class="line">    depth = query.shape[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate scaled query key dot product according to formula above</span></span><br><span class="line">    dots = jnp.matmul(query, jnp.swapaxes(key, <span class="number">1</span>, <span class="number">2</span>)) / jnp.sqrt(depth)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Apply the mask</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># The &#x27;None&#x27; in this line does not need to be replaced</span></span><br><span class="line">        dots = jnp.where(mask, dots, jnp.full_like(dots, -<span class="number">1e9</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Softmax formula implementation</span></span><br><span class="line">    <span class="comment"># Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers</span></span><br><span class="line">    <span class="comment"># Hint: Last axis should be used and keepdims should be True</span></span><br><span class="line">    <span class="comment"># Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)</span></span><br><span class="line">    logsumexp = trax.fastmath.logsumexp(dots,axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Take exponential of dots minus logsumexp to get softmax</span></span><br><span class="line">    <span class="comment"># Use jnp.exp()</span></span><br><span class="line">    dots = jnp.exp( dots - logsumexp)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Multiply dots by value to get self-attention</span></span><br><span class="line">    <span class="comment"># Use jnp.matmul()</span></span><br><span class="line">    attention = jnp.matmul(dots, value)</span><br><span class="line"></span><br><span class="line">    <span class="comment">## END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> attention</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DotProductAttention(q_with_batch, k_with_batch, v_with_batch, m_bool)</span><br></pre></td></tr></table></figure>




<pre><code>DeviceArray([[[0.8496746 , 0.15032545, 0.8496746 ],
              [1.        , 0.        , 1.        ]]], dtype=float32)
</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">DeviceArray</span>([[[<span class="number">0.8496746</span> , <span class="number">0.15032545</span>, <span class="number">0.8496746</span> ],</span><br><span class="line">              [<span class="number">1.</span>        , <span class="number">0.</span>        , <span class="number">1.</span>        ]]], dtype=float32)</span><br><span class="line">```    </span><br><span class="line"></span><br><span class="line">&lt;a name=<span class="string">&#x27;2.2&#x27;</span>&gt;&lt;/a&gt;</span><br><span class="line"></span><br><span class="line">## <span class="number">2.2</span> Causal Attention</span><br><span class="line"></span><br><span class="line">Now you are going to implement causal attention: multi-headed attention with a mask to attend only to words that occurred before. </span><br><span class="line"></span><br><span class="line">&lt;img src = <span class="string">&quot;causal.png&quot;</span>&gt;</span><br><span class="line"></span><br><span class="line">In the image above, a word can see everything that is before it, but <span class="keyword">not</span> what is after it. To implement causal attention, you will have to transform vectors <span class="keyword">and</span> <span class="keyword">do</span> many reshapes. You will need to implement the functions below.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;a name=<span class="string">&#x27;ex02&#x27;</span>&gt;&lt;/a&gt;</span><br><span class="line">### Exercise <span class="number">02</span></span><br><span class="line"></span><br><span class="line">Implement the following functions that will be needed <span class="keyword">for</span> Causal Attention:</span><br><span class="line"></span><br><span class="line">- &lt;span style=<span class="string">&#x27;color:blue&#x27;</span>&gt; compute_attention_heads &lt;/span&gt;: Gets an input $x$ of <span class="built_in">dimension</span> (batch_size, seqlen, n_heads $\times$ d_head) <span class="keyword">and</span> splits the <span class="built_in">last</span> (depth) dimension <span class="keyword">and</span> stacks it to the zeroth dimension to allow matrix <span class="built_in">multiplication</span> (batch_size $\times$ n_heads, seqlen, d_head).</span><br><span class="line">- &lt;span style=<span class="string">&#x27;color:blue&#x27;</span>&gt; dot_product_self_attention &lt;/span&gt;: Creates a mask matrix with `False` values above the diagonal <span class="keyword">and</span> `True` values below <span class="keyword">and</span> calls DotProductAttention which implements dot product self attention.</span><br><span class="line">- &lt;span style=<span class="string">&#x27;color:blue&#x27;</span>&gt; compute_attention_output &lt;/span&gt;: Undoes compute_attention_heads by splitting <span class="built_in">first</span> (vertical) dimension <span class="keyword">and</span> stacking in the <span class="built_in">last</span> (depth) <span class="built_in">dimension</span> (batch_size, seqlen, n_heads $\times$ d_head). These operations <span class="built_in">concatenate</span> (stack/merge) the heads. </span><br><span class="line"></span><br><span class="line">Next there are some toy tensors which may serve to give you an idea of the data shapes <span class="keyword">and</span> opperations involved in Causal Attention. They are also useful to test out your functions! </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">tensor2d = <span class="built_in">create_tensor</span>(q)</span><br><span class="line"><span class="built_in">display_tensor</span>(tensor2d, <span class="string">&#x27;query matrix (2D tensor)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">tensor4d2b = <span class="built_in">create_tensor</span>([[q, q], [q, q]])</span><br><span class="line"><span class="built_in">display_tensor</span>(tensor4d2b, <span class="string">&#x27;batch of two (multi-head) collections of query matrices (4D tensor)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">tensor3dc = <span class="built_in">create_tensor</span>([jnp.<span class="built_in">concatenate</span>([q, q], axis = <span class="number">-1</span>)])</span><br><span class="line"><span class="built_in">display_tensor</span>(tensor3dc, <span class="string">&#x27;one batch of concatenated heads of query matrices (3d tensor)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">tensor3dc3b = <span class="built_in">create_tensor</span>([jnp.<span class="built_in">concatenate</span>([q, q], axis = <span class="number">-1</span>), jnp.<span class="built_in">concatenate</span>([q, q], axis = <span class="number">-1</span>), jnp.<span class="built_in">concatenate</span>([q, q], axis = <span class="number">-1</span>)])</span><br><span class="line"><span class="built_in">display_tensor</span>(tensor3dc3b, <span class="string">&#x27;three batches of concatenated heads of query matrices (3d tensor)&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>query matrix (2D tensor) shape: (2, 3)

[[1 0 0]
 [0 1 0]]

batch of two (multi-head) collections of query matrices (4D tensor) shape: (2, 2, 2, 3)

[[[[1 0 0]
   [0 1 0]]

  [[1 0 0]
   [0 1 0]]]


 [[[1 0 0]
   [0 1 0]]

  [[1 0 0]
   [0 1 0]]]]

one batch of concatenated heads of query matrices (3d tensor) shape: (1, 2, 6)

[[[1 0 0 1 0 0]
  [0 1 0 0 1 0]]]

three batches of concatenated heads of query matrices (3d tensor) shape: (3, 2, 6)

[[[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]]
</code></pre>
<p>It is important to know that the following 3 functions would normally be defined within the <code>CausalAttention</code> function further below. </p>
<p>However this makes these functions harder to test. Because of this, these functions are shown individually using a <code>closure</code> (when necessary) that simulates them being inside of the <code>CausalAttention</code> function. This is done because they rely on some variables that can be accessed from within <code>CausalAttention</code>.</p>
<h3 id="Support-Functions"><a href="#Support-Functions" class="headerlink" title="Support Functions"></a>Support Functions</h3><p><span style='color:blue'> compute_attention_heads </span>: Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\times$ n_heads, seqlen, d_head).</p>
<p><strong>For the closures you only have to fill the inner function.</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C2</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: compute_attention_heads_closure</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_attention_heads_closure</span>(<span class="params">n_heads, d_head</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Function that simulates environment inside CausalAttention function.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_head (int):  dimensionality of heads.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        function: compute_attention_heads function</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_attention_heads</span>(<span class="params">x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Compute the attention heads.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Size of the x&#x27;s batch dimension</span></span><br><span class="line">        batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># Length of the sequence</span></span><br><span class="line">        <span class="comment"># Should be size of x&#x27;s first dimension without counting the batch dim</span></span><br><span class="line">        seqlen = x.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># Reshape x using jnp.reshape()</span></span><br><span class="line">        <span class="comment"># batch_size, seqlen, n_heads*d_head -&gt; batch_size, seqlen, n_heads, d_head</span></span><br><span class="line">        x = jnp.reshape(x, (batch_size, seqlen,n_heads, d_head))</span><br><span class="line">        <span class="comment"># Transpose x using jnp.transpose()</span></span><br><span class="line">        <span class="comment"># batch_size, seqlen, n_heads, d_head -&gt; batch_size, n_heads, seqlen, d_head</span></span><br><span class="line">        <span class="comment"># Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them</span></span><br><span class="line">        x = jnp.transpose(x, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">        <span class="comment"># Reshape x using jnp.reshape()</span></span><br><span class="line">        <span class="comment"># batch_size, n_heads, seqlen, d_head -&gt; batch_size*n_heads, seqlen, d_head</span></span><br><span class="line">        x = jnp.reshape(x, (batch_size*n_heads, seqlen, d_head))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> compute_attention_heads</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">display_tensor(tensor3dc3b, <span class="string">&quot;input tensor&quot;</span>)</span><br><span class="line">result_cah = compute_attention_heads_closure(<span class="number">2</span>,<span class="number">3</span>)(tensor3dc3b)</span><br><span class="line">display_tensor(result_cah, <span class="string">&quot;output tensor&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>input tensor shape: (3, 2, 6)

[[[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]]

output tensor shape: (6, 2, 3)

[[[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]]
</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">input tensor shape: (<span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br><span class="line"></span><br><span class="line">output tensor shape: (<span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br></pre></td></tr></table></figure>

<p><span style='color:blue'> dot_product_self_attention </span>: Creates a mask matrix with <code>False</code> values above the diagonal and <code>True</code> values below and calls DotProductAttention which implements dot product self attention.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C3</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: dot_product_self_attention</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dot_product_self_attention</span>(<span class="params">q, k, v</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Masked dot product self attention.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        q (jax.interpreters.xla.DeviceArray): queries.</span></span><br><span class="line"><span class="string">        k (jax.interpreters.xla.DeviceArray): keys.</span></span><br><span class="line"><span class="string">        v (jax.interpreters.xla.DeviceArray): values.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Hint: mask size should be equal to L_q. Remember that q has shape (batch_size, L_q, d)</span></span><br><span class="line">    mask_size = q.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)</span></span><br><span class="line">    <span class="comment"># Notice that 1&#x27;s and 0&#x27;s get casted to True/False by setting dtype to jnp.bool_</span></span><br><span class="line">    <span class="comment"># Use jnp.tril() - Lower triangle of an array and jnp.ones()</span></span><br><span class="line">    mask = jnp.tril(jnp.ones((<span class="number">1</span>, mask_size, mask_size), dtype=jnp.bool_), k=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> DotProductAttention(q, k, v, mask)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dot_product_self_attention(q_with_batch, k_with_batch, v_with_batch)</span><br></pre></td></tr></table></figure>




<pre><code>DeviceArray([[[0.        , 1.        , 0.        ],
              [0.8496746 , 0.15032543, 0.8496746 ]]], dtype=float32)
</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">DeviceArray</span>([[[<span class="number">0.</span>        , <span class="number">1.</span>        , <span class="number">0.</span>        ],</span><br><span class="line">              [<span class="number">0.8496746</span> , <span class="number">0.15032543</span>, <span class="number">0.8496746</span> ]]], dtype=float32)</span><br></pre></td></tr></table></figure>

<p><span style='color:blue'> compute_attention_output </span>: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads $\times$ d_head). These operations concatenate (stack/merge) the heads. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C4</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: compute_attention_output_closure</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_attention_output_closure</span>(<span class="params">n_heads, d_head</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Function that simulates environment inside CausalAttention function.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_head (int):  dimensionality of heads.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        function: compute_attention_output function</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_attention_output</span>(<span class="params">x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; Compute the attention output.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Length of the sequence</span></span><br><span class="line">        <span class="comment"># Should be size of x&#x27;s first dimension without counting the batch dim</span></span><br><span class="line">        seqlen = x.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># Reshape x using jnp.reshape() to shape (batch_size, n_heads, seqlen, d_head)</span></span><br><span class="line">        x = jnp.reshape(x,(-<span class="number">1</span>, n_heads, seqlen, d_head))</span><br><span class="line">        <span class="comment"># Transpose x using jnp.transpose() to shape (batch_size, seqlen, n_heads, d_head)</span></span><br><span class="line">        x = jnp.transpose(x, (<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>)) </span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Reshape to allow to concatenate the heads</span></span><br><span class="line">        <span class="keyword">return</span> jnp.reshape(x, (-<span class="number">1</span>, seqlen, n_heads * d_head))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> compute_attention_output</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">display_tensor(result_cah, <span class="string">&quot;input tensor&quot;</span>)</span><br><span class="line">result_cao = compute_attention_output_closure(<span class="number">2</span>,<span class="number">3</span>)(result_cah)</span><br><span class="line">display_tensor(result_cao, <span class="string">&quot;output tensor&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>input tensor shape: (6, 2, 3)

[[[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]

 [[1 0 0]
  [0 1 0]]]

output tensor shape: (3, 2, 6)

[[[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]]
</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">input tensor shape: (<span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br><span class="line"></span><br><span class="line">output tensor shape: (<span class="number">3</span>, <span class="number">2</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">[[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br></pre></td></tr></table></figure>

<h3 id="Causal-Attention-Function"><a href="#Causal-Attention-Function" class="headerlink" title="Causal Attention Function"></a>Causal Attention Function</h3><p>Now it is time for you to put everything together within the <code>CausalAttention</code> or Masked multi-head attention function:</p>
<img src = "masked-attention.png"> 

<p><strong>Instructions:</strong> Implement the causal attention.<br>Your model returns the causal attention through a $tl.Serial$ with the following:</p>
<ul>
<li><span style='color:blue'> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch">tl.Branch</a> </span>: consisting of 3 [tl.Dense(d_feature), ComputeAttentionHeads] to account for the queries, keys, and values.</li>
<li><span style='color:blue'> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn">tl.Fn</a></span>: Takes in dot_product_self_attention function and uses it to compute the dot product using $Q$, $K$, $V$.</li>
<li><span style='color:blue'> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn">tl.Fn</a></span>: Takes in compute_attention_output_closure to allow for parallel computing.</li>
<li><span style='color:blue'> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">tl.Dense</a></span>: Final Dense layer, with dimension <code>d_feature</code>.</li>
</ul>
<p>Remember that in order for trax to properly handle the functions you just defined, they need to be added as layers using the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn"><code>tl.Fn()</code></a> function. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C5</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: CausalAttention</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CausalAttention</span>(<span class="params">d_feature, </span></span></span><br><span class="line"><span class="params"><span class="function">                    n_heads, </span></span></span><br><span class="line"><span class="params"><span class="function">                    compute_attention_heads_closure=compute_attention_heads_closure,</span></span></span><br><span class="line"><span class="params"><span class="function">                    dot_product_self_attention=dot_product_self_attention,</span></span></span><br><span class="line"><span class="params"><span class="function">                    compute_attention_output_closure=compute_attention_output_closure,</span></span></span><br><span class="line"><span class="params"><span class="function">                    mode=<span class="string">&#x27;train&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer-style multi-headed causal attention.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_feature (int):  dimensionality of feature embedding.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">        compute_attention_heads_closure (function): Closure around compute_attention heads.</span></span><br><span class="line"><span class="string">        dot_product_self_attention (function): dot_product_self_attention function. </span></span><br><span class="line"><span class="string">        compute_attention_output_closure (function): Closure around compute_attention_output. </span></span><br><span class="line"><span class="string">        mode (str): &#x27;train&#x27; or &#x27;eval&#x27;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.layers.combinators.Serial: Multi-headed self-attention model.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> d_feature % n_heads == <span class="number">0</span></span><br><span class="line">    d_head = d_feature // n_heads</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)</span></span><br><span class="line">    <span class="comment"># Since you are dealing with closures you might need to call the outer </span></span><br><span class="line">    <span class="comment"># function with the correct parameters to get the actual uncalled function.</span></span><br><span class="line">    ComputeAttentionHeads = tl.Fn(<span class="string">&#x27;AttnHeads&#x27;</span>, compute_attention_heads_closure(n_heads, d_head), n_out=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tl.Serial(</span><br><span class="line">        tl.Branch( <span class="comment"># creates three towers for one input, takes activations and creates queries keys and values</span></span><br><span class="line">            [tl.Dense(d_feature), ComputeAttentionHeads], <span class="comment"># queries</span></span><br><span class="line">            [tl.Dense(d_feature), ComputeAttentionHeads], <span class="comment"># keys</span></span><br><span class="line">            [tl.Dense(d_feature), ComputeAttentionHeads], <span class="comment"># values</span></span><br><span class="line">        ),</span><br><span class="line">        </span><br><span class="line">        tl.Fn(<span class="string">&#x27;DotProductAttn&#x27;</span>, dot_product_self_attention, n_out=<span class="number">1</span>), <span class="comment"># takes QKV</span></span><br><span class="line">        <span class="comment"># HINT: The second argument to tl.Fn() is an uncalled function</span></span><br><span class="line">        <span class="comment"># Since you are dealing with closures you might need to call the outer </span></span><br><span class="line">        <span class="comment"># function with the correct parameters to get the actual uncalled function.</span></span><br><span class="line">        tl.Fn(<span class="string">&#x27;AttnOutput&#x27;</span>, compute_attention_output_closure(n_heads, d_head), n_out=<span class="number">1</span>), <span class="comment"># to allow for parallel</span></span><br><span class="line">        tl.Dense(d_feature) <span class="comment"># Final dense layer</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Take a look at the causal attention model</span></span><br><span class="line"><span class="built_in">print</span>(CausalAttention(d_feature=<span class="number">512</span>, n_heads=<span class="number">8</span>))</span><br></pre></td></tr></table></figure>

<pre><code>Serial[
  Branch_out3[
    [Dense_512, AttnHeads]
    [Dense_512, AttnHeads]
    [Dense_512, AttnHeads]
  ]
  DotProductAttn_in3
  AttnOutput
  Dense_512
]
</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Serial[</span><br><span class="line">  Branch_out3[</span><br><span class="line">    [Dense_512, AttnHeads]</span><br><span class="line">    [Dense_512, AttnHeads]</span><br><span class="line">    [Dense_512, AttnHeads]</span><br><span class="line">  ]</span><br><span class="line">  DotProductAttn_in3</span><br><span class="line">  AttnOutput</span><br><span class="line">  Dense_512</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p><a name='2.3'></a></p>
<h2 id="2-3-Transformer-decoder-block"><a href="#2-3-Transformer-decoder-block" class="headerlink" title="2.3 Transformer decoder block"></a>2.3 Transformer decoder block</h2><p>Now that you have implemented the causal part of the transformer, you will implement the transformer decoder block. Concretely you will be implementing this image now.</p>
<img src = "transformer_decoder_1.png" style = "height:300px"> 

<p>To implement this function, you will have to call the <code>CausalAttention</code> or Masked multi-head attention function you implemented above. You will have to add a feedforward which consists of: </p>
<ul>
<li><span style='color:blue'> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm">tl.LayerNorm</a> </span>: used to layer normalize</li>
<li><span style='color:blue'> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">tl.Dense</a> </span>: the dense layer</li>
<li><span style='color:blue'> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu">ff_activation</a> </span>: feed forward activation (we use ReLu) here.</li>
<li><span style='color:blue'> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout">tl.Dropout</a> </span>: dropout layer</li>
<li><span style='color:blue'> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">tl.Dense</a> </span>: dense layer</li>
<li><span style='color:blue'> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout">tl.Dropout</a> </span>: dropout layer</li>
</ul>
<p>Finally once you implement the feedforward, you can go ahead and implement the entire block using: </p>
<ul>
<li><p><span style='color:blue'> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual">tl.Residual</a> </span>: takes in the tl.LayerNorm(), causal attention block, tl.dropout. </p>
</li>
<li><p><span style='color:blue'> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual">tl.Residual</a> </span>: takes in the feedforward block you will implement. </p>
</li>
</ul>
<p><a name='ex03'></a></p>
<h3 id="Exercise-03"><a href="#Exercise-03" class="headerlink" title="Exercise 03"></a>Exercise 03</h3><p><strong>Instructions:</strong> Implement the transformer decoder block. Good luck!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C6</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: DecoderBlock</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DecoderBlock</span>(<span class="params">d_model, d_ff, n_heads,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout, mode, ff_activation</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns a list of layers that implements a Transformer decoder block.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input is an activation tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model (int):  depth of embedding.</span></span><br><span class="line"><span class="string">        d_ff (int): depth of feed-forward layer.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate (how much to drop out).</span></span><br><span class="line"><span class="string">        mode (str): &#x27;train&#x27; or &#x27;eval&#x27;.</span></span><br><span class="line"><span class="string">        ff_activation (function): the non-linearity in feed-forward layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create masked multi-head attention block using CausalAttention function</span></span><br><span class="line">    causal_attention = CausalAttention( </span><br><span class="line">                        d_model,</span><br><span class="line">                        n_heads=n_heads,</span><br><span class="line">                        mode=mode</span><br><span class="line">                        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create feed-forward block (list) with two dense layers with dropout and input normalized</span></span><br><span class="line">    feed_forward = [ </span><br><span class="line">        <span class="comment"># Normalize layer inputs</span></span><br><span class="line">        tl.LayerNorm(),</span><br><span class="line">        <span class="comment"># Add first feed forward (dense) layer (don&#x27;t forget to set the correct value for n_units)</span></span><br><span class="line">        tl.Dense(d_ff),</span><br><span class="line">        <span class="comment"># Add activation function passed in as a parameter (you need to call it!)</span></span><br><span class="line">        ff_activation(), <span class="comment"># Generally ReLU</span></span><br><span class="line">        <span class="comment"># Add dropout with rate and mode specified (i.e., don&#x27;t use dropout during evaluation)</span></span><br><span class="line">        tl.Dropout(rate=dropout, mode=mode),</span><br><span class="line">        <span class="comment"># Add second feed forward layer (don&#x27;t forget to set the correct value for n_units)</span></span><br><span class="line">        tl.Dense(d_model),</span><br><span class="line">        <span class="comment"># Add dropout with rate and mode specified (i.e., don&#x27;t use dropout during evaluation)</span></span><br><span class="line">        tl.Dropout(rate=dropout, mode=mode)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks</span></span><br><span class="line">    <span class="keyword">return</span> [</span><br><span class="line">      tl.Residual(</span><br><span class="line">          <span class="comment"># Normalize layer input</span></span><br><span class="line">          tl.LayerNorm(),</span><br><span class="line">          <span class="comment"># Add causal attention block previously defined (without parentheses)</span></span><br><span class="line">          causal_attention,</span><br><span class="line">          <span class="comment"># Add dropout with rate and mode specified</span></span><br><span class="line">          tl.Dropout()</span><br><span class="line">        ),</span><br><span class="line">      tl.Residual(</span><br><span class="line">          <span class="comment"># Add feed forward block (without parentheses)</span></span><br><span class="line">          feed_forward</span><br><span class="line">        ),</span><br><span class="line">      ]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Take a look at the decoder block</span></span><br><span class="line"><span class="built_in">print</span>(DecoderBlock(d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, n_heads=<span class="number">8</span>, dropout=<span class="number">0.1</span>, mode=<span class="string">&#x27;train&#x27;</span>, ff_activation=tl.Relu))</span><br></pre></td></tr></table></figure>

<pre><code>[Serial[
  Branch_out2[
    None
    Serial[
      LayerNorm
      Serial[
        Branch_out3[
          [Dense_512, AttnHeads]
          [Dense_512, AttnHeads]
          [Dense_512, AttnHeads]
        ]
        DotProductAttn_in3
        AttnOutput
        Dense_512
      ]
      Dropout
    ]
  ]
  Add_in2
], Serial[
  Branch_out2[
    None
    Serial[
      LayerNorm
      Dense_2048
      Relu
      Dropout
      Dense_512
      Dropout
    ]
  ]
  Add_in2
]]
</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[Serial[</span><br><span class="line">  Branch_out2[</span><br><span class="line">    None</span><br><span class="line">    Serial[</span><br><span class="line">      LayerNorm</span><br><span class="line">      Serial[</span><br><span class="line">        Branch_out3[</span><br><span class="line">          [Dense_512, AttnHeads]</span><br><span class="line">          [Dense_512, AttnHeads]</span><br><span class="line">          [Dense_512, AttnHeads]</span><br><span class="line">        ]</span><br><span class="line">        DotProductAttn_in3</span><br><span class="line">        AttnOutput</span><br><span class="line">        Dense_512</span><br><span class="line">      ]</span><br><span class="line">      Dropout</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">  Add_in2</span><br><span class="line">], Serial[</span><br><span class="line">  Branch_out2[</span><br><span class="line">    None</span><br><span class="line">    Serial[</span><br><span class="line">      LayerNorm</span><br><span class="line">      Dense_2048</span><br><span class="line">      Relu</span><br><span class="line">      Dropout</span><br><span class="line">      Dense_512</span><br><span class="line">      Dropout</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">  Add_in2</span><br><span class="line">]]</span><br></pre></td></tr></table></figure>

<p><a name='2.4'></a></p>
<h2 id="2-4-Transformer-Language-Model"><a href="#2-4-Transformer-Language-Model" class="headerlink" title="2.4 Transformer Language Model"></a>2.4 Transformer Language Model</h2><p>You will now bring it all together. In this part you will use all the subcomponents you previously built to make the final model. Concretely, here is the image you will be implementing.<br><img src = "transformer_decoder.png" style = "height:400px"></p>
<p><a name='ex04'></a></p>
<h3 id="Exercise-04"><a href="#Exercise-04" class="headerlink" title="Exercise 04"></a>Exercise 04</h3><p><strong>Instructions:</strong> Previously you coded the decoder block. Now you will code the transformer language model. Here is what you will need. </p>
<ul>
<li><p><span style="color:blue"> positional_enconder </span>- a list containing the following layers:</p>
<ul>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding">tl.Embedding</a></li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout">tl.Dropout</a></li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.PositionalEncoding">tl.PositionalEncoding</a></li>
</ul>
</li>
<li><p>A list of <code>n_layers</code> <span style="color:blue"> decoder blocks</span>.</p>
</li>
<li><p><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial">tl.Serial</a>: </span> takes in the following layers or lists of layers:</p>
<ul>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight">tl.ShiftRight</a>: </span>: shift the tensor to the right by padding on axis 1.</li>
<li><span style="color:blue"> positional_encoder </span>: encodes the text positions.</li>
<li><span style="color:blue"> decoder_blocks </span>: the ones you created.</li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm">tl.LayerNorm</a> </span>: a layer norm.</li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">tl.Dense</a> </span>: takes in the vocab_size.</li>
<li><span style="color:blue"> <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax">tl.LogSoftmax</a> </span>: to predict.</li>
</ul>
</li>
</ul>
<p>Go go go!! You can do it :)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C7</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: TransformerLM</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TransformerLM</span>(<span class="params">vocab_size=<span class="number">33300</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                  d_model=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                  d_ff=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                  n_layers=<span class="number">6</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                  n_heads=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                  dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                  max_len=<span class="number">4096</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                  mode=<span class="string">&#x27;train&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                  ff_activation=tl.Relu</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns a Transformer language model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input to the model is a tensor of tokens. (This model uses only the</span></span><br><span class="line"><span class="string">    decoder part of the overall Transformer.)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab_size (int): vocab size.</span></span><br><span class="line"><span class="string">        d_model (int):  depth of embedding.</span></span><br><span class="line"><span class="string">        d_ff (int): depth of feed-forward layer.</span></span><br><span class="line"><span class="string">        n_layers (int): number of decoder layers.</span></span><br><span class="line"><span class="string">        n_heads (int): number of attention heads.</span></span><br><span class="line"><span class="string">        dropout (float): dropout rate (how much to drop out).</span></span><br><span class="line"><span class="string">        max_len (int): maximum symbol length for positional encoding.</span></span><br><span class="line"><span class="string">        mode (str): &#x27;train&#x27;, &#x27;eval&#x27; or &#x27;predict&#x27;, predict mode is for fast inference.</span></span><br><span class="line"><span class="string">        ff_activation (function): the non-linearity in feed-forward layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens</span></span><br><span class="line"><span class="string">        to activations over a vocab set.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Embedding inputs and positional encoder</span></span><br><span class="line">    positional_encoder = [ </span><br><span class="line">        <span class="comment"># Add embedding layer of dimension (vocab_size, d_model)</span></span><br><span class="line">        tl.Embedding(vocab_size,d_model),</span><br><span class="line">        <span class="comment"># Use dropout with rate and mode specified</span></span><br><span class="line">        tl.Dropout(rate = dropout, mode = mode),</span><br><span class="line">        <span class="comment"># Add positional encoding layer with maximum input length and mode specified</span></span><br><span class="line">        tl.PositionalEncoding()]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create stack (list) of decoder blocks with n_layers with necessary parameters</span></span><br><span class="line">    decoder_blocks = [ </span><br><span class="line">        DecoderBlock(d_model, d_ff, n_heads,</span><br><span class="line">                 dropout, mode, ff_activation) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create the complete model as written in the figure</span></span><br><span class="line">    <span class="keyword">return</span> tl.Serial(</span><br><span class="line">        <span class="comment"># Use teacher forcing (feed output of previous step to current step)</span></span><br><span class="line">        tl.ShiftRight(), <span class="comment"># Specify the mode!</span></span><br><span class="line">        <span class="comment"># Add positional encoder</span></span><br><span class="line">        positional_encoder,</span><br><span class="line">        <span class="comment"># Add decoder blocks</span></span><br><span class="line">        decoder_blocks,</span><br><span class="line">        <span class="comment"># Normalize layer</span></span><br><span class="line">        tl.LayerNorm(),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add dense layer of vocab_size (since need to select a word to translate to)</span></span><br><span class="line">        <span class="comment"># (a.k.a., logits layer. Note: activation already set by ff_activation)</span></span><br><span class="line">        tl.Dense(vocab_size),</span><br><span class="line">        <span class="comment"># Get probabilities with Logsoftmax</span></span><br><span class="line">        tl.LogSoftmax()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Take a look at the Transformer</span></span><br><span class="line"><span class="built_in">print</span>(TransformerLM(n_layers=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<pre><code>Serial[
  ShiftRight(1)
  Embedding_33300_512
  Dropout
  PositionalEncoding
  Serial[
    Branch_out2[
      None
      Serial[
        LayerNorm
        Serial[
          Branch_out3[
            [Dense_512, AttnHeads]
            [Dense_512, AttnHeads]
            [Dense_512, AttnHeads]
          ]
          DotProductAttn_in3
          AttnOutput
          Dense_512
        ]
        Dropout
      ]
    ]
    Add_in2
  ]
  Serial[
    Branch_out2[
      None
      Serial[
        LayerNorm
        Dense_2048
        Relu
        Dropout
        Dense_512
        Dropout
      ]
    ]
    Add_in2
  ]
  LayerNorm
  Dense_33300
  LogSoftmax
]
</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">Serial[</span><br><span class="line">  <span class="built_in">ShiftRight</span>(<span class="number">1</span>)</span><br><span class="line">  Embedding_33300_512</span><br><span class="line">  Dropout</span><br><span class="line">  PositionalEncoding</span><br><span class="line">  Serial[</span><br><span class="line">    Branch_out2[</span><br><span class="line">      None</span><br><span class="line">      Serial[</span><br><span class="line">        LayerNorm</span><br><span class="line">        Serial[</span><br><span class="line">          Branch_out3[</span><br><span class="line">            [Dense_512, AttnHeads]</span><br><span class="line">            [Dense_512, AttnHeads]</span><br><span class="line">            [Dense_512, AttnHeads]</span><br><span class="line">          ]</span><br><span class="line">          DotProductAttn_in3</span><br><span class="line">          AttnOutput</span><br><span class="line">          Dense_512</span><br><span class="line">        ]</span><br><span class="line">        Dropout</span><br><span class="line">      ]</span><br><span class="line">    ]</span><br><span class="line">    Add_in2</span><br><span class="line">  ]</span><br><span class="line">  Serial[</span><br><span class="line">    Branch_out2[</span><br><span class="line">      None</span><br><span class="line">      Serial[</span><br><span class="line">        LayerNorm</span><br><span class="line">        Dense_2048</span><br><span class="line">        Relu</span><br><span class="line">        Dropout</span><br><span class="line">        Dense_512</span><br><span class="line">        Dropout</span><br><span class="line">      ]</span><br><span class="line">    ]</span><br><span class="line">    Add_in2</span><br><span class="line">  ]</span><br><span class="line">  LayerNorm</span><br><span class="line">  Dense_33300</span><br><span class="line">  LogSoftmax</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p><a name='3'></a></p>
<h1 id="Part-3-Training"><a href="#Part-3-Training" class="headerlink" title="Part 3: Training"></a>Part 3: Training</h1><p>Now you are going to train your model. As usual, you have to define the cost function, the optimizer, and decide whether you will be training it on a <code>gpu</code> or <code>cpu</code>. In this case, you will train your model on a cpu for a few steps and we will load in a pre-trained model that you can use to predict with your own words.</p>
<p><a name='3.1'></a></p>
<h3 id="3-1-Training-the-model"><a href="#3-1-Training-the-model" class="headerlink" title="3.1 Training the model"></a>3.1 Training the model</h3><p>You will now write a function that takes in your model and trains it. To train your model you have to decide how many times you want to iterate over the entire data set. Each iteration is defined as an <code>epoch</code>. For each epoch, you have to go over all the data, using your training iterator.</p>
<p><a name='ex05'></a></p>
<h3 id="Exercise-05"><a href="#Exercise-05" class="headerlink" title="Exercise 05"></a>Exercise 05</h3><p><strong>Instructions:</strong> Implement the <code>train_model</code> program below to train the neural network above. Here is a list of things you should do:</p>
<ul>
<li>Create the train task by calling <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask"><code>trax.supervised.training.TrainTask</code></a> and pass in the following: <ul>
<li><span style='color:blue'> labeled_data </span> = train_gen</li>
<li><span style='color:blue'> loss_fn </span> = <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss">tl.CrossEntropyLoss()</a></li>
<li><span style='color:blue'> optimizer </span> = <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam">trax.optimizers.Adam(0.01)</a></li>
<li><span style='color:blue'> lr_schedule </span> = <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.lr_schedules.warmup_and_rsqrt_decay">lr_schedule</a></li>
</ul>
</li>
</ul>
<ul>
<li>Create the eval task by calling <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask"><code>trax.supervised.training.EvalTask</code></a> and pass in the following: <ul>
<li><span style='color:blue'> labeled_data </span> = eval_gen</li>
<li><span style='color:blue'> metrics </span> = tl.CrossEntropyLoss() and <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy">tl.Accuracy()</a></li>
</ul>
</li>
<li>Create the training loop by calling <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop"><code>trax.supervised.Training.Loop</code></a> and pass in the following: <ul>
<li><span style='color:blue'> TransformerLM </span> </li>
<li><span style='color:blue'> train_task </span> </li>
<li><span style='color:blue'> eval_task </span> = [eval_task]</li>
<li><span style='color:blue'> output_dir</span> = output_dir</li>
</ul>
</li>
</ul>
<p>You will be using a cross entropy loss, with Adam optimizer. Please read the <a target="_blank" rel="noopener" href="https://trax-ml.readthedocs.io/en/latest/index.html">Trax</a> documentation to get a full understanding. </p>
<p>The training loop that this function returns can be runned using the <code>run()</code> method by passing in the desired number of steps.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trax.supervised <span class="keyword">import</span> training</span><br><span class="line"></span><br><span class="line"><span class="comment"># UNQ_C8</span></span><br><span class="line"><span class="comment"># GRADED FUNCTION: train_model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training_loop</span>(<span class="params">TransformerLM, train_gen, eval_gen, output_dir = <span class="string">&quot;~/model&quot;</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        TransformerLM (trax.layers.combinators.Serial): The model you are building.</span></span><br><span class="line"><span class="string">        train_gen (generator): Training stream of data.</span></span><br><span class="line"><span class="string">        eval_gen (generator): Evaluation stream of data.</span></span><br><span class="line"><span class="string">        output_dir (str): folder to save your file.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        trax.supervised.training.Loop: Training loop.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    output_dir = os.path.expanduser(output_dir)  <span class="comment"># trainer is an object</span></span><br><span class="line">    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=<span class="number">1000</span>, max_value=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    train_task = training.TrainTask( </span><br><span class="line">      labeled_data=train_gen, <span class="comment"># The training generator</span></span><br><span class="line">      loss_layer= tl.CrossEntropyLoss(), <span class="comment"># Loss function </span></span><br><span class="line">      optimizer= trax.optimizers.Adam(<span class="number">0.01</span>), <span class="comment"># Optimizer (Don&#x27;t forget to set LR to 0.01)</span></span><br><span class="line">      lr_schedule= lr_schedule,</span><br><span class="line">      n_steps_per_checkpoint = <span class="number">10</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    eval_task = training.EvalTask( </span><br><span class="line">      labeled_data=eval_gen, <span class="comment"># The evaluation generator</span></span><br><span class="line">      metrics=[tl.CrossEntropyLoss(),tl.Accuracy()] <span class="comment"># CrossEntropyLoss and Accuracy</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    loop = training.Loop(TransformerLM(d_model=<span class="number">4</span>,</span><br><span class="line">                                       d_ff=<span class="number">16</span>,</span><br><span class="line">                                       n_layers=<span class="number">1</span>,</span><br><span class="line">                                       n_heads=<span class="number">2</span>,</span><br><span class="line">                                       mode=<span class="string">&#x27;train&#x27;</span>),</span><br><span class="line">                         train_task,</span><br><span class="line">                         eval_tasks=[eval_task],</span><br><span class="line">                         output_dir=output_dir)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loop</span><br></pre></td></tr></table></figure>

<p>Notice that the model will be trained for only 10 steps. </p>
<p>Even with this constraint the model with the original default arguments took a very long time to finish. Because of this some parameters are changed when defining the model that is fed into the training loop in the function above.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Should take around 1.5 minutes</span></span><br><span class="line">!rm -f ~/model/model.pkl.gz</span><br><span class="line">loop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream)</span><br><span class="line">loop.run(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Step      1: Ran 1 train steps in 9.11 secs
Step      1: train CrossEntropyLoss |  10.41297626
Step      1: eval  CrossEntropyLoss |  10.41586781
Step      1: eval          Accuracy |  0.00000000

Step     10: Ran 9 train steps in 58.21 secs
Step     10: train CrossEntropyLoss |  10.41278458
Step     10: eval  CrossEntropyLoss |  10.41440201
Step     10: eval          Accuracy |  0.00000000
</code></pre>
<p> <a name='4'></a></p>
<h1 id="Part-4-Evaluation"><a href="#Part-4-Evaluation" class="headerlink" title="Part 4:  Evaluation"></a>Part 4:  Evaluation</h1><p><a name='4.1'></a></p>
<h3 id="4-1-Loading-in-a-trained-model"><a href="#4-1-Loading-in-a-trained-model" class="headerlink" title="4.1 Loading in a trained model"></a>4.1 Loading in a trained model</h3><p>In this part you will evaluate by loading in an almost exact version of the model you coded, but we trained it for you to save you time. Please run the cell below to load in the model.</p>
<p>As you may have already noticed the model that you trained and the pretrained model share the same overall architecture but they have different values for some of the parameters:</p>
<p>   <code>Original (pretrained) model: </code>                                 </p>
<pre><code>TransformerLM(vocab_size=33300, d_model=512, d_ff=2048, n_layers=6, n_heads=8, 
               dropout=0.1, max_len=4096, ff_activation=tl.Relu)
               
</code></pre>
<p>   <code>Your model:</code></p>
<pre><code>TransformerLM(d_model=4, d_ff=16, n_layers=1, n_heads=2)
</code></pre>
<p>   <strong>Only the parameters shown for your model were changed. The others stayed the same.</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get the model architecture</span></span><br><span class="line">model = TransformerLM(mode=<span class="string">&#x27;eval&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pre-trained weights</span></span><br><span class="line">model.init_from_file(<span class="string">&#x27;model.pkl.gz&#x27;</span>, weights_only=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><a name='5'></a></p>
<h1 id="Part-5-Testing-with-your-own-input"><a href="#Part-5-Testing-with-your-own-input" class="headerlink" title="Part 5: Testing with your own input"></a>Part 5: Testing with your own input</h1><p>You will now test your input. You are going to implement greedy decoding. This consists of two functions. The first one allows you to identify the next symbol. It gets the argmax of the output of your model and then returns that index. </p>
<p><a name='ex06'></a></p>
<h3 id="Exercise-06"><a href="#Exercise-06" class="headerlink" title="Exercise 06"></a>Exercise 06</h3><p><strong>Instructions:</strong> Implement the next symbol function that takes in the cur_output_tokens and the trained model to return the index of the next word. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C9</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_symbol</span>(<span class="params">cur_output_tokens, model</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns the next symbol for a given sentence.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.</span></span><br><span class="line"><span class="string">        model (trax.layers.combinators.Serial): The transformer model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        int: tokenized symbol.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># current output tokens length</span></span><br><span class="line">    token_length = <span class="built_in">len</span>(cur_output_tokens)</span><br><span class="line">    <span class="comment"># calculate the minimum power of 2 big enough to store token_length</span></span><br><span class="line">    <span class="comment"># HINT: use np.ceil() and np.log2()</span></span><br><span class="line">    <span class="comment"># add 1 to token_length so np.log2() doesn&#x27;t receive 0 when token_length is 0</span></span><br><span class="line">    padded_length = <span class="number">2</span>**<span class="built_in">int</span>(np.ceil(np.log2(token_length + <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Fill cur_output_tokens with 0&#x27;s until it reaches padded_length</span></span><br><span class="line">    padded = cur_output_tokens + [<span class="number">0</span>] * (padded_length - token_length)</span><br><span class="line">    padded_with_batch = np.array(padded)[<span class="literal">None</span>, :] <span class="comment"># Don&#x27;t replace this &#x27;None&#x27;! This is a way of setting the batch dim</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># model expects a tuple containing two padded tensors (with batch)</span></span><br><span class="line">    output, _ = model((padded_with_batch, padded_with_batch)) </span><br><span class="line">    <span class="comment"># HINT: output has shape (1, padded_length, vocab_size)</span></span><br><span class="line">    <span class="comment"># To get log_probs you need to index output with 0 in the first dim</span></span><br><span class="line">    <span class="comment"># token_length in the second dim and all of the entries for the last dim.</span></span><br><span class="line">    log_probs = output[<span class="number">0</span>, token_length, :]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">int</span>(np.argmax(log_probs))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test it out!</span></span><br><span class="line">sentence_test_nxt_symbl = <span class="string">&quot;I want to fly in the sky.&quot;</span></span><br><span class="line">detokenize([next_symbol(tokenize(sentence_test_nxt_symbl)+[<span class="number">0</span>], model)])</span><br></pre></td></tr></table></figure>




<pre><code>&#39;The&#39;
</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;The&#x27;</span></span><br></pre></td></tr></table></figure>

<p><a name='5.1'></a></p>
<h3 id="5-1-Greedy-decoding"><a href="#5-1-Greedy-decoding" class="headerlink" title="5.1 Greedy decoding"></a>5.1 Greedy decoding</h3><p>Now you will implement the greedy_decode algorithm that will call the <code>next_symbol</code> function. It takes in the input_sentence, the trained model and returns the decoded sentence. </p>
<p><a name='ex07'></a></p>
<h3 id="Exercise-07"><a href="#Exercise-07" class="headerlink" title="Exercise 07"></a>Exercise 07</h3><p><strong>Instructions</strong>: Implement the greedy_decode algorithm. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># UNQ_C10</span></span><br><span class="line"><span class="comment"># Decoding functions.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greedy_decode</span>(<span class="params">input_sentence, model</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Greedy decode function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        input_sentence (string): a sentence or article.</span></span><br><span class="line"><span class="string">        model (trax.layers.combinators.Serial): Transformer model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        string: summary of the input.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE (REPLACE INSTANCES OF &#x27;None&#x27; with your code) ###</span></span><br><span class="line">    <span class="comment"># Use tokenize()</span></span><br><span class="line">    cur_output_tokens = tokenize(input_sentence) + [<span class="number">0</span>]</span><br><span class="line">    generated_output = [] </span><br><span class="line">    cur_output = <span class="number">0</span> </span><br><span class="line">    EOS = <span class="number">1</span> </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> cur_output != EOS:</span><br><span class="line">        <span class="comment"># Get next symbol</span></span><br><span class="line">        cur_output = next_symbol(cur_output_tokens, model)</span><br><span class="line">        <span class="comment"># Append next symbol to original sentence</span></span><br><span class="line">        cur_output_tokens.append(cur_output)</span><br><span class="line">        <span class="comment"># Append next symbol to generated sentence</span></span><br><span class="line">        generated_output.append(cur_output)</span><br><span class="line">        <span class="built_in">print</span>(detokenize(generated_output))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> detokenize(generated_output)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test it out on a sentence!</span></span><br><span class="line">test_sentence = <span class="string">&quot;It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips.&quot;</span></span><br><span class="line"><span class="built_in">print</span>(wrapper.fill(test_sentence), <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(greedy_decode(test_sentence, model))</span><br></pre></td></tr></table></figure>

<pre><code>It was a sunny day when I went to the market to buy some flowers. But
I only found roses, not tulips. 

:
: I
: I just
: I just found
: I just found ros
: I just found roses
: I just found roses,
: I just found roses, not
: I just found roses, not tu
: I just found roses, not tulips
: I just found roses, not tulips
: I just found roses, not tulips.
: I just found roses, not tulips.&lt;EOS&gt;
: I just found roses, not tulips.&lt;EOS&gt;
</code></pre>
<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">:</span><br><span class="line">: I</span><br><span class="line">: I just</span><br><span class="line">: I just found</span><br><span class="line">: I just found ros</span><br><span class="line">: I just found roses</span><br><span class="line">: I just found roses,</span><br><span class="line">: I just found roses, <span class="keyword">not</span></span><br><span class="line">: I just found roses, <span class="keyword">not</span> tu</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips.</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips.&lt;EOS&gt;</span><br><span class="line">: I just found roses, <span class="keyword">not</span> tulips.&lt;EOS&gt;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test it out with a whole article!</span></span><br><span class="line">article = <span class="string">&quot;It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the &#x27;Tebowing&#x27; craze at the school was blocking the hallway and presenting a safety hazard to students.&quot;</span></span><br><span class="line"><span class="built_in">print</span>(wrapper.fill(article), <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(greedy_decode(article, model))</span><br></pre></td></tr></table></figure>

<p><strong>Expected Output:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Jordan</span><br><span class="line">Jordan Ful</span><br><span class="line">Jordan Fulcol</span><br><span class="line">Jordan Fulcoly</span><br><span class="line">Jordan Fulcoly,</span><br><span class="line">Jordan Fulcoly, Wayne</span><br><span class="line">Jordan Fulcoly, Wayne Dre</span><br><span class="line">Jordan Fulcoly, Wayne Drexe</span><br><span class="line">Jordan Fulcoly, Wayne Drexel</span><br><span class="line">Jordan Fulcoly, Wayne Drexel,</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line"></span><br><span class="line">Final summary:</span><br><span class="line"></span><br><span class="line">Jordan Fulcoly, Wayne Drexel, Tyler Carroll <span class="keyword">and</span> Connor Carroll were</span><br><span class="line">suspended <span class="keyword">for</span> one day. Four students were suspended <span class="keyword">for</span> one day</span><br><span class="line">because they allegedly did <span class="keyword">not</span> heed to warnings that the <span class="string">&#x27;Tebowing&#x27;</span></span><br><span class="line">craze was blocking the hallway <span class="keyword">and</span> presenting a safety hazard to</span><br><span class="line">students.&lt;EOS&gt;</span><br></pre></td></tr></table></figure>

<p><strong>Congratulations on finishing this week’s assignment!</strong> You did a lot of work and now you should have a better understanding of the encoder part of Transformers and how Transformers can be used for text summarization.</p>
<p><strong>Keep it up!</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Using-RL-to-Solve-Blackjack/2020/09/17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Using-RL-to-Solve-Blackjack/2020/09/17/" class="post-title-link" itemprop="url">Using RL to Solve Blackjack</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-09-17 17:44:04 / Modified: 17:45:27" itemprop="dateCreated datePublished" datetime="2020-09-17T17:44:04+08:00">2020-09-17</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Using-RL-to-Solve-Blackjack/2020/09/17/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Using-RL-to-Solve-Blackjack/2020/09/17/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>转载自: <a target="_blank" rel="noopener" href="https://github.com/zht007/tensorflow-practice/blob/master/7_Renforcement_Learning_blackjack/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94MC(%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B)%E7%8E%A921%E7%82%B9%E6%89%91%E5%85%8B%E6%B8%B8%E6%88%8F.md">https://github.com/zht007/tensorflow-practice/blob/master/7_Renforcement_Learning_blackjack/强化学习——MC(蒙特卡洛)玩21点扑克游戏.md</a></strong></p>
<h3 id="1-关于21点游戏"><a href="#1-关于21点游戏" class="headerlink" title="1. 关于21点游戏"></a>1. 关于21点游戏</h3><h4 id="1-1-规则简介"><a href="#1-1-规则简介" class="headerlink" title="1.1 规则简介"></a>1.1 规则简介</h4><p>21点的游戏规则详细很容易就能够找到，这里进行简单的介绍。</p>
<blockquote>
<ul>
<li><p>在这里**智能体(Agent)<strong>扮演</strong>玩家(Player)<strong>，对方是</strong>庄家(Dealer)**。</p>
</li>
<li><p><strong>点数(Score)<strong>：2-10的点数为牌面数字；J，Q，K是10点；</strong>A有两种算法</strong>，1或者11，算11总点数不超过21时则必须算成11(<strong>usable</strong>)，否则算作1。</p>
</li>
<li><p>庄家需要**亮(Show)<strong>一张牌，玩家根据自己手中的牌和庄家亮的牌决定是</strong>要牌(hits)<strong>还是</strong>停牌(sticks)**。</p>
</li>
<li><p>庄家要牌和停牌的规则是固定的，即点数小于17必须要牌，否则停牌。</p>
</li>
<li><p>**爆牌(goes bust)**：牌总数操过21点，谁爆牌谁输，谁首先凑到21点谁赢，每有爆牌的时候谁大谁赢，同时凑到21为和局。</p>
</li>
</ul>
</blockquote>
<p>####1.2 转换成MDP</p>
<p>了解规则后，我们将游戏转换成MDP，MDP的几大要素：状态(S: State)，行动(A: Action)，奖励(R: Reward)，策略Policy，状态值函数V(s): State-Value Function，行动值函数Q(s, a)Action-Value Function。</p>
<blockquote>
<p><strong>行动A</strong>：<strong>要牌(hits)<strong>还是</strong>停牌(sticks)</strong></p>
<p><strong>状态S</strong>：状态是由双方目前牌的点数决定的，但是当玩家点数小于等于11时，当然会毫不犹豫选择要牌，所以真正涉及到做选择的状态是12-21点的状态，此时庄家亮牌有A-10种情况，再加上是否有11的A(usable A)，所以21点游戏中所有的状态一<strong>共只有200个</strong>。</p>
<p><strong>奖励R</strong>：玩家赢牌奖励为1，输牌奖励为-1，和局和其他状态奖励为0。</p>
<p><strong>策略Policy</strong>：该状态下，要牌和停牌的概率</p>
</blockquote>
<h3 id="2-MC策略评估"><a href="#2-MC策略评估" class="headerlink" title="2. MC策略评估"></a>2. MC策略评估</h3><p>在<strong>给定策略</strong>下，为什么我们不用上一篇文章提到的DP方法进行策略评估呢？DP方法需要look one step ahead，假设玩家手里牌点数为14，庄家亮牌为10，你需要计算要牌和停牌之后所有可能性，下一张牌是什么？庄家可能抽到什么？离获得奖励有多远？等等，这几乎是不可能的。</p>
<p>MC可以通过抽样方式，直接根据策略实践，从而获取奖励和学习V(s)，克服了DP方法的限制。这里采用首次访问MC方法。大致分为三步：</p>
<p><strong>第一步</strong>：根据策略采样，直到游戏结束，获得一个episode的 (S0, A0, R1), (S1, A1, R2), . . . , (ST-1, AT-1, RT)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">episode = []</span><br><span class="line">state = env.reset()      </span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    action = policy(state)</span><br><span class="line">    next_state, reward, done, _ = env.step(action)</span><br><span class="line">    episode.append((state, action, reward))</span><br><span class="line">    <span class="keyword">if</span> done:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    state = next_state</span><br></pre></td></tr></table></figure>

<p><em>该部分代码参考<a target="_blank" rel="noopener" href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Prediction%20Solution.ipynb">github</a> with MIT license</em></p>
<p><strong>第二步:</strong>   计算首次出现s状态的Reward，直到这个episode结束总共累积的Reward。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">states_in_episode = <span class="built_in">set</span>([<span class="built_in">tuple</span>(x[<span class="number">0</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> episode])</span><br><span class="line">        <span class="keyword">for</span> state <span class="keyword">in</span> states_in_episode:</span><br><span class="line">            <span class="comment"># Find the first occurance of the state in the episode</span></span><br><span class="line">            first_occurence_idx = <span class="built_in">next</span>(i <span class="keyword">for</span> i,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(episode) <span class="keyword">if</span> x[<span class="number">0</span>] == state)</span><br><span class="line">            <span class="comment"># Sum up all rewards since the first occurance</span></span><br><span class="line">            G = <span class="built_in">sum</span>([x[<span class="number">2</span>]*(discount_factor**i) <span class="keyword">for</span> i,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(episode[first_occurence_idx:])])</span><br><span class="line">            <span class="comment"># Calculate average return for this state over all sampled episodes</span></span><br><span class="line">            returns_sum[state] += G</span><br><span class="line">            returns_count[state] += <span class="number">1.0</span></span><br></pre></td></tr></table></figure>

<p><em>该部分代码参考<a target="_blank" rel="noopener" href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Prediction%20Solution.ipynb">github</a> with MIT license</em></p>
<p><strong>第三步</strong>：若干个epsoide之后，将累积的R平均就得到该s下的V(s)了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">V[state] = returns_sum[state] / returns_count[state]</span><br></pre></td></tr></table></figure>

<p>给定玩家的策略，当分数小于20则要牌，否则停牌</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_policy</span>(<span class="params">observation</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A policy that sticks if the player score is &gt;= 20 and hits otherwise.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    score, dealer_score, usable_ace = observation</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span> <span class="keyword">if</span> score &gt;= <span class="number">20</span> <span class="keyword">else</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p><em>该部分代码参考<a target="_blank" rel="noopener" href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Prediction%20Solution.ipynb">github</a> with MIT license</em></p>
<p>下图为500,000个epsoide之后的V(s)![image-20190424164753631](/Users/hongtao/Library/Application Support/typora-user-images/image-20190424164753631.png)</p>
<p>V(s)的分布只能告诉我们<strong>当前策略下</strong>每个<strong>状态</strong>(你的点数，庄家亮牌，是否有usable A)的<strong>价值</strong>，我们如何使用V(s)来改进我们的策略，从而获得最大几率获胜的可能性呢？这就是我们下一节要讨论的内容。</p>
<h3 id="3-MC控制"><a href="#3-MC控制" class="headerlink" title="3. MC控制"></a>3. MC控制</h3><p>当然我们的目的不仅仅是对当前策略进行评估，我们希望改进策略在游戏中获得最大的收益。与DP一样，MC可以采用评估加改进(Policy Evaluation and Policy Improvement)的方式，迭代更新策略，最终可以收敛到一个最佳的策略。</p>
<p>当然我们在MC控制中采用策略评估的时候，需要加入对行动的评估，即<strong>Q(s, a)行动值函数</strong>的评估。但是如果我们采用DP中Greedy的方式来改进策略会遇到问题。由于MC是用<strong>采样</strong>的方式更新**Q(s, a)<strong>，这就意味着我们很可能错过一些状态和行动，而且永远也无法更新该状态和行动的Q函数了。这就是典型的</strong>探索利用困境(Explore Exploit Delima)**。</p>
<p>解决探索利用困境，我们可以使用epsilon-greedy 方法，或者将探索和利用的policy分开，采用off-policy的方法更新策略。</p>
<h4 id="3-1-On-Policy-的-epsilon-greedy采样法"><a href="#3-1-On-Policy-的-epsilon-greedy采样法" class="headerlink" title="3.1 On-Policy 的 epsilon-greedy采样法"></a>3.1 On-Policy 的 epsilon-greedy采样法</h4><p>On-Policy即评估和改进的策略是同一个策略，为避免探索利用困境，我们采用 epsilon-greedy的方法。</p>
<p><strong>第一步</strong>：对于21点的游戏，我们定义 epsilon-greedy policy.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_epsilon_greedy_policy</span>(<span class="params">Q, epsilon, nA</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span>(<span class="params">observation</span>):</span></span><br><span class="line">        A = np.ones(nA, dtype=<span class="built_in">float</span>) * epsilon / nA </span><br><span class="line">        best_action = np.argmax(Q[observation])</span><br><span class="line">        A[best_action] += (<span class="number">1.0</span> - epsilon)</span><br><span class="line">        <span class="keyword">return</span> A </span><br><span class="line">    <span class="keyword">return</span> policy_fn</span><br></pre></td></tr></table></figure>

<p>该部分代码参考<a target="_blank" rel="noopener" href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Prediction%20Solution.ipynb">github</a> with MIT license</p>
<p>其中Q是一个dictionary，为该状态下对应的行动，这样定义epsilon greedy policy 既保证了最优行动的几率最大，同时也让采取其他行动几率为一个非零的小值(epsilon / nA )。这样就保证了智能体在采样的时候能够探索未知的状态和行动。</p>
<p><strong>第二步</strong>：与MC评估的第一步一致，根据策略采样，直到游戏结束，获得一个episode的 (S0, A0, R1), (S1, A1, R2), . . . , (ST-1, AT-1, RT)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)</span><br><span class="line">       episode = []</span><br><span class="line">       state = env.reset()</span><br><span class="line">       <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">           probs = policy(state)</span><br><span class="line">           action = np.random.choice(np.arange(<span class="built_in">len</span>(probs)), p=probs)</span><br><span class="line">           next_state, reward, done, _ = env.step(action)</span><br><span class="line">           episode.append((state, action, reward))</span><br><span class="line">           <span class="keyword">if</span> done:</span><br><span class="line">               <span class="keyword">break</span></span><br><span class="line">           state = next_state</span><br></pre></td></tr></table></figure>

<p><em>该部分代码参考<a target="_blank" rel="noopener" href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Control%20with%20Epsilon-Greedy%20Policies.ipynb">github</a> MIT license</em></p>
<p>注意与MC评估不同的是，action无法从policy中直接得出，而是根据概率随机选择的，也就是有可能智能体会”探索”非最优行动。</p>
<p><strong>第三步</strong>：计算首次出现该s 和 a 的Reward，直到这个episode结束，总共累积的Reward。平均Reward并更新Q表。Q表更新的同时，Policy也就自动更新了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sa_in_episode = <span class="built_in">set</span>([(<span class="built_in">tuple</span>(x[<span class="number">0</span>]), x[<span class="number">1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> episode])</span><br><span class="line">        <span class="keyword">for</span> state, action <span class="keyword">in</span> sa_in_episode:</span><br><span class="line">            sa_pair = (state, action)</span><br><span class="line">            <span class="comment"># Find the first occurance of the (state, action) pair in the episode</span></span><br><span class="line">            first_occurence_idx = <span class="built_in">next</span>(i <span class="keyword">for</span> i,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(episode)</span><br><span class="line">                                       <span class="keyword">if</span> x[<span class="number">0</span>] == state <span class="keyword">and</span> x[<span class="number">1</span>] == action)</span><br><span class="line">            <span class="comment"># Sum up all rewards since the first occurance</span></span><br><span class="line">            G = <span class="built_in">sum</span>([x[<span class="number">2</span>]*(discount_factor**i) <span class="keyword">for</span> i,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(episode[first_occurence_idx:])])</span><br><span class="line">            <span class="comment"># Calculate average return for this state over all sampled episodes</span></span><br><span class="line">            returns_sum[sa_pair] += G</span><br><span class="line">            returns_count[sa_pair] += <span class="number">1.0</span></span><br><span class="line">            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># The policy is improved implicitly by changing the Q dictionary</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Q, policy</span><br></pre></td></tr></table></figure>

<p><em>该部分代码参考<a target="_blank" rel="noopener" href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/MC%20Control%20with%20Epsilon-Greedy%20Policies.ipynb">github</a> MIT license</em></p>
<p>下图是500,000个episode之后Q表中各个状态对应的Action值，Action只有两个值0(停牌)和1(要牌)，读者就可以尝试用下图的策略指导玩21点的游戏啦。举个例子，比如你现在手上牌是14点，没有可作为11的A，庄家亮牌为8，那么根据左图所示，最好的策略就是要牌。</p>
<p>![image-20190429164937096](/Users/hongtao/Library/Application Support/typora-user-images/image-20190429164937096.png)</p>
<h4 id="3-2-Off-Policy的-Weighted-Importance采样法"><a href="#3-2-Off-Policy的-Weighted-Importance采样法" class="headerlink" title="3.2 Off-Policy的 Weighted Importance采样法"></a>3.2 Off-Policy的 Weighted Importance采样法</h4><p>Off-Policy就是将最终想要得到的**目标策略(Target Policy)<strong>和用于探索的</strong>行为策略(Behavior Policy)**分离，对目标策略采取Greedy的改进方式，而对实际行动的行为策略采用随机探索的改进方式从而解决了探索利用困境。当然Off-Policy 还有很多其他的优点比如学习历史经验，学习别人的经验等等。</p>
<p>这部分涉及到的理论比较复杂，可参考<a target="_blank" rel="noopener" href="http://incompleteideas.net/book/RLbook2018.pdf">[1]</a> <a target="_blank" rel="noopener" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">[2]</a>中的相关内容。简单解释即首先用Behavior Policy指导智能体进行MC采样，然后用包含<strong>Importance Sampling Ratio</strong> 函数来更新Target Policy。Importance Sampling Ratio是Target Policy和Behavior Policy在同一路径下的概率比值。</p>
<p>Target Policy的Q(s, a)函数在MC采样下需要平均，这里采用加权平均的方法，包含Importatnce Sampling Ratio的权重简记为 W，最后，我们通过递推的方法更新 W 即可跟新Q(s, a)。</p>
<p><strong>第一步</strong>：生成两种policy方法，random policy 用于Behavior Policy，greedy policy用于Target Policy.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_random_policy</span>(<span class="params">nA</span>):</span></span><br><span class="line">    A = np.ones(nA, dtype=<span class="built_in">float</span>) / nA</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span>(<span class="params">observation</span>):</span></span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line">    <span class="keyword">return</span> policy_fn</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_greedy_policy</span>(<span class="params">Q</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span>(<span class="params">state</span>):</span></span><br><span class="line">        A = np.zeros_like(Q[state], dtype=<span class="built_in">float</span>)</span><br><span class="line">        best_action = np.argmax(Q[state])</span><br><span class="line">        A[best_action] = <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line">    <span class="keyword">return</span> policy_fn</span><br></pre></td></tr></table></figure>

<p><em>该部分代码参考<a target="_blank" rel="noopener" href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/Off-Policy%20MC%20Control%20with%20Weighted%20Importance%20Sampling%20Solution.ipynb">github</a> with MIT license</em></p>
<p><strong>第二步</strong>：用Behavior Policy进行MC采样，这里与On-Policy 的方法类似。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">target_policy = create_greedy_policy(Q)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_episodes + <span class="number">1</span>):</span><br><span class="line">        episode = []</span><br><span class="line">        state = env.reset()</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">            <span class="comment"># Sample an action from our policy</span></span><br><span class="line">            probs = behavior_policy(state)</span><br><span class="line">            action = np.random.choice(np.arange(<span class="built_in">len</span>(probs)), p=probs)</span><br><span class="line">            next_state, reward, done, _ = env.step(action)</span><br><span class="line">            episode.append((state, action, reward))</span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            state = next_state</span><br></pre></td></tr></table></figure>

<p><em>该部分代码参考<a target="_blank" rel="noopener" href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/Off-Policy%20MC%20Control%20with%20Weighted%20Importance%20Sampling%20Solution.ipynb">github</a> with MIT license</em></p>
<p>第三步**：递推的方法更新W和Q，Target Policy 也就自动更新了。注意由于是采用递推的方法，该episode是从后往前计算的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">    G = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># The importance sampling ratio (the weights of the returns)</span></span><br><span class="line">    W = <span class="number">1.0</span></span><br><span class="line">    <span class="comment"># For each step in the episode, backwards</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(episode))[::-<span class="number">1</span>]:</span><br><span class="line">        state, action, reward = episode[t]</span><br><span class="line">        <span class="comment"># Update the total reward since step t</span></span><br><span class="line">        G = discount_factor * G + reward</span><br><span class="line">        <span class="comment"># Update weighted importance sampling formula denominator</span></span><br><span class="line">        C[state][action] += W</span><br><span class="line">        <span class="comment"># Update the action-value function using the incremental update formula (5.7)</span></span><br><span class="line">        <span class="comment"># This also improves our target policy which holds a reference to Q</span></span><br><span class="line">        Q[state][action] += (W / C[state][action]) * (G - Q[state][action])</span><br><span class="line">        <span class="comment"># If the action taken by the behavior policy is not the action </span></span><br><span class="line">        <span class="comment"># taken by the target policy the probability will be 0 and we can break</span></span><br><span class="line">        <span class="keyword">if</span> action !=  np.argmax(target_policy(state)):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        W = W * <span class="number">1.</span>/behavior_policy(state)[action]</span><br><span class="line">    </span><br><span class="line"><span class="keyword">return</span> Q, target_policy</span><br></pre></td></tr></table></figure>

<p><em>该部分代码参考<a target="_blank" rel="noopener" href="https://github.com/dennybritz/reinforcement-learning/blob/master/MC/Off-Policy%20MC%20Control%20with%20Weighted%20Importance%20Sampling%20Solution.ipynb">github</a> with MIT license</em></p>
<p>最后，经过500,000 个episod我们得到的最佳策略，与上一节采用On-Policy MC 方法的结果稍有差异，但基本一致。</p>
<p>![image-20190426125404672](/Users/hongtao/Library/Application Support/typora-user-images/image-20190426125404672.png)</p>
<hr>
<p>参考资料</p>
<p>[1] <a target="_blank" rel="noopener" href="http://incompleteideas.net/book/RLbook2018.pdf">Reinforcement Learning: An Introduction (2nd Edition)</a></p>
<p>[2] <a target="_blank" rel="noopener" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">David Silver’s Reinforcement Learning Course (UCL, 2015)</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://github.com/dennybritz/reinforcement-learning">Github repo: Reinforcement Learning</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Generalized-Policy-Iteration/2020/09/10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Generalized-Policy-Iteration/2020/09/10/" class="post-title-link" itemprop="url">Generalized Policy Iteration</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-09-10 14:26:25 / Modified: 15:20:31" itemprop="dateCreated datePublished" datetime="2020-09-10T14:26:25+08:00">2020-09-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Generalized-Policy-Iteration/2020/09/10/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Generalized-Policy-Iteration/2020/09/10/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies.  As discussed there, we can easily obtain optimal policies once we have found the optimal value functions, $v_{\star}$ or $q_{\star}$, which satisfy the Bellman optimality equations:</p>
<p>$$V_{\star} = max_a \sum_{s\prime,r} p(s\prime,r | s, a)[ r + \gamma v_{\star} (s\prime)]$$</p>
<p>$$q_{\star}(s,a) = \sum_{s\prime,r} p(s\prime,r | s,a)[ r + \gamma max_{a\prime} q_{\star}(s\prime,a\prime)]$$</p>
<h2 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h2><p>We consider how to compute the state-value function $V_{\pi}$ for an arbitrary policy $\pi$. This is called policy evaluation in the DP literature. </p>
<p>$$V_{\pi}(s) = \sum_a \pi(a | s) \sum_{s\prime,r} p(s\prime, r | s, a) [ r + \gamma v_{\pi} (s\prime)]$$</p>
<p>Consider a sequence of approximate value functions $v_0,v_1, \cdots $,  each mapping $S+$ to $\mathbb{R}$ (the real numbers). The initial approximation, $v_0$, is chosen arbitrarily, and each successive approximation is obtained by using the Bellman equation for $V_{\pi}$ as an update rule:</p>
<p>$$V_{k+1}(s) = \sum_a \pi(a | s) \sum_{s\prime,r} p(s\prime, r | s, a) [ r + \gamma v_{k} (s\prime)]$$</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="1.png" width = "80%" height="80%">
</center>


<h2 id="Policy-Improvement"><a href="#Policy-Improvement" class="headerlink" title="Policy Improvement"></a>Policy Improvement</h2><p>Our reason for computing the value function for a policy is to help find better policies. </p>
<p>Consider selecting a in s and thereafter following the existing policy, $\pi$,  The value of this way of behaving is</p>
<p>$$q_{\pi}(s,a) = \sum_{s\prime,r}p(s\prime,r | s, a) [ r + \gamma  v_{\pi}(s\prime)]$$</p>
<p>The key criterion is whether this is greater than or less than $V_{\pi}(s)$.  If it is greater—that is, if it is better to select a once in s and thereafter follow ⇡ than it would be to follow $\pi$ all the time—then one would expect it to be better still to select a every time s is encountered, and that the new policy would in fact be a better one overall. </p>
<p>That this is true is a special case of a general result called the <strong>policy improvement theorem</strong>. Let $\pi$ and $\pi\prime$ be any pair of deterministic policies such that, for all $s\in S$, </p>
<p>$$q_{\pi}(s, \pi\prime(s)) \geq v_{\pi}(s)$$</p>
<p>Then the policy $\pi\prime$ must be as good as, or better than, $\pi$. That is, it must obtain greater or equal expected return from all states $s\in S$:</p>
<p>$$V_{\pi\prime}(s) \geq V_{\pi}(s)$$</p>
<h2 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h2><p>Once a policy, $\pi$, has been improved using $v_{\pi}$ to yield a better policy, $\pi\prime$, we can then compute $\pi\prime$ and  improve it again to yield an even better $\pi\prime\prime$. We can thus obtain a sequence of monotonically improving policies and value functions:</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="2.png" width = "80%" height="80%">
</center>

<p>This way of finding an optimal policy is called policy iteration. A complete algorithm is given in the box below.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="3.png" width = "80%" height="80%">
</center>


<h2 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h2><p>$$V_{k+1}(s) = max_a \sum_{s\prime,r} p(s\prime,r | s, a)[ r + \gamma v_k (s\prime)]$$</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="4.png" width = "80%" height="80%">
</center>


<h2 id="Generalized-Policy-Iteration"><a href="#Generalized-Policy-Iteration" class="headerlink" title="Generalized Policy Iteration"></a>Generalized Policy Iteration</h2><p>Policy iteration consists of two simultaneous, interacting processes, one making the value function consistent with the current policy (policy evaluation), and the other making the policy greedy with respect to the current value function (policy improvement). In policy iteration, these two processes alternate, each completing before the other begins, but this is not really necessary. In value iteration, for example, only a single iteration of policy evaluation is performed in between each policy improvement. In asynchronous DP methods, the evaluation and improvement processes are interleaved at an even finer grain. In some cases a single state is updated in one process before returning to the other. As long as both processes continue to update all states, the ultimate result is typically the same—convergence to the optimal value function and an optimal policy.</p>
<p>We use the term generalized policy iteration (GPI) to refer to the general idea of letting policy-evaluation and policy improvement processes interact, independent of the granularity and other details of the two processes. Almost all reinforcement learning methods are well described as GPI. That is, all have identifiable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy, as suggested by the diagram to the right. If both the evaluation process and the improvement process stabilize, that is, no longer produce changes, then the value function and policy must be optimal. The value function stabilizes only when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function. Thus, both processes stabilize only when a policy has been found that is greedy with respect to its own evaluation function. This implies that the Bellman optimality equation holds, and thus that the policy and the value function are optimal.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="5.png" width = "50%" height="50%">
</center>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Optimal-Policies-with-Dynamic-Programming/2020/09/10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Optimal-Policies-with-Dynamic-Programming/2020/09/10/" class="post-title-link" itemprop="url">Optimal Policies with Dynamic Programming</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-09-10 14:17:39 / Modified: 14:18:14" itemprop="dateCreated datePublished" datetime="2020-09-10T14:17:39+08:00">2020-09-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Optimal-Policies-with-Dynamic-Programming/2020/09/10/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Optimal-Policies-with-Dynamic-Programming/2020/09/10/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Assignment-2-Optimal-Policies-with-Dynamic-Programming"><a href="#Assignment-2-Optimal-Policies-with-Dynamic-Programming" class="headerlink" title="Assignment 2: Optimal Policies with Dynamic Programming"></a>Assignment 2: Optimal Policies with Dynamic Programming</h1><p>Welcome to Assignment 2. This notebook will help you understand:</p>
<ul>
<li>Policy Evaluation and Policy Improvement.</li>
<li>Value and Policy Iteration.</li>
<li>Bellman Equations.</li>
</ul>
<h2 id="Gridworld-City"><a href="#Gridworld-City" class="headerlink" title="Gridworld City"></a>Gridworld City</h2><p>Gridworld City, a thriving metropolis with a booming technology industry, has recently experienced an influx of grid-loving software engineers. Unfortunately, the city’s street parking system, which charges a fixed rate, is struggling to keep up with the increased demand. To address this, the city council has decided to modify the pricing scheme to better promote social welfare. In general, the city considers social welfare higher when more parking is being used, the exception being that the city prefers that at least one spot is left unoccupied (so that it is available in case someone really needs it). The city council has created a Markov decision process (MDP) to model the demand for parking with a reward function that reflects its preferences. Now the city has hired you &mdash; an expert in dynamic programming &mdash; to help determine an optimal policy.</p>
<h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><p>You’ll need two imports to complete this assigment:</p>
<ul>
<li>numpy: The fundamental package for scientific computing with Python.</li>
<li>tools: A module containing an environment and a plotting function.</li>
</ul>
<p>There are also some other lines in the cell below that are used for grading and plotting &mdash; you needn’t worry about them.</p>
<p>In this notebook, all cells are locked except those that you are explicitly asked to modify. It is up to you to decide how to implement your solution in these cells, <strong>but please do not import other libraries</strong> &mdash; doing so will break the autograder.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tools</span><br><span class="line"><span class="keyword">import</span> grader</span><br></pre></td></tr></table></figure>


<pre><code>&lt;Figure size 432x288 with 0 Axes&gt;
</code></pre>
<p>In the city council’s parking MDP, states are nonnegative integers indicating how many parking spaces are occupied, actions are nonnegative integers designating the price of street parking, the reward is a real value describing the city’s preference for the situation, and time is discretized by hour. As might be expected, charging a high price is likely to decrease occupancy over the hour, while charging a low price is likely to increase it.</p>
<p>For now, let’s consider an environment with three parking spaces and three price points. Note that an environment with three parking spaces actually has four states &mdash; zero, one, two, or three spaces could be occupied.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------</span></span><br><span class="line"><span class="comment"># Discussion Cell</span></span><br><span class="line"><span class="comment"># ---------------</span></span><br><span class="line">num_spaces = <span class="number">3</span></span><br><span class="line">num_prices = <span class="number">3</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces, num_prices)</span><br><span class="line">V = np.zeros(num_spaces + <span class="number">1</span>)</span><br><span class="line">pi = np.ones((num_spaces + <span class="number">1</span>, num_prices)) / num_prices</span><br></pre></td></tr></table></figure>

<p>The value function is a one-dimensional array where the $i$-th entry gives the value of $i$ spaces being occupied.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">V</span><br></pre></td></tr></table></figure>




<pre><code>array([0., 0., 0., 0.])
</code></pre>
<p>We can represent the policy as a two-dimensional array where the $(i, j)$-th entry gives the probability of taking action $j$ in state $i$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pi</span><br></pre></td></tr></table></figure>




<pre><code>array([[0.33333333, 0.33333333, 0.33333333],
       [0.33333333, 0.33333333, 0.33333333],
       [0.33333333, 0.33333333, 0.33333333],
       [0.33333333, 0.33333333, 0.33333333]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pi[<span class="number">0</span>] = [<span class="number">0.75</span>, <span class="number">0.11</span>, <span class="number">0.14</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> s, pi_s <span class="keyword">in</span> <span class="built_in">enumerate</span>(pi):</span><br><span class="line">    <span class="keyword">for</span> a, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(pi_s):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;pi(A=<span class="subst">&#123;a&#125;</span>|S=<span class="subst">&#123;s&#125;</span>) = <span class="subst">&#123;p.<span class="built_in">round</span>(<span class="number">2</span>)&#125;</span>    &#x27;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br></pre></td></tr></table></figure>

<pre><code>pi(A=0|S=0) = 0.75    pi(A=1|S=0) = 0.11    pi(A=2|S=0) = 0.14    
pi(A=0|S=1) = 0.33    pi(A=1|S=1) = 0.33    pi(A=2|S=1) = 0.33    
pi(A=0|S=2) = 0.33    pi(A=1|S=2) = 0.33    pi(A=2|S=2) = 0.33    
pi(A=0|S=3) = 0.33    pi(A=1|S=3) = 0.33    pi(A=2|S=3) = 0.33    
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">V[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">tools.plot(V, pi)</span><br></pre></td></tr></table></figure>


<p><img src="output_11_0.png" alt="png"></p>
<p>We can visualize a value function and policy with the <code>plot</code> function in the <code>tools</code> module. On the left, the value function is displayed as a barplot. State zero has an expected return of ten, while the other states have an expected return of zero. On the right, the policy is displayed on a two-dimensional grid. Each vertical strip gives the policy at the labeled state. In state zero, action zero is the darkest because the agent’s policy makes this choice with the highest probability. In the other states the agent has the equiprobable policy, so the vertical strips are colored uniformly.</p>
<p>You can access the state space and the action set as attributes of the environment.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.S</span><br></pre></td></tr></table></figure>




<pre><code>[0, 1, 2, 3]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.A</span><br></pre></td></tr></table></figure>




<pre><code>[0, 1, 2]
</code></pre>
<p>You will need to use the environment’s <code>transitions</code> method to complete this assignment. The method takes a state and an action and returns a 2-dimensional array, where the entry at $(i, 0)$ is the reward for transitioning to state $i$ from the current state and the entry at $(i, 1)$ is the conditional probability of transitioning to state $i$ given the current state and action.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">state = <span class="number">3</span></span><br><span class="line">action = <span class="number">1</span></span><br><span class="line">transitions = env.transitions(state, action)</span><br><span class="line">transitions</span><br></pre></td></tr></table></figure>




<pre><code>array([[1.        , 0.12390437],
       [2.        , 0.15133714],
       [3.        , 0.1848436 ],
       [2.        , 0.53991488]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> sp, (r, p) <span class="keyword">in</span> <span class="built_in">enumerate</span>(transitions):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;p(S\&#x27;=<span class="subst">&#123;sp&#125;</span>, R=<span class="subst">&#123;r&#125;</span> | S=<span class="subst">&#123;state&#125;</span>, A=<span class="subst">&#123;action&#125;</span>) = <span class="subst">&#123;p.<span class="built_in">round</span>(<span class="number">2</span>)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>p(S&#39;=0, R=1.0 | S=3, A=1) = 0.12
p(S&#39;=1, R=2.0 | S=3, A=1) = 0.15
p(S&#39;=2, R=3.0 | S=3, A=1) = 0.18
p(S&#39;=3, R=2.0 | S=3, A=1) = 0.54
</code></pre>
<h2 id="Section-1-Policy-Evaluation"><a href="#Section-1-Policy-Evaluation" class="headerlink" title="Section 1: Policy Evaluation"></a>Section 1: Policy Evaluation</h2><p>You’re now ready to begin the assignment! First, the city council would like you to evaluate the quality of the existing pricing scheme. Policy evaluation works by iteratively applying the Bellman equation for $v_{\pi}$ to a working value function, as an update rule, as shown below.</p>
<p>$$\large v(s) \leftarrow \sum_a \pi(a | s) \sum_{s’, r} p(s’, r | s, a)[r + \gamma v(s’)]$$<br>This update can either occur “in-place” (i.e. the update rule is sequentially applied to each state) or with “two-arrays” (i.e. the update rule is simultaneously applied to each state). Both versions converge to $v_{\pi}$ but the in-place version usually converges faster. <strong>In this assignment, we will be implementing all update rules in-place</strong>, as is done in the pseudocode of chapter 4 of the textbook. </p>
<p>We have written an outline of the policy evaluation algorithm described in chapter 4.1 of the textbook. It is left to you to fill in the <code>bellman_update</code> function to complete the algorithm.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lock</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_policy</span>(<span class="params">env, V, pi, gamma, theta</span>):</span></span><br><span class="line">    delta = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">    <span class="keyword">while</span> delta &gt; theta:</span><br><span class="line">        delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">            v = V[s]</span><br><span class="line">            bellman_update(env, V, pi, s, gamma)</span><br><span class="line">            delta = <span class="built_in">max</span>(delta, <span class="built_in">abs</span>(v - V[s]))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> V</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bellman_update</span>(<span class="params">env, V, pi, s, gamma</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Mutate ``V`` according to the Bellman update equation.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    actions = pi[s]</span><br><span class="line">    G = [<span class="number">0</span>] * <span class="built_in">len</span>(actions)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> env.A:</span><br><span class="line">        transitions = env.transitions(s, action)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> s_, (r,p) <span class="keyword">in</span> <span class="built_in">enumerate</span>(transitions):</span><br><span class="line">            G[action] += p * (r + gamma * V[s_])</span><br><span class="line">            </span><br><span class="line">    V[s] = np.<span class="built_in">sum</span>(G * actions)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<p>The cell below uses the policy evaluation algorithm to evaluate the city’s policy, which charges a constant price of one.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set up test environment</span></span><br><span class="line">num_spaces = <span class="number">10</span></span><br><span class="line">num_prices = <span class="number">4</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces, num_prices)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build test policy</span></span><br><span class="line">city_policy = np.zeros((num_spaces + <span class="number">1</span>, num_prices))</span><br><span class="line">city_policy[:, <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">V = np.zeros(num_spaces + <span class="number">1</span>)</span><br><span class="line">V = evaluate_policy(env, V, city_policy, gamma, theta)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(V)</span><br></pre></td></tr></table></figure>

<pre><code>[80.04173399 81.65532303 83.37394007 85.12975566 86.87174913 88.55589131
 90.14020422 91.58180605 92.81929841 93.78915889 87.77792991]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set up test environment</span></span><br><span class="line">num_spaces = <span class="number">10</span></span><br><span class="line">num_prices = <span class="number">4</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces, num_prices)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build test policy</span></span><br><span class="line">city_policy = np.zeros((num_spaces + <span class="number">1</span>, num_prices))</span><br><span class="line">city_policy[:, <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">V = np.zeros(num_spaces + <span class="number">1</span>)</span><br><span class="line">V = evaluate_policy(env, V, city_policy, gamma, theta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># test the value function</span></span><br><span class="line">answer = [<span class="number">80.04</span>, <span class="number">81.65</span>, <span class="number">83.37</span>, <span class="number">85.12</span>, <span class="number">86.87</span>, <span class="number">88.55</span>, <span class="number">90.14</span>, <span class="number">91.58</span>, <span class="number">92.81</span>, <span class="number">93.78</span>, <span class="number">87.77</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure the value function is within 2 decimal places of the correct answer</span></span><br><span class="line"><span class="keyword">assert</span> grader.near(V, answer, <span class="number">1e-2</span>)</span><br></pre></td></tr></table></figure>

<p>You can use the <code>plot</code> function to visualize the final value function and policy.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lock</span></span><br><span class="line">tools.plot(V, city_policy)</span><br></pre></td></tr></table></figure>


<p><img src="output_26_0.png" alt="png"></p>
<p>Observe that the value function qualitatively resembles the city council’s preferences &mdash; it monotonically increases as more parking is used, until there is no parking left, in which case the value is lower. Because of the relatively simple reward function (more reward is accrued when many but not all parking spots are taken and less reward is accrued when few or all parking spots are taken) and the highly stochastic dynamics function (each state has positive probability of being reached each time step) the value functions of most policies will qualitatively resemble this graph. However, depending on the intelligence of the policy, the scale of the graph will differ. In other words, better policies will increase the expected return at every state rather than changing the relative desirability of the states. Intuitively, the value of a less desirable state can be increased by making it less likely to remain in a less desirable state. Similarly, the value of a more desirable state can be increased by making it more likely to remain in a more desirable state. That is to say, good policies are policies that spend more time in desirable states and less time in undesirable states. As we will see in this assignment, such a steady state distribution is achieved by setting the price to be low in low occupancy states (so that the occupancy will increase) and setting the price high when occupancy is high (so that full occupancy will be avoided).</p>
<h2 id="Section-2-Policy-Iteration"><a href="#Section-2-Policy-Iteration" class="headerlink" title="Section 2: Policy Iteration"></a>Section 2: Policy Iteration</h2><p>Now the city council would like you to compute a more efficient policy using policy iteration. Policy iteration works by alternating between evaluating the existing policy and making the policy greedy with respect to the existing value function. We have written an outline of the policy iteration algorithm described in chapter 4.3 of the textbook. We will make use of the policy evaluation algorithm you completed in section 1. It is left to you to fill in the <code>q_greedify_policy</code> function, such that it modifies the policy at $s$ to be greedy with respect to the q-values at $s$, to complete the policy improvement algorithm.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">improve_policy</span>(<span class="params">env, V, pi, gamma</span>):</span></span><br><span class="line">    policy_stable = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">        old = pi[s].copy()</span><br><span class="line">        q_greedify_policy(env, V, pi, s, gamma)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> np.array_equal(pi[s], old):</span><br><span class="line">            policy_stable = <span class="literal">False</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> pi, policy_stable</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">policy_iteration</span>(<span class="params">env, gamma, theta</span>):</span></span><br><span class="line">    V = np.zeros(<span class="built_in">len</span>(env.S))</span><br><span class="line">    pi = np.ones((<span class="built_in">len</span>(env.S), <span class="built_in">len</span>(env.A))) / <span class="built_in">len</span>(env.A)</span><br><span class="line">    policy_stable = <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> policy_stable:</span><br><span class="line">        V = evaluate_policy(env, V, pi, gamma, theta)</span><br><span class="line">        pi, policy_stable = improve_policy(env, V, pi, gamma)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> V, pi</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_greedify_policy</span>(<span class="params">env, V, pi, s, gamma</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Mutate ``pi`` to be greedy with respect to the q-values induced by ``V``.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    G = [<span class="number">0</span>] * <span class="built_in">len</span>(env.A)</span><br><span class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> env.A:</span><br><span class="line">        transitions = env.transitions(s, action)</span><br><span class="line">        <span class="keyword">for</span> s_, (r,p) <span class="keyword">in</span> <span class="built_in">enumerate</span>(transitions):</span><br><span class="line">            G[action] += p * (r + gamma * V[s_])</span><br><span class="line">            </span><br><span class="line">    best_a = np.argmax(G)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i,_ <span class="keyword">in</span> <span class="built_in">enumerate</span>(pi[s]):</span><br><span class="line">        <span class="keyword">if</span> i == best_a:</span><br><span class="line">            pi[s][i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pi[s][i] = <span class="number">0</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">6</span>, num_prices=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">V = np.array([<span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">pi = np.ones((<span class="number">7</span>, <span class="number">4</span>)) / <span class="number">4</span></span><br><span class="line"></span><br><span class="line">new_pi, stable = improve_policy(env, V, pi, gamma)</span><br><span class="line"></span><br><span class="line"><span class="comment"># expect first call to greedify policy</span></span><br><span class="line">expected_pi = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">])</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(new_pi == expected_pi)</span><br><span class="line"><span class="keyword">assert</span> stable == <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the value function has not changed, so the greedy policy should not change</span></span><br><span class="line">new_pi, stable = improve_policy(env, V, new_pi, gamma)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(new_pi == expected_pi)</span><br><span class="line"><span class="keyword">assert</span> stable == <span class="literal">True</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">10</span>, num_prices=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">V, pi = policy_iteration(env, gamma, theta)</span><br><span class="line"></span><br><span class="line">V_answer = [<span class="number">81.60</span>, <span class="number">83.28</span>, <span class="number">85.03</span>, <span class="number">86.79</span>, <span class="number">88.51</span>, <span class="number">90.16</span>, <span class="number">91.70</span>, <span class="number">93.08</span>, <span class="number">94.25</span>, <span class="number">95.25</span>, <span class="number">89.45</span>]</span><br><span class="line">pi_answer = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure value function is within 2 decimal places of answer</span></span><br><span class="line"><span class="keyword">assert</span> grader.near(V, V_answer, <span class="number">1e-2</span>)</span><br><span class="line"><span class="comment"># make sure policy is exactly correct</span></span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(pi == pi_answer)</span><br></pre></td></tr></table></figure>

<p>When you are ready to test the policy iteration algorithm, run the cell below.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">10</span>, num_prices=<span class="number">4</span>)</span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line">V, pi = policy_iteration(env, gamma, theta)</span><br></pre></td></tr></table></figure>

<p>You can use the <code>plot</code> function to visualize the final value function and policy.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tools.plot(V, pi)</span><br></pre></td></tr></table></figure>


<p><img src="output_36_0.png" alt="png"></p>
<p>You can check the value function (rounded to one decimal place) and policy against the answer below:<br><br>State $\quad\quad$    Value $\quad\quad$ Action<br><br>0 $\quad\quad\quad;$        81.6 $\quad\quad;$ 0<br><br>1 $\quad\quad\quad;$        83.3 $\quad\quad;$ 0<br><br>2 $\quad\quad\quad;$        85.0 $\quad\quad;$ 0<br><br>3 $\quad\quad\quad;$        86.8 $\quad\quad;$ 0<br><br>4 $\quad\quad\quad;$        88.5 $\quad\quad;$ 0<br><br>5 $\quad\quad\quad;$        90.2 $\quad\quad;$ 0<br><br>6 $\quad\quad\quad;$        91.7 $\quad\quad;$ 0<br><br>7 $\quad\quad\quad;$        93.1 $\quad\quad;$ 0<br><br>8 $\quad\quad\quad;$        94.3 $\quad\quad;$ 0<br><br>9 $\quad\quad\quad;$        95.3 $\quad\quad;$ 3<br><br>10 $\quad\quad;;,,$      89.5 $\quad\quad;$ 3<br></p>
<h2 id="Section-3-Value-Iteration"><a href="#Section-3-Value-Iteration" class="headerlink" title="Section 3: Value Iteration"></a>Section 3: Value Iteration</h2><p>The city has also heard about value iteration and would like you to implement it. Value iteration works by iteratively applying the Bellman optimality equation for $v_{\ast}$ to a working value function, as an update rule, as shown below.</p>
<p>$$\large v(s) \leftarrow \max_a \sum_{s’, r} p(s’, r | s, a)[r + \gamma v(s’)]$$<br>We have written an outline of the value iteration algorithm described in chapter 4.4 of the textbook. It is left to you to fill in the <code>bellman_optimality_update</code> function to complete the value iteration algorithm.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">value_iteration</span>(<span class="params">env, gamma, theta</span>):</span></span><br><span class="line">    V = np.zeros(<span class="built_in">len</span>(env.S))</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">            v = V[s]</span><br><span class="line">            bellman_optimality_update(env, V, s, gamma)</span><br><span class="line">            delta = <span class="built_in">max</span>(delta, <span class="built_in">abs</span>(v - V[s]))</span><br><span class="line">        <span class="keyword">if</span> delta &lt; theta:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    pi = np.ones((<span class="built_in">len</span>(env.S), <span class="built_in">len</span>(env.A))) / <span class="built_in">len</span>(env.A)</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">        q_greedify_policy(env, V, pi, s, gamma)</span><br><span class="line">    <span class="keyword">return</span> V, pi</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Graded Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bellman_optimality_update</span>(<span class="params">env, V, s, gamma</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Mutate ``V`` according to the Bellman optimality update equation.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># YOUR CODE HERE</span></span><br><span class="line">    G = np.zeros(<span class="built_in">len</span>(env.A))    </span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> env.A:</span><br><span class="line">        transitions = env.transitions(s, a)</span><br><span class="line">        <span class="keyword">for</span> s_, (r, p) <span class="keyword">in</span> <span class="built_in">enumerate</span>(transitions):</span><br><span class="line">            G[a] += p*(r + gamma*V[s_])</span><br><span class="line">    V[s] = np.<span class="built_in">max</span>(G)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Debugging Cell</span></span><br><span class="line"><span class="comment"># --------------</span></span><br><span class="line"><span class="comment"># Feel free to make any changes to this cell to debug your code</span></span><br><span class="line"></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">6</span>, num_prices=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">V = np.array([<span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># only state 0 updated</span></span><br><span class="line">bellman_optimality_update(env, V, <span class="number">0</span>, gamma)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">list</span>(V) == [<span class="number">5</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># only state 2 updated</span></span><br><span class="line">bellman_optimality_update(env, V, <span class="number">2</span>, gamma)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">list</span>(V) == [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># Tested Cell</span></span><br><span class="line"><span class="comment"># -----------</span></span><br><span class="line"><span class="comment"># The contents of the cell will be tested by the autograder.</span></span><br><span class="line"><span class="comment"># If they do not pass here, they will not pass there.</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">10</span>, num_prices=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">V = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">        bellman_optimality_update(env, V, s, gamma)</span><br><span class="line"></span><br><span class="line"><span class="comment"># make sure value function is exactly correct</span></span><br><span class="line">answer = [<span class="number">61</span>, <span class="number">63</span>, <span class="number">65</span>, <span class="number">67</span>, <span class="number">69</span>, <span class="number">71</span>, <span class="number">72</span>, <span class="number">74</span>, <span class="number">75</span>, <span class="number">76</span>, <span class="number">71</span>]</span><br><span class="line"><span class="keyword">assert</span> np.<span class="built_in">all</span>(V == answer)</span><br></pre></td></tr></table></figure>

<p>When you are ready to test the value iteration algorithm, run the cell below.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">10</span>, num_prices=<span class="number">4</span>)</span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line">V, pi = value_iteration(env, gamma, theta)</span><br></pre></td></tr></table></figure>

<p>You can use the <code>plot</code> function to visualize the final value function and policy.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tools.plot(V, pi)</span><br></pre></td></tr></table></figure>


<p><img src="output_46_0.png" alt="png"></p>
<p>You can check your value function (rounded to one decimal place) and policy against the answer below:<br><br>State $\quad\quad$    Value $\quad\quad$ Action<br><br>0 $\quad\quad\quad;$        81.6 $\quad\quad;$ 0<br><br>1 $\quad\quad\quad;$        83.3 $\quad\quad;$ 0<br><br>2 $\quad\quad\quad;$        85.0 $\quad\quad;$ 0<br><br>3 $\quad\quad\quad;$        86.8 $\quad\quad;$ 0<br><br>4 $\quad\quad\quad;$        88.5 $\quad\quad;$ 0<br><br>5 $\quad\quad\quad;$        90.2 $\quad\quad;$ 0<br><br>6 $\quad\quad\quad;$        91.7 $\quad\quad;$ 0<br><br>7 $\quad\quad\quad;$        93.1 $\quad\quad;$ 0<br><br>8 $\quad\quad\quad;$        94.3 $\quad\quad;$ 0<br><br>9 $\quad\quad\quad;$        95.3 $\quad\quad;$ 3<br><br>10 $\quad\quad;;,,$      89.5 $\quad\quad;$ 3<br></p>
<p>In the value iteration algorithm above, a policy is not explicitly maintained until the value function has converged. Below, we have written an identically behaving value iteration algorithm that maintains an updated policy. Writing value iteration in this form makes its relationship to policy iteration more evident. Policy iteration alternates between doing complete greedifications and complete evaluations. On the other hand, value iteration alternates between doing local greedifications and local evaluations. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">value_iteration2</span>(<span class="params">env, gamma, theta</span>):</span></span><br><span class="line">    V = np.zeros(<span class="built_in">len</span>(env.S))</span><br><span class="line">    pi = np.ones((<span class="built_in">len</span>(env.S), <span class="built_in">len</span>(env.A))) / <span class="built_in">len</span>(env.A)</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        delta = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> env.S:</span><br><span class="line">            v = V[s]</span><br><span class="line">            q_greedify_policy(env, V, pi, s, gamma)</span><br><span class="line">            bellman_update(env, V, pi, s, gamma)</span><br><span class="line">            delta = <span class="built_in">max</span>(delta, <span class="built_in">abs</span>(v - V[s]))</span><br><span class="line">        <span class="keyword">if</span> delta &lt; theta:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> V, pi</span><br></pre></td></tr></table></figure>

<p>You can try the second value iteration algorithm by running the cell below.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">env = tools.ParkingWorld(num_spaces=<span class="number">10</span>, num_prices=<span class="number">4</span>)</span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">0.1</span></span><br><span class="line">V, pi = value_iteration2(env, gamma, theta)</span><br><span class="line">tools.plot(V, pi)</span><br></pre></td></tr></table></figure>


<p><img src="output_51_0.png" alt="png"></p>
<h2 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h2><p>Congratulations, you’ve completed assignment 2! In this assignment, we investigated policy evaluation and policy improvement, policy iteration and value iteration, and Bellman updates. Gridworld City thanks you for your service!</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhangruochi.com/Markov-Decision-Processes-II/2020/09/06/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ruochi Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RUOCHI.AI">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/Markov-Decision-Processes-II/2020/09/06/" class="post-title-link" itemprop="url">Markov Decision Processes II</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-09-06 19:36:56" itemprop="dateCreated datePublished" datetime="2020-09-06T19:36:56+08:00">2020-09-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-08 18:08:41" itemprop="dateModified" datetime="2020-09-08T18:08:41+08:00">2020-09-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Artificial-Intelligence/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/Markov-Decision-Processes-II/2020/09/06/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/Markov-Decision-Processes-II/2020/09/06/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Lesson-1-Policies-and-Value-Functions"><a href="#Lesson-1-Policies-and-Value-Functions" class="headerlink" title="Lesson 1: Policies and Value Functions"></a>Lesson 1: Policies and Value Functions</h2><h3 id="Recognize-that-a-policy-is-a-distribution-over-actions-for-each-possible-state"><a href="#Recognize-that-a-policy-is-a-distribution-over-actions-for-each-possible-state" class="headerlink" title="Recognize that a policy is a distribution over actions for each possible state."></a>Recognize that a policy is a distribution over actions for each possible state.</h3><p>a policy is a mapping from states to probabilities of selecting each possible action.  If the agent is following policy $\pi$ at time $t$, then $\pi(a | s)$ is the probability that $A_t = a$ if $S_t = s$</p>
<h3 id="Describe-the-similarities-and-differences-between-stochastic-and-deterministic-policies"><a href="#Describe-the-similarities-and-differences-between-stochastic-and-deterministic-policies" class="headerlink" title="Describe the similarities and differences between stochastic and deterministic policies"></a>Describe the similarities and differences between stochastic and deterministic policies</h3><p>Deterministic policies: a policy assigns probabilities to each action in each state.</p>
<p>Stochastic policy: a policy where multiple actions may be selected with non-zero probability.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="3.png" width = "50%" height="50%">
</center>

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="4.png" width = "50%" height="50%">
</center>

<h3 id="Identify-the-characteristics-of-a-well-defined-policy"><a href="#Identify-the-characteristics-of-a-well-defined-policy" class="headerlink" title="Identify the characteristics of a well-defined policy"></a>Identify the characteristics of a well-defined policy</h3><ul>
<li>An agent’s behavior is specified by a policy that maps the state to a probability distribution over actions</li>
<li>The policy can depend only on the current state, and not other things like time or previous states. See you next time.</li>
</ul>
<h3 id="Describe-the-roles-of-state-value-and-action-value-functions-in-reinforcement-learning"><a href="#Describe-the-roles-of-state-value-and-action-value-functions-in-reinforcement-learning" class="headerlink" title="Describe the roles of state-value and action-value functions in reinforcement learning"></a>Describe the roles of state-value and action-value functions in reinforcement learning</h3><p>Similarly, we define the value of taking action a in state s under a policy $\pi$, denoted $q_{\pi}(s,a)$,  as the expected return starting from $s$,  taking the action a, and thereafter following policy $\pi$:</p>
<p>$$q_{\pi}(s) = \mathbb{E}<em>{\pi}[ G_t | S_t = s, A_t = a] = \mathbb{E}</em>{\pi}[ \sum_{k=0}^{\infty} \eta^k R_{t+k+1} | S_t = s, A_t = a ] $$</p>
<p>We call $q_{\pi}$ the action-value function for policy $\pi$.</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="1.png" width = "70%" height="70%">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">the roles of state-value and action-value functions</div>
</center>


<h3 id="Describe-the-relationship-between-value-functions-and-policies"><a href="#Describe-the-relationship-between-value-functions-and-policies" class="headerlink" title="Describe the relationship between value functions and policies"></a>Describe the relationship between value functions and policies</h3><p>Value function enable us to judge the quality of different policies.</p>
<p>The value function of a state s under a policy $\pi$, denoted $V_\pi(s)$, is the expected return when starting in s and following $\pi$ thereafter. For MDPs, we can define $V_{\pi}$ formally by</p>
<p>$$V_{\pi}(s) = \mathbb{E}[ G_t | S_t = s] = \mathbb{E}<em>{\pi}[ \sum</em>{k=0}^{\infty} \eta^k R_{t+k+1} | S_t = s ], \text{for all} \quad s \in \mathbb{S}$$</p>
<p>where $\mathbb{E}[\dot]$ denotes the expected value of a random variable given that the agent follows policy $\pi$, and t is any time step.</p>
<h3 id="Create-examples-of-valid-value-functions-for-a-given-MDP"><a href="#Create-examples-of-valid-value-functions-for-a-given-MDP" class="headerlink" title="Create examples of valid value functions for a given MDP"></a>Create examples of valid value functions for a given MDP</h3><h2 id="Lesson-2-Bellman-Equations"><a href="#Lesson-2-Bellman-Equations" class="headerlink" title="Lesson 2: Bellman Equations"></a>Lesson 2: Bellman Equations</h2><h3 id="Derive-the-Bellman-equation-for-state-value-functions"><a href="#Derive-the-Bellman-equation-for-state-value-functions" class="headerlink" title="Derive the Bellman equation for state-value functions"></a>Derive the Bellman equation for state-value functions</h3><p>$$V_{\pi}(s) = \mathbb{E}[ G_t | S_t = s] = \sum_a \pi (a | s)\sum_{s\prime,r}p(s\prime,r | s, a)[ r + \eta v_{\pi}(s\prime)], \quad \text{ for all } \quad s \in \mathbb{S}$$</p>
<h3 id="Derive-the-Bellman-equation-for-action-value-functions"><a href="#Derive-the-Bellman-equation-for-action-value-functions" class="headerlink" title="Derive the Bellman equation for action-value functions"></a>Derive the Bellman equation for action-value functions</h3><p>$$q_{\pi}(s,a) = \mathbb{E}<em>{\pi}[ G_t | S_t = s, A_t = a] = \sum</em>{s\prime}\sum_{r}p(s\prime,r | s, a)[ r + \eta\sum_{a\prime} \pi (a\prime | q_{\pi}(s\prime, a\prime))]$$</p>
<h3 id="Understand-how-Bellman-equations-relate-current-and-future-values"><a href="#Understand-how-Bellman-equations-relate-current-and-future-values" class="headerlink" title="Understand how Bellman equations relate current and future values"></a>Understand how Bellman equations relate current and future values</h3><center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="2.png" width = "70%" height="70%">
</center>

<p>The current time-step’s state/action values can be written recursivelu in terms of future state/action values</p>
<p>Bellman equation for $V_{\pi}$,  It expresses a relationship between the value of a state and the values of its successor states. Think of looking ahead from a state to its possible successor states, as suggested by the diagram to the right. Each open circle represents a state and each solid circle represents a state–action pair. Starting from state s, the root node at the top, the agent could take any of some set of actions—three are shown in the diagram—based on its policy $\pi$.  From each of these, the environment could respond with one of several next states, $s\prime$ (two are shown in the figure), along with a reward, r, depending on its dynamics given by the function $p$. The Bellman equation averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the(discounted) value of the expected next state, plus the reward expected along the way.</p>
<p>We call diagrams like that above backup diagrams because they diagram relationships that form the basis of the update or backup operations that are at the heart of reinforcement learning methods. These operations transfer value information back to a state (or a state–action pair) from its successor states (or state–action pairs)</p>
<h3 id="Use-the-Bellman-equations-to-compute-value-functions"><a href="#Use-the-Bellman-equations-to-compute-value-functions" class="headerlink" title="Use the Bellman equations to compute value functions"></a>Use the Bellman equations to compute value functions</h3><p>The value function  is the unique solution to its Bellman equation.</p>
<h2 id="Lesson-3-Optimality-Optimal-Policies-amp-Value-Functions"><a href="#Lesson-3-Optimality-Optimal-Policies-amp-Value-Functions" class="headerlink" title="Lesson 3: Optimality (Optimal Policies &amp; Value Functions)"></a>Lesson 3: Optimality (Optimal Policies &amp; Value Functions)</h2><h3 id="Define-an-optimal-policy-understand-how-a-policy-can-be-at-least-as-good-as-every-other-policy-in-every-state"><a href="#Define-an-optimal-policy-understand-how-a-policy-can-be-at-least-as-good-as-every-other-policy-in-every-state" class="headerlink" title="Define an optimal policy, understand how a policy can be at least as good as every other policy in every state"></a>Define an optimal policy, understand how a policy can be at least as good as every other policy in every state</h3><p>For finite MDPs, we can precisely define an optimal policy in the following way. Value functions define a partial ordering over policies.  A policy $\pi$ is defined to be better than or equal to a policy $\pi\prime$ if its expected return is greater than or equal to that of $\pi\prime$ for all states.  In other words, $\pi \geq \pi\prime$ if and only if $V_{\pi}(s) \geq V_{\pi\prime}(s)$ for all $s \in \mathbb{S}$. There is always at least one policy that is better than or equal to all other policies. This is an optimal policy. Although there may be more than one, we denote all the optimal policies by $\pi_{\star}$. They share the same state-value function, called the optimal state-value function, denoted $V_{\star}$, and defined as</p>
<p>$$V_{\star} = max_{\pi}V_{\pi}(s) \quad \text{for all} \quad s \in \mathbb{S}$$</p>
<p>Optimal policies also share the same optimal action-value function, denoted $q_{\star}$, and defined as</p>
<p>$$q_{\star}(s,a) = max_{\pi} q_{\pi}(s,a) \quad \text{for all} \quad s \in \mathbb{S} , \text{and} , a \in \mathbb{A}(s)$$</p>
<p>For the state–action pair (s, a), this function gives the expected return for taking action $a$ in state $s$ and thereafter following an optimal policy. Thus, we can write $q_{\star}$ in terms of $V_{\star}$ as follows:</p>
<p>$$q_{\star}(s,a) = \mathbb{E}[ R_{t+1} + \eta v_{\star}(S_{t+1}) | S_t = s, A_t = a]$$</p>
<h3 id="Derive-the-Bellman-optimality-equation-for-state-value-functions"><a href="#Derive-the-Bellman-optimality-equation-for-state-value-functions" class="headerlink" title="Derive the Bellman optimality equation for state-value functions"></a>Derive the Bellman optimality equation for state-value functions</h3><p>$$V_{\star}(s) = \sum_a \pi_{\star} (a | s)\sum_{s\prime,r}p(s\prime,r | s, a)[ r + \eta v_{\star}(s\prime)], \quad \text{ for all } \quad s \in \mathbb{S}$$</p>
<p>$$V_{\star}(s) = max_a \sum_{s\prime,r}p(s\prime,r | s, a)[ r + \eta v_{\star}(s\prime)], \quad \text{ for all } \quad s \in \mathbb{S}$$</p>
<h3 id="Derive-the-Bellman-optimality-equation-for-action-value-functions"><a href="#Derive-the-Bellman-optimality-equation-for-action-value-functions" class="headerlink" title="Derive the Bellman optimality equation for action-value functions"></a>Derive the Bellman optimality equation for action-value functions</h3><p>$$q_{\star}(s,a) = \sum_{s\prime}\sum_{r}p(s\prime,r | s, a)[ r + \eta\sum_{a\prime} \pi_{\star} (a\prime | q_{\star}(s\prime, a\prime))]$$</p>
<p>$$q_{\star}(s,a) = \sum_{s\prime}\sum_{r}p(s\prime,r | s, a)[ r + \eta max_{a\prime}q_{\star}(s\prime, a\prime))]$$</p>
<h3 id="Understand-the-connection-between-the-optimal-value-function-and-optimal-policies"><a href="#Understand-the-connection-between-the-optimal-value-function-and-optimal-policies" class="headerlink" title="Understand the connection between the optimal value function and optimal policies"></a>Understand the connection between the optimal value function and optimal policies</h3><p>Once we had the optimal state-value function, it’s relatively easy to work out the optimal policy. If we have the optimal action-value function, working out the optimal policy is even easier. This correspondence between optimal-value functions and optimal-policies will help us to derive many of the reinforced learning algorithms we will explore later in this specialization.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ruochi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">266</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhangruochi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhangruochi" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zrc720@gmail.com" title="E-Mail → mailto:zrc720@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://www.healthinformaticslab.org/" title="http:&#x2F;&#x2F;www.healthinformaticslab.org" rel="noopener" target="_blank">HILab</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.shihaizhou.com/" title="http:&#x2F;&#x2F;www.shihaizhou.com" rel="noopener" target="_blank">Rose</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/cherish_CX/" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;cherish_CX&#x2F;" rel="noopener" target="_blank">Chunxia</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ruochi Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '[object Object]';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'qW3MLcAgcX96sB6qbegeL7rP-gzGzoHsz',
      appKey     : 'GL6JvT9DgGxqYrY5Vj6bXVuv',
      placeholder: "Thank you for your reply",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
